[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science 2: Textdaten als Grundlage prÃ¤diktiver Modelle ğŸ“šğŸ”®",
    "section": "",
    "text": "Zu diesem Buch\nBild von mcmurryjulie auf Pixabay"
  },
  {
    "objectID": "index.html#was-rÃ¤t-meister-yoda",
    "href": "index.html#was-rÃ¤t-meister-yoda",
    "title": "Data Science 2: Textdaten als Grundlage prÃ¤diktiver Modelle ğŸ“šğŸ”®",
    "section": "Was rÃ¤t Meister Yoda?",
    "text": "Was rÃ¤t Meister Yoda?\nMeister Yoda rÃ¤t: Lesen Sie die Hinweise (AbbildungÂ 1).\n\n\nAbbildungÂ 1: Lesen Sie die folgenden Hinweise im eigenen Interesse\n\n\nQuelle: made at imageflip"
  },
  {
    "objectID": "index.html#zitation",
    "href": "index.html#zitation",
    "title": "Data Science 2: Textdaten als Grundlage prÃ¤diktiver Modelle ğŸ“šğŸ”®",
    "section": "Zitation",
    "text": "Zitation\nNutzen Sie folgende DOI, um dieses Buch zu zitieren: \nHier ist die Zitation im Bibtex-Format:"
  },
  {
    "objectID": "index.html#literatur",
    "href": "index.html#literatur",
    "title": "Data Science 2: Textdaten als Grundlage prÃ¤diktiver Modelle ğŸ“šğŸ”®",
    "section": "Literatur",
    "text": "Literatur\nZentrale Begleitliteratur ist Hvitfeldt und Silge (2021); der Volltext ist hier verfÃ¼gbar.\nPro Thema wird ggf. weitere Literatur ausgewiesen."
  },
  {
    "objectID": "index.html#quellcode",
    "href": "index.html#quellcode",
    "title": "Data Science 2: Textdaten als Grundlage prÃ¤diktiver Modelle ğŸ“šğŸ”®",
    "section": "Quellcode",
    "text": "Quellcode\nDer Quellcode liegt Ã¶ffentlich zugÃ¤nglich in diesem Github-Repositorium."
  },
  {
    "objectID": "index.html#technische-details",
    "href": "index.html#technische-details",
    "title": "Data Science 2: Textdaten als Grundlage prÃ¤diktiver Modelle ğŸ“šğŸ”®",
    "section": "Technische Details",
    "text": "Technische Details\n\nDiese Version des Buches wurde erstellt am: 2023-10-22 21:38:07\nSie haben Feedback, Fehlerhinweise oder WÃ¼nsche zur Weiterentwicklung? Am besten stellen Sie hier einen Issue ein.\nDieses Projekt steht unter der MIT-Lizenz.\nDieses Buch wurde in RStudio mit Hilfe von bookdown geschrieben.\nDiese Version des Buches wurde mit der R-Version R version 4.2.1 (2022-06-23) und den folgenden technischen Spezifikationen erstellt:\n\n\n## â”€ Session info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n##  setting  value\n##  version  R version 4.2.1 (2022-06-23)\n##  os       macOS Big Sur ... 10.16\n##  system   x86_64, darwin17.0\n##  ui       X11\n##  language (EN)\n##  collate  en_US.UTF-8\n##  ctype    en_US.UTF-8\n##  tz       Europe/Berlin\n##  date     2023-08-24\n##  pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n## \n## â”€ Packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n##  package     * version date (UTC) lib source\n##  cli           3.6.1   2023-03-23 [1] CRAN (R 4.2.0)\n##  colorout    * 1.2-2   2022-06-13 [1] local\n##  digest        0.6.32  2023-06-26 [1] CRAN (R 4.2.0)\n##  evaluate      0.21    2023-05-05 [1] CRAN (R 4.2.0)\n##  fastmap       1.1.1   2023-02-24 [1] CRAN (R 4.2.0)\n##  htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.2.0)\n##  htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.2.0)\n##  jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.2.0)\n##  knitr         1.43    2023-05-25 [1] CRAN (R 4.2.0)\n##  rlang         1.1.1   2023-04-28 [1] CRAN (R 4.2.0)\n##  rmarkdown     2.23    2023-07-01 [1] CRAN (R 4.2.0)\n##  rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.2.0)\n##  sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n##  xfun          0.39    2023-04-20 [1] CRAN (R 4.2.0)\n##  yaml          2.3.7   2023-01-23 [1] CRAN (R 4.2.0)\n## \n##  [1] /Users/sebastiansaueruser/Rlibs\n##  [2] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n## \n## â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n\n\n\n\nHvitfeldt, Emil, und Julia Silge. 2021. Supervised Machine Learning for Text Analysis in R. 1. Aufl. Boca Raton: Chapman and Hall/CRC. https://doi.org/10.1201/9781003093459."
  },
  {
    "objectID": "010-Hinweise.html#ihr-lernerfolg",
    "href": "010-Hinweise.html#ihr-lernerfolg",
    "title": "Lernhilfen",
    "section": "\n1.1 Ihr Lernerfolg",
    "text": "1.1 Ihr Lernerfolg\n\n1.1.1 Was Sie hier lernen und wozu das gut ist\nAlle Welt spricht von Big Data, aber ohne die Analyse sind die groÃŸen Daten nur groÃŸes Rauschen. Was letztlich interessiert, sind die Erkenntnisse, die Einblicke, nicht die Daten an sich. Dabei ist es egal, ob die Daten groÃŸ oder klein sind. NatÃ¼rlich erlauben die heutigen Datenmengen im Verbund mit leistungsfÃ¤higen Rechnern und neuen Analysemethoden ein VerstÃ¤ndnis, das vor Kurzem noch nicht mÃ¶glich war. Und wir stehen erst am Anfang dieser Entwicklung. Vielleicht handelt es sich bei diesem Feld um eines der dynamischsten Fachgebiete der heutigen Zeit. Sie sind dabei: Sie lernen einiges Handwerkszeugs des â€œDatenwissenschaftlersâ€. Wir konzentrieren uns auf das vielleicht bekannteste Teilgebiet: Ereignisse vorhersagen auf Basis von hoch strukturierten Daten und geeigneter Algorithmen und Verfahren. Nach diesem Kurs sollten Sie in der Lage sein, typisches Gebabbel des Fachgebiet mit LÃ¤ssigkeit mitzumachen. Ach ja, und mit einigem Erfolg Vorhersagemodelle entwickeln.\n\n1.1.2 Lernziele\n\n\n\n\n\n\nWichtig\n\n\n\nKurz gesagt: Sie lernen die Grundlagen von Data Science zur Analyse von Text.\\(\\square\\)\n\n\nNach diesem Kurs sollten Sie â€¦\n\nDaten aus Sozialen Netzwerken wie Twitter automatisiert in groÃŸer Menge auslesen kÃ¶nnen\nGÃ¤ngige Methoden des Textminings mit R anwenden kÃ¶nnen (z.B. Tokenizing, Stemming, Regex)\nVerfahren des Maschinenlernens auf Textdaten anwenden kÃ¶nnen\nDen Forschungsstand zum Thema Erkennung von Hatespeech in Ausschnitten kennen\n\n1.1.3 Ãœberblick\nAbb. AbbildungÂ 1.1 gibt einen Ãœberblick Ã¼ber den Verlauf und die Inhalte des Buches. Das Diagramm hilft Ihnen zu verorten, wo welches Thema im Gesamtzusammenhang steht.\n\n\n\n\nflowchart LR\n  subgraph R[Rahmen]\n    direction LR\n    subgraph V[Grundlagen]\n      direction TB\n      E[R] --- Um[Statistisches&lt;br&gt;Lernen]\n      Um --- tm[tidymodels]\n    end\n    subgraph M[Lernalgorithmen]\n      direction TB\n      M1[Regression] --- Vis[Baeume]\n      Vis --- U[Regularisierung]\n      U --- G[...]\n    end\n    subgraph N[Anwendung]\n      direction TB\n      D[Fallstudien]\n    end\n  V --&gt; M\n  M --&gt; N\n  end\n\n\nAbbildungÂ 1.1: Ein â€˜Fahrplanâ€™ als â€˜Big Pictureâ€™ dieses Buches"
  },
  {
    "objectID": "010-Hinweise.html#selbstÃ¤ndige-vorbereitung-vor-kursbeginn",
    "href": "010-Hinweise.html#selbstÃ¤ndige-vorbereitung-vor-kursbeginn",
    "title": "Lernhilfen",
    "section": "\n1.2 SelbstÃ¤ndige Vorbereitung vor Kursbeginn",
    "text": "1.2 SelbstÃ¤ndige Vorbereitung vor Kursbeginn\nDie folgenden Inhalte werden in diesem Buch/Kurs vorausgesetzt. Falls Ihnen der Stoff nicht gelÃ¤ufig ist, sollten Sie sich selbstÃ¤ndig damit vertraut machen.\n\nGrundlagen der Statistik wie im Kurs Statistik1 vermittelt\nEinfÃ¼hrung in die Inferenzstatistik wie im Kurs Bayes:Start! vermittelt\nGrundlagen der Prognosemodellierung wie im Kurs Data Science 1 vermittelt"
  },
  {
    "objectID": "010-Hinweise.html#lernhilfen",
    "href": "010-Hinweise.html#lernhilfen",
    "title": "Lernhilfen",
    "section": "\n1.3 Lernhilfen",
    "text": "1.3 Lernhilfen\n\n1.3.1 PDF-Version\nUm eine PDF-Version eines Kapitels zu erhalten, kÃ¶nnen Sie im Browser die Druckfunktion nutzen (Strg-P). WÃ¤hlen Sie dort â€œPDFâ€ als Ziel.\n\n1.3.2 Videos\nAuf dem YouTube-Kanal des Autors finden sich eine Reihe von Videos mit Bezug zum Inhalt dieses Buchs. Besonders diese Playlist passt zu den Inhalten dieses Buchs.\n\n1.3.3 Software allgemein\nInstallieren Sie R und seine Freunde.\nInstallieren Sie bitte auch die folgende R-Pakete1:\n\ntidyverse\neasystats\nweitere Pakete werden im Unterricht bekannt gegeben (es schadet aber nichts, jetzt schon Pakete nach eigenem Ermessen zu installieren)\n\nR Syntax aus dem Unterricht findet sich im Github-Repo bzw. Ordner zum jeweiligen Semester.\n\n\n\n\n\n\nRStudio-Cloud-Project\n\n\n\nWenn Ihnen die Lehrkraft ein RStudio-Cloud-Projekt zur VerfÃ¼gung stellt, nutzen Sie es. Dort sind alle R-Pakete, DatensÃ¤tze und Syntax-Vorlagen schon bereit gestellt. Sie sparen sich also eine Menge Installationsarbeit.\\(\\square\\)\n\n\n\n\n\n\n\n\nBei Installationsproblemen\n\n\n\n\nGibt R eine Warning aus, ist das zumeist kein Problem und kann ignoriert werden.\nStarten Sie R neu, bevor Sie R-Pakete installieren.\nWenn Sie Probleme mit der Installation auf Ihrem Computer haben, kÃ¶nnen Sie (Ã¼bergangsweise oder dauerhaft) die Online-Version von RStudio, RStudio Cloud verwenden (in gewissem Umfang kostenlos).\\(\\square\\)\n\n\n\n\n\n1.3.4 Software: Bayes\nWenn in diesem Modul Inferenzstatistik nÃ¶tig ist, benÃ¶tigen Sie Software fÃ¼r Bayes-Inferenz.\nFolgendes R-Paket ist fÃ¼r die Bayes-Inferenz nÃ¶tig:\n\nrstanarm\n\n\n1.3.5 Online-UnterstÃ¼tzung\nDieser Kurs kann in PrÃ¤senz und Online angeboten werden. Wenn Sie die Wahl haben, empfehle ich die Teilnahme in PrÃ¤senz, da der Lernerfolg hÃ¶her ist. Online ist es meist schwieriger, sich zu konzentrieren. Aber auch online ist es mÃ¶glich, den Stoff gut zu lernen, s. AbbildungÂ 1.2.\n\n\nAbbildungÂ 1.2: We believe in you! Image Credit: Allison Horst\n\nBitte beachten Sie, dass bei einer Teilnahme in PrÃ¤senz eine aktive Mitarbeit erwartet wird. Hingegen ist bei einer Online-Teilnahme keine/kaum aktive Mitarbeit mÃ¶glich.\nHier finden Sie einige Werkzeuge, die das Online-Zusammenarbeiten vereinfachen:\n\n\nFrag-Jetzt-Raum zum anonymen Fragen stellen wÃ¤hrend des Unterrichts. Der Keycode wird Ihnen bei Bedarf vom Dozenten bereitgestellt.\n\nPadlet zum einfachen (und anonymen) Hochladen von Arbeitsergebnissen der Studentis im Unterricht. Wir nutzen es als eine Art Pinwand zum Sammeln von ArbeitsbeitrÃ¤gen. Die Zugangsdaten stellt Ihnen der Dozent bereit.\nNutzen Sie das vom Dozenten bereitgestelle Forum, um Fragen zu stellen und Fragen zu beantworten.\n\n1.3.6 Fundorte fÃ¼r DatensÃ¤tze\nHier finden Sie DatensÃ¤tze, die sich eignen, um die Analyse von Daten zu lernen:\n\nVincent Arel-Bundocks Datenseite\nDie Datenseite der University of California in Irvine (UCI)\n\n1.3.7 Aufgabensammlung\nDie Webseite Datenwerk beherbergt eine Sammlung an Ãœbungsaufgaben rund um das Thema Datenanalyse. es gibt eine Suchfunktion (wenn Sie den Namen der Aufgabe wissen) und eine Tag-Liste, wenn Sie Aufgaben nach Themengebiet durchsehen wollen.\n\n1.3.8 Tipps zum Lernerfolg\n\n\n\n\n\n\nHinweis\n\n\n\nStetige Mitarbeit - auch und gerade auÃŸerhalb des Unterrichts - ist der SchlÃ¼ssel zum PrÃ¼fungserfolg. Vermeiden Sie, das Lernen aufzuschieben. Bleiben Sie dran!\\(\\square\\)\n\n\n\n\nLerngruppe: Treten Sie einer Lerngruppe bei.\n\nTutorium: Besuchen Sie ein Tutorium, falls eines angeboten wird.\n\nVor- und Nachbereitung: Bereiten Sie den Unterricht vor und nach.\n\nSelbsttest: Testen Sie sich mit Flashcards (Karteikarten mit Vor- und RÃ¼ckseite). Wenn Sie alle Aufgaben dieses Kurses aus dem FF beherrschen, sollte die PrÃ¼fung kein Problem sein.\n\nÃœbungen: Bearbeiten Sie alle Ãœbungsaufgaben gewissenhaft.\nPortal Datenwerk: Gehen Sie die Aufgaben auf dem Portal Datenwerk durch (soweit relevant).\n\nFallstudien: Schauen Sie sich meine Fallstudiensammlungen an: https://sebastiansauer-academic.netlify.app/courseware/casestudies/\n\nLehrkraft ansprechen: Sprechen Sie die Lehrkraft an, wenn Sie Fragen haben. Haben Sie keine Scheu! Bitte lesen Sie aber vorab die Hinweise, um Redundanz zu vermeiden.\n\nDabei bleiben: Vermeiden Sie â€œBullimie-Lernenâ€ (lange nix, dann alles auf einmal), sondern bevorzugen Sie â€œLern-Snacksâ€ (immer wieder ein bisschen)\n\n1.3.9 Selbstlernkontrolle\nFÃ¼r jedes Kapitel sind (am Kapitelende) Aufgaben eingestellt, jeweils mit LÃ¶sung. Ein Teil dieser Aufgaben hat eine kurze, eindeutige LÃ¶sung (z.B. â€œ42â€ oder â€œAntwort Câ€); ein (kleiner) Teil der Aufgaben verlangen komplexere Antworten (z.B. â€œWelche Arten von Prioris gibt es bei stan_glm()?). Nutzen Sie die Fragen mit eindeutiger, kurzer LÃ¶sung um sich selber zu prÃ¼fen. Nutzen Sie die Fragen mit komplexerer, lÃ¤ngerer LÃ¶sung, um ein Themengebiet tiefer zu erarbeiten.\n\n\n\n\n\n\nHinweis\n\n\n\nFortwÃ¤hrendes Feedback zu Ihrem Lernfortschritt ist wichtig, damit Sie Ihre LernbemÃ¼hungen steuern kÃ¶nnen. Bearbeiten Sie daher die bereitgestellten Arbeiten ernsthaft.\\(\\square\\)\n\n\n\n1.3.10 Lernen lernen\nHier sind einige Quellen (Literatur), die Ihnen helfen sollen, das Lernen (noch besser) zu lernen:\n\nEssentielle Tipps fÃ¼r Bachelor-Studierende der Psychologie\nKonzentriert arbeiten: Regeln fÃ¼r eine Welt voller Ablenkungen\nWie man ein Buch liest\nErsti-Hilfe: 112 Tipps fÃ¼r StudienanfÃ¤nger - erfolgreich studieren ab der ersten Vorlesung\nVon der KÃ¼rze des Lebens\nBlog â€œStudienscheissâ€"
  },
  {
    "objectID": "010-Hinweise.html#literatur",
    "href": "010-Hinweise.html#literatur",
    "title": "Lernhilfen",
    "section": "\n1.4 Literatur",
    "text": "1.4 Literatur\nZentrale Kursliteratur fÃ¼r die theoretischen Konzepte ist Hvitfeldt und Silge (2021); das Buch ist frei online verfÃ¼gbar.\nEine gute ErgÃ¤nzung ist das Lehrbuch von Chollet, Kalinowski, und Allaire (2022), welches grundlegende Data-Science-Konzepte erlÃ¤utert und mit tidymodels umsetzt. Es ist in einer [Online-Version beim Verlag frei zugÃ¤nglich](https://livebook.manning.com/book/deep-learning-with-r-second-edition.\nJames u.Â a. (2021) haben ein weithin renommiertes und sehr bekanntes Buch verfasst. Es ist allerdings etwas anspruchsvoller aus Rhys (2020), daher steht es nicht im Fokus dieses Kurses, aber einige Schwenker zu Inhalten von James u.Â a. (2021) gibt es. Schauen Sie mal rein, das Buch ist gut!"
  },
  {
    "objectID": "010-Hinweise.html#faq",
    "href": "010-Hinweise.html#faq",
    "title": "Lernhilfen",
    "section": "\n1.5 FAQ",
    "text": "1.5 FAQ\n\n\nFolien\n\nFrage: Gibt es ein Folienskript?\nAntwort: Wo es einfache, gute Literatur gibt, gibt es kein Skript. Wo es keine gute oder keine einfach zugÃ¤ngliche Literatur gibt, dort gibt es ein Skript.\n\n\n\nEnglisch\n\nIst die Literatur auf Englisch?\nJa. Allerdings ist die Literatur gut zugÃ¤nglich. Das Englisch ist nicht schwer. Bedenken Sie: Englisch ist die lingua franca in Wissenschaft und Wirtschaft. Ein solides VerstÃ¤ndnis englischer (geschriebener) Sprache ist fÃ¼r eine gute Ausbildung unerlÃ¤sslich. Zu dem sollte die Kursliteratur fachlich passende und gute BÃ¼cher umfassen; oft sind das englische Titel.\n\n\n\nAnstrengend\n\nIst der Kurs sehr anstrengend, aufwÃ¤ndig?\nDer Kurs hat ein mittleres Anspruchsniveau.\n\n\n\nMathe\n\nMuss man ein Mathe-Crack sein, um eine gute Note zu erreichen?\nNein. Mathe steht nicht im Vordergrund. Schauen Sie sich die Literatur an, sie werden wenig Mathe darin finden.\n\n\n\nPrÃ¼fungsliteratur\n\nWelche Literatur ist prÃ¼fungsrelevant?\nPrÃ¼fungsrelevant im engeren Sinne ist das Skript sowie alles, was im Unterricht behandelt wurde.\n\n\n\nPrÃ¼fung\n\nWie sieht die PrÃ¼fung aus?\nDie PrÃ¼fung ist angewandt, z.B. eine Datenanalyse. Es wird keine Klausur geben, in der reines Wissen abgefragt wird.\n\n\n\nNur R?\n\nWird nur R in dem Kurs gelehrt? Andere Programmiersprachen sind doch auch wichtig.\nIn der Datenanalyse gibt es zwei zentrale Programmiersprachen, R und Python. Beide sind gut und beide werden viel verwendet. In einer Grundausbildung sollte man sich auf eine Sprache begrenzen, da sonst den Sprachen zu viel Zeit eingerÃ¤umt werden muss. Wichtiger als eine zweite Programmiersprache zu lernen, mit der man nicht viel mehr kann als mit der ersten, ist es, die Inhalte des Fachs zu lernen.\n\n\n\n\n\n\n\nChollet, FranÃ§ois, Tomasz Kalinowski, und J. J. Allaire. 2022. Deep Learning with R. Second edition. Shelter Island, NY: Manning.\n\n\nHvitfeldt, Emil, und Julia Silge. 2021. Supervised Machine Learning for Text Analysis in R. 1. Aufl. Boca Raton: Chapman and Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, und Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York: Springer. https://link.springer.com/book/10.1007/978-1-0716-1418-1.\n\n\nRhys, Hefin. 2020. Machine Learning with R, the Tidyverse, and Mlr. Shelter Island, NY: Manning publications."
  },
  {
    "objectID": "010-Hinweise.html#footnotes",
    "href": "010-Hinweise.html#footnotes",
    "title": "Lernhilfen",
    "section": "",
    "text": "falls Sie die Pakete schon installiert haben, kÃ¶nnen Sie in RStudio auf â€œupdate.packagesâ€ klickenâ†©ï¸"
  },
  {
    "objectID": "020-pruefung.html#prÃ¼fungsform-datenanalyse-als-quarto-blog-post",
    "href": "020-pruefung.html#prÃ¼fungsform-datenanalyse-als-quarto-blog-post",
    "title": "2Â  PrÃ¼fung",
    "section": "2.1 PrÃ¼fungsform: Datenanalyse als Quarto-Blog-Post",
    "text": "2.1 PrÃ¼fungsform: Datenanalyse als Quarto-Blog-Post\nAls PrÃ¼fungsleistung ist ein Corpus an Twitter-Daten, die an deutsche, aktuelle Politiker gerichtet sind, auf Hate Speech hin zu untersuchen.\n\nDer Dozent weiÃŸt jedis Studenti einen deutschen Politiker (bzw. dessen Twitter-Account) zu.\nDer Bericht der Analyse ist als Quarto Blog-Posts zu formatieren.\nEinzureichen ist die URL des Posts.\nDer Post muss wÃ¤hrend des gesamten PrÃ¼fungszeitraums online sein, gehostet von einem beliebigen Provider (z.B. Netlify oder Github).\nNach Einreichen des Posts dÃ¼rfen keine Ã„nderungen mehr vorgenommen werden.\nZu Dokumentationszwecken soll ein PDF-Print des Posts in die Abgabe mit hochgeladen werden. Das PDF-Print des Posts muss identisch (exakt gleich) sein zum Post, der Ã¼ber die URL verfÃ¼gbar ist.\nDer Quelltext des Posts soll bei Github vorliegen.\nDie Methoden des Textminings aus dem Unterricht sollen angewendet werden\nZusÃ¤tzlich dÃ¼rfen sonstige Techniken des Textminings (die nicht im Unterricht behandelt wurden), angewendet werden\nDarÃ¼ber hinaus sollen prÃ¤diktive Modelle zur Klassifikation von Hate-Speech (ja/nein) berechnet werden.\nEin Trainingsdatensatz wird gemeinsam erstellt.\nMethoden der Inferenzstatistik (wie Bayes) sind nicht nÃ¶tig.\nEs soll eine mittlere vierstellige Zahl an Tweets verarbeitet werden oder wenigstens so viele Tweets wie verfÃ¼gbar."
  },
  {
    "objectID": "020-pruefung.html#politiker-accounts",
    "href": "020-pruefung.html#politiker-accounts",
    "title": "2Â  PrÃ¼fung",
    "section": "2.2 Politiker-Accounts",
    "text": "2.2 Politiker-Accounts\nU.a. folgende Politiker-Accounts kÃ¶nnen als PrÃ¼fungsgegenstand verwendet werden (nach Hinweisen des Dozenten):\n\nOlaf Scholz\nAnnalena Baerbock\nChristian Lindner\nRobert Habeck (bzw. der Account seines Ministeriums)\nCem Ã–zdemir\nVolker Wissing\nNancy Faeser\nFriedrich Merz\nBjÃ¶rn HÃ¶cke\nSarah Wagenknecht"
  },
  {
    "objectID": "020-pruefung.html#hinweise-zur-prÃ¼fungsform-datenanalyse",
    "href": "020-pruefung.html#hinweise-zur-prÃ¼fungsform-datenanalyse",
    "title": "2Â  PrÃ¼fung",
    "section": "2.3 Hinweise zur PrÃ¼fungsform Datenanalyse",
    "text": "2.3 Hinweise zur PrÃ¼fungsform Datenanalyse\n\nAlle folgenden Hinweise gelten nur insoweit Ihre Lehrkraft Ihnen keine anders lautenden Hinweise gegeben hat (schriftlich)."
  },
  {
    "objectID": "020-pruefung.html#allgemeines",
    "href": "020-pruefung.html#allgemeines",
    "title": "2Â  PrÃ¼fung",
    "section": "2.4 Allgemeines",
    "text": "2.4 Allgemeines\n\nGegenstand dieser PrÃ¼fungsform ist eine Projektarbeit in Form von Analyse eines Datensatzes nach einer Forschungsfrage und die Dokumentation dieser Analyse.\nSchreiben Sie Ihre Datenanalyse in Form eines Berichts, der sich an den Gliederungspunkten wie unten dargestellt orientiert.\nWenden Sie die passenden, im Unterricht eingefÃ¼hrten, statistischen Verfahren an. Es steht Ihnen frei, andere (nicht im Unterricht behandelte) Verfahren zur Analyse der Daten anzuwenden, nach MaÃŸgabe der fachlichen Angemessenheit.\nWerten Sie die Daten mit R oder Python aus.\nDie R-Syntax soll im Hauptteil des Berichts dokumentiert werden. R-Output darf ggf. gekÃ¼rzt wiedergegeben werden.\nFÃ¼gen Sie keine ErklÃ¤rungen oder Definitionen von statistischen Verfahren an.\nBeschreiben und interpretieren Sie jede Analyse bzw. jeden R-Code bzw. jedes Ergebnis (jede R-Ausgabe).\nVon hoher Bedeutung ist die Korrektheit der Beschreibung und Interpretation der statistischen Modellierung (z.B. mit der Regressionsanalyse).\nEs hat keinen Einfluss auf Ihre Note, ob sich ein (erwarteter) Effekt zeigt und wie stark dieser Effekt ggf. ist.\nZu Beginn der Analyse mÃ¼ssen folgende Metadaten gut ersichtlich platziert sein (z.B. auf einem Deckblatt):\n\n\nVorname Nachname der Autors/der Autorin\nMatrikelnummer\nModulname\nAbgabedatum\n\n\nDie Abgabefrist endet mit Verstreichen des regulÃ¤ren PrÃ¼fungszeitraums (soweit nicht vom PrÃ¼fer anderweitig angegeben).\nStudentis mit Nachteilsausgleich melden sich beim PrÃ¼fer und zeigen ihren Antrag auf Nachteilsausgleich an."
  },
  {
    "objectID": "020-pruefung.html#einzureichende-dateien",
    "href": "020-pruefung.html#einzureichende-dateien",
    "title": "2Â  PrÃ¼fung",
    "section": "2.5 Einzureichende Dateien",
    "text": "2.5 Einzureichende Dateien\n\nEinzureichen sind folgende Dateien:\n\n\nder Bericht in menschenlesbarer Form (s. Formatierungshinweise)\nalle Dateien, die Quellcode der Analyse beinhalten.\ndie Rohdaten\n\n\nDer Name der Dateien kann frei gewÃ¤hlt werden (bzw. folgt keinen technischen Restriktionen)."
  },
  {
    "objectID": "020-pruefung.html#formatierung-des-berichts",
    "href": "020-pruefung.html#formatierung-des-berichts",
    "title": "2Â  PrÃ¼fung",
    "section": "2.6 Formatierung des Berichts",
    "text": "2.6 Formatierung des Berichts\n\nDer Bericht ist nur elektronisch, nicht ausgedruckt einzureichen.\nDer Bericht kann in einem paginierten Format (z.B. Word) oder einem nicht-paginierten Format (HTML-Dokument) verfasst werden. Abzugeben ist aber eine PDF-Datei oder eine HTML-Datei, die alle Bilder und sonstige Medien enthÃ¤lt (â€œStand-Alone-HTMLâ€).\nDie Wahl eines bestimmten Stylesheets ist nicht von Bedeutung. Lesbarkeit und Ãœbersichtlichkeit in der Formatierung sind unabhÃ¤ngig davon anzustreben.\nIm Kopfbereich (oder auf einem Deckblatt) sind die relevanten Metadatan anzugeben wie Name (Nachname, Vorname) der Autorin/des Autors, Abgabedatum, Titel der Arbeit, Modul."
  },
  {
    "objectID": "020-pruefung.html#formalia",
    "href": "020-pruefung.html#formalia",
    "title": "2Â  PrÃ¼fung",
    "section": "2.7 Formalia",
    "text": "2.7 Formalia\n\nRichtlinien einer Wortzahl gibt es nicht. Entscheidend ist, dass relevante Analysen durchgefÃ¼hrt und beschrieben wurden. Schreiben Sie so knapp wie mÃ¶glich und so ausfÃ¼hrich wie nÃ¶tig.\nDer Anspruch richtet sich nach dem Inhalt und Niveau des auf diese PrÃ¼fung vorbereitenden Unterricht (auch aus Modulen vorheriger Semester). Oft sind das Module in quantitativer Datenanalyse (und wissenschaftliches Arbeiten). Deren Inhalte sollen im Rahmen dieser PrÃ¼fungsleistung als selbstÃ¤ndig und flÃ¼ssig verfÃ¼gbare Kompetenz von den Studentis demonstriert werden.\nDie Gliederung der Arbeit kann sich an den PPDAC-Zyklus und am Data Science Model von Wickham und Grolemund orientieren."
  },
  {
    "objectID": "020-pruefung.html#beurteilungskriterien",
    "href": "020-pruefung.html#beurteilungskriterien",
    "title": "2Â  PrÃ¼fung",
    "section": "2.8 Beurteilungskriterien",
    "text": "2.8 Beurteilungskriterien\nDie Arbeit wird im Hinblick auf drei Kriterien bewertet:\n\nFormalia (z. B. VollstÃ¤ndigkeit der Abarbeitung, Angemessenheit der Ã¤uÃŸeren Gestaltung, Fokus auf Wesentliche, Ãœbersichtlichkeit, Ã„sthetik, Reproduzierbarkeit)\nMethodik (z. B. Richtige Auswahl und Anwendung der Verfahren, methodisches VerstÃ¤ndnis)\nInhalt (z. B. VerstÃ¤ndlichkeit, Breite und Tiefe der ProblemlÃ¶sung, Korrektheit der Interpretation)\n\nSie erhalten fÃ¼r jedes der drei Kriterien eine Teilnote sowie eine Gesamtnote. AuÃŸerdem erhalten Sie ggf. fÃ¼r die Kriterien noch ausformulierte Hinweise.\nDie Gesamtnote muss sich nicht als Mittelwert der Teilnoten ergeben.\nInsbesondere kann eine FÃ¼nf in einem der Kriterien zum Durchfallen fÃ¼hren, auch wenn die anderen beiden Kriterien gut oder sehr gut beurteilt wurden."
  },
  {
    "objectID": "020-pruefung.html#beispiele-fÃ¼r-aspekte-der-beurteilungskriterien",
    "href": "020-pruefung.html#beispiele-fÃ¼r-aspekte-der-beurteilungskriterien",
    "title": "2Â  PrÃ¼fung",
    "section": "2.9 Beispiele fÃ¼r Aspekte der Beurteilungskriterien",
    "text": "2.9 Beispiele fÃ¼r Aspekte der Beurteilungskriterien\n\nWurden deskriptive Statistiken (an angemessenen Ort) berichtet?\nWurden Diagramme und Tabellen angemessen eingesetzt?\nWurde Inferenzstatistik (angemessen) eingesetzt?\nWurden EffektstÃ¤rkemaÃŸe (idealerweise mit Konfidenzintervallen dazu) berichtet?\nWurden alle relevanten Informationen fÃ¼r ein statistisches Verfahren angegeben (z.B. zum gewÃ¤hlten Prior)?\nWurde die Aussagekraft von Modellergebnissen richtig eingeschÃ¤tzt?\nWaren die Schlussfolgerungen, die aus den statistischen Ergebnissen gezogen wurden, angemessen (z. B. wurde erkannt, dass ein Nicht-Verwerfeen einer Hypothese nicht automatisch ein BestÃ¤tigen derselben bedeutet)?\nWurde angemessen gerundet (inkl. konsistente Anzahl von Nachkommastellen)?\nPassen die statistischen Verfahren zu den Hypothesen?\nWurden die Voraussetzungen der statistischen Verfahren geprÃ¼ft?\nSind die Ergebnisse reproduzierbar (Daten und Syntax eingereicht)?"
  },
  {
    "objectID": "020-pruefung.html#beispiele-fÃ¼r-fehler",
    "href": "020-pruefung.html#beispiele-fÃ¼r-fehler",
    "title": "2Â  PrÃ¼fung",
    "section": "2.10 Beispiele fÃ¼r Fehler",
    "text": "2.10 Beispiele fÃ¼r Fehler\nSchwere Fehler, die zum Durchfallen oder deutlichem Abwerten der Note fÃ¼hren kÃ¶nnen, sind z.B.:\n\nfehlende Inferenzstatistik (oder adÃ¤quatem Ersatz)\nfalsche Interpretation von Posteriori-Verteilungen oder p-Werten\nkeine Angabe von Konfidenzintervallen\nfalsche Interpretation von Konfidenzintervallen\nWahl des falschen Intervalls (Vorhersageintervall vs.Â Perzentilintervall vs.Â HDI)\nfalsche Entscheidung zum Hypothesentest auf Basis entsprechender Kennwerte (wie ROPE-Wahrscheinlichkeit oder p-Wert)\nfalsche Wahl des statistischen Verfahrens\nfehlende Deskriptivstatistik\n\nHÃ¤ufige kleinere MÃ¤ngel sind z. B.\n\npixelige Abbildungen\nR-Ausgaben oder R-Syntax als Screenshot\nfehlende Seitenzahlen (nur bei paginierten Formaten, nicht bei HTML)\nunÃ¼bersichtliche Diagramme\nkein (verlinktes) Inhaltsverzeichnis ï¸\nfehlende oder unverstÃ¤ndliche Achsenbeschriftung bei Diagrammen\nfehlende oder falsche Beschreibung der/des Skalenniveau(s) der untersuchten Variablen"
  },
  {
    "objectID": "020-pruefung.html#sonstiges",
    "href": "020-pruefung.html#sonstiges",
    "title": "2Â  PrÃ¼fung",
    "section": "2.11 Sonstiges",
    "text": "2.11 Sonstiges\nEine automatische PrÃ¼fung auf Plagiate mittels geeigneter, von der Hochschule bereitgestellter Software ist mÃ¶glich."
  },
  {
    "objectID": "025-twittermining.html#vorab",
    "href": "025-twittermining.html#vorab",
    "title": "\n3Â  Twitter Mining\n",
    "section": "\n3.1 Vorab",
    "text": "3.1 Vorab\n\n3.1.1 Lernziele\n\nTwitterdaten via API von Twitter auslesen\n\n3.1.2 Vorbereitung\n\nLesen Sie in Hvitfeldt und Silge (2021) Kap. 1.\nLegen Sie sich ein Konto bei Github an.\nLegen Sie sich ein Konto bei Twitter an.\nLesen Sie diesen Artikel zur Anmeldung bei der Twitter API1\n\n\n3.1.3 BenÃ¶tigte R-Pakete\n\nlibrary(tidyverse)\nlibrary(rtweet)\nlibrary(rio)  \nlibrary(glue)\nlibrary(tweetbotornot)  # optional\nlibrary(keyring)  # optional\nlibrary(askpass)  # optional\nlibrary(academictwitteR)\n\n\n\nR-Paket {rtweet}\n\nEinen Ãœberblick Ã¼ber die Funktionen des Pakets (function reference) findet sich hier."
  },
  {
    "objectID": "025-twittermining.html#anmelden-bei-twitter",
    "href": "025-twittermining.html#anmelden-bei-twitter",
    "title": "\n3Â  Twitter Mining\n",
    "section": "\n3.2 Anmelden bei Twitter",
    "text": "3.2 Anmelden bei Twitter\n\n3.2.1 Welche Accounts interessieren uns?\nHier ist eine (subjektive) Auswahl von deutschen Politikern2, die einen Startpunkt gibt zur Analyse von Art und AusmaÃŸ von Hate Speech gerichtet an deutsche Politiker:innen.\n\nd_path &lt;- \"data/twitter-german-politicians.csv\"\n\npoliticians &lt;- import(d_path)\npoliticians\n\n\n\n  \n\n\n\n\n3.2.2 Twitter App erstellen\nTutorial\nAuf der Twitter Developer Seite kÃ¶nnen Sie sich ein Konto erstellen und dann anmelden.\n\n3.2.3 Intro\nDie Seite von rtweet gibt eine gute Starthilfe in die Benutzung des Pakets.\n\n3.2.4 Zugangsdaten\nZugangsdaten sollte man geschÃ¼tzt speichern, also z.B. nicht in einem geteilten Ordner.\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\n\nAnmelden:\n\nauth &lt;- rtweet_bot(api_key = API_Key,\n                   api_secret = API_Key_Secret,\n                   access_token = Access_Token,\n                   access_secret = Access_Token_Secret)\n\nAlternativ kann man sich auch als App anmelden, damit kann man z.B. nicht posten, aber dafÃ¼r mehr herunterladen Quelle.\n\nauth &lt;- rtweet_app(bearer_token = Bearer_Token)\n\nJetzt haben wir ein Anmeldeobjekt, das wir fÃ¼r die weiteren Anfragen dieser Session nutzen kÃ¶nnen. Das sagen wir jetzt der Twitter-API:\n\nauth_as(auth)\n\nInfos Ã¼ber Ihre aktuellen Raten kann man sich mittels rate_limit() ausgeben lassen.\n\n3.2.5 SchÃ¼tzen Sie Ihre Zugangsdaten\nAchtung, Sicherheitshinweis â€¦ PasswÃ¶rter und andere sensitive (Anmelde-)Informationen muss man schÃ¼tzen, das weiÃŸ jeder. Konkret bedeutet es, dass Sie diese Daten nicht in einem Ã¶ffentlichen oder geteilten Repo herumliegen lassen. Achten Sie auch darauf, dass, wenn Sie diese Information sourceen, so wie ich gerade, diese dann ungeschÃ¼tzt in Ihrem RStudio Environment Fenster zu sehen sind. Falls Sie also den Bildschirm teilen, oder Ihnen jemand Ã¼ber die Schulter schaut, sind Ihre Zugangsdaten nicht geschÃ¼tzt.\nEin Ã¤hnlicher Fehler wÃ¤re, die History-Dateien von R in ein Ã¶ffentliches Repo einzustellen (z.B. via Git). In der Datei .gitignore sollten daher folgende Dateien aufgefÃ¼hrt sein:\n.Rhistory\n.Rapp.history\nEin Rat von rtweet dazu:\n\nItâ€™s good practice to only provide secrets interactively, because that makes it harder to accidentally share them in either your .Rhistory or an .R file.\n\nEinen alternativen, sichereren Zugang bietet z.B. das Paket keyring. Dieses Paket bietet eine Anbindung zur SchlÃ¼sselbundverwaltung Ihres Betriebssystems:\n\nPlatform independent API to access the operating systems credential store.\n\nIm MacOS wird die zentrale SchlÃ¼sselbundverwaltung genutzt, in Windows und Linux die analoge Vorrichtungen.\nWir erstellen uns einen SchlÃ¼sselbund:\n\nkeyring_create(keyring = \"hate-speech-twitter\")\n\nDann kÃ¶nnen wir einen Eintrag im SchlÃ¼sselbund erstellen. Es Ã¶ffnet sich eine Maske, die nach einem Passwort fragt. Geben Sie dort die sensitiven Informationen ein, etwa die Client-ID. Ggf. werden Sie noch nach dem Passwort des SchlÃ¼sselbunds an sich gefragt3\n\nkey_set(service = \"client_id\",\n        keyring = \"hate-speech-twitter\")\n\n\nkey_set(service = \"client_secret\",\n        keyring = \"hate-speech-twitter\")\n\n\nkey_set(service = \"bearer_token\",\n        keyring = \"hate-speech-twitter\")\n\nKÃ¼nftig kÃ¶nnen wir dann die PasswÃ¶rter aus dem SchlÃ¼sselbund abrufen:\n\nkey_get(service = \"client_id\",\n        keyring = \"hate-speech-twitter\")\n\n\nkey_get(service = \"bearer_token\",\n        keyring = \"hate-speech-twitter\")"
  },
  {
    "objectID": "025-twittermining.html#tweets-einlesen",
    "href": "025-twittermining.html#tweets-einlesen",
    "title": "\n3Â  Twitter Mining\n",
    "section": "\n3.3 Tweets einlesen",
    "text": "3.3 Tweets einlesen\nZu beachten ist, dass es Limits gibt, wie viele Informationen (pro Zeiteinheit) man Ã¼ber die Twitter-API auslesen darf. Informationen dazu findet man z.B. hier oder auch mit rate_limit().\nEin gÃ¤ngiges Limit der Twitter-API sind 900 Anfragen (z.B. Tweets auslesen) pro 15 Minuten.\n\n3.3.1 Timeline einlesen einzelner Accounts\nMal ein paar Tweets zur Probe:\n\nsesa_test &lt;- get_timeline(user = \"sauer_sebastian\", n = 3) %&gt;% \n  select(full_text)\n\n\n## RT @pia_lamberty: Ein Ansatz, der sich beim Debunking wissenschaftlich als erfolgreich herausgestellt hat, ist das sog. Faktensandwich: httâ€¦\n## RT @ianbremmer: sure, itâ€™s the hottest summer europe has ever had in history \n## \n## but look at the upside\n## \n## itâ€™s one of the coolest summers euroâ€¦\n## RT @twisteddoodles: Balanced news reporting https://t.co/O1iiItEQrs\n\n\ntweets &lt;- get_timeline(user = politicians$screenname)\nsaveRDS(tweets, file = \"data/tweets/tweets01.rds\")\n\nMichael Kearney rÃ¤t uns:\n\nPRO TIP #4: (for developer accounts only) Use bearer_token() to increase rate limit to 45,000 per fifteen minutes.\n\n\n3.3.2 Retweets einlesen\n\ntweets01_retweets &lt;- \n  tweets$id_str %&gt;% \n  head(3) %&gt;% \n  map_dfr( ~ get_retweets(status_id = .x, retryonratelimit = TRUE))\n\nDa die meisten Retweets aber nix sagen, sondern nur auf das einen Tweet wiederholen, ist das Sammeln der Retweets ziemlich langweilig.\nMÃ¶chte man retry on rate limit im Standard auf TRUE setzen, so kann man das Ã¼ber die Optionen von R tun.\n\noptions(rtweet.retryonratelimit = TRUE)\n\n\n3.3.3 EPINetz Twitter Politicians 2021\nKÃ¶nig u.Â a. (2022) Volltext hier haben einen Datensatz mit knapp 2500 Twitter Accounts deutscher Politiker zusammengestellt, zum Stand 2021.\nDer Datensatz kann Ã¼ber Gesis bezogen werden.\nAuf der gleichen Seite findet sich auch eine Dokumentation des Vorgehens.\nNachdem wir den Datensatz heruntergeladen haben, kÃ¶nnen wir ihn einlesen:\n\npoliticians_path &lt;- \"data/tweets/EPINetz_TwitterPoliticians_2021.RDs\"\npoliticians_twitter &lt;- read_rds(politicians_path)\n\nhead(politicians_twitter)\n\n\n\n  \n\n\n\nDann lesen wir die Timelines (die Tweets) dieser Konten aus; in diesem Beispiel nur 10 Tweets pro Account:\n\nepi_tweets &lt;- get_timeline(user = head(politicians_twitter$twitter_name), n = 10)\nhead(epi_tweets)\n\nNatÃ¼rlich kÃ¶nnte man auch mehr als 10 Tweets pro Konto einsammeln, braucht nur seine Zeit.\n\n3.3.4 Followers suchen\n\nfollowers01 &lt;-\n  politicians$screenname %&gt;% \n map_dfr( ~ get_followers(user = .x, retryonratelimit = TRUE))\n\nDa es dauern kann, Daten auszulesen (wir dÃ¼rfen pro 15 Min. nur eine begrenzte Zahl an Information abrufen), kann es Sinn machen, die Daten lokal zu speichern.\n\nsaveRDS(followers01, file = \"data/tweets/followers01.rds\")\n\nUnd ggf. wieder importieren:\n\nfollowers01 &lt;- read_rds(file = \"data/tweets/followers01.rds\")\n\nWie viele unique Followers haben wir identifiziert?\n\nfollowers02 &lt;- \n  followers01 %&gt;% \n  distinct(from_id)\n\nDie Screennames wÃ¤ren noch nÃ¼tzlich:\n\nlookup_users(users = \"1690868335\")\n\nDie Anzahl der Users, die man nachschauen kann, ist begrenzt auf 180 pro 15 Minuten.\n\nfollowers03 &lt;-\n  followers02 %&gt;% \n  mutate(screenname = \n           list(lookup_users(users = from_id, retryonratelimit = TRUE,verbose = TRUE)))\n\nEntsprechend kann man wieder einlesen:\nDamit haben wir eine Liste an Followers, deren Tweets wir einlesen und analysieren kÃ¶nnen, z.B. nach Hate Speech.\nIm Gegensatz zu Followers heiÃŸen bei Twitter die Accounts, denen ei Nutzi folgt â€œFriendsâ€.\nLesen wir mal die Followers von karl_lauterbach ein:\n\nkarl_followers &lt;- get_followers(user = \"karl_lauterbach\", verbose = TRUE)\n\nUm nicht jedes Mal aufs Neue die Daten herunterzuladen, bietet es sich an, die Daten lokal zu speichern:\n\nwrite_rds(karl_followers, file = \"data/tweets/karl_followers.rds\",\n          compress = \"gz\")\n\nEntsprechend kann man die Daten dann auch wieder einlesen:\n\nkarl_followers &lt;- read_rds(file = \"data/tweets/karl_followers.rds\")\n\n\n3.3.5 Follower Tweets einlesen\n\nfollowers_tweets &lt;- get_timeline(user = head(followers01$from_id), n = 10)\n\n\n3.3.6 Tweets nach Stichwort suchen\nUm nach einem Stichwort, allgemeiner nach einem bestimmten Text, in einem Tweet zu suchen, kann man die Funktion search_tweets nutzen:\n\nmy_tweet &lt;- search_tweets(\"Sebastian Sauer\", n = 1)\nmy_tweet$full_text\n\nSchaut man sich das zurÃ¼ckgelieferte Objekt (einen Tibble) nÃ¤her an, entdeckt man eine FÃ¼lle an Informationen. Satte 43 Spalten (teilweise Listenspalten) finden sich dort:\n\nnames(my_tweet)\n##  [1] \"created_at\"                    \"id\"                           \n##  [3] \"id_str\"                        \"full_text\"                    \n##  [5] \"truncated\"                     \"display_text_range\"           \n##  [7] \"entities\"                      \"metadata\"                     \n##  [9] \"source\"                        \"in_reply_to_status_id\"        \n## [11] \"in_reply_to_status_id_str\"     \"in_reply_to_user_id\"          \n## [13] \"in_reply_to_user_id_str\"       \"in_reply_to_screen_name\"      \n## [15] \"geo\"                           \"coordinates\"                  \n## [17] \"place\"                         \"contributors\"                 \n## [19] \"retweeted_status\"              \"is_quote_status\"              \n## [21] \"quoted_status_id\"              \"quoted_status_id_str\"         \n## [23] \"retweet_count\"                 \"favorite_count\"               \n## [25] \"favorited\"                     \"retweeted\"                    \n## [27] \"lang\"                          \"possibly_sensitive\"           \n## [29] \"quoted_status\"                 \"text\"                         \n## [31] \"favorited_by\"                  \"scopes\"                       \n## [33] \"display_text_width\"            \"quoted_status_permalink\"      \n## [35] \"quote_count\"                   \"timestamp_ms\"                 \n## [37] \"reply_count\"                   \"filter_level\"                 \n## [39] \"query\"                         \"withheld_scope\"               \n## [41] \"withheld_copyright\"            \"withheld_in_countries\"        \n## [43] \"possibly_sensitive_appealable\"\n\nDie Tweet-ID dieses Tweets bekommen Sie, wenn Sie die Variable id_str auslesen:\n\nmy_tweet$id_str\n## [1] \"1593598440675500032\"\n\n\nmy_tweet$source\n## [1] \"&lt;a href=\\\"https://mobile.twitter.com\\\" rel=\\\"nofollow\\\"&gt;Twitter Web App&lt;/a&gt;\"\n\nDabei ist source nicht etwa die Person, die tweetet, wie man vielleicht meinen kÃ¶nnte, sondern das Frontend, das dabei verwendet wurde, also z.B. die iphone-App oder die Twitter-Webseite.\nLeider sucht man den screenname zu einen Tweet vergeblich in my_tweet.\nGegeben eines Dataframes mit Tweets kann man sich aber wie folgt den Nutzernamen (screen_name) ausgeben lassen.\n\nusers_data(my_tweet)\n\n\n\n  \n\n\n\nAuÃŸerdem gibt es einen â€œTrickâ€ laut dieser Quelle, vgl. auch diesen SO-Post: Gibt man in die URL eines Tweets einen beliebigen Nutzernamen - das kann ein Fantasiename sein - so wird man automatisch zum richtigen Nutzer geleitet.\nDie Rohform der URL sieht also so aus:\nhttps://twitter.com/irgendeinnutzer/status/&lt;id_str&gt;\nGeben Sie also z.B. Folgende URL in Ihren Browser sein:\nhttps://twitter.com/irgendeinnutzer/status/1593598440675500032\nUnd Sie werden zum Nutzer sauer_sebastian weitergeleitet bzw. zu seinem Tweet mit obiger ID.\n\n3.3.7 Der Volltext ist manchmal abgeschniten\nManchmal ist der Volltext abgeschnitten\n\nmy_tweet$full_text\n\nHier steht der Beginn des Tweet-Textes, aber dann endet der Text abrup...\"\nGlÃ¼cklicherweise - sofern man bei einer so umstÃ¤ndlichen Darstellung von GlÃ¼ck reden kann - findet man den kompletten Text andernorts Quelle.\nDazu schreibt in diesem SO-Post der Nutzer Jonas:\n\nYou will need to check if the tweet is a retweet. If it is, use the retweetâ€™s full_text. If it is not, use the tweetâ€™s full_text. â€“ Jonas, Nov 13, 2017 at 15:00\n\n\nmy_tweet$retweeted_status[[1]][[\"full_text\"]]\n\nHier steht der Beginn des Tweet-Textes, aber dann endet der Text abrupt? \nNein,er geht weiter und irgendwann ist der dann wirklich aus.\"\n\n3.3.8 Tweets nach ID suchen\nMit lookup_tweets(id_des_tweets) kÃ¶nnen Sie sich die Informationen zu einen Tweet ausgeben lassen. Das ist natÃ¼rlich primÃ¤r der Volltext:\n\ntweet_example &lt;- lookup_tweets(\"1593598440675500032\")\ntweet_example$full_text\n\nAber auch die Ã¼brigen Informationen kÃ¶nnen interessant sein."
  },
  {
    "objectID": "025-twittermining.html#tweets-verarbeiten",
    "href": "025-twittermining.html#tweets-verarbeiten",
    "title": "\n3Â  Twitter Mining\n",
    "section": "\n3.4 Tweets verarbeiten",
    "text": "3.4 Tweets verarbeiten\n\n3.4.1 Grundlegende Verarbeitung\nSind die Tweets eingelesen, kann man z.B. eine Sentimentanalyse, s. KapitelÂ 4.2.10, durchfÃ¼hren, oder schlicht vergleichen, welche Personen welche WÃ¶rter hÃ¤ufig verwenden, s. KapitelÂ 4.2.3.\n\n3.4.2 Bot or not?\nEine interessante Methode, Tweets zu verarbeiten, bietet das R-Paket tweetbotornot von M. Kearney.\nAus der Readme:\n\nDue to Twitterâ€™s REST API rate limits, users are limited to only 180 estimates per every 15 minutes. To maximize the number of estimates per 15 minutes (at the cost of being less accurate), use the fast = TRUE argument. This method uses only users-level data, which increases the maximum number of estimates per 15 minutes to 90,000! Due to losses in accuracy, this method should be used with caution!\n\n\nusers &lt;- c(\"sauer_sebastian\")\nbot01 &lt;-\n  tweetbotornot(users)\n\n\n\n\n\n\n\nWichtig\n\n\n\nIch habe ein Fehlermeldung bekommen bei tweetbotornot. Da kÃ¶nnte ein technisches Problem in der Funktion vorliegen."
  },
  {
    "objectID": "025-twittermining.html#cron-jobs",
    "href": "025-twittermining.html#cron-jobs",
    "title": "\n3Â  Twitter Mining\n",
    "section": "\n3.5 Cron Jobs",
    "text": "3.5 Cron Jobs\n\n3.5.1 Was ist ein Cron Job?\nCron ist ein Programm auf Unix-artigen Betriebssystemen, das Skripte zu eingestellten Zeiten (wiederholt) ausfÃ¼hrt, das sind dann â€œCron Jobsâ€. Auf Windows gibt es aber analoge Funktionen. Cron Jobs sind praktisch, da man nicht jedes Mal selber z.B. Tweets, die heute zu einem Thema getweetet wurden, herunterladen muss. Das wird dann vom Cron Job Ã¼bernommen.\nIn R gibt es eine API zum Programm Cron mit dem Paket cronR, s. Anleitung hier.\nDas analoge R-Paket fÃ¼r Windows heiÃŸt {taskscheduleR}.\n\n3.5.2 Beispiel fÃ¼r einen Cron Job\n\nlibrary(cronR)\n\nscrape_script &lt;- cron_rscript(\"scrape_tweets.R\")\n\n# Cron Job hinzufÃ¼gen:\ncron_add(command = scrape_script, \n         frequency = 'daily', \n         at = \"10AM\",\n         id = 'Hate Speech')  # Name des Cron Jobs\n\ncron_clear(ask = FALSE)  # Alle Cron Jobs lÃ¶schen\ncron_ls()  # Liste aller Cron Jobs\n\nIm obigen Beispiel wird das R-Skript scrape_tweets.R tÃ¤glich um 10h ausgefÃ¼hrt.\nDer Inhalt von scrape_tweets.R kÃ¶nnte dann, in GrundzÃ¼gen, so aussehen:\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(rtweet)\nfollowers_lauterbach &lt;-\n  followers01 %&gt;% \n  filter(to_id == \"Karl_Lauterbach\")\n\nfollowers_lauterbach_tweets &lt;- \n  get_timeline(user = followers_lauterbach$from_id[1:10], n = 10, retryonratelimit = TRUE, verbose = FALSE)\n\n\npath_output &lt;- \"/Users/sebastiansaueruser/Google Drive/RRRing/Scrape-Tweets/tweets/\"\n\nwrite_csv(x = followers_lauterbach_tweets,\n          file = paste0(path_output, \"followers_lauterbach_tweets.csv\"),\n          append = TRUE)\n\nWir schreiben nicht jedes Mal (jeden Tag) eine neue CSV-Datei, sondern wir hÃ¤ngen hier die neu ausgelesenen Daten an die Datei an.\nLeider ist es mit rtweet nicht mÃ¶glich, ein Datum anzugeben, ab dem man Tweets auslesen mÃ¶chte4"
  },
  {
    "objectID": "025-twittermining.html#datenbank-an-tweets-aufbauen",
    "href": "025-twittermining.html#datenbank-an-tweets-aufbauen",
    "title": "\n3Â  Twitter Mining\n",
    "section": "\n3.6 Datenbank an Tweets aufbauen",
    "text": "3.6 Datenbank an Tweets aufbauen\n\n3.6.1 Stamm an bisherigen Tweets\nIn diesem Abschnitt kÃ¼mmern wir uns in grÃ¶ÃŸerem Detail um das Aufbauen einer Tweets-Datenbank.\nDiese Pakete benÃ¶tigen wir:\n\nlibrary(rtweet)\nlibrary(tidyverse)\nlibrary(rio)  # R Data import/export\n\nDann melden wir uns an:\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\nauth &lt;- rtweet_app(bearer_token = Bearer_Token)\n\nDann brauchen wir eine Liste an Twitterkonten, die uns interessieren. Im Kontext von Hate Speech soll uns hier interessieren, welche Tweets an deutsche Spitzenpolitikis5 gesendet werden. Wir suchen also nach Tweets mit dem Text @karl_lauterbach, um ein Beispiel fÃ¼r einen Spitzenpolitiker zu nennen, der vermutlich von Hate Speech in hÃ¶herem MaÃŸe betroffen ist.\n\npoliticians_twitter_path &lt;- \"/Users/sebastiansaueruser/github-repos/datascience-text/data/twitter-german-politicians.csv\"\n\npoliticians_twitter &lt;- rio::import(file = politicians_twitter_path)\n\nIn der Liste befinden sich 13 Politiker. Es macht die Sache vielleicht einfacher, wenn wir die Rate nicht Ã¼berziehen. Bleiben wir daher bei 1000 Tweets pro Politiki:\n\nn_tweets_per_politician &lt;- 1e3\n\nDie R-Syntax, die die Arbeit leistet, ist in Funktionen ausgelagert, der Ãœbersichtlichkeit halber.\n\nsource(\"funs/filter_recent_tweets.R\")\nsource(\"funs/download_recent_tweets.R\")\nsource(\"funs/add_tweets_to_tweet_db.R\")\nsource(\"funs/sanitize_tweets.R\")\n\nJetzt laden wir einfach die aktuellsten 1000 Tweets pro Konto herunter, daher brauchen wir keine Tweet-ID angeben, die ein Mindest- oder Maximum-Datum (bzw. ID) fÃ¼r einen Tweet angibt:\n\ntweets_older &lt;-\n  download_recent_tweets(screenname = politicians_twitter$screenname,\n                         max_or_since_id_str = NULL,\n                         n = n_tweets_per_politician,\n                         strip_columns = TRUE,\n                         reverse = TRUE)\n\nWie weit in die Vergangenheit reicht unsere Tweet-Sammlung?\n\noldest_tweets &lt;- filter_recent_tweets(tweets_older, max_or_min_id_str = is_min_id_str)\noldest_tweets\n\n\n\n  \n\n\n\nWas sind die neuesten Tweets, die wir habven?\n\nmost_recent_tweets &lt;- filter_recent_tweets(oldest_tweets)\nmost_recent_tweets\n\n\n\n  \n\n\n\nJetzt laden wir die neueren Tweets herunter, also mit einer ID grÃ¶ÃŸer als die grÃ¶ÃŸte in unserer Sammlung:\n\ntweets_new &lt;- \n  download_recent_tweets(screenname = most_recent_tweets$screenname,\n                         max_or_since_id_str = most_recent_tweets$id_str)\n\ntweets_new %&gt;% \n  select(screenname, created_at, id_str) %&gt;% \n  head()\n\nJetzt - und jedes Mal, wenn wir Tweets herunterladen - fÃ¼gen wir diese einer Datenbank (oder zumindest einer â€œGesamt-Tabelleâ€) hinzu:\n\ntweets_db &lt;- add_tweets_to_tweets_db(tweets_new, tweets_older)\n\nnrow(tweets_db)\n## [1] 10969\n\nSchlieÃŸlich sollten wir nicht vergessen diese in einer Datei zu speichern:\n\nwrite_rds(tweets_db, file = \"~/datasets/Twitter/tweets-db-2022-11-11.rds\")\n\nâ€¦ â€¦ So, einige Zeit ist vergangen. Laden wir noch Ã¤ltere Tweets herunter und fÃ¼gen Sie unserer Datenbank hinzu:\n\ntweets_older2 &lt;-\n  download_recent_tweets(screenname = politicians_twitter$screenname,\n                         max_or_since_id_str = oldest_tweets$id_str,\n                         n = 1e3,\n                         strip_columns = TRUE,\n                         reverse = TRUE)\n\n\ntweets_db &lt;- add_tweets_to_tweets_db(tweets_new, tweets_older2)\n\nnrow(tweets_db)\n## [1] 10011\n\nUnd wieder speichern wir die vergrÃ¶ÃŸerte Datenbasis auf der Festplatte:\n\nwrite_rds(tweets_db, file = \"~/datasets/Twitter/hate-speech-twitter.rds\")\n\nLeider ist die Datenbasis nicht mehr deutlich gewachsen. Eine plausible Ursache ist, dass Twitter den Zugriff auf alte Tweets einschrÃ¤nkt.\nAus der Hilfe von search_tweets:\n\nReturns Twitter statuses matching a user provided search query. ONLY RETURNS DATA FROM THE PAST 6-9 DAYS.\n\nMit Hilfe des Academic Research Access sind deutlich hÃ¶here Raten mÃ¶glich.\n\n3.6.2 Neue Tweets per Cron Job\nWie oben schon ausprobiert, legen wir uns einen Cron Job an.\nDas ist Ã¼brigens auch eine komfortable LÃ¶sung.\n\nlibrary(cronR)\n\nscrape_script &lt;- cron_rscript(\"/Users/sebastiansaueruser/github-repos/datascience-text/funs/get_tweets_politicians.R\")\n\n# Cron Job hinzufÃ¼gen:\ncron_add(command = scrape_script, \n         frequency = 'daily', \n         at = \"10AM\",\n         id = 'Hate Speech')  # Name des Cron Jobs\n\nDas Skript get_tweets_politicians.R birgt die Schritte, die wir in diesem Abschnitt ausprobiert haben, hier liegt es. Kurz gesagt sucht es nach neuen Tweets, die also noch nicht in Ihrer â€œDatenbankâ€ vorhanden sind, und lÃ¤dt diese herunter. Dabei werden maximal 1000 Tweets pro Konto (derer sind es 13) heruntergeladen.\nBei einem Cronjob sollten absolute Pfade angegeben werden, da der Cronjob nicht aus dem aktuellen Projekt-Repo startet.\nDie Ergebnisse eines Cronjob-Durchlaufs werden in einer Log-Datei abgelegt, und zwar in dem Ordner, in dem auch das Skript liegt, das im Rahmen des Cronjobs durchgefÃ¼hrt wird.\n\n\n\n\n\n\nHinweis\n\n\n\nSchauen Sie sich die Funktionen im Ordner /funs einmal in Ruhe an. Hier geht es zu dem Ordner im Github-Repo. Es ist alles keine Zauberei, aber im Detail gibt es immer wieder Schwierigkeiten. Am meisten lernt man, wenn man selber Hand anlegt.\n\n\nMÃ¶chte man den Cron Job wieder lÃ¶schen, so kann man das so tun:\n\ncron_clear(ask = FALSE)  # Alle Cron Jobs lÃ¶schen\ncron_ls()  # Liste aller Cron Jobs\n\nUm die Tweets â€œhÃ¤ndischâ€ herunterzuladen, kann man get_tweets_politicians() aufrufen:\n\nsource(\"funs/get_tweets_politicians.R\")\nget_tweets_politicians()\n\n\n3.6.3 Tweets in Excel exportieren\nUm prÃ¤diktive Modelle zu erstellen, braucht man ein Trainingsset, Tweets also, die schon vorklassifiziert sind, z.B. im Hinblick auf Hassrede mit ja oder nein. Technisch bietet sich ein 1 vs.Â 0 an.\nDazu laden wir einen Datensatz mit Tweets, z.B. diesen hier:\n\ntweets_to_kl &lt;- import(\"/Users/sebastiansaueruser/datasets/Twitter/tweets_to_karl_lauterbach.rds\")\n\nDa es viele Spalten gibt, die teilweise Listenspalten sind, also komplex, begrenzen wir uns auf das Wesentliche, den Tweet-Text und die ID des Tweets.\n\ntweets_to_kl2 &lt;-\n  tweets_to_kl %&gt;% \n  select(id_str, full_text) \n\n\n\n\n\n\n\nHinweis\n\n\n\nDie Tweet-ID wird einmal als String und einmal als Integer gespeichert. Allerdings Ã¼bersteigt die Anzahl der Ziffern die SpeichergrÃ¶ÃŸe von (normalen) Integer-Formaten in R. Daher ist die Twitter-ID als Integer nicht zuverlÃ¤ssig; als Text hingegen schon.\n\n\nUnd schlieÃŸlich kÃ¶nnen wir die Excel-Datei importieren.\n\nexport(tweets_to_kl2, file = \"~/datasets/Twitter/tweets_to_kl.xlsx\")\n\nDie Excel-Tabelle kÃ¶nnen wir dann bequem hernehmen, um Tweets manuell zu klassifizieren.\n\n3.6.4 Twitterkonten fÃ¼r Wissenschaftler\nTwitter stellt spezielle Konten fÃ¼r Wissentschaftlis bereit, die Ã¼ber hÃ¶here Raten und mehr Funktionen verfÃ¼gen, also mehr Tweets herunterladen kÃ¶nnen, z.B. 10 Millionen Tweets pro Monat pro Projekt.\n\nauth_academic &lt;- rtweet_app(bearer_token = askpass::askpass(\"bearer token\"))\nauth_academic\n\nDas R-Paket askpass stellt eine weitere MÃ¶glichkeit bereit, um Zugangsdaten zu schÃ¼tzen. Es Ã¶ffnet eine Maske, die interaktiv und als Punkte geschÃ¼tzte Buchstaben nach einem Passwort fragt, in diesem Fall nach dem Bearer-Token.\nAus der Hilfe:\n\nPrompt the user for a password to authenticate or read a protected key. By default, this function automatically uses the most appropriate method based on the user platform and front-end. Users or IDEs can override this and set a custom password entry function via the askpass option.\n\nTwitter bietet in diesem Repo einen nÃ¼tzlichen Kurs an, um sich mit der API vertraut zu machen.\n\ntweets_to__FriedrichMerz_2022 &lt;-\n  get_all_tweets(query = \"to:_FriedrichMerz -is:retweet\",\n                 start_tweets = \"2022-01-01T00:00:00Z\",\n                 end_tweets = \"2022-11-23T23:59:59Z\",\n                 bearer_token = askpass(\"Bearer token\"),\n                 file = \"~/datasets/Twitter/tweets-to-_FriedrichMerz_2022.rds\",\n                 n = 1e5)\n\nOder als Funktion, das ist praktischer, wenn man die Syntax mehrfach verwendet:\n\nget_all_tweets_politicians &lt;- function(screenname, bearer_token, n = 1e5) {\n  get_all_tweets(query = paste0(\"to:\", screenname, \" -is:retweet\"),\n                 start_tweets = \"2021-01-01T00:00:00Z\",\n                 end_tweets = \"2021-12-31T23:59:59Z\",\n                 bearer_token = bearer_token,\n                 file = glue::glue(\"~/datasets/Twitter/tweets_to_{screenname}_2021.rds\"),\n                 data_path = glue::glue(\"~/datasets/Twitter/{screenname}\"),\n                 n = n)\n}\n\n\n#debug(get_all_tweets_politicians)\nget_all_tweets_politicians(screenname = politicians$screenname[5],\n                           bearer_token = askpass(\"Bearer token\"),\n                           n = 1e05)\n\nDann kann man die Objekte abespeichern, etwas als RDS-Datei oder als Feather-Datei.\nDen Datensatz politicians hatten wir oben angelegt, s. KapitelÂ 3.2.1. Er beinhaltet die Kontonamen (screennames) einiger deutscher Politikis.\nWichtig ist, mit den Lizenzregeln in Einklang zu bleiben.\nZentral ist dabei sicherlich die Frage, ob und wie man Tweets weitergeben darf. Dazu:\n\nAcademic researchers are permitted to distribute an unlimited number of Tweet IDs and/or User IDs if they are doing so for the purposes of non-commercial research and to validate or replicate previous academic studies. You should not share the entire Tweet text directly. Instead, you can build a list of Tweet IDs and share those. The researchers who you share this set of Tweet IDs with, can then use the Twitter API to hydrate and get the full Tweet objects from the Tweet IDs.\n\nQuelle\nMehr Details finden sich den Entwicklerrichtlinien von Twitter.\nTwitter stellt eine Reihe von Lehrmaterialien fÃ¼r die wissenschaftliche Nutzung von Tweets bereit."
  },
  {
    "objectID": "025-twittermining.html#aufgaben",
    "href": "025-twittermining.html#aufgaben",
    "title": "\n3Â  Twitter Mining\n",
    "section": "\n3.7 Aufgaben",
    "text": "3.7 Aufgaben\n\nÃœberlegen Sie, wie Sie das AusmaÃŸ an Hate Speech, dem deutsche Politikerinnen und Politiker konfrontiert sind, messen kÃ¶nnen.\nArgumentieren Sie die Vorteile und Nachteile Ihres Ansatzes. AuÃŸerdem, auf welches Ergebnis dieser Analyse sie gespannt sind bzw. wÃ¤ren.\nÃœberlegen Sie Korrelate, oder besser noch: (mÃ¶gliche) Ursachen, des Hasses in den Tweets, gerichtet auf Polikter:innen. Sie kÃ¶nnen auch Gruppen von Ursachen bilden, etwas personengebundene Variablen der Politiker:innen (z.B. Alter? Geschlecht? Migrationshintergrund?).\nErstellen Sie sich eine Liste an Personen, deren Tweets sich lohnen (kÃ¶nnten), auf Hate Speech hin analysiert zu werden. Laden Sie deren Tweets (ggf. in AuszÃ¼gen) herunter.\nDas Skript zu scrape_tweets.R kÃ¶nnte man noch verbessern, in dem man jeden Tag nur die neuesten Tweets herunterlÃ¤dt. Dazu kann man bei get_timeline() mit dem Argument since_id eine Untergrenze der ID festlegen, so dass nur neuere Tweets (d.h. mit grÃ¶ÃŸerem Wert bei ID) ausgelesen werden. Ã„ndern Sie das Skript entsprechend, so dass nur neuerer Tweets gelesen werden.\nErarbeiten Sie die Folien zu diesem rtweet-Workshop. Eine Menge guter Tipps!\n\n\n\n\n\nHvitfeldt, Emil, und Julia Silge. 2021. Supervised Machine Learning for Text Analysis in R. 1. Aufl. Boca Raton: Chapman and Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nKÃ¶nig, Tim, Wolf J. SchÃ¼nemann, Alexander Brand, Julian Freyberg, und Michael Gertz. 2022. â€The EPINetz Twitter Politicians Dataset 2021. A New Resource for the Study of the German Twittersphere and Its Application for the 2021 Federal Electionsâ€œ. Politische Vierteljahresschrift 63 (3): 529â€“47. https://doi.org/10.1007/s11615-022-00405-7."
  },
  {
    "objectID": "025-twittermining.html#footnotes",
    "href": "025-twittermining.html#footnotes",
    "title": "\n3Â  Twitter Mining\n",
    "section": "",
    "text": "Sie kÃ¶nnen hier nachlesen, was eine API ist.â†©ï¸\nStand November 2022â†©ï¸\nwas beruhigend ist: Man darf nicht ohne Erlaubnis in Ihrer Passwort-Sammlung herumfuhrwerken.â†©ï¸\nMit dem R-Paket twitteR, das mittlerweile zugunsten von rtweet aufgegeben wurde, war das mÃ¶glich. Allerdings zeigt ein Blick in die Dokumentation der Twitter-API, das Datumsangaben offenbar gar nicht unterstÃ¼tzt werden.â†©ï¸\nzur Zeit, als diese Zeilen geschrieben wurdenâ†©ï¸"
  },
  {
    "objectID": "030-textmining1.html#vorab",
    "href": "030-textmining1.html#vorab",
    "title": "\n4Â  Textmining1\n",
    "section": "\n4.1 Vorab",
    "text": "4.1 Vorab\n\n4.1.1 Lernziele\n\nDie vorgestellten Techniken des Textminings mit R anwenden kÃ¶nnen\n\n4.1.2 Vorbereitung\n\nLesen Sie in Hvitfeldt und Silge (2021) Kap. 2.\n\n4.1.3 BenÃ¶tigte R-Pakete\n\nlibrary(tidyverse)\nlibrary(tokenizers)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(hcandersenr)\nlibrary(SnowballC)  # Stemming\nlibrary(lsa)  # StopwÃ¶rter\nlibrary(easystats)  # Komfort fÃ¼r deskriptive Statistiken, wie `describe_distribution`\nlibrary(textclean)  # Emojis ersetzen\nlibrary(wordcloud)"
  },
  {
    "objectID": "030-textmining1.html#einfache-methoden-des-textminings",
    "href": "030-textmining1.html#einfache-methoden-des-textminings",
    "title": "\n4Â  Textmining1\n",
    "section": "\n4.2 Einfache Methoden des Textminings",
    "text": "4.2 Einfache Methoden des Textminings\nArbeiten Sie die folgenden grundlegenden Methoden des Textminigs durch.\n\n4.2.1 Tokenisierung\nErarbeiten Sie dieses Kapitel: Hvitfeldt und Silge (2021), Kap. 2\nWie viele Zeilen hat das MÃ¤rchen â€œThe Fir treeâ€ (in der englischen Fassung?)\n\nhcandersen_en %&gt;% \n  filter(book == \"The fir tree\") %&gt;% \n  nrow()\n## [1] 253\n\n\n4.2.2 StopwÃ¶rter entfernen\nErarbeiten Sie dieses Kapitel: s. Hvitfeldt und Silge (2021), Kap. 3\nEine alternative Quelle von StopwÃ¶rtern - in verschiedenen Sprachen - biwetet das Paket quanteda:\n\nstop2 &lt;-\n  tibble(word = quanteda::stopwords(\"german\"))\n\nhead(stop2)\n\n\n\n  \n\n\n\nEs bestehst (in der deutschen Version) aus 231 WÃ¶rtern.\n\n4.2.3 WÃ¶rter zÃ¤hlen\nIst der Text tokenisiert, kann man einfach mit â€œBordmittelnâ€ die WÃ¶rter zÃ¤hlen.\n\nhc_andersen_count &lt;- \n  hcandersen_de %&gt;% \n  filter(book == \"Das Feuerzeug\") %&gt;% \n  unnest_tokens(output = word, input = text) %&gt;% \n  anti_join(stop2) %&gt;% \n  count(word, sort = TRUE) \n## Joining with `by = join_by(word)`\n\nhc_andersen_count %&gt;% \n  head()\n\n\n\n  \n\n\n\nZur Visualisierung eignen sich Balkendiagramme, s. ?fig-hcandersen-count.\n\nhc_andersen_count %&gt;% \n  slice_max(order_by = n, n = 10) %&gt;% \n  mutate(word = factor(word)) %&gt;% \n  ggplot() +\n  aes(y = reorder(word, n), x = n) +\n  geom_col()\n  \n\n\n\nAbbildungÂ 4.1: Die hÃ¤ufigsten WÃ¶rter in H.C. Anderssens Feuerzeug\n\n\n\nDabei macht es Sinn, aus word einen Faktor zu machen, denn Faktorstufen kann man sortieren, zumindest ist das die einfachste LÃ¶sung in ggplot2 (wenn auch nicht super komfortabel).\nEine (beliebite?) Methode, um WorthÃ¤ufigkeiten in Corpora darzustellen, sind Wortwolken, s. AbbildungÂ 4.2. Es sei hinzugefÃ¼gt, dass solche Wortwolken nicht gerade optimale perzeptorische QualitÃ¤ten aufweisen.\n\nwordcloud(words = hc_andersen_count$word,\n          freq = hc_andersen_count$n,\n          max.words = 50,\n          rot.per = 0.35,\n          colors = brewer.pal(8, \"Dark2\"))\n\n\n\nAbbildungÂ 4.2: Eine Wortwolke zu den hÃ¤ufigsten WÃ¶rtern in H.C. Andersens Feuerzeug\n\n\n\n\n4.2.4 Stemming (Wortstamm finden)\nErarbeiten Sie dieses Kapitel: Hvitfeldt und Silge (2021), Kap. 4\nVertiefende Hinweise zum UpSet plot finden Sie hier, Lex u.Â a. (2014).\nFÃ¼r welche Sprachen gibt es Stemming im Paket SnowballC?\n\nlibrary(SnowballC)\ngetStemLanguages()\n##  [1] \"arabic\"     \"basque\"     \"catalan\"    \"danish\"     \"dutch\"     \n##  [6] \"english\"    \"finnish\"    \"french\"     \"german\"     \"greek\"     \n## [11] \"hindi\"      \"hungarian\"  \"indonesian\" \"irish\"      \"italian\"   \n## [16] \"lithuanian\" \"nepali\"     \"norwegian\"  \"porter\"     \"portuguese\"\n## [21] \"romanian\"   \"russian\"    \"spanish\"    \"swedish\"    \"tamil\"     \n## [26] \"turkish\"\n\nEinfacher Test: Suchen wir den Wordstamm fÃ¼r das Wort â€œwissensdurstigenâ€, wie in â€œdie wissensdurstigen Studentis lÃ¶cherten dis armi Professiâ€1.\n\nwordStem(\"wissensdurstigen\", language = \"german\")\n## [1] \"wissensdurst\"\n\nWerfen Sie mal einen Blick in das Handbuch von SnowballC.\n\n4.2.5 Fallstudie AfD-Parteiprogramm\nDaten einlesen:\n\nd_link &lt;- \"https://raw.githubusercontent.com/sebastiansauer/pradadata/master/data-raw/afd_2022.csv\"\nafd &lt;- read_csv(d_link, show_col_types = FALSE)\n\nWie viele Seiten hat das Dokument?\n\nnrow(afd)\n## [1] 190\n\nUnd wie viele WÃ¶rter?\n\nstr_count(afd$text, pattern = \"\\\\w\") %&gt;% sum(na.rm = TRUE)\n## [1] 179375\n\nAus breit mach lang, oder: wir tokenisieren (nach WÃ¶rtern):\n\nafd %&gt;% \n  unnest_tokens(output = token, input = text) %&gt;% \n  filter(str_detect(token, \"[a-z]\")) -&gt; afd_long\n\nStopwÃ¶rter entfernen:\n\ndata(stopwords_de, package = \"lsa\")\n\nstopwords_de &lt;- tibble(word = stopwords_de)\n\n# FÃ¼r das Joinen werden gleiche Spaltennamen benÃ¶tigt:\nstopwords_de &lt;- stopwords_de %&gt;% \n  rename(token = word)  \n\nafd_long %&gt;% \n  anti_join(stopwords_de) -&gt; afd_no_stop\n## Joining with `by = join_by(token)`\n\nWÃ¶rter zÃ¤hlen:\n\nafd_no_stop %&gt;% \n  count(token, sort = TRUE) -&gt; afd_count\n\nhead(afd_count)\n\n\n\n  \n\n\n\nWÃ¶rter trunkieren:\n\nafd_no_stop %&gt;% \n  mutate(token_stem = wordStem(token, language = \"de\")) %&gt;% \n  count(token_stem, sort = TRUE) -&gt; afd_count_stemmed\n\nhead(afd_no_stop)\n\n\n\n  \n\n\n\n\n4.2.6 Stringverarbeitung\nErarbeiten Sie dieses Kapitel: Wickham und Grolemund (2016), Kap. 14\n\n4.2.6.1 RegulÃ¤rausdrÃ¼cke\nDas \"[a-z]\" in der Syntax oben steht fÃ¼r â€œalle Buchstaben von a-zâ€. D iese flexible Art von â€œString-Verarbeitung mit Jokernâ€ nennt man RegulÃ¤rausdrÃ¼cke (regular expressions; regex). Es gibt eine ganze Reihe von diesen RegulÃ¤rausdrÃ¼cken, die die Verarbeitung von Texten erleichert. Mit dem Paket stringr geht das - mit etwas Ãœbung - gut von der Hand. Nehmen wir als Beispiel den Text eines Tweets:\n\nstring &lt;- \"Correlation of unemployment and #AfD votes at #btw17: ***r = 0.18***\\n\\nhttps://t.co/YHyqTguVWx\"  \n\nMÃ¶chte man Ziffern identifizieren, so hilft der ReulÃ¤rausdruck [:digit:]:\nâ€œGibt es mindestens eine Ziffer in dem String?â€\n\nstr_detect(string, \"[:digit:]\")\n## [1] TRUE\n\nâ€œFinde die Position der ersten Ziffer! Welche Ziffer ist es?â€\n\nstr_locate(string, \"[:digit:]\")\n##      start end\n## [1,]    51  51\nstr_extract(string, \"[:digit:]\")\n## [1] \"1\"\n\nâ€œFinde alle Ziffern!â€\n\nstr_extract_all(string, \"[:digit:]\")\n## [[1]]\n## [1] \"1\" \"7\" \"0\" \"1\" \"8\"\n\nâ€œFinde alle Stellen an denen genau 2 Ziffern hintereinander folgen!â€\n\nstr_extract_all(string, \"[:digit:]{2}\")\n## [[1]]\n## [1] \"17\" \"18\"\n\nDer QuantitÃ¤tsoperator {n} findet alle Stellen, in der der der gesuchte Ausdruck genau \\(n\\) mal auftaucht.\nâ€œZeig die Hashtags!â€\n\nstr_extract_all(string, \"#[:alnum:]+\")\n## [[1]]\n## [1] \"#AfD\"   \"#btw17\"\n\nDer Operator [:alnum:] steht fÃ¼r â€œalphanumerischer Charakterâ€ - also eine Ziffer oder ein Buchstabe; synonym hÃ¤tte man auch \\\\w schreiben kÃ¶nnen (w wie word). Warum werden zwei Backslashes gebraucht? Mit \\\\w wird signalisiert, dass nicht der Buchstabe w, sondern etwas Besonderes, eben der Regex-Operator \\w gesucht wird.\nâ€œZeig die URLs!â€\n\nstr_extract_all(string, \"https?://[:graph:]+\")\n## [[1]]\n## [1] \"https://t.co/YHyqTguVWx\"\n\nDas Fragezeichen ? ist eine QuantitÃ¤tsoperator, der einen Treffer liefert, wenn das vorherige Zeichen (hier s) null oder einmal gefunden wird. [:graph:] ist die Summe von [:alpha:] (Buchstaben, groÃŸ und klein), [:digit:] (Ziffern) und [:punct:] (Satzzeichen u.Ã¤.).\nâ€œZÃ¤hle die WÃ¶rter im String!â€\n\nstr_count(string, boundary(\"word\"))\n## [1] 13\n\nâ€œLiefere nur Buchstabenfolgen zurÃ¼ck, lÃ¶sche alles Ã¼brigeâ€\n\nstr_extract_all(string, \"[:alpha:]+\")\n## [[1]]\n##  [1] \"Correlation\"  \"of\"           \"unemployment\" \"and\"          \"AfD\"         \n##  [6] \"votes\"        \"at\"           \"btw\"          \"r\"            \"https\"       \n## [11] \"t\"            \"co\"           \"YHyqTguVWx\"\n\nDer QuantitÃ¤tsoperator + liefert alle Stellen zurÃ¼ck, in denen der gesuchte Ausdruck einmal oder hÃ¤ufiger vorkommt. Die Ergebnisse werden als Vektor von WÃ¶rtern zurÃ¼ckgegeben. Ein anderer QuantitÃ¤tsoperator ist *, der fÃ¼r 0 oder mehr Treffer steht. MÃ¶chte man einen Vektor, der aus Stringen-Elementen besteht zu einem Strring zusammenfÃ¼ngen, hilft paste(string) oder str_c(string, collapse = \" \").\n\nstr_replace_all(string, \"[^[:alpha:]+]\", \"\")\n## [1] \"CorrelationofunemploymentandAfDvotesatbtwrhttpstcoYHyqTguVWx\"\n\nMit dem Negationsoperator [^x] wird der RegulÃ¤rausrck x negiert; die Syntax oben heiÃŸt also â€œersetze in string alles auÃŸer Buchstaben durch Nichtsâ€. Mit â€œNichtsâ€ sind hier Strings der LÃ¤nge Null gemeint; ersetzt man einen belieibgen String durch einen String der LÃ¤nge Null, so hat man den String gelÃ¶scht.\nDas Cheatsheet zur Strings bzw zu stringr von RStudio gibt einen guten Ãœberblick Ã¼ber Regex; im Internet finden sich viele Beispiele.\n\n4.2.6.2 Regex im Texteditor\nEinige Texteditoren unterstÃ¼tzen Regex, so auch RStudio.\nDas ist eine praktische Sache. Ein Beispiel: Sie haben eine Liste mit Namen der Art:\n\nNachname1, Vorname1\nNachname2, Vorname2\nNachname3, Vorname3\n\nUnd Sie mÃ¶chten jetzt aber die Liste mit Stil Vorname Nachname sortiert haben.\nRStudio mit Regex machtâ€™s mÃ¶glich, s. ?fig-vorher-regex.\n\n\nAbbildungÂ 4.3: ?(caption)\n\n\n4.2.7 Emoji-Analyse\nEine einfache Art, Emojis in einer Textmining-Analyse zu verarbeiten, bietet das Paket textclean:\n\nfls &lt;- system.file(\"docs/emoji_sample.txt\", package = \"textclean\")\nx &lt;- readLines(fls)[1]\nx\n## [1] \"Proin ğŸ˜ ut maecenas ğŸ˜ condimentum ğŸ˜” purus eget. Erat, ğŸ˜‚vitae nunc elit. Condimentum ğŸ˜¢ semper iaculis bibendum sed tellus. Ut suscipit interdumğŸ˜‘ in. FaucibğŸ˜ us nunc quis a vitae posuere. ğŸ˜› Eget amet sit condimentum non. Nascetur vitae â˜¹ et. Auctor ornare â˜º vestibulum primis justo congue ğŸ˜€urna ac magna. Quam ğŸ˜¥ pharetra ğŸ˜Ÿ eros ğŸ˜’facilisis ac lectus nibh est ğŸ˜™vehicula ğŸ˜ ornare! Vitae, malesuada ğŸ˜ erat sociosqu urna, ğŸ˜ nec sed ad aliquet ğŸ˜® .\"\n\n\nreplace_emoji(x)\n## [1] \"Proin smiling face with heart-eyes ut maecenas smirking face condimentum pensive face purus eget. Erat, face with tears of joy vitae nunc elit. Condimentum crying face semper iaculis bibendum sed tellus. Ut suscipit interdum expressionless face in. Faucib disappointed face us nunc quis a vitae posuere. face with tongue Eget amet sit condimentum non. Nascetur vitae frowning face et. Auctor ornare smiling face vestibulum primis justo congue grinning face urna ac magna. Quam sad but relieved face pharetra worried face eros unamused face facilisis ac lectus nibh est kissing face with smiling eyes vehicula neutral face ornare! Vitae, malesuada smiling face with sunglasses erat sociosqu urna, smirking face nec sed ad aliquet face with open mouth .\"\nreplace_emoji_identifier(x)\n## [1] \"Proin lexiconwiutsdotskrupggpgmhm ut maecenas lexiconwizbukzesopzflfinotj condimentum lexiconwlnxqescoesytfatoevi purus eget. Erat, lexiconwcaiviebiytolowkanmb vitae nunc elit. Condimentum lexiconwpujksvgujncexktvyrn semper iaculis bibendum sed tellus. Ut suscipit interdum lexiconwknnasgueiicggptyzbx in. Faucib lexiconwoxfeslcareuqfkbyjgy us nunc quis a vitae posuere. lexiconwobmhqdrrzgygdexhnkk Eget amet sit condimentum non. Nascetur vitae lexiconbfalxvockmnmtmycmwyq et. Auctor ornare lexiconbgmujofaalvxqrklfqgd vestibulum primis justo congue lexiconvygwtlyrpywfarytvfis urna ac magna. Quam lexiconwurhpvewhizayynmfxqo pharetra lexiconwpmuduwgbxxrxeltrueb eros lexiconwkrvakxddtqckcjxeksl facilisis ac lectus nibh est lexiconwmsjgfnelqfeyhgudmfj vehicula lexiconwjfhkpcsgcjtotwlapxa ornare! Vitae, malesuada lexiconwivnupleicqgksianinp erat sociosqu urna, lexiconwizbukzesopzflfinotj nec sed ad aliquet lexiconxbwhfeflxbuupjezgdwl .\"\n\n\n4.2.8 Text aufrÃ¤umen\nEine Reihe generischer Tests bietet das Paket textclean von Tyler Rinker:\nHier ist ein â€œunaufgerÃ¤umeterâ€ Text:\n\nx &lt;- c(\"i like\", \"&lt;p&gt;i want. &lt;/p&gt;. thet them ther .\", \"I am ! that|\", \"\", NA, \n    \"&quot;they&quot; they,were there\", \".\", \"   \", \"?\", \"3;\", \"I like goud eggs!\", \n    \"bi\\xdfchen Z\\xfcrcher\", \"i 4like...\", \"\\\\tgreat\",  \"She said \\\"yes\\\"\")\n\nLassen wir uns dazu ein paar Diagnostiken ausgeben.\n\nEncoding(x) &lt;- \"latin1\"\nx &lt;- as.factor(x)\ncheck_text(x)\n## \n## =============\n## NON CHARACTER\n## =============\n## \n## The text variable is not a character column (likely `factor`):\n## \n## \n## *Suggestion: Consider using `as.character` or `stringsAsFactors = FALSE` when reading in\n##              Also, consider rerunning `check_text` after fixing\n## \n## \n## =====\n## DIGIT\n## =====\n## \n## The following observations contain digits/numbers:\n## \n## 10, 13\n## \n## This issue affected the following text:\n## \n## 10: 3;\n## 13: i 4like...\n## \n## *Suggestion: Consider using `replace_number`\n## \n## \n## ========\n## EMOTICON\n## ========\n## \n## The following observations contain emoticons:\n## \n## 6\n## \n## This issue affected the following text:\n## \n## 6: &quot;they&quot; they,were there\n## \n## *Suggestion: Consider using `replace_emoticons`\n## \n## \n## =====\n## EMPTY\n## =====\n## \n## The following observations contain empty text cells (all white space):\n## \n## 1\n## \n## This issue affected the following text:\n## \n## 1: i like\n## \n## *Suggestion: Consider running `drop_empty_row`\n## \n## \n## =======\n## ESCAPED\n## =======\n## \n## The following observations contain escaped back spaced characters:\n## \n## 14\n## \n## This issue affected the following text:\n## \n## 14: \\tgreat\n## \n## *Suggestion: Consider using `replace_white`\n## \n## \n## ====\n## HTML\n## ====\n## \n## The following observations contain HTML markup:\n## \n## 2, 6\n## \n## This issue affected the following text:\n## \n## 2: &lt;p&gt;i want. &lt;/p&gt;. thet them ther .\n## 6: &quot;they&quot; they,were there\n## \n## *Suggestion: Consider running `replace_html`\n## \n## \n## ==========\n## INCOMPLETE\n## ==========\n## \n## The following observations contain incomplete sentences (e.g., uses ending punctuation like '...'):\n## \n## 13\n## \n## This issue affected the following text:\n## \n## 13: i 4like...\n## \n## *Suggestion: Consider using `replace_incomplete`\n## \n## \n## =============\n## MISSING VALUE\n## =============\n## \n## The following observations contain missing values:\n## \n## 5\n## \n## *Suggestion: Consider running `drop_NA`\n## \n## \n## ========\n## NO ALPHA\n## ========\n## \n## The following observations contain elements with no alphabetic (a-z) letters:\n## \n## 4, 7, 8, 9, 10\n## \n## This issue affected the following text:\n## \n## 4: \n## 7: .\n## 8:    \n## 9: ?\n## 10: 3;\n## \n## *Suggestion: Consider cleaning the raw text or running `filter_row`\n## \n## \n## ==========\n## NO ENDMARK\n## ==========\n## \n## The following observations contain elements with missing ending punctuation:\n## \n## 1, 3, 4, 6, 8, 10, 12, 14, 15\n## \n## This issue affected the following text:\n## \n## 1: i like\n## 3: I am ! that|\n## 4: \n## 6: &quot;they&quot; they,were there\n## 8:    \n## 10: 3;\n## 12: biÃŸchen ZÃ¼rcher\n## 14: \\tgreat\n## 15: She said \"yes\"\n## \n## *Suggestion: Consider cleaning the raw text or running `add_missing_endmark`\n## \n## \n## ====================\n## NO SPACE AFTER COMMA\n## ====================\n## \n## The following observations contain commas with no space afterwards:\n## \n## 6\n## \n## This issue affected the following text:\n## \n## 6: &quot;they&quot; they,were there\n## \n## *Suggestion: Consider running `add_comma_space`\n## \n## \n## =========\n## NON ASCII\n## =========\n## \n## The following observations contain non-ASCII text:\n## \n## 12\n## \n## This issue affected the following text:\n## \n## 12: biÃŸchen ZÃ¼rcher\n## \n## *Suggestion: Consider running `replace_non_ascii`\n## \n## \n## ==================\n## NON SPLIT SENTENCE\n## ==================\n## \n## The following observations contain unsplit sentences (more than one sentence per element):\n## \n## 2, 3\n## \n## This issue affected the following text:\n## \n## 2: &lt;p&gt;i want. &lt;/p&gt;. thet them ther .\n## 3: I am ! that|\n## \n## *Suggestion: Consider running `textshape::split_sentence`\n\n\n4.2.9 Diverse Wortlisten\nTyler Rinker stellt mit dem Paket lexicon eine Zusammenstellung von Wortlisten zu diversen Zwecken zur VerfÃ¼gung. Allerding nur fÃ¼r die englische Sprache.\n\n4.2.10 Sentimentanalyse\n\n4.2.10.1 EinfÃ¼hrung\nEine weitere interessante Analyse ist, die â€œStimmungâ€ oder â€œEmotionenâ€ (Sentiments) eines Textes auszulesen. Die AnfÃ¼hrungszeichen deuten an, dass hier ein MaÃŸ an VerstÃ¤ndnis suggeriert wird, welches nicht (unbedingt) von der Analyse eingehalten wird. Jedenfalls ist das Prinzip der Sentiment-Analyse im einfachsten Fall so:\n\nSchau dir jeden Token aus dem Text an.\n\nPrÃ¼fe, ob sich das Wort im Lexikon der Sentiments wiederfindet.\n\nWenn ja, dann addiere den Sentimentswert dieses Tokens zum bestehenden Sentiments-Wert.\n\nWenn nein, dann gehe weiter zum nÃ¤chsten Wort.\n\nLiefere zum Schluss die Summenwerte pro Sentiment zurÃ¼ck.\n\nEs gibt Sentiment-Lexika, die lediglich einen Punkt fÃ¼r â€œpositive Konnotationâ€ bzw. â€œnegative Konnotationâ€ geben; andere Lexiko weisen differenzierte GefÃ¼hlskonnotationen auf. Wir nutzen hier das deutsche Sentimentlexikon sentiws (Remus, Quasthoff, und Heyer 2010). Sie kÃ¶nnen das Lexikon als CSV hier herunterladen:\n\nsentiws &lt;- read_csv(\"https://osf.io/x89wq/?action=download\")\n\nDen Volltext zum Paper finden Sie z.B. hier.\nAlternativ kÃ¶nnen Sie die Daten aus dem Paket pradadata laden. Allerdings mÃ¼ssen Sie dieses Paket von Github installieren:\n\ninstall.packages(\"devtools\", dep = TRUE)\ndevtools::install_github(\"sebastiansauer/pradadata\")\n\n\ndata(sentiws, package = \"pradadata\")\n\nTabelleÂ 4.1 zeigt einen Ausschnitt aus dem Sentiment-Lexikon SentiWS.\n\n\n\n\n TabelleÂ 4.1:  Auszug aus SentiWS \n  \n\n\n\n\n\n4.2.10.2 Ungewichtete Sentiment-Analyse\nNun kÃ¶nnen wir jedes Token des Textes mit dem Sentiment-Lexikon abgleichen; dabei zÃ¤hlen wir die Treffer fÃ¼r positive bzw. negative Terme. Zuvor mÃ¼ssen wir aber noch die Daten (afd_long) mit dem Sentimentlexikon zusammenfÃ¼hren (joinen). Das geht nach bewÃ¤hrter Manier mit inner_join; â€œinnerâ€ sorgt dabei dafÃ¼r, dass nur Zeilen behalten werden, die in beiden Dataframes vorkommen. Tabelle TabelleÂ 4.2 zeigt Summe, Anzahl und Anteil der Emotionswerte.\nWir nutzen die Tabelle afd_long, die wir oben definiert haben.\n\nafd_long %&gt;% \n  inner_join(sentiws, by = c(\"token\" = \"word\")) %&gt;% \n  select(-inflections) -&gt; afd_senti  # die Spalte brauchen wir nicht\n## Warning in inner_join(., sentiws, by = c(token = \"word\")): Detected an unexpected many-to-many relationship between `x` and `y`.\n## â„¹ Row 9101 of `x` matches multiple rows in `y`.\n## â„¹ Row 3190 of `y` matches multiple rows in `x`.\n## â„¹ If a many-to-many relationship is expected, set `relationship =\n##   \"many-to-many\"` to silence this warning.\n\nafd_senti %&gt;% \n  group_by(neg_pos) %&gt;% \n  summarise(polarity_sum = sum(value),\n            polarity_count = n()) %&gt;% \n  mutate(polarity_prop = (polarity_count / sum(polarity_count)) %&gt;% round(2)) -&gt;\n  afd_senti_tab\n\n\n\n\n\n TabelleÂ 4.2:  Zusammenfassung von SentiWS \n  \n\n\n\n\nDie Analyse zeigt, dass die emotionale Bauart des Textes durchaus interessant ist: Es gibt viel mehr positiv getÃ¶nte WÃ¶rter als negativ getÃ¶nte. Allerdings sind die negativen WÃ¶rter offenbar deutlich stÃ¤rker emotional aufgeladen, denn die Summe an Emotionswert der negativen WÃ¶rter ist (Ã¼berraschenderweise?) deutlich grÃ¶ÃŸer als die der positiven.\nBetrachten wir also die intensivsten negativ und positive konnotierten WÃ¶rter nÃ¤her.\n\nafd_senti %&gt;% \n  distinct(token, .keep_all = TRUE) %&gt;% \n  mutate(value_abs = abs(value)) %&gt;% \n  top_n(20, value_abs) %&gt;% \n  pull(token)\n##  [1] \"ungerecht\"    \"besonders\"    \"gefÃ¤hrlich\"   \"Ã¼berflÃ¼ssig\"  \"behindern\"   \n##  [6] \"gelungen\"     \"brechen\"      \"unzureichend\" \"gemein\"       \"verletzt\"    \n## [11] \"zerstÃ¶ren\"    \"trennen\"      \"falsch\"       \"vermeiden\"    \"zerstÃ¶rt\"    \n## [16] \"schwach\"      \"belasten\"     \"schÃ¤dlich\"    \"tÃ¶ten\"        \"verbieten\"\n\nDiese â€œHitlisteâ€ wird zumeist (19/20) von negativ polarisierten Begriffen aufgefÃ¼llt, wobei â€œbesondersâ€ ein Intensivierwort ist, welches das Bezugswort verstÃ¤rt (â€œbesonders gefÃ¤hrlichâ€). Das Argument keep_all = TRUE sorgt dafÃ¼r, dass alle Spalten zurÃ¼ckgegeben werden, nicht nur die durchsuchte Spalte token. Mit pull haben wir aus dem Dataframe, der von den dplyr-Verben Ã¼bergeben wird, die Spalte pull â€œherausgezogenâ€; hier nur um Platz zu sparen bzw. der Ãœbersichtlichkeit halber.\nNun kÃ¶nnte man noch den erzielten â€œNetto-Sentimentswertâ€ des Corpus ins VerhÃ¤ltnis setzen Sentimentswert des Lexikons: Wenn es insgesamt im Sentiment-Lexikon sehr negativ zuginge, wÃ¤re ein negativer Sentimentwer in einem beliebigen Corpus nicht Ã¼berraschend. describe_distribution aus easystats gibt uns einen Ãœberblick der Ã¼blichen deskriptiven Statistiken.\n\nsentiws %&gt;% \n  select(value, neg_pos) %&gt;% \n  #group_by(neg_pos) %&gt;% \n  describe_distribution()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nMean\nSD\nIQR\nRange\nSkewness\nKurtosis\nn\nn_Missing\n\n\nvalue\n-0.05\n0.20\n0.05\n(-1.00, 1.00)\n-0.68\n2.36\n3468\n0\n\n\n\n\nInsgesamt ist das Lexikon ziemlich ausgewogen; negative Werte sind leicht in der Ãœberzahl im Lexikon. Unser Corpus hat eine Ã¤hnliche mittlere emotionale Konnotation wie das Lexikon:\n\nafd_senti %&gt;% \n  summarise(senti_sum = mean(value) %&gt;% round(2))\n\n\n\n  \n\n\n\n\n4.2.11 Weitere Sentiment-Lexika\nTyler Rinker stellt das Paket sentimentr zur VerfÃ¼gung. Matthew Jockers stellt das Paket Syushet zur VerfÃ¼gung.\n\n4.2.12 Google Trends\nEine weitere MÃ¶glichkeit, â€œWorthÃ¤ufigkeitenâ€ zu identifizieren ist Google Trends. Dieser Post zeigt Ihnen eine EinsatzmÃ¶glichkeit."
  },
  {
    "objectID": "030-textmining1.html#aufgaben",
    "href": "030-textmining1.html#aufgaben",
    "title": "\n4Â  Textmining1\n",
    "section": "\n4.3 Aufgaben",
    "text": "4.3 Aufgaben\n\npurrr-map01\npurrr-map02\npurrr-map03\npurrr-map04\nRegex-Ãœbungen\nAufgaben zum Textmining von Tweets"
  },
  {
    "objectID": "030-textmining1.html#fallstudie-hate-speech",
    "href": "030-textmining1.html#fallstudie-hate-speech",
    "title": "\n4Â  Textmining1\n",
    "section": "\n4.4 Fallstudie Hate-Speech",
    "text": "4.4 Fallstudie Hate-Speech\n\n4.4.1 Daten\nEs finden sich mehrere DatensÃ¤tze zum Thema Hate-Speech im Ã¶ffentlichen Internet, eine Quelle ist Hate Speech Data, ein Repositorium, das mehrere DatensÃ¤tze beinhaltet.\n\nKaggle Hate Speech and Offensive Language Dataset\nBretschneider and Peters Prejudice on Facebook Dataset\nDaten zum Fachartikelâ€Large Scale Crowdsourcing and Characterization of Twitter Abusive Behaviorâ€\n\nFÃ¼r Textmining kann eine Liste mit anstÃ¶ÃŸigen (obszÃ¶nen) WÃ¶rten nÃ¼tzlich sein, auch wenn man solche Dinge ungern anfÃ¤sst, verstÃ¤ndlicherweise. Jenyay bietet solche Listen in verschiedenen Sprachen an. Die Liste von KDNOOBW sieht sehr Ã¤hnlich aus (zumindest die deutsche Version). Eine lange Sammlung deutscher SchimpfwÃ¶rter findet sich im insult.wiki; Ã¤hnlich bei Hyperhero.\nTwitterdaten dÃ¼rfen nur in â€œdehydrierterâ€ Form weitergegeben werden, so dass kein RÃ¼ckschluss von ID zum Inhalt des Tweets mÃ¶glich ist. Daher werden Ã¶ffentlich nur die IDs der Tweets, als einzige Information zum Tweet, also ohne den eigentlichen Inhalt des Tweets, bereitgestellt.\nÃœber die Twitter-API kann man sich, wie oben dargestellt, dann die Tweets wieder â€œrehydrierenâ€, also wieder mit dem zugehÃ¶rigen Tweet-Text (und sonstigen Infos des Tweets) zu versehen.\n\n4.4.2 Grundlegendes Text Mining\nWenden Sie die oben aufgefÃ¼hrten Techniken des grundlegenden Textminings auf einen der oben dargestellten Hate-Speech-DatensÃ¤tze an. Erstellen Sie ein (HTML-Dokument) mit Ihren Ergebnissen. Stellen Sie die Ergebnisse auf dem Github-Repo dieses Kurses ein. Vergleichen Sie Ihre LÃ¶sung mit den LÃ¶sungen der anderen Kursmitglieder.\nWir nutzen noch nicht eigene Daten, die wir von Twitter ausgelesen haben, das heben wir uns fÃ¼r spÃ¤ter auf.\n\n\n\n\nHvitfeldt, Emil, und Julia Silge. 2021. Supervised Machine Learning for Text Analysis in R. 1. Aufl. Boca Raton: Chapman and Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nLex, Alexander, Nils Gehlenborg, Hendrik Strobelt, Romain Vuillemot, und Hanspeter Pfister. 2014. â€UpSet: Visualization of Intersecting Setsâ€œ. IEEE Transactions on Visualization and Computer Graphics 20 (12): 1983â€“92. https://doi.org/10.1109/TVCG.2014.2346248.\n\n\nRemus, Robert, Uwe Quasthoff, und Gerhard Heyer. 2010. â€SentiWS - a Publicly Available German-Language Resource for Sentiment Analysisâ€œ. Proceedings of the 7th International Language Ressources and Evaluation (LRECâ€™10), 1168â€“71.\n\n\nWickham, Hadley, und Garrett Grolemund. 2016. R for Data Science: Visualize, Model, Transform, Tidy, and Import Data. Oâ€™Reilly Media. https://r4ds.had.co.nz/index.html."
  },
  {
    "objectID": "030-textmining1.html#footnotes",
    "href": "030-textmining1.html#footnotes",
    "title": "\n4Â  Textmining1\n",
    "section": "",
    "text": "Gender-iâ†©ï¸"
  },
  {
    "objectID": "040-populismus.html#vorab",
    "href": "040-populismus.html#vorab",
    "title": "\n5Â  Fallstudie Populismus\n",
    "section": "\n5.1 Vorab",
    "text": "5.1 Vorab\n\n5.1.1 Lernziele\n\nDie Fallstudie erklÃ¤ren kÃ¶nnen\n\n5.1.2 Vorbereitung\n\n\nClonen Sie das Projekt-Repositorium oder laden Sie es herunter1.\nArbeiten Sie die Syntax zu dem Projekt durch.\n\n5.1.3 BenÃ¶tigte R-Pakete\nIn dem vorgestellten Projekt werden die folgenden R-Pakete verwendet.\n\nlibrary(tidyverse)\nlibrary(twitteR)\nlibrary(tidytext)\nlibrary(lubridate)\nlibrary(magrittr)\nlibrary(tidytext)\nlibrary(stringr)\nlibrary(viridis)\nlibrary(wordcloud)\nlibrary(SnowballC)\nlibrary(knitr)\nlibrary(testthat)"
  },
  {
    "objectID": "040-populismus.html#wie-populistisch-tweeten-unsere-politikerinnen",
    "href": "040-populismus.html#wie-populistisch-tweeten-unsere-politikerinnen",
    "title": "\n5Â  Fallstudie Populismus\n",
    "section": "\n5.2 Wie populistisch tweeten unsere Politiker:innen?",
    "text": "5.2 Wie populistisch tweeten unsere Politiker:innen?\nVerschaffen Sie sich einen Ãœberblick Ã¼ber dieses Projekt! Im Rahmen dieses Projekts vergleicht der Autor den Populismus von deutschen Politiker:innen, so wie er sich in den Tweets dieser Personen niederschlÃ¤gt. Auf dieser Basis wird ein Populismuswert, bestehend aus mehreren Teilwerten, berechnet und auf Parteiebenen (als Mittel der zugehÃ¶rigen Politiker:innen) berechnet. NatÃ¼rlich fragt man sich, wie Populismus definiert ist und wie diese Definition in den Berechnungen umgesetzt wurde. Finden Sie es selber heraus: Im Github-Repo sind alle Details dokumentiert.\nZum Einstieg hilft ein Ãœberblick Ã¼ber die Ergebnisse der Analyse, die in diesem Vortrag zusammengefasst sind.\nDieser Post stellt die Ergebnisse mit etwas Kontext dar."
  },
  {
    "objectID": "040-populismus.html#footnotes",
    "href": "040-populismus.html#footnotes",
    "title": "\n5Â  Fallstudie Populismus\n",
    "section": "",
    "text": "Hier finden Sie Hinweise, wie man ein Github-Repo clont oder herunterlÃ¤dt.â†©ï¸"
  },
  {
    "objectID": "065-Information.html#lernsteuerung",
    "href": "065-Information.html#lernsteuerung",
    "title": "\n6Â  Informationstheorie\n",
    "section": "\n6.1 Lernsteuerung",
    "text": "6.1 Lernsteuerung\n\n6.1.1 Lernziele\n\nDie grundlegenden Konzepte der Informationstheorie erklÃ¤ren kÃ¶nnen\n\n6.1.2 Vorbereitung\n\nLesen Sie diesen Text als Vorbereitung.\n\n6.1.3 BenÃ¶tigte R-Pakete\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(hcandersenr)   # Textdaten\nlibrary(easystats)  # Komfort fÃ¼r deskriptive Statistiken, wie `describe_distribution`\nlibrary(entropy)  # Entropie berechnen"
  },
  {
    "objectID": "065-Information.html#grundlagen",
    "href": "065-Information.html#grundlagen",
    "title": "\n6Â  Informationstheorie\n",
    "section": "\n6.2 Grundlagen",
    "text": "6.2 Grundlagen\nDie Informationstheorie ist eine der Sternstunden der Wissenschaft. Manche sagen dass Claude Shannon, der Autor der Theorie, auf einer Stufe mit Darwin und Einstein stehen sollte:\n\nIn this single paper, Shannon introduced this new fundamental theory. He raised the right questions, which no one else even thought of asking. This would have been enough to make this contribution earthshaking. But amazingly enough, Shannon also provided most of the right answers with class and elegance. In comparison, it took decades for a dozen of top physicists to define the basics of quantum theory. Meanwhile, Shannon constructed something equivalent, all by himself, in a single paper. Shannonâ€™s theory has since transformed the world like no other ever had, from information technologies to telecommunications, from theoretical physics to economical globalization, from everyday life to philosophy. (â€¦) I donâ€™t think Shannon has had the credits he deserves. He should be right up there, near Darwin and Einstein, among the few greatest scientists mankind has ever had the chance to have.\n\nFÃ¼r die Statistik ist die Informationstheorie von hoher Bedeutung. Im Folgenden schauen wir uns einige Grundlagen an.\n\n6.2.1 Shannon-Information\nMit der Shannon-Information (Information, Selbstinformation) quantifizieren wir, wie viel â€œÃœberraschungâ€ sich in einem Ereignis verbirgt (Shannon 1948).\nEin Ereignis mit â€¦\n\n\ngeringer Wahrscheinlichkeit: Viel Ãœberraschung (Information)\n\nhoher Wahrscheinlichkeit: Wenig Ãœberraschung (Information)\n\nWenn wir also erfahren, dass ein unwahrscheinliches Ereignis eingetreten ist (Schnee im Sommer), sind wir Ã¼berraschter als wenn wir hÃ¶ren, dass ein wahrscheinliches Ereignis eingetreten ist (Schnee im Winter).\nDie Information eines Ereignis ist also eng verbunden mit seiner Wahrscheinlichkeit oder auch mit den Odds eines Ereignisses.\nDie Shannon-Information ist die einzige GrÃ¶ÃŸe, die einige wÃ¼nschenswerte Anforderungen1 erfÃ¼llt:\n\nStetig\nJe mehr Ereignisse in einem Zufallsexperiment mÃ¶glich sind, desto hÃ¶her die Information, wenn ein bestimmtes Ereignis eintritt\nAdditiv: Die Summe der Information zweier Teilereignisse ist gleich der Information des Gesamtereignis\n\n\nDefinition 6.1 (Shannon-Information) Die Information, \\(I\\), ist so definiert:\n\\[I(x) = - \\log_2 \\left( Pr(x) \\right)\\qquad \\square\\]\n\nAndere Logarithmusbasen (als 2) sind mÃ¶glich. Bei einem binÃ¤ren Logarithmus (Basis 2, logarithmus dualis) nennt man die Einheit Bit2.\nEin MÃ¼nwzurf3 hat 1 Bit Information:\n\n-log(1/2, base = 2)\n## [1] 1\n\n\nDefinition 6.2 (Bit) Von 1 Bit Information spricht man, wenn ein Zufallsvorgang zwei AusgÃ¤nge hat und wir indifferent gegenÃ¼ber den AusgÃ¤ngen sind (also beide AusgÃ¤nge fÃ¼r gleich wahrscheinlich halten).\\(\\square\\)\n\nDamit gilt: \\(I = \\log_2\\left( \\frac{1}{Pr(x)} \\right)\\)\nDie Information ist eng verwandt mit den Odds bzw. Log-Odds (Logits):\n\\(\\text{log-odds}(x)=\\log \\left({\\frac {p(x)}{p(\\lnot x)}}\\right)\\)\nLogits kÃ¶nnen als Differenz zweier Shannon-Infos ausgedrÃ¼ckt werden:\n\\(\\text{log-odds}(x)=I(\\lnot x)-I(x)\\)\nDie Information zweier unabhÃ¤ngiger Ereignisse ist additiv.\nDie gemeinsame Wahrscheinlichkeit zweier unabhÃ¤ngiger Ereignisse ist das Produkt der einzelnen Wahrscheinlichkeiten:\n\\(Pr(x,y) = Pr(x) \\cdot Pr(y)\\)\nDie gemeinsame Information ist dann\n\\[\n{\\displaystyle {\\begin{aligned}\\operatorname {I}(x,y)&=-\\log _{2}\\left[p(x,y)\\right]=-\\log _{2}\\left[p(x)p(y)\\right]\\\\[5pt]&=-\\log _{2}\\left[p{(x)}\\right]-\\log _{2}\\left[p{(y)}\\right]\\\\[5pt]&=\\operatorname {I} (x)+\\operatorname {I} (y)\\end{aligned}}}\n\\]\n\nBeispiel 6.1 (Information eines wahrscheinlichen Ereignisses) Â \n\n-log(99/100, base = 2)\n## [1] 0.01449957\n\nDie Information eines fast sicheren Ereignisses ist gering. \\(\\square\\)\n\n\nBeispiel 6.2 (Information eines unwahrscheinlichen Ereignisses) Â \n\n-log(01/100, base = 2)\n## [1] 6.643856\n\nDie Information eines unwahrscheinlichen Ereignisses ist hoch. \\(\\square\\)\n\n\nBeispiel 6.3 (Information eines WÃ¼rfelwurfs) Die Wahrscheinlichkeitsfunktion eines WÃ¼rfel ist\n\\({\\displaystyle Pr(k)={\\begin{cases}{\\frac {1}{6}},&k\\in \\{1,2,3,4,5,6\\}\\\\0,&{\\text{ansonsten}}\\end{cases}}}\\)\nDie Wahrscheinlichkeit, eine 6 zu wÃ¼rfeln, ist \\(Pr(X=6) = \\frac{1}{6}\\).\nDie Information von \\(X=6\\) betrÃ¤gt also\n\\(I(X=6) = -\\log_2 \\left( Pr(X=6) \\right) = -\\log_2(1/6) \\approx 2.585 \\, \\text{bits}\\).\n\n-log(1/6, base = 2)\n## [1] 2.584963\n\n\n\nBeispiel 6.4 (Information zweier WÃ¼rfelwurfe) Die Wahrscheinlichkeit, mit zwei WÃ¼rfeln, \\(X\\) und \\(Y\\), jeweils 6 zu wÃ¼rfeln, betrÃ¤gt \\(Pr(X=6, Y=6) = \\frac{1}{36}\\)\nDie Information betrÃ¤gt also\n\\(I(X=6, Y=6) = -\\log_2 \\left( Pr(6,6) \\right)\\)\n\n-log(1/36, base = 2)\n## [1] 5.169925\n\nAufgrund der AdditivitÃ¤t der Information gilt\n\\(I(6,6) = I(6) + I(6)\\).\n\n-log(1/6, base = 2) + -log(1/6, base = 2)\n## [1] 5.169925\n\n\n\n6.2.2 Entropie\n(Informations)entropie ist die Summe der Informationen der Ereignisse einer Zufallsvariablen, \\(X\\).\n\nDefinition 6.3 (Informationsentropie) Informationsentropie ist so definiert:\n\\[H(p) = - \\text{E log} (p_i) = - \\sum_{i = 1}^n p_i \\text{log} (p_i) = E\\left[I(X) \\right]\\]\n\nDie Informationsentropie ist also die â€œmittlereâ€ oder â€œerwartete Information einer Zufallsvariablen.\nDie Entropie eines MÃ¼nzwurf-Versuchs (Bernoulli-Experiment) ist dann maximal (1 bit), wenn die Trefferwahrscheinlichkeit 50% betrÃ¤gt: \\(Pr(X=x) = 1/2\\), s. Abb. AbbildungÂ 6.1.\n\n\nAbbildungÂ 6.1: Der Zusammenhang von Wahrscheinlichkeit und Entropie bei einem Bernoulli-Versuch, Quelle: Wikipedia, CC-BY-SA, Brona, Rubber Duck\n\n\n6.2.3 Gemeinsame Information\nDie gemeinsame Information (mutual information, MI) zweier Zufallsvariablen \\(X\\) und \\(Y\\), \\(I(X,Y)\\), quantifiziert die Informationsmenge, die man Ã¼ber \\(Y\\) erhÃ¤lt, wenn man \\(X\\) beobachtet. Mit anderen Worten: Die MI ist ein MaÃŸ des Zusammenhangs zweier (nominaler) Variablen. Im Gegensatz zur Korrelation ist die MI nicht auf lineare AbhÃ¤ngigkeiten beschrÃ¤nkt.\nDie MI quantifiziert den Unterschied zwischen der gemeinsamen Verteilung \\(Pr(X,Y)\\) und dem Produkt einer einzelnen4 Wahrscheinlichkeitsverteilungen, d.h. \\(Pr(X)\\) und \\(Pr(Y)\\).\nWenn die beiden Variablen (stochastisch) unabhÃ¤ngig5 sind, ist ihre gemeinsame Information Null:\n\\(I(X,Y) = 0 \\quad \\text{gdw} \\quad \\bot(X,Y)\\).\nDann gilt nÃ¤mlich:\n\\(\\log \\left( \\frac{Pr(X,Y)} {Pr(X) \\cdot Pr(Y)} \\right) =\\log(1) = 0\\).\nDas macht intuitiv Sinn: Sind zwei Variablen unabhÃ¤ngig, so erfÃ¤hrt man nichts Ã¼ber die zweite, wenn man die erste kennt. So ist Kenntnis der Sternzeichens einer Person und ihrer KÃ¶rpergrÃ¶ÃŸe unabhÃ¤ngig.\nDas Gegenteil ist auch wahr: Sind zwei Variablen voneinander komplett abhÃ¤ngig, so weiÃŸ man alles Ã¼ber die zweite, wenn man die erste kennt.\nDie gemeinsame Information kann man sich als Summe der einzelnen gemeinsamen Informationen von \\(XY\\) sehen (s. TabelleÂ 6.1):\n\n\n\n\n TabelleÂ 6.1:  Summe der punktweisen gemeinsamen Informationen \n  \n\n\n\n\n\\(I(X,Y) = \\Sigma_Y \\Sigma_y Pr(x,y) \\underbrace{\\log \\left( \\frac{Pr(X,Y)}{Pr(X) Pr(Y)} \\right)}_\\text{punktweise MI}\\)\nDie Summanden der gemeinsamen Information bezeichnet man auch als punktweise gemeinsame Information (pointwise mutual information, PMI), entsprechend, s. GleichungÂ 6.1. MI ist also der Erwartungswert der PMI.\n\\[{\\displaystyle \\operatorname {PMI} (x,y)\\equiv \\log_{2}{\\frac {p(x,y)}{p(x)p(y)}}=\\log _{2}{\\frac {p(x|y)}{p(x)}}=\\log _{2}{\\frac {p(y|x)}{p(y)}}}\n\\tag{6.1}\\]\nAndere Basen als log2 sind gebrÃ¤uchlich, vor allem der natÃ¼rliche Logarithmus.\n\nAnmerkung. Die zwei rechten Umformungen in GleichungÂ 6.1 basieren auf der Umformung der gemeinsamen Wahrscheinlichkeit.\nZur Erinnerung: \\(p(x,y) = p(y)p(x|y) = p(x)p(y|x)\\)\n\n\nBeispiel 6.5 (Interpretation der PMI) Sei \\(p(x) = p(y) = 1/10\\) und \\(p(x,y) = 1/10\\). WÃ¤ren \\(x\\) und \\(y\\) unabhÃ¤ngig, dann wÃ¤re \\(p^{\\prime}(x,y) = p(x)p(y) = 1/100\\). Das VerhÃ¤ltnis der Produkte der einzelnen Wahrscheinlichkeit zur gemeinsamen Wahrscheinlichkeit wÃ¤re dann 1 und der Logarithmus von 1 ist 0. Das VerhÃ¤ltnis von 1 entspricht also der UnabhÃ¤ngigkeit. Ist das VerhÃ¤ltnis z.B. 5, so zeigt das eine gewisse AbhÃ¤ngigkeit an. Im obigen Beispiel gilt: \\(\\frac{1/20}{1/100}=5\\).\n\nDie MI wird auch Ã¼ber die sog. Kullback-Leibler-Divergenz definiert, die die Differenz zwischen gemeinsamer Wahrscheinlichkeitsfunktion bzw. -dichte und dem Produkt der jeweiligen Randwahrscheinlichkeiten bzw. -dichte angibt.\n\n6.2.4 Maximumentropie\n\nDefinition 6.4 (Maximumentropie) Die Verteilungsform, fÃ¼r die es die meisten MÃ¶glichkeiten (Pfade im Baumdiagramm) gibt, hat die hÃ¶chste Informationsentropie.\n\nAbbildungÂ 6.2 zeigt ein Baumdiagramm fÃ¼r einen 3-fachen MÃ¼nzwurf. In den â€œBlÃ¤tternâ€ (Endknoten) sind die Ergebnisse des Experiments dargestellt sowie die Zufallsvariable \\(X\\), die die Anzahl der â€œTrefferâ€ (Kopf) fasst. Wie man sieht, vereinen einige Werte der Zufallsvariable mehr Pfade auf sich als andere: Der Wert \\(X=1\\) vereinigt 3 Pfade (von 8) auf sich; der Wert \\(X=3\\) nur 1 Pfad.\n\n\nAbbildungÂ 6.2: Pfade im Baumdiagramm: 3-facher MÃ¼nzwurf\n\n\n6.2.4.1 Ilustration\nSagen wir, Sie stehen vor 5 Eimern und haben 10 Kieselsteine bei sich, die mit den Nummern 1 bis 10 beschriftet sind (McElreath 2020). Weil Sie nichts besseres zu tun haben, werfen Sie die Kiesel in die Eimer und zwar so, dass die Wahrscheinlichkeit fÃ¼r einen Kiesel in einen bestimmten Eimer zu landen fÃ¼r alle Eimer gleich ist. Sie werfen also Ihre 10 Kiesel und betrachten das Ergebnis; die Kiesel sind jetzt in einem bestimmten (zufÃ¤lligen) Arrangement auf die Eimer verteilt. Jede Aufteilung (der 10 Kiesel in den 5 Eimern) ist gleich wahrscheinlich6 â€“ die Wahrscheinlichkeit, dass alle 10 Kiesel in Eimer 1 landen ist also gleich hoch wie die Wahrscheinlichkeit, dass jeder Eimer einen Kiesel abkriegt. Jetzt kommtâ€™s: Manche Arrangements kÃ¶nnen auf mehrere Arten erzielt werden als andere. So gibt es nur eine Aufteilung fÃ¼r alle 10 Kiesel in einem Eimer (Teildiagramm a, in AbbildungÂ 6.3). Aber es gibt 90 MÃ¶glichkeiten, die Kiesel so aufzuteilen, dass 2 in Eimer 2 landen, 8 in Eimer 4 und 2 in Eimer 4, s. Teildiagramm b in AbbildungÂ 6.3. Teildiagramme c bis e zeigen, dass die Anzahl der Aufteilungen schnell astronomisch hoch wird, wenn sich die Kiesel â€œgleichmÃ¤ÃŸigerâ€ auf die Eimer verteilen. Die gleichmÃ¤ÃŸigste Aufteilung (Diagramm e) hat die grÃ¶ÃŸte Zahl an mÃ¶glichen Anordnungen. Eine Aufteilung der Kiesel auch als Pfad durch ein Baumdiagramm beschrieben werden.\nHier sind ein paar verschiedene Arrangements, deren Anzahl von Aufteilungen wir hier anschauen:\n\nd &lt;-\n  tibble(a = c(0, 0, 10, 0, 0),\n         b = c(0, 1, 8, 1, 0),\n         c = c(0, 2, 6, 2, 0),\n         d = c(1, 2, 4, 2, 1),\n         e = 2) \nd\n\n\n\n  \n\n\n\n\n\n\n\nAbbildungÂ 6.3: Anzahl von Aufteilungen der 10 Kiesel in 5 Eimer\n\n\n\nHier sind die Wahrscheinlichkeitsverteilungen der 5 Arrangements7:\n\nd %&gt;% \n  mutate_all(~. / sum(.))\n\n\n\n  \n\n\n\nDann kann man die Wahrscheinlichkeit einfach in Entropie umrechnen8:\n\nd %&gt;% \n  mutate_all(~ . / sum(.)) %&gt;% \n  gather() %&gt;% \n  group_by(key) %&gt;% \n  summarise(h = -sum(ifelse(value == 0, 0, value * log(value))))\n\n\n\n  \n\n\n\nDas ifelse dient nur dazu, eine Wahrscheinlichkeit von 0 eine Entropie von 0 zu verpassen9, denn sonst wÃ¼rden wir ein Problem rennen, wenn wir \\(log(0)\\) ausrechnen.\n\nlog(0)\n## [1] -Inf\n\n\n6.2.5 Kreuzentropie\n\nDefinition 6.5 Die Kreuzentropie (cross entropy) ist die Anzahl der benÃ¶tigten Bits, um ein ein Ereignis aus der Verteilung \\(X\\) mit einer anderen Verteilung \\(Y\\) darzustellen, s. ?eq-cr. \\(\\square\\)\n\n\\[H(X,Y) = - \\sum_x X(x) \\cdot log(Y(x)) \\tag{6.2}\\]\nAnschaulich gesprochen gibt die Kreuzentropie die Differenz zwischen zwei Verteilugen an.\n\n6.2.6 Kullback-Leibler-Divergenz\nDie Kullback-Leibler-Divergenz, \\(D_{KL} (X\\, || \\, Y)\\), ist verwandt mit der Kreuzentropie, da\n\\[H(X,Y) = H(X) + D_{KL} (X\\, || \\, Y)\\]"
  },
  {
    "objectID": "065-Information.html#zufallstext-erkennen",
    "href": "065-Information.html#zufallstext-erkennen",
    "title": "\n6Â  Informationstheorie\n",
    "section": "\n6.3 Zufallstext erkennen",
    "text": "6.3 Zufallstext erkennen\n\n6.3.1 Entropie von Zufallstext\nKann man wohl Zufallstext maschinell erkennen? Sicher gibt es viele AnsÃ¤tze, um das Problem anzugehen. Lassen Sie uns einen Ansatz erforschen. Erforschen heiÃŸt, wir erforschen fÃ¼r uns, es handelt sich um eine didaktische Ãœbung, das Ziel ist nicht, Neuland fÃ¼r die Menschheit zu betreten.\nAber zuerst mÃ¼ssen wir Ã¼berlegen, was â€œZufallstextâ€ bedeuten soll.\nNehmen wir uns dazu zuerst einen richtigen Text, ein MÃ¤rchen von H.C. Andersen zum Beispiel. Nehmen wir das Erste aus der Liste in dem Tibble hcandersen_de, â€œdas Feuerzeugâ€.\n\ndas_feuerzeug &lt;-\n  hcandersen_de  %&gt;% \n  filter(book == \"Das Feuerzeug\") %&gt;% \n  unnest_tokens(input = text, output = word) %&gt;% \n  pull(word) \n\nhead(das_feuerzeug)\n## [1] \"es\"         \"kam\"        \"ein\"        \"soldat\"     \"die\"       \n## [6] \"landstraÃŸe\"\n\nDas MÃ¤rchen ist 2688 WÃ¶rter lang.\n\nwortliste &lt;- \nhcandersen_de  %&gt;% \n  filter(book == \"Das Feuerzeug\") %&gt;% \n  unnest_tokens(output = word, input = text) %&gt;% \n  pull(word) %&gt;% \n  unique()\n\nhead(wortliste)\n## [1] \"es\"         \"kam\"        \"ein\"        \"soldat\"     \"die\"       \n## [6] \"landstraÃŸe\"\n\nJetzt ziehen wir Stichproben (mit ZurÃ¼cklegen) aus dieser Liste und erstellen damit eine Art Zufallstext.\n\nzufallstext &lt;- \n  sample(x = wortliste, size = length(das_feuerzeug)*10^2, replace = TRUE)\nhead(zufallstext)\n## [1] \"nÃ¤chsten\" \"hunde\"    \"rundum\"   \"stumpf\"   \"wollte\"   \"sÃ¼nder\"\n\nZÃ¤hlen wir, wie hÃ¤ufig jedes Wort vorkommt:\n\nzufallstext_count &lt;-\ntibble(zufallstext = zufallstext) %&gt;% \n  count(zufallstext)\n\nhead(zufallstext_count)\n\n\n\n  \n\n\n\nDer HÃ¤ufigkeitsvektor von wortliste besteht nur aus Einsen, so haben wir ja gerade die Wortliste definiert:\n\nwortliste_count &lt;-\ntibble(wortliste = wortliste) %&gt;% \n  count(wortliste)\n\nhead(wortliste_count)\n\n\n\n  \n\n\n\nDaher ist ihre Informationsentropy maximal.\n\nentropy(wortliste_count$n, unit = \"log2\")\n## [1] 9.47978\n\nDie HÃ¤ufigkeiten der WÃ¶rter in zufallstext hat eine hohe Entropie.\n\nentropy(zufallstext_count$n, unit = \"log2\")\n## [1] 9.477878\n\nZÃ¤hlen wir die HÃ¤ufigkeiten in der Geschichte â€œDas Feuerzeugâ€.\n\ndas_feuerzeug_count &lt;-\n  tibble(text = das_feuerzeug) %&gt;% \n  count(text)\n\nhead(das_feuerzeug_count)\n\n\n\n  \n\n\n\nUnd berechnen dann die Entropie:\n\nentropy(das_feuerzeug_count$n, unit = \"log2\")\n## [1] 8.075194\n\nDer Zufallstext hat also eine hÃ¶here Entropie als der echte MÃ¤rchentext. Der Zufallstext ist also gleichverteilter in den WorthÃ¤ufigkeiten.\nPro Bit weniger Entropie halbiert sich die Anzahl der MÃ¶glichkeiten einer HÃ¤ufigkeitsverteilung.\n\n6.3.2 MI von Zufallstext\nLeft as an exercises for the reader10 ğŸ¥³."
  },
  {
    "objectID": "065-Information.html#literatur",
    "href": "065-Information.html#literatur",
    "title": "\n6Â  Informationstheorie\n",
    "section": "\n6.4 Literatur",
    "text": "6.4 Literatur\nStone (2019) bietet einen nÃ¼tzlichen Einstieg in das Thema der Informationsentropie. Shannons (1948) berÃ¼hmter Artikel setzt hÃ¶here AnsprÃ¼che.\nEs gibt eine Reihe nÃ¼tzlicher (und recht informationsdichter) Wikipedia-EintrÃ¤ge zum Thema Informationstheorie.\n\nWikipedia: Pointwise Mutual Information\nWikipedia: Mutual information\nWikipedia: Information theory\n\n\n\n\n\nKurz, A. Solomon. 2021. Statistical Rethinking with Brms, Ggplot2, and the Tidyverse: Second Edition. https://bookdown.org/content/4857/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2. Aufl. CRC Texts in Statistical Science. Boca Raton: Taylor and Francis, CRC Press.\n\n\nShannon, C. E. 1948. â€A Mathematical Theory of Communicationâ€œ. Bell System Technical Journal 27 (3): 379â€“423. https://doi.org/10.1002/j.1538-7305.1948.tb01338.x.\n\n\nStone, James V. 2019. â€Information Theory: A Tutorial Introductionâ€œ. 13. Juni 2019. http://arxiv.org/abs/1802.05968."
  },
  {
    "objectID": "065-Information.html#footnotes",
    "href": "065-Information.html#footnotes",
    "title": "\n6Â  Informationstheorie\n",
    "section": "",
    "text": "Desiderata, sagt manâ†©ï¸\noder shannonâ†©ï¸\nwie immer, als fair angenommen, wenn sonst nichts anderes angegeben istâ†©ï¸\nauch als marginalen Wahrscheinlichkeiten oder Randwahrscheinlichkeiten bezeichnetâ†©ï¸\nFÃ¼r stochastische UnabhÃ¤ngigkeit kann das Zeichen \\(\\bot\\) verwendet werdenâ†©ï¸\nso Ã¤hnlich wie mit den Lottozahlenâ†©ï¸\nIst das nicht eine elegante Syntax?! Solomon Kurz, der Autor der Syntax, hielt sich nicht an die aktuelle Version von dplyr; mutate_all ist eigentlich Ã¼berholt zugunsten von mutate mit across, aber die PrÃ¤gnanz der Syntax hier ist schon beeindruckend, wie ich finde.â†©ï¸\nSyntax aus Kurz (2021)â†©ï¸\nRegel von Lâ€™Hopitalâ†©ï¸\nVgl. hierâ†©ï¸"
  },
  {
    "objectID": "050-word-embedding.html#vorab",
    "href": "050-word-embedding.html#vorab",
    "title": "\n7Â  Word Embedding\n",
    "section": "\n7.1 Vorab",
    "text": "7.1 Vorab\n\n7.1.1 Lernziele\n\n\nDie Erstellung von Word-Embeddings anhand grundlegender R-Funktionen erlÃ¤utern kÃ¶nnen.\n\n7.1.2 Vorbereitung\n\n\nArbeiten Sie Hvitfeldt und Silge (2021), Kap. 5 durch.\n\n7.1.3 BenÃ¶tigte R-Pakete\n\nlibrary(tidyverse)\nlibrary(easystats)  # Komfort fÃ¼r deskriptive Statistiken, wie `describe_distribution`\nlibrary(tidytext)\nlibrary(hcandersenr)   # Textdaten\nlibrary(slider)  # slide\nlibrary(widyr)  # pairwise_pmi\nlibrary(furrr)  # mehrere Kerne gleichzeitig nutzen\nlibrary(textdata)  # Worteinbettungen, vorgekocht\nlibrary(entropy)  # Entropie berechnen\nlibrary(widyr)  # Ã„hnlichkeit berechnen mit widyr_svd\nlibrary(furrr)  # Mehrere Kerne gleichzeitig"
  },
  {
    "objectID": "050-word-embedding.html#daten",
    "href": "050-word-embedding.html#daten",
    "title": "\n7Â  Word Embedding\n",
    "section": "\n7.2 Daten",
    "text": "7.2 Daten\n\n7.2.1 Complaints-Datensatz\nDer Datensatz complaints stammt aus dieser Quelle.\nDen Datensatz complaints kann man hier herunterladen. Im Buch ist die Webseite nicht (direkt?) angegeben. Die Datei ist mit gz gepackt; read_csv sollte das automatisch entpacken. Achtung: Die Datei ist recht groÃŸ.\n\nd_path &lt;- \"https://github.com/EmilHvitfeldt/smltar/raw/master/data/complaints.csv.gz\"\ncomplaints &lt;- read_csv(d_path)\n\nGeschickter als jedes Mal die Datei herunterzuladen, ist es, sie einmal herunterzuladen, und dann lokal zu speichern, etwa im Unterordner data des RStudio-Projektordners.\nNach dem Importieren wird der Datensatz in eine Tidy-Form gebracht (mit unnest_tokens) und dann verschachtelt, mit nest.\n\n7.2.2 Complaints verkÃ¼rzt und geschachtelt\nUm ein Herumprobieren zu erleichtern, ist hier der Datensatz complaints in zwei verkÃ¼rzten Formen bereitgestellt:\n\nnested_words2_path &lt;- \"https://github.com/sebastiansauer/datascience-text/raw/main/data/nested_words2.rds\"\nnested_words3_path &lt;- \"https://github.com/sebastiansauer/datascience-text/raw/main/data/nested_words3.rds\"\n\nnested_words2 enthÃ¤lt die ersten 10% des Datensatz nested_wordsund ist gut 4 MB groÃŸ (mit gz gezippt); er besteht aus ca. 11 Tausend Beschwerden. nested_words3 enthÃ¤lt nur die ersten 10 Beschwerden, was ihn gut zum Handhaben macht.\nBeide sind verschachtelt und aus tidy_complaints (s. Kap. 5.1) hervorgegangen.\n\nnested_words3 &lt;- read_rds(nested_words3_path)\n\nDas sieht dann so aus:\n\nnested_words3 %&gt;% \n  head(3)\n\n\n\n  \n\n\n\nWerfen wir einen Blick in den TExt der ersten Beschwerde des Datensatzes mit der ID nested_words3_path$complaint_id[1].\n\nbeschwerde1_text &lt;- nested_words3$words[[1]]\n\nDas ist ein Tibble mit einer Spalte und 17 WÃ¶rtern; da wir schon auf Unigramme aufgeteilt haben, ist jedes Wort ein Element des Vektors word:\n\nbeschwerde1_text %&gt;% \n  head()\n\n\n\n  \n\n\n\n\nbeschwerde1_text$word\n##  [1] \"systems\"    \"inc\"        \"is\"         \"trying\"     \"to\"        \n##  [6] \"collect\"    \"a\"          \"debt\"       \"that\"       \"is\"        \n## [11] \"not\"        \"mine\"       \"not\"        \"owed\"       \"and\"       \n## [16] \"is\"         \"inaccurate\""
  },
  {
    "objectID": "050-word-embedding.html#wordembeddings-selber-erstellen",
    "href": "050-word-embedding.html#wordembeddings-selber-erstellen",
    "title": "\n7Â  Word Embedding\n",
    "section": "\n7.3 Wordembeddings selber erstellen",
    "text": "7.3 Wordembeddings selber erstellen\n\n7.3.1 PMI berechnen\nRufen Sie sich die Definition der PMI ins GedÃ¤chtnis, s. GleichungÂ 6.1.\nMit R kann man die PMI z.B. so berechnen, s. ? pairwise_pmi aus dem Paket widyr.\nZum Paket widyr von Robinson und Silge:\n\nThis package wraps the pattern of un-tidying data into a wide matrix, performing some processing, then turning it back into a tidy form. This is useful for several mathematical operations such as co-occurrence counts, correlations, or clustering that are best done on a wide matrix.\n\nQuelle\nErzeugen wir uns Dummy-Daten:\n\ndat &lt;- tibble(feature = rep(1:5, each = 2),\n              item = c(\"a\", \"b\",\n                       \"a\", \"c\",\n                       \"a\", \"c\",\n                       \"b\", \"e\",\n                       \"b\", \"f\"))\n\ndat\n\n\n\n  \n\n\n\nAus der Hilfe der Funktion:\n\nFind pointwise mutual information of pairs of items in a column, based on a â€œfeatureâ€ column that links them together. This is an example of the spread-operate-retidy pattern.\n\nDie Argumente der Funktion sind:\nitem\nItem to compare; will end up in item1 and item2 columns\nfeature\nColumn describing the feature that links one item to others\nManche Berechnungen (Operationen) lassen sich vielleicht leichter nicht in der Tidy-Form (Langform), sondern in der â€œbreitenâ€ oder Matrixform ausfÃ¼hren. Wandeln wir mal dat von der Langform in die Breitform um:\n\ntable(dat$item, dat$feature)\n##    \n##     1 2 3 4 5\n##   a 1 1 1 0 0\n##   b 1 0 0 1 1\n##   c 0 1 1 0 0\n##   e 0 0 0 1 0\n##   f 0 0 0 0 1\n\nSilge und Robinson verdeutlichen das Prinzip von widyr so, s. AbbildungÂ 7.1.\n\n\nAbbildungÂ 7.1: Die Funktionsweise von widyr, Quelle: Silge und Robinson\n\n(Vgl. auch die ErklÃ¤rung hier.)\nBauen wir das mal von Hand nach.\nRandwahrscheinlichkeiten von a und c sowie deren Produkt, p_a_p_c:\n\np_a &lt;- 3/5\np_c &lt;- 2/5\n\np_a_p_c &lt;- p_a * p_c\np_a_p_c\n## [1] 0.24\n\nGemeinsame Wahrscheinlichkeit von a und c:\n\np_ac &lt;- 2/5\n\nPMI von Hand berechnet:\n\nlog(p_ac/p_a_p_c)\n## [1] 0.5108256\n\nMan beachte, dass hier als Basis \\(e\\), der natÃ¼rliche Logarithmus, verwendet wurde (nicht 2).\nJetzt berechnen wir die PMI mit pairwise_pmi.\n\npairwise_pmi(dat, item = item, feature = feature)\n\n\n\n  \n\n\n\nWie man sieht, entspricht unserer Berechnung von Hand der Berechnung mit pairwise_pmi.\n\n7.3.2 Sliding\nSliding ist ein interessantes Konzept, aber man braucht vielleicht etwas Zeit, um sein Hirn um das Konzept zu wickelnâ€¦\nHier eine Illustration:\n\ntxt_vec &lt;- \"Das ist ein Test, von dem nicht viel zu erwarten ist\"\n\nslider::slide(txt_vec, ~ .x, .before = 2)\n## [[1]]\n## [1] \"Das ist ein Test, von dem nicht viel zu erwarten ist\"\n\nOh, da passiert nichts?! Kaputt? Nein, wir mÃ¼ssen jedes Wort als ein Element des Vektors auffassen.\n\ntxt_df &lt;-\n  tibble(txt = txt_vec) %&gt;% \n  unnest_tokens(input = txt, output = word)\n\nhead(txt_df)\n\n\n\n  \n\n\n\n\nslider::slide(txt_df$word, ~ .x, .before = 2)\n## [[1]]\n## [1] \"das\"\n## \n## [[2]]\n## [1] \"das\" \"ist\"\n## \n## [[3]]\n## [1] \"das\" \"ist\" \"ein\"\n## \n## [[4]]\n## [1] \"ist\"  \"ein\"  \"test\"\n## \n## [[5]]\n## [1] \"ein\"  \"test\" \"von\" \n## \n## [[6]]\n## [1] \"test\" \"von\"  \"dem\" \n## \n## [[7]]\n## [1] \"von\"   \"dem\"   \"nicht\"\n## \n## [[8]]\n## [1] \"dem\"   \"nicht\" \"viel\" \n## \n## [[9]]\n## [1] \"nicht\" \"viel\"  \"zu\"   \n## \n## [[10]]\n## [1] \"viel\"     \"zu\"       \"erwarten\"\n## \n## [[11]]\n## [1] \"zu\"       \"erwarten\" \"ist\"\n\nAh!\nDas Aufteilen in einzelne WÃ¶rter pro Element des Vektors kÃ¶nnte man auch so erreichen:\n\ntxt_vec2 &lt;- str_split(txt_vec, pattern = boundary(\"word\")) %&gt;% \n  simplify() \n\nslide(txt_vec2, .f = ~.x, .before = 2)\n## [[1]]\n## [1] \"Das\"\n## \n## [[2]]\n## [1] \"Das\" \"ist\"\n## \n## [[3]]\n## [1] \"Das\" \"ist\" \"ein\"\n## \n## [[4]]\n## [1] \"ist\"  \"ein\"  \"Test\"\n## \n## [[5]]\n## [1] \"ein\"  \"Test\" \"von\" \n## \n## [[6]]\n## [1] \"Test\" \"von\"  \"dem\" \n## \n## [[7]]\n## [1] \"von\"   \"dem\"   \"nicht\"\n## \n## [[8]]\n## [1] \"dem\"   \"nicht\" \"viel\" \n## \n## [[9]]\n## [1] \"nicht\" \"viel\"  \"zu\"   \n## \n## [[10]]\n## [1] \"viel\"     \"zu\"       \"erwarten\"\n## \n## [[11]]\n## [1] \"zu\"       \"erwarten\" \"ist\"\n\nIn unserem Beispiel mit den Beschwerden:\n\nslide(beschwerde1_text$word,  ~.x, .before = 2)\n## [[1]]\n## [1] \"systems\"\n## \n## [[2]]\n## [1] \"systems\" \"inc\"    \n## \n## [[3]]\n## [1] \"systems\" \"inc\"     \"is\"     \n## \n## [[4]]\n## [1] \"inc\"    \"is\"     \"trying\"\n## \n## [[5]]\n## [1] \"is\"     \"trying\" \"to\"    \n## \n## [[6]]\n## [1] \"trying\"  \"to\"      \"collect\"\n## \n## [[7]]\n## [1] \"to\"      \"collect\" \"a\"      \n## \n## [[8]]\n## [1] \"collect\" \"a\"       \"debt\"   \n## \n## [[9]]\n## [1] \"a\"    \"debt\" \"that\"\n## \n## [[10]]\n## [1] \"debt\" \"that\" \"is\"  \n## \n## [[11]]\n## [1] \"that\" \"is\"   \"not\" \n## \n## [[12]]\n## [1] \"is\"   \"not\"  \"mine\"\n## \n## [[13]]\n## [1] \"not\"  \"mine\" \"not\" \n## \n## [[14]]\n## [1] \"mine\" \"not\"  \"owed\"\n## \n## [[15]]\n## [1] \"not\"  \"owed\" \"and\" \n## \n## [[16]]\n## [1] \"owed\" \"and\"  \"is\"  \n## \n## [[17]]\n## [1] \"and\"        \"is\"         \"inaccurate\"\n\n\n7.3.3 Funktion slide_windows\n\nDie Funktion slide_windows im Kapitel 5.2 ist recht kompliziert. In solchen FÃ¤llen ist es hilfreich, sich jeden Schritt einzeln ausfÃ¼hren zu lassen. Das machen wir jetzt mal.\nHier ist die Syntax der Funktion slide_windows:\n\nslide_windows &lt;- function(tbl, window_size) {\n  skipgrams &lt;- slider::slide(\n    tbl, \n    ~.x,  # Syntax Ã¤hnlich zu purrr::map()\n    .after = window_size - 1, \n    .step = 1, \n    .complete = TRUE\n  )\n  \n  safe_mutate &lt;- safely(mutate)\n  \n  out &lt;- map2(skipgrams,\n              1:length(skipgrams),\n              ~ safe_mutate(.x, window_id = .y))\n  \n  out %&gt;%\n    transpose() %&gt;%\n    pluck(\"result\") %&gt;%\n    compact() %&gt;%\n    bind_rows()\n}\n\nErschwerend kommt eine groÃŸe Datenmenge und eine lange Berechnungszeit dazu, was das Debuggen (Nachvollziehen und Durchdenken) der Schritte zusÃ¤tzlich erschwert. In solchen FÃ¤llen hilft die goldene Regel: Mach es dir so einfach wie mÃ¶glich (aber nicht einfacher). Wir nutzen also den stark verkleinerten Datensatz nested_words3, den wir oben importiert haben.\nZuerst erlauben wir mal, dasss unsere R-Session mehrere Kerne benutzen darf.\n\nplan(multisession)  ## for parallel processing\n\nDie Funktion slide_windows ist recht kompliziert. Es hilft oft, sich mit debug(fun) eine Funktion Schritt fÃ¼r Schritt anzuschauen.\nGehen wir Schritt fÃ¼r Schritt durch die Syntax von slide_windows.\nWerfen wir einen Blick in words, erstes Element (ein Tibble mit einer Spalte). Denn die einzelnen Elemente vonwordswerden an die Funktionslide_windows` als â€œFutterâ€ Ã¼bergeben.\n\nfutter1 &lt;- nested_words3[[\"words\"]][[1]]\nfutter1\n\n\n\n  \n\n\n\nDas ist der Text der ersten Beschwerde.\nOkay, also dann gehtâ€™s los durch die einzelnen Schritte der Funktion slide_windows.\nZunÃ¤chst holen wir uns die â€œFensterâ€ oder â€œSkipgramsâ€:\n\nskipgrams1 &lt;- slider::slide(\n   futter1, \n    ~.x, \n    .after = 3, \n    .step = 1, \n    .complete = TRUE\n  )\n\nBei slide(tbl, ~.x) geben wir die Funktion an, die auf tbl angewendet werden soll. Daher auch die Tilde, die uns von purrr::map() her bekannt ist. In unserem Fall wollen wir nur die Elemente auslesen; Elemente auslesen erreicht man, in dem man sie mit Namen anspricht, in diesem Fall mit dem Platzhalter .x.\nJedes Element von skipgrams1 ist ein 4*1-Tibble und ist ein Skripgram.\n\nskipgrams1 %&gt;% str()\n## List of 17\n##  $ : tibble [4 Ã— 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"systems\" \"inc\" \"is\" \"trying\"\n##  $ : tibble [4 Ã— 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"inc\" \"is\" \"trying\" \"to\"\n##  $ : tibble [4 Ã— 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"is\" \"trying\" \"to\" \"collect\"\n##  $ : tibble [4 Ã— 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"trying\" \"to\" \"collect\" \"a\"\n##  $ : tibble [4 Ã— 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"to\" \"collect\" \"a\" \"debt\"\n##  $ : tibble [4 Ã— 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"collect\" \"a\" \"debt\" \"that\"\n##  $ : tibble [4 Ã— 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"a\" \"debt\" \"that\" \"is\"\n##  $ : tibble [4 Ã— 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"debt\" \"that\" \"is\" \"not\"\n##  $ : tibble [4 Ã— 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"that\" \"is\" \"not\" \"mine\"\n##  $ : tibble [4 Ã— 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"is\" \"not\" \"mine\" \"not\"\n##  $ : tibble [4 Ã— 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"not\" \"mine\" \"not\" \"owed\"\n##  $ : tibble [4 Ã— 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"mine\" \"not\" \"owed\" \"and\"\n##  $ : tibble [4 Ã— 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"not\" \"owed\" \"and\" \"is\"\n##  $ : tibble [4 Ã— 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"owed\" \"and\" \"is\" \"inaccurate\"\n##  $ : NULL\n##  $ : NULL\n##  $ : NULL\n\nDas zweite Skipgram von skipgrams1 enthÃ¤lt, naja, das zweite Skipgram.\n\nskipgrams1[[2]] %&gt;% str()\n## tibble [4 Ã— 1] (S3: tbl_df/tbl/data.frame)\n##  $ word: chr [1:4] \"inc\" \"is\" \"trying\" \"to\"\n\nUnd so weiter.\nOkay, weiter im Programm. Jetzt mappen wir das Erstellen der Skipgrams\n\nsafe_mutate &lt;- safely(mutate)\n  \nout1 &lt;- map2(skipgrams1,\n             1:length(skipgrams1),\n             ~ safe_mutate(.x, window_id = .y))\n  \nout1 %&gt;% \n  head(2) %&gt;% \n  str()\n## List of 2\n##  $ :List of 2\n##   ..$ result: tibble [4 Ã— 2] (S3: tbl_df/tbl/data.frame)\n##   .. ..$ word     : chr [1:4] \"systems\" \"inc\" \"is\" \"trying\"\n##   .. ..$ window_id: int [1:4] 1 1 1 1\n##   ..$ error : NULL\n##  $ :List of 2\n##   ..$ result: tibble [4 Ã— 2] (S3: tbl_df/tbl/data.frame)\n##   .. ..$ word     : chr [1:4] \"inc\" \"is\" \"trying\" \"to\"\n##   .. ..$ window_id: int [1:4] 2 2 2 2\n##   ..$ error : NULL\n\nout1 ist eine Liste mit 17 Elementen; jedes Element mit jeweils zwei Elementen: Den Ergebnissen und ob es einen Fehler gab bei safe_mutate. Die 10 Elemente entsprechen den 10 Skipgrams. Wir kÃ¶nnen aber out1 auch â€œdrehenâ€, transponieren genauer gesagt. so dass wir eine Liste mit zwei Elementen bekommen: das erste Element hat die (zehn) Ergebnisse (nÃ¤mlich die Skipgrams) und das zweite Elemente, ob es Fehler gab.\nDas Prinzip des Transponierens ist in AbbildungÂ 7.2 dargestellt.\n\n\nAbbildungÂ 7.2: Transponieren einer Matrix (â€œTabelleâ€)\n\n\nout2 &lt;-\nout1 %&gt;%\n  transpose() \n\nPuh, das ist schon anstrengendes Datenyogaâ€¦\nAber jetzt ist es einfach. Wir ziehen das erste der beiden Elemente, die Ergebnisse heraus (pluck), entfernen leere Elemente (compact) und machen einen Tibble daraus (bind_rows):\n\nout2 %&gt;% \n  pluck(\"result\") %&gt;%\n  compact() %&gt;%\n  bind_rows() %&gt;% \n  head()\n\n\n\n  \n\n\n\nGeschafft!\n\n7.3.4 Ã„hnlichkeit berechnen\nNachdem wir jetzt slide_windows kennen, schauen wir uns die nÃ¤chsten Schritte an:\n\ntidy_pmi1 &lt;- nested_words3 %&gt;%  # &lt;--- Kleiner Datensatz!\n  mutate(words = future_map(words, slide_windows, 4L))\n\nWir werden slide_windows auf die Liste words an, die die Beschwerden enthÃ¤lt. FÃ¼r jede Beschwerde erstellen wir die Skipgrams; diese Schleife wird realisiert Ã¼ber map bzw. future_map, die uns erlaubt, mehrere Kerne des Rechners gleichzeitig zu nutzen, damit es schneller geht.\nHier sehen wir z.B. die Skipgram-IDs der ersten Beschwerde.\n\ntidy_pmi1[[\"words\"]][[1]] %&gt;% \n  head()\n\n\n\n  \n\n\n\nGenestet siehst es so aus:\n\ntidy_pmi1 %&gt;% \n  head(1)\n\n\n\n  \n\n\n\nDie Listenspalte entschachteln wir mal:\n\ntidy_pmi2 &lt;- tidy_pmi1 %&gt;% \n  unnest(words)  # entschachtele\n\ntidy_pmi2 %&gt;% \n  head()\n\n\n\n  \n\n\n\nZum Berechnen der Ã„hnlichkeit brauchen wir eineindeutige IDs, nach dem Prinzip â€œ1. Skipgram der 1. Beschwerdeâ€ etc:\n\ntidy_pmi3 &lt;- tidy_pmi2 %&gt;% \n  unite(window_id, complaint_id, window_id)  # fÃ¼hre Spalten zusammen\n\ntidy_pmi3 %&gt;% \n  head()\n\n\n\n  \n\n\n\nSchlieÃŸlich berechnen wir die Ã„hnlichkeit mit pairwise_pmi, das hatten wir uns oben schon mal nÃ¤her angeschaut:\n\ntidy_pmi4 &lt;- tidy_pmi3 %&gt;% \n  pairwise_pmi(word, window_id)  # berechne Ã„hnlichkeit\n\ntidy_pmi &lt;- tidy_pmi4  # mit dem Objekt arbeiten wir dann weiter\n\ntidy_pmi %&gt;% \n  head()\n\n\n\n  \n\n\n\n\n7.3.5 SVD\nDie SingulÃ¤rwertzerlegung (singular value decomposition, SVD) ist ein Verfahren zur Verringerung der Spaltenzahl (Dimension), vergleichbar zur Faktorenanalyse. Zur Anschaulichkeit - und ohne substanziellen Hintergrund - sei folgendes Beispiel genannt: Die Verben â€œgehenâ€, â€œrennenâ€, â€œlaufenâ€, â€œschwimmenâ€, â€œfahrenâ€, â€œrutschenâ€ kÃ¶nnten zu einer gemeinsamen Dimension, etwa â€œfortbewegenâ€ reduziert werden. Jedes einzelne der eingehenden Verben erhÃ¤lt eine Zahl von 0 bis 1, das die konzeptionelle NÃ¤he des Verbs zur â€œdahinterliegendenâ€ Dimension (fortbewegen) quantifiziert; die Zahl nennt man auch die â€œLadungâ€ des Items (Worts) auf die Dimension. Sagen wir, wir identifizieren 10 Dimensionen. Man erhÃ¤lt dann fÃ¼r jedes unique Wort im Corpus einen Vektor mit den Ladungen auf die Dimensionen. Im genannten Beispiel wÃ¤re es ein 10-stelliger Vektor. So wie ein 3-stelliger Vektor die Position einer Biene im Raum beschreibt1, beschreibt hier unser 10-stelliger Vektor die â€œPositionâ€ eines Worts in unserem Einbettungsvektor.\nDie Syntax dazu ist dieses Mal einfach:\n\ntidy_word_vectors &lt;- \n  tidy_pmi %&gt;%\n  widely_svd(\n    item1, item2, pmi,\n    nv = 100\n  )\n\ntidy_word_vectors %&gt;% \n  (head)\n\n\n\n  \n\n\n\nMit nv = 100 haben wir die Anzahl (n) der Dimensionen (Variablen, v) auf 100 bestimmt.\n\n7.3.6 WortÃ¤hnlichkeit\nJetzt, da wir mit der SVD jedes Wort in einem Koordinatensystem verortet haben, kÃ¶nnen wir die AbstÃ¤nde der WÃ¶rter im Koordinatensystem bestimmen. Das geht mit Hilfe des alten Pythagoras, s. AbbildungÂ 7.3. Der Abstand, den man mit Hilfe des Satz des Pythagoras berechnet, nennt man auch euklidische Distanz.\n\n\nAbbildungÂ 7.3: Euklidische Distanz in 2D, Quelle: Wikipedia, CC BY4.0, Kmhkmh\n\nOkay, wir sind in einem Raum mit vielen Dimensionen, was mein Hirn nicht mitmacht, aber der Algebra ist das egal. Pythagorasâ€™ Satz lÃ¤sst sich genauso anwenden, wenn es mehr als Dimensionen sind.\nDie Autoren basteln sich selber eine Funktion in Kap. 5.3, aber der Einfachheit halber nehme ich (erstmal) die entsprechende Funktion aus widyr:\n\nword_neighbors &lt;- \ntidy_word_vectors %&gt;% \n  pairwise_dist(item1, dimension, value)\n\nword_neighbors %&gt;% \n  head()\n\n\n\n  \n\n\n\nSchauen wir uns ein Beispiel an. Was sind die Nachbarn von â€œinaccurateâ€?\n\nword_neighbors %&gt;% \n  filter(item1 == \"inaccurate\") %&gt;% \n  arrange(distance) %&gt;% \n  head()\n\n\n\n  \n\n\n\nHier ist die Datenmenge zu klein, um vernÃ¼nftige SchlÃ¼sse zu ziehen. Aber â€œincorrectlyâ€, â€œcorrectâ€, â€œbalanceâ€ sind wohl plausible Nachbarn von â€œinaccurateâ€.\n\n7.3.7 Cosinus-Ã„hnlichkeit\nDie NÃ¤he zweier Vektoren lÃ¤sst sich, neben der euklidischen Distanz, auch z.B. Ã¼ber die Cosinus-Ã„hnlichkeit (Cosine similarity) berechnen, vgl. auch AbbildungÂ 7.4:\n\n\nAbbildungÂ 7.4: Die Cosinus-Ã„hnlichkeit zweier Vektoren\n\nQuelle: Mazin07, Lizenz: PD\n\\[{\\displaystyle {\\text{Cosinus-Ã„hnlichkeit}}=S_{C}(A,B):=\\cos(\\theta )={\\mathbf {A} \\cdot \\mathbf {B}  \\over \\|\\mathbf {A} \\|\\|\\mathbf {B} \\|}={\\frac {\\sum \\limits _{i=1}^{n}{A_{i}B_{i}}}{{\\sqrt {\\sum \\limits _{i=1}^{n}{A_{i}^{2}}}}{\\sqrt {\\sum \\limits _{i=1}^{n}{B_{i}^{2}}}}}},}\\]\nwobei \\(A\\) und \\(B\\) zwei Vektoren sind und \\(\\|\\mathbf {A} \\|\\) das Skalarprodukt von A (und B genauso). Das Skalarprodukt von \\(\\color {red} {a = {\\displaystyle [a_{1},a_{2},\\cdots ,a_{n}]}}\\) und \\(\\color {blue} {b = {\\displaystyle [b_{1},b_{2},\\cdots ,b_{n}]}}\\) ist so definiert:\n\\[{\\displaystyle \\mathbf {\\color {red}a} \\cdot \\mathbf {\\color {blue}b} =\\sum _{i=1}^{n}{\\color {red}a}_{i}{\\color {blue}b}_{i}={\\color {red}a}_{1}{\\color {blue}b}_{1}+{\\color {red}a}_{2}{\\color {blue}b}_{2}+\\cdots +{\\color {red}a}_{n}{\\color {blue}b}_{n}}\\]\nEntsprechend ist die Funktion nearest_neighbors zu verstehen aus Kap. 5.3:\n\nnearest_neighbors &lt;- function(df, token) {\n  df %&gt;%\n    widely(\n      ~ {\n        y &lt;- .[rep(token, nrow(.)), ]\n        res &lt;- rowSums(. * y) / \n          (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))\n        \n        matrix(res, ncol = 1, dimnames = list(x = names(res)))\n      },\n      sort = TRUE\n    )(item1, dimension, value) %&gt;%\n    select(-item2)\n}\n\nWobei mit widely zuerst noch von der Langform in die Breitform umformatiert wird, da die Breitform von der Formel verlangt wird bzw. das Rechnen einfacher macht.\nDer eine Vektor ist das Embedding des Tokens, der andere Vektor ist das mittlere Embedding Ã¼ber alle Tokens des Corpus. Wenn die Anzahl der Elemente konstant bleibt, kann man sich das Teilen durch \\(n\\) schenken, wenn man einen Mittelwert berechnen; so hÃ¤lt es auch die Syntax von nearest_neighbors.\nEin nÃ¼tzlicher Post zur Cosinus-Ã„hnlichkeit findet sich hier. Dieses Bild zeigt das Konzept der Cosinus-Ã„hnlichkeit anschaulich.\nZur Erinnerung: Der Cosinus eines Winkels ist definiert als VerhÃ¤ltnis der LÃ¤nge der Ankathete (das ist jene Kathete, die einen Schenkel des Winkels bildet) zur LÃ¤nge der Hypotenuse2 in einem rechtwinkligen, vgl. AbbildungÂ 7.5.\n\n\nAbbildungÂ 7.5: Ein rechtwinkliges Dreieck als Grundlage der trigonometrischen Funktionen\n\nAlso: \\({\\displaystyle \\cos \\alpha ={\\frac {b}{c}}}\\)\nQuelle: PaterSigmund, Wikipedia,dre CC-BY-SA 2.5\nHilfreich ist auch die Visualisierung von Sinus und Cosinus am Einheitskreis; gerne animiert betrachten."
  },
  {
    "objectID": "050-word-embedding.html#word-embeddings-vorgekocht",
    "href": "050-word-embedding.html#word-embeddings-vorgekocht",
    "title": "\n7Â  Word Embedding\n",
    "section": "\n7.4 Word-Embeddings vorgekocht",
    "text": "7.4 Word-Embeddings vorgekocht\n\n7.4.1 Glove6B\nIn Kap. 5.4 schreiben die Autoren:\n\nIf your data set is too small, you typically cannot train reliable word embeddings.\n\nEin paar Millionen WÃ¶rter sollte der Corpus schon enthalten, so die Autoren. Da solche â€œWorteinbettungenâ€ (word embedings) aufwÃ¤ndig zu erstellen sind, kann man fertige, â€œvorgekochteâ€ Produkte nutzen.\nGlove6B wurde anhand von Wikipedia und anderen Datenquellen erstellt (Pennington, Socher, und Manning 2014).\n\n\n\n\n\n\nHinweis\n\n\n\nDie zugehÃ¶rigen Daten sind recht groÃŸ; fÃ¼r glove6b (Pennington, Socher, und Manning 2014) ist fast ein Gigabyte fÃ¤llig. Sie sollten sich die Daten in einem ruhigen Moment (mit stabiler Internetverbindung) herunterladen und in einem Verzeichnis meiner Wahl abgespeichert (datasets). Da bei mir Download abbrach, als ich embedding_glove6b(dimensions = 100) aufrief, habe ich die Daten manuell heruntergeladen, s.u.\n\n\n\nglove6b &lt;- \n  embedding_glove6b(dir = \"~/datasets\", dimensions = 50, manual_download = TRUE)\n\n\nglove6b &lt;- read_rds(\"/Users/sebastiansaueruser/datasets/glove6b/glove_6b_50.rds\")\n\n\nglove6b %&gt;% \n  select(1:5) %&gt;% \n  head()\n\n\n\n  \n\n\n\nDie ersten paar Tokens sind:\n\nglove6b$token %&gt;% head(20)\n##  [1] \"the\"  \",\"    \".\"    \"of\"   \"to\"   \"and\"  \"in\"   \"a\"    \"\\\"\"   \"'s\"  \n## [11] \"for\"  \"-\"    \"that\" \"on\"   \"is\"   \"was\"  \"said\" \"with\" \"he\"   \"as\"\n\nIn eine Tidyform bringen:\n\ntidy_glove &lt;- \n  glove6b %&gt;%\n  pivot_longer(contains(\"d\"),\n               names_to = \"dimension\") %&gt;%\n  rename(item1 = token)\n\nhead(tidy_glove)\n\n\n\n  \n\n\n\nGanz schÃ¶n groÃŸ:\n\ndim(glove6b)\n## [1] 400000     51\n\n\nobject.size(tidy_glove)\n## 503834736 bytes\n\nIn Megabyte3\n\nobject.size(tidy_glove) / 2^20\n## 480.5 bytes\n\nEinfacher und genauer geht es so:\n\npryr::object_size(tidy_glove)\n## 503.83 MB\n\n\npryr::mem_used()\n## 841 MB\n\nUm Speicher zu sparen, kÃ¶nnte man glove6b wieder direkt lÃ¶schen, wenn man nur mit der Tidyform weiterarbeitet.\n\nrm(glove6b)\n\nJetzt kÃ¶nnen wir wieder nach Nachbarn fragen, im euklidischen Sinne, wie oben. Probieren wir aus, welche WÃ¶rter nah zu â€œinaccurateâ€ stehen.\n\n\n\n\n\n\nHinweis\n\n\n\nWie wir oben gesehen haben, ist der Datensatz riesig4, was die Berechnungen (zeitaufwÃ¤ndig) und damit nervig machen kÃ¶nnen. DarÃ¼ber hinaus kann es nÃ¶tig sein, dass Sie mehr Speicher auf Ihrem Computer zur VerfÃ¼gung stellen mÃ¼ssen5. Wir mÃ¼ssen noch maximum_size = NULL, um das Jonglieren mit riesigen Matrixen zu erlauben. MÃ¶ge der Gott der RAMs und Arbeitsspeicher uns gnÃ¤dig sein!\n\n\nMit pairwise_dist dauert die Berechnung sehr lange und braucht wohl sehr viel Speicher. Mitunter kam folgender Fehler auf: â€œR error: vector memory exhausted (limit reached?)â€.\n\nword_neighbors_glove6b &lt;- \ntidy_glove %&gt;% \n  slice_head(prop = .1) %&gt;% \n  pairwise_dist(item1, dimension, value, maximum_size = NULL)\n\nhead(word_neighbors_glove6b)\n\ntidy_glove %&gt;% \n  filter(item1 == \"inaccurate\") %&gt;% \n  arrange(-value) %&gt;% \n  slice_head(n = 5)\n\nDeswegen probieren wir doch die Funktion nearest_neighbors, so wie es im Buch vorgeschlagen wird, s. Kap 5.3.\n\nnearest_neighbors &lt;- function(df, token) {\n  df %&gt;%\n    widely(\n      ~ {\n        y &lt;- .[rep(token, nrow(.)), ]\n        res &lt;- rowSums(. * y) / \n          (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))\n        matrix(res, ncol = 1, dimnames = list(x = names(res)))\n      },\n      sort = TRUE,\n      maximum_size = NULL\n    )(item1, dimension, value) %&gt;%\n    select(-item2)\n}\n\n\ntidy_glove %&gt;%\n  # slice_head(prob = .1) %&gt;% \n  nearest_neighbors(\"error\") %&gt;% \n  head()\n\n\n\n  \n\n\n\nEntschachteln wir unsere Daten zu complaints:\n\ntidy_complaints3 &lt;-\n  nested_words3 %&gt;% \n  unnest(words)\n\nDann erstellen wir uns eine Tabelle, in der nur die Schnittmenge der WÃ¶rter aus den Beschwerden und Glove vorkommen. Dazu nutzen winr einen inneren Join\n\n\nInner Join, Quelle: Garrick Adenbuie\n\nQuelle\n\ncomplaints_glove &lt;- \ntidy_complaints3 %&gt;% \n  inner_join(by = \"word\", \n  tidy_glove %&gt;% \n  distinct(item1) %&gt;% \n  rename(word = item1)) \n\nhead(complaints_glove)\n\n\n\n  \n\n\n\nWie viele unique (distinkte) WÃ¶rter gibt es in unserem Corpus?\n\ntidy_complaints3_distinct_words_n &lt;- \ntidy_complaints3 %&gt;% \n  distinct(word) %&gt;% \n  nrow()\n\ntidy_complaints3_distinct_words_n\n## [1] 222\n\nIn tidy_complaints gibt es Ã¼brigens 222 verschiedene WÃ¶rter.\n\nword_matrix &lt;- tidy_complaints3 %&gt;%\n  inner_join(by = \"word\",\n             tidy_glove %&gt;%\n               distinct(item1) %&gt;%\n               rename(word = item1)) %&gt;%\n  count(complaint_id, word) %&gt;%\n  cast_sparse(complaint_id, word, n)\n\n#word_matrix\n\nword_matrix zÃ¤hlt fÃ¼r jede der 10 Beschwerden, welche WÃ¶rter (und wie hÃ¤ufig) vorkommen.\n\ndim(word_matrix)\n## [1]  10 222\n\n10 Beschwerden (Dokumente) und 222 unique WÃ¶rter.\n\nglove_matrix &lt;- tidy_glove %&gt;%\n  inner_join(by = \"item1\",\n             tidy_complaints3 %&gt;%\n               distinct(word) %&gt;%\n               rename(item1 = word)) %&gt;%\n  cast_sparse(item1, dimension, value)\n\n#glove_matrix\n\nglove_matrix gibt fÃ¼r jedes unique Wort den Einbettungsvektor an.\n\ndim(glove_matrix)\n## [1] 222  50\n\nDas sind 222 unique WÃ¶rter und 50 Dimensionen des Einbettungsvektors.\nJetzt kÃ¶nnen wir noch pro Dokument (10 in diesem Beispiel) die mittlere â€œPositionâ€ jedes Dokuments im Einbettungsvektor ausrechnen. Bildlich gesprochen: Was ist der mittlere Raumpunkt (Zentroid) des Bienenschwarms, wobei eine Biene ein Dokument darstellt und die Raumachsen die Dimensionen des Einbettungsvektors.\nDazu gewichten wir jedes Wort eines Dokuments mit den Ladungen des Einbettungsvektor und summieren diese Terme. Es resultiert eine Matrix mit einem Einbettungsvektor pro Dokument. Diese Matrix kÃ¶nnen wir jetzt als PrÃ¤diktorenmatrix hernehmen.\n\ndoc_matrix &lt;- word_matrix %*% glove_matrix\n#doc_matrix %&gt;% head()\n\n\ndim(doc_matrix)\n## [1] 10 50\n\nDie Anzahl der Dokumente ist 10 und die Anzahl der Dimensionen (des Einbettungsvektors) ist 50.\n\n7.4.2 Wordembeddings fÃ¼r die deutsche Sprache\nIn diesem Github-Projekt finden sich die Materialien fÃ¼r ein deutsches Wordembedding (mueller2015?)."
  },
  {
    "objectID": "050-word-embedding.html#fazit",
    "href": "050-word-embedding.html#fazit",
    "title": "\n7Â  Word Embedding\n",
    "section": "\n7.5 Fazit",
    "text": "7.5 Fazit\nWorteinbettungen sind eine aufwÃ¤ndige Angelegenheit. Positiv gesprochen kann ein Analysti die Muskeln spielen lassen und zeigen, was sie oder er so alles drauf hat. Ist ja schon cooles Zeugs, die Word Embeddings. Es besteht die Chance, dass man mit dieser Methode bessere Vorhersagen erreicht, als mit anderen, einfachen AnsÃ¤tzen wir WorthÃ¤ufigkeiten oder tf-idf. Auf der anderen Seite ist es oft sinnvoll, mit einfachen AnsÃ¤tzen zu starten, und zu sehen, wie weit man kommt. Vielleicht ja weit genug."
  },
  {
    "objectID": "050-word-embedding.html#literatur",
    "href": "050-word-embedding.html#literatur",
    "title": "\n7Â  Word Embedding\n",
    "section": "\n7.6 Literatur",
    "text": "7.6 Literatur\n\n7.6.1 Wikipedia\nEs gibt eine Reihe nÃ¼tzlicher (und recht informationsdichter) Wikipedia-EintrÃ¤ge zum Thema Informationstheorie.\n\nWikipedia: Pointwise Mutual Information\nWikipedia: Mutual information\nWikipedia: Information theory\n\n\n\n\n\nHvitfeldt, Emil, und Julia Silge. 2021. Supervised Machine Learning for Text Analysis in R. 1. Aufl. Boca Raton: Chapman and Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nPennington, Jeffrey, Richard Socher, und Christopher Manning. 2014. â€GloVe: Global Vectors for Word Representationâ€œ. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1532â€“43. Doha, Qatar: Association for Computational Linguistics. https://doi.org/10.3115/v1/D14-1162."
  },
  {
    "objectID": "050-word-embedding.html#footnotes",
    "href": "050-word-embedding.html#footnotes",
    "title": "\n7Â  Word Embedding\n",
    "section": "",
    "text": "Man kÃ¶nnte ergÃ¤nzen: plus eine 4. Dimension fÃ¼r Zeit, plus noch ein paar Weitere fÃ¼r die Beschleunigung in verschiedene Richtungenâ€¦â†©ï¸\nQuelle: https://de.wikipedia.org/wiki/Sinus_und_Kosinusâ†©ï¸\n\\(1024 \\cdot 1024\\) Byte, und \\(1024 =2^{10}\\), daher \\(2^{10} \\cdot 2^{10} = 2^{20}\\)â†©ï¸\nzugegeben, ein subjektiver Ausdruckâ†©ï¸\nKaufenâ€¦â†©ï¸"
  },
  {
    "objectID": "060-hassrede.html#vorab",
    "href": "060-hassrede.html#vorab",
    "title": "\n8Â  Hassrede\n",
    "section": "\n8.1 Vorab",
    "text": "8.1 Vorab\n\n8.1.1 Lernziele\n\nFinden Sie eine operationale Definition fÃ¼r Hassrede (engl. hate speech) bzw. Hatespeech1!\n\n8.1.2 Vorbereitung\n\nLesen Sie die unten aufgefÃ¼hrte Literatur\n\n8.1.3 BenÃ¶tigte R-Pakete\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "060-hassrede.html#worum-gehts",
    "href": "060-hassrede.html#worum-gehts",
    "title": "\n8Â  Hassrede\n",
    "section": "\n8.2 Worum gehtâ€™s?",
    "text": "8.2 Worum gehtâ€™s?\nWir mÃ¶chten eine treffende und praktikable Definition, um zu erkennen, wann eis deutschis Politiki Hass entgegenschlÃ¤gt.\nTreffend meint, dass Hassrede als Hassrede erkannt wird von unserer Definition, und Nicht-Hassrede als Nicht-Hassrede erkannt wird. Mit anderen Worten: Wir verlangen, dass die SensitivitÃ¤t und SpezifitÃ¤t unserer Definition hoch ist.\nPraktikabel meint, dass wir diese Definition in der Praxis gut umsetzen kÃ¶nnen. Wir denken dabei an die Schwierigkeiten, einer (tumben) Maschine unsere Regeln beizubringen. Insbesondere muss die Definition objektiv sein in dem Sinne, dass mehrere Gutachtis zur gleichen EinschÃ¤tzung kommen wÃ¼rden."
  },
  {
    "objectID": "060-hassrede.html#einstieg",
    "href": "060-hassrede.html#einstieg",
    "title": "\n8Â  Hassrede\n",
    "section": "\n8.3 Einstieg",
    "text": "8.3 Einstieg\n\n8.3.1 Einstiegsdefinition\nHier ist eine Definition als Startpunkt fÃ¼r Ihre Ãœberlegungen.\nHassrede liegt vor, wenn eine oder mehrere der folgenden Inhalte in einem Text verwendet werden:\n\nSchimpfwÃ¶rter (â€œVollpfostenâ€)\nRassismus, Sexismus, Antisemitismus oder andere Formen von gruppenbezogener Menschenfeindlichkeit (â€œDer Schwarze schnackselt gerneâ€)\nAufruf oder Androhung zur Gewalt, auch in indirekter Form (â€œDa kÃ¶nnte mal jemand mit der Pistole bei dir vorbeikommenâ€)\nHerabsetzung (â€œVolksverrÃ¤terâ€)\n\nDabei sollten wir uns mit Blick auf das Ziel, Hass gegen einzelne Personen zu erkennen, nicht auf gruppenbezogene Menschenfeindlichkeit begrenzen, sondern auch Hass auf Individuen einbeziehen. Vielleicht ist daher der Begriff Cybermobbing passender als Hatespeech.\n\n8.3.2 Einstiegsliteratur\nDer Artikel zu Hatespeech der Stanford-EnzyklopÃ¤die birgt (am Anfang) gute Hinweise; im weiteren Verlauf geht der Text mehr in die Tiefe.\nIn dieser Zotero-Gruppe finden Sie empfehlenswerte (und Ã¶ffentlich zugÃ¤ngliche) Artikel zum Thema Hatespeech und Hate-Speech-Erkennung.\n\n8.3.3 Trainingsdaten\nDie UniversitÃ¤t Heidelberg verÃ¶ffentlicht Daten, die Tweets (oder Ã¤hnliche Kurztexte) nach Hatespeech hin untersucht (wiegand_germeval-2018_2019?). Nutzen Sie dieser Ressource.\n\n8.3.4 Los gehtâ€™s!\nLesen Sie diese und weitere Literatur, um zu einer Arbeitsdefinition von Hassrede zu kommen."
  },
  {
    "objectID": "060-hassrede.html#footnotes",
    "href": "060-hassrede.html#footnotes",
    "title": "\n8Â  Hassrede\n",
    "section": "",
    "text": "Zur korrekten deutschen Schreibweise vgl. Dudenâ†©ï¸"
  },
  {
    "objectID": "067-miniprojekt1.html#aufgabe",
    "href": "067-miniprojekt1.html#aufgabe",
    "title": "9Â  Miniprojekt",
    "section": "9.1 Aufgabe",
    "text": "9.1 Aufgabe\nFÃ¼hren Sie eine deskriptive Textanalyse durch. Verwenden Sie einen Text (ca. 10-30k WÃ¶rter) Ihrer Wahl, von dem Sie einen engen Fokus auf bestimmte Themen sowie eine gewisse EmotionalitÃ¤t erwarten. Wenden Sie dann die Methoden an, die im Teil Textmining dieses Buch vorgestellt sind. ZusÃ¤tzlich es es empfehlenswert, auf typische Methoden der explorativen Datenanalyse zurÃ¼ckzugreifen inklusive der Datenvisualisierung.\nTypische Arbeitsschritte dabei sind:\n\nImportieren und Aufbereiten der Daten\nZÃ¤hlen hÃ¤ufiger Worte und n-Gramme\nEntfernen von StopwÃ¶rtern\nBerechnen von SentimentstÃ¤rken\nAssoziationsanalyse von WÃ¶rtern und n-Grammen\nBerechnung von Wortdistanzen\nWorteinbettungen\nDimensionsreduktion von Worteinbettungen\nClusteranalysen von WÃ¶rtern anhand ihrer Einbettungen\nThemenanalyse anhand einer Latenten-Dirichlet-Analyse"
  },
  {
    "objectID": "080-klassifikation.html#vorab",
    "href": "080-klassifikation.html#vorab",
    "title": "\n10Â  Klassifikation von Hatespeech\n",
    "section": "\n10.1 Vorab",
    "text": "10.1 Vorab\n\n10.1.1 Lernziele\n\nSie kÃ¶nnen grundlegende Verfahren zur Klassifikation von Hatespeech einsetzen und erklÃ¤ren\n\n10.1.2 BenÃ¶tigte R-Pakete\n\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(tidymodels)\nlibrary(tidytext)\nlibrary(textrecipes)\nlibrary(lsa)  # stopwords\nlibrary(discrim)  # naive bayes classification\nlibrary(naivebayes)\nlibrary(tictoc)  # Zeitmessung\nlibrary(fastrtext)  # Worteinbettungen\nlibrary(remoji)  # Emojis\nlibrary(tokenizers)  # Vektoren tokenisieren"
  },
  {
    "objectID": "080-klassifikation.html#daten",
    "href": "080-klassifikation.html#daten",
    "title": "\n10Â  Klassifikation von Hatespeech\n",
    "section": "\n10.2 Daten",
    "text": "10.2 Daten\nFÃ¼r Maschinenlernen brauchen wir Trainingsdaten, Daten also, bei denen wir pro Beobachtung der Wert der Zielvariablen kennen. Man spricht auch von â€œgelabeltenâ€ Daten.\nWir nutzen die Daten von (wiegand_germeval?) bzw. (wiegand-data?). Die Daten sind unter CC-By-4.0 Int. lizensiert.\n\nd_raw &lt;- \n  data_read(\"data/germeval2018.training.txt\",\n         header = FALSE)\n## Warning in data.table::fread(input = path, encoding = encoding, ...): Found and\n## resolved improper quoting out-of-sample. First healed line 111: &lt;&lt;\"Edel sei der\n## Mensch, hilfreich und gut\" - Nicht eine dieser Charaktereigenschaften kann\n## Merkel fÃ¼r sich beanspruchen. OTHER OTHER&gt;&gt;. If the fields are not quoted (e.g.\n## field separator does not appear within any field), try quote=\"\" to avoid this\n## warning.\n\nDie Daten finden sich auch im Paket pradadata.\nDa die Daten keine SpaltenkÃ¶pfe haben, informieren wir die Funktion dazu mit header = FALSE.\nBenennen wir die die Spalten um:\n\nnames(d_raw) &lt;- c(\"text\", \"c1\", \"c2\")\n\nDabei soll c1 und c2 fÃ¼r die 1. bzw. 2. Klassifikation stehen.\nIn c1 finden sich diese Werte:\n\nd_raw %&gt;% \n  count(c1)\n\n\n\n  \n\n\n\nHier wurde klassifiziert, ob beleidigende Sprache (offensive language) vorlag oder nicht (isch-etal-2021-overview?):\n\nTask 1 was to decide whether a tweet includes some form of offensive language or not. The tweets had to be classiï¬ed into the two classes OFFENSE and OTHER. The OFFENSE category covered abusive language, insults, as well as merely profane statements.\n\nUnd in c2 finden sich folgende AusprÃ¤gungen:\n\nd_raw %&gt;% \n  count(c2)\n\n\n\n  \n\n\n\nIn c2 ging es um eine feinere Klassifikation beleidigender Sprache (isch-etal-2021-overview?):\n\nThe second task involved four categories, a nonoffensive OTHER class and three sub-categories of what is OFFENSE in Task 1. In the case of PROFANITY, profane words are used, however, the tweet does not want to insult anyone. This typically concerns the usage of swearwords (ScheiÃŸe, Fuck etc.) and cursing (Zur HÃ¶lle! Verdammt! etc.). This can be often found in youth language. Swearwords and cursing may, but need not, co-occur with insults or abusive speech. Profane language may in fact be used in tweets with positive sentiment to express emphasis. Whenever profane words are not directed towards a speciï¬c person or group of persons and there are no separate cues of INSULT or ABUSE, then tweets are labeled as simple cases of PROFANITY.\n\nSind Texte, die als OFFENSE klassifiziert sind, auch (fast) immer als ABUSE, INSULT oder PROFANITY klassifiziert?\n\nd_raw %&gt;% \n  filter(c1 == \"OTHER\", c2 == \"OTHER\") %&gt;% \n  nrow() / nrow(d_raw)\n## [1] 0.6630066\n\nIn ca. 2/3 der FÃ¤lle wurden in beiden Klassifikation OTHER klassifiziert.\n\nd_raw %&gt;% \n  filter(c1 != \"OTHER\", c2 != \"OTHER\") %&gt;% \n  nrow() / nrow(d_raw)\n## [1] 0.3369934\n\nEntsprechend in ca. 1/3 der FÃ¤lle wurde jeweils nicht mit OTHER klassifiziert.\nWir begnÃ¼gen uns hier mit der ersten, grÃ¶beren Klassifikation.\nFÃ¼gen wir abschlieÃŸend noch eine ID-Variable hinzu:\n\nd1 &lt;-\n  d_raw %&gt;% \n  mutate(id = as.character(1:nrow(.)))\n\nDie ID-Variable definieren als Text (nicht als Integer), da die Twitter-IDs zwar natÃ¼rliche Zahlen sind, aber zu groÃŸ, um von R als Integer verarbeitet zu werden. Faktisch sind sie fÃ¼r uns auch nur nominal skalierte Variablen, so dass wir keinen Informationsverlust haben.\n\n#write_rds(d1, \"objects/d1.rds\")"
  },
  {
    "objectID": "080-klassifikation.html#feature-engineering",
    "href": "080-klassifikation.html#feature-engineering",
    "title": "\n10Â  Klassifikation von Hatespeech\n",
    "section": "\n10.3 Feature Engineering",
    "text": "10.3 Feature Engineering\nReichern wir die Daten mit weiteren Features an, in der Hoffnung, damit eine bessere Klassifikation erzielen zu kÃ¶nnen.\n\n10.3.1 TextlÃ¤nge\n\nd2 &lt;-\n  d1 %&gt;% \n  mutate(text_length = str_length(text))\n\nhead(d2)\n\n\n\n  \n\n\n\n\n10.3.2 Sentimentanalyse\nWir nutzen dazu SentiWS (Remus, Quasthoff, und Heyer 2010).\n\nsentiws &lt;- read_csv(\"https://osf.io/x89wq/?action=download\")\n## Rows: 3468 Columns: 4\n## â”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n## Delimiter: \",\"\n## chr (3): neg_pos, word, inflections\n## dbl (1): value\n## \n## â„¹ Use `spec()` to retrieve the full column specification for this data.\n## â„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nd2_long &lt;-\n  d2 %&gt;% \n  unnest_tokens(input = text, output = token)\n\nhead(d2_long)\n\n\n\n  \n\n\n\nJetzt filtern wir unsere Textdaten so, dass nur WÃ¶rter mit Sentimentwert Ã¼brig bleiben:\n\nd2_long_senti &lt;- \n  d2_long %&gt;%  \n  inner_join(sentiws %&gt;% select(-inflections), by = c(\"token\" = \"word\"))\n## Warning in inner_join(., sentiws %&gt;% select(-inflections), by = c(token = \"word\")): Detected an unexpected many-to-many relationship between `x` and `y`.\n## â„¹ Row 1559 of `x` matches multiple rows in `y`.\n## â„¹ Row 2572 of `y` matches multiple rows in `x`.\n## â„¹ If a many-to-many relationship is expected, set `relationship =\n##   \"many-to-many\"` to silence this warning.\n\nhead(d2_long)\n\n\n\n  \n\n\n\nSchlieÃŸlich berechnen wir die Sentimentwert pro PolaritÃ¤t und pro Tweet:\n\nd2_sentis &lt;-\n  d2_long_senti %&gt;% \n  group_by(id, neg_pos) %&gt;% \n  summarise(senti_avg = mean(value))\n## `summarise()` has grouped output by 'id'. You can override using the `.groups`\n## argument.\n\nhead(d2_sentis)\n\n\n\n  \n\n\n\nDiese Tabelle bringen wir wieder eine breitere Form, um sie dann wieder mit den Hauptdaten zu vereinigen.\n\nd2_sentis_wide &lt;-\n  d2_sentis %&gt;% \n  pivot_wider(names_from = \"neg_pos\", values_from = \"senti_avg\")\n\nd2_sentis_wide %&gt;% head()\n\n\n\n  \n\n\n\n\nd3 &lt;-\n  d2 %&gt;% \n  full_join(d2_sentis_wide)\n## Joining with `by = join_by(id)`\n\nhead(d3)\n\n\n\n  \n\n\n\n\n\n\n\n\n\nHinweis\n\n\n\nDie Sentimentanalyse hier vernachlÃ¤ssigt Flexionen der WÃ¶rter. Der Autor fÃ¼hlt den Drang zu schreiben: â€œLeft as an exercise for the readerâ€ :-)\n\n\n\n10.3.3 SchimpfwÃ¶rter\nZÃ¤hlen wir die SchimpfwÃ¶rter pro Text. Dazu nutzen wir die Daten von LDNOOBW, lizensiert nach CC-BY-4.0-Int.\n\nschimpf1 &lt;- read_csv(\"https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/de\", col_names = FALSE)\n## Rows: 66 Columns: 1\n## â”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n## Delimiter: \",\"\n## chr (1): X1\n## \n## â„¹ Use `spec()` to retrieve the full column specification for this data.\n## â„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nLÃ¤nger aber noch ist die Liste aus dem InsultWiki, lizensiert CC0.\n\nschimpf2 &lt;- \n  data_read(\"data/insult-de.txt\", header = FALSE) %&gt;% \n  mutate_all(str_to_lower)\n\nDie Daten finden sich auch im Paket pradadata.\nBinden wir die Listen zusammen:\n\nschimpf &lt;-\n  schimpf1 %&gt;% \n  bind_rows(schimpf2) %&gt;% \n  distinct() %&gt;% \n  rename(word = \"V1\")\n\nnrow(schimpf)\n## [1] 6235\n\nUm die Lesis vor (unnÃ¶tiger?) Kopfverschmutzung zu bewahren, sind diese SchimpfwÃ¶rter hier nicht abgedruckt.\nJetzt zÃ¤hlen wir, ob unsere Tweets/Texte solcherlei WÃ¶rter enthalten.\n\nd_schimpf &lt;- \nd2_long %&gt;% \n  select(id, token) %&gt;% \n  mutate(schimpf = token %in% schimpf$word)\n\nWie viele SchimpfwÃ¶rter haben wir gefunden?\n\nd_schimpf %&gt;% \n  count(schimpf)\n\n\n\n  \n\n\n\nEtwa ein Prozent der WÃ¶rter sind SchimpfwÃ¶rter in unserem Corpus.\n\nd_schimpf2 &lt;-\n  d_schimpf %&gt;% \n  group_by(id) %&gt;% \n  summarise(schimpf_n = sum(schimpf))\n\nhead(d_schimpf2)\n\n\n\n  \n\n\n\n\nd_main &lt;-\n  d3 %&gt;% \n  full_join(d_schimpf2)\n## Joining with `by = join_by(id)`\n\n\n\n\n\n\n\nWichtig\n\n\n\nNamen wie final, main oder result sind gefÃ¤hrlich, da es unter Garantie ein â€œfinal-final geben wird, oder derâ€Haupt-Datensatâ€ plÃ¶tzlich nicht mehr so wichtig erscheint und so weiter.\n\n\n\n10.3.4 Emojis\n\nemj &lt;- emoji(list_emoji(), pad = FALSE)\n\nhead(emj)\n## [1] \"ğŸ˜„\" \"ğŸ˜ƒ\" \"ğŸ˜€\" \"ğŸ˜Š\" \"â˜ºï¸\"  \"ğŸ˜‰\"\n\nDiese Liste umfasst knapp 900 Emojis, das sind allerdings noch nicht alle, die es gibt. Diese Liste umfasst mit gut 1800 Emojis gut das Doppelte.\nSelbstkuratierte Liste an â€œwildenâ€ Emoji; diese Liste ist inspiriert von emojicombos.com.\n\nwild_emojis &lt;- \n  c(\n    emoji(find_emoji(\"gun\")),\n    emoji(find_emoji(\"bomb\")),\n    emoji(find_emoji(\"fist\")),\n    emoji(find_emoji(\"knife\"))[1],\n    emoji(find_emoji(\"ambulance\")),\n    emoji(find_emoji(\"fist\")),\n    emoji(find_emoji(\"skull\")),\n    \"â˜ ï¸\",     \"ğŸ—‘\",       \"ğŸ˜ \",    \"ğŸ‘¹\",    \"ğŸ’©\" ,\n    \"ğŸ–•\",    \"ğŸ‘ï¸\",\n    emoji(find_emoji(\"middle finger\")),    \"ğŸ˜¡\",    \"ğŸ¤¢\",    \"ğŸ¤®\",  \n    \"ğŸ˜–\",    \"ğŸ˜£\",    \"ğŸ˜©\",    \"ğŸ˜¨\",    \"ğŸ˜\",    \"ğŸ˜³\",    \"ğŸ˜¬\",    \"ğŸ˜±\",    \"ğŸ˜µ\",\n       \"ğŸ˜¤\",    \"ğŸ¤¦â€â™€ï¸\",    \"ğŸ¤¦â€\"\n  )\n\n\nwild_emojis_df &lt;-\n  tibble(emoji = wild_emojis)\n\nsave(wild_emojis_df, file = \"data/wild_emojis.RData\")\n\nAuf dieser Basis kÃ¶nnen wir einen PrÃ¤diktor erstellen, der zÃ¤hlt, ob ein Tweet einen oder mehrere der â€œwildenâ€ Emojis enthÃ¤lt."
  },
  {
    "objectID": "080-klassifikation.html#workflow-1-rezept-1-naive-bayes",
    "href": "080-klassifikation.html#workflow-1-rezept-1-naive-bayes",
    "title": "\n10Â  Klassifikation von Hatespeech\n",
    "section": "\n10.4 Workflow 1: Rezept 1 + Naive-Bayes",
    "text": "10.4 Workflow 1: Rezept 1 + Naive-Bayes\n\n10.4.1 Dummy-Rezept\nHier ist ein einfaches Beispiel, um die Textvorbereitung mit textrecipes zu verdeutlichen.\nWir erstellen uns einen Dummy-Text:\n\ndummy &lt;- \n  tibble(text = c(\"Ich gehe heim und der die das nicht in ein and the\"))\n\nDann tokenisieren wir den Text:\n\nrec_dummy &lt;-\n  recipe(text ~ 1, data = dummy) %&gt;% \n  step_tokenize(text)\n  \nrec_dummy\n## \n## â”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n## \n## â”€â”€ Inputs\n## Number of variables by role\n## outcome: 1\n## \n## â”€â”€ Operations\n## â€¢ Tokenization for: text\n\nDie Tokens kann man sich so zeigen lassen:\n\nshow_tokens(rec_dummy, text)\n## [[1]]\n##  [1] \"ich\"   \"gehe\"  \"heim\"  \"und\"   \"der\"   \"die\"   \"das\"   \"nicht\" \"in\"   \n## [10] \"ein\"   \"and\"   \"the\"\n\nJetzt entfernen wir die StopwÃ¶rter deutscher Sprache; dafÃ¼r nutzen wir die Stopwort-Quelle snowball:\n\nrec_dummy &lt;-\n  recipe(text ~ 1, data = dummy) %&gt;% \n  step_tokenize(text) %&gt;% \n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\")\n\nrec_dummy\n## \n## â”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n## \n## â”€â”€ Inputs\n## Number of variables by role\n## outcome: 1\n## \n## â”€â”€ Operations\n## â€¢ Tokenization for: text\n## â€¢ Stop word removal for: text\n\nPrÃ¼fen wir die Tokens; sind die StopwÃ¶rter wirklich entfernt?\n\nshow_tokens(rec_dummy, text)\n## [[1]]\n## [1] \"gehe\" \"heim\" \"and\"  \"the\"\n\nJa, die deutschen StopwÃ¶rter sind entfernt. Die englischen nicht; das macht Sinn!\n\n10.4.2 Datenaufteilung\n\nd_split &lt;- initial_split(d_main, strata = c1)\n\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\n\n10.4.3 Rezept 1\nRezept definieren:\n\nrec1 &lt;- \n  recipe(c1 ~ ., data = select(d_train, text, c1, id)) %&gt;% \n  update_role(id, new_role = \"id\") %&gt;% \n  step_tokenize(text) %&gt;% \n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %&gt;% \n  step_stem(text) %&gt;% \n  step_tokenfilter(text, max_tokens = 1e2) %&gt;% \n  step_tfidf(text) %&gt;% \n  step_normalize(all_numeric_predictors())\n\nrec1\n## \n## â”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n## \n## â”€â”€ Inputs\n## Number of variables by role\n## outcome:   1\n## predictor: 1\n## id:        1\n## \n## â”€â”€ Operations\n## â€¢ Tokenization for: text\n## â€¢ Stop word removal for: text\n## â€¢ Stemming for: text\n## â€¢ Text filtering for: text\n## â€¢ Term frequency-inverse document frequency with: text\n## â€¢ Centering and scaling for: all_numeric_predictors()\n\nPreppen:\n\nrec1_prepped &lt;- prep(rec1)\n\nUnd backen:\n\nd_rec1 &lt;- bake(rec1_prepped, new_data = NULL)\n\nhead(d_rec1)\n\n\n\n  \n\n\n\n\n10.4.4 Modellspezifikation 1\nWir definiere einen Naive-Bayes-Algorithmus:\n\nnb_spec &lt;- naive_Bayes() %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"naivebayes\")\n\nnb_spec\n## Naive Bayes Model Specification (classification)\n## \n## Computational engine: naivebayes\n\nUnd setzen auf die klassische zehnfache Kreuzvalidierung.\n\nset.seed(42)\nfolds1 &lt;- vfold_cv(d_train)\n\n\n10.4.5 Workflow 1\n\nwf1 &lt;-\n  workflow() %&gt;% \n  add_recipe(rec1) %&gt;% \n  add_model(nb_spec)\n\nwf1\n## â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n## Preprocessor: Recipe\n## Model: naive_Bayes()\n## \n## â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n## 6 Recipe Steps\n## \n## â€¢ step_tokenize()\n## â€¢ step_stopwords()\n## â€¢ step_stem()\n## â€¢ step_tokenfilter()\n## â€¢ step_tfidf()\n## â€¢ step_normalize()\n## \n## â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n## Naive Bayes Model Specification (classification)\n## \n## Computational engine: naivebayes\n\n\n10.4.6 Fitting 1\n\nfit1 &lt;-\n  fit_resamples(\n    wf1,\n    folds1,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nDie Vorhersagen speichern wir ab, um die Performanz in den Faltungen des Hold-out-Samples zu berechnen.\nMÃ¶chte man sich die Zeit sparen, die Syntax wieder durchlaufen zu lassen, kann man das Objekt speichern. Aber Vorsicht: Dabei kann es passieren, dass man mit veralteten Objekten arbeitet.\n\n#write_rds(fit1, \"objects/chap_classific_fit1.rds\")\n\n\n10.4.7 Performanz 1\n\nwf1_performance &lt;-\n  collect_metrics(fit1)\n\nwf1_performance\n\n\nwf_preds &lt;-\n  collect_predictions(fit1)\n\nwf_preds %&gt;% \n  group_by(id) %&gt;% \n  roc_curve(truth = c1, .pred_OFFENSE) %&gt;% \n  autoplot()\n\n\n\n\nconf_mat_resampled(fit1, tidy = FALSE) %&gt;% autoplot(type = â€œheatmapâ€)\n\nconf_mat_resampled(fit1, tidy = FALSE) %&gt;% \n  autoplot(type = \"heatmap\")"
  },
  {
    "objectID": "080-klassifikation.html#nullmodell",
    "href": "080-klassifikation.html#nullmodell",
    "title": "\n10Â  Klassifikation von Hatespeech\n",
    "section": "\n10.5 Nullmodell",
    "text": "10.5 Nullmodell\n\nnull_classification &lt;- \n  parsnip::null_model() %&gt;%\n  set_engine(\"parsnip\") %&gt;%\n  set_mode(\"classification\")\n\nnull_rs &lt;- workflow() %&gt;%\n  add_recipe(rec1) %&gt;%\n  add_model(null_classification) %&gt;%\n  fit_resamples(\n    folds1\n  )\n\nHier ist die Performanz des Nullmodells.\n\nnull_rs %&gt;%\n  collect_metrics()\n\n\nshow_best(null_rs)\n## Warning: No value of `metric` was given; metric 'roc_auc' will be used."
  },
  {
    "objectID": "080-klassifikation.html#workflow-2-rezept-1-lasso",
    "href": "080-klassifikation.html#workflow-2-rezept-1-lasso",
    "title": "\n10Â  Klassifikation von Hatespeech\n",
    "section": "\n10.6 Workflow 2: Rezept 1 + Lasso",
    "text": "10.6 Workflow 2: Rezept 1 + Lasso\n\nlasso_spec &lt;- logistic_reg(penalty = tune(), mixture = 1) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"glmnet\")\n\nlasso_spec\n## Logistic Regression Model Specification (classification)\n## \n## Main Arguments:\n##   penalty = tune()\n##   mixture = 1\n## \n## Computational engine: glmnet\n\nWir definieren die AusprÃ¤gungen von penalty, die wir ausprobieren wollen:\n\nlambda_grid &lt;- grid_regular(penalty(), levels = 3)  # hier nur 3 Werte, um Rechenzeit zu sparen\n\n\nwf2 &lt;-\n  workflow() %&gt;% \n  add_recipe(rec1) %&gt;% \n  add_model(lasso_spec)\n\nwf2\n## â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n## Preprocessor: Recipe\n## Model: logistic_reg()\n## \n## â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n## 6 Recipe Steps\n## \n## â€¢ step_tokenize()\n## â€¢ step_stopwords()\n## â€¢ step_stem()\n## â€¢ step_tokenfilter()\n## â€¢ step_tfidf()\n## â€¢ step_normalize()\n## \n## â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n## Logistic Regression Model Specification (classification)\n## \n## Main Arguments:\n##   penalty = tune()\n##   mixture = 1\n## \n## Computational engine: glmnet\n\nTunen und Fitten:\n\nset.seed(42)\n\nfit2 &lt;-\n  tune_grid(\n    wf2,\n    folds1,\n    grid = lambda_grid,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nfit2\n\nVorsicht beim Abspeichern.\n\n#write_rds(fit2, \"objects/chap_classific_fit2.rds\")\n\nHier ist die Performanz:\n\ncollect_metrics(fit2) %&gt;% \n  filter(.metric == \"roc_auc\") %&gt;% \n  slice_max(mean, n = 3)\n\n\nautoplot(fit2)\n\n\n\n\n\nfit2 %&gt;% \n  show_best(\"roc_auc\")\n\n\n\n  \n\n\n\n\nchosen_auc &lt;- \n  fit2 %&gt;%\n  select_by_one_std_err(metric = \"roc_auc\", -penalty)\n\nFinalisieren:\n\nwf2_final &lt;-\n  finalize_workflow(wf2, chosen_auc)\n\nwf2_final\n## â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n## Preprocessor: Recipe\n## Model: logistic_reg()\n## \n## â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n## 6 Recipe Steps\n## \n## â€¢ step_tokenize()\n## â€¢ step_stopwords()\n## â€¢ step_stem()\n## â€¢ step_tokenfilter()\n## â€¢ step_tfidf()\n## â€¢ step_normalize()\n## \n## â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n## Logistic Regression Model Specification (classification)\n## \n## Main Arguments:\n##   penalty = 0.00853167852417281\n##   mixture = 1\n## \n## Computational engine: glmnet\n\n\nfit2_final_train &lt;-\n  fit(wf2_final, d_train)\n\n\nfit2_final_train %&gt;% \n  extract_fit_parsnip() %&gt;% \n  tidy() %&gt;% \n  arrange(-abs(estimate)) %&gt;% \n  head()\n## Loading required package: Matrix\n## \n## Attaching package: 'Matrix'\n## The following objects are masked from 'package:tidyr':\n## \n##     expand, pack, unpack\n## Loaded glmnet 4.1-8\n\n\n\n  \n\n\n\n\nfit2_final_test &lt;-\n  last_fit(wf2_final, d_split)\n\ncollect_metrics(fit2_final_test)\n\n\n\n  \n\n\n\n\n10.6.1 Vorhersage\n\n10.6.2 Vohersagedaten\nPfad zu den Daten:\n\ntweet_data_path &lt;- \"/Users/sebastiansaueruser/github-repos/hate-speech/data-raw/tweets/\"\n\n\ntweet_data_files_names &lt;- list.files(path = tweet_data_path,\n                                     pattern  = \"tweets-to-.*\\\\.rds$\")\nhead(tweet_data_files_names)\n## [1] \"tweets-to-_FriedrichMerz_2021.rds\" \"tweets-to-_FriedrichMerz_2022.rds\"\n## [3] \"tweets-to-ABaerbock_2021.rds\"      \"tweets-to-ABaerbock_2022.rds\"     \n## [5] \"tweets-to-Alice_Weidel_2021.rds\"   \"tweets-to-Alice_Weidel_2022.rds\"\n\nWie viele Dateien sind es?\n\nlength(tweet_data_files_names)\n## [1] 26\n\nWir geben den Elementen des Vektors gÃ¤ngige Namen, das hilft uns gleich bei map:\n\nnames(tweet_data_files_names) &lt;- str_remove(tweet_data_files_names, \"\\\\.rds\")\n\nOK, weiter: So kÃ¶nnen wir eine der Datendateien einlesen:\n\nd_raw &lt;-\n  read_rds(file = paste0(tweet_data_path, tweet_data_files_names[1])) \n\nd &lt;- \n  d_raw %&gt;% \n  select(id, author_id, created_at, public_metrics) %&gt;% \n  unnest_wider(public_metrics)\n\nhead(d)\n\nUnd so lesen wir alle ein:\nZunÃ¤chst erstellen wir uns eine Helper-Funktion:\n\nread_and_select &lt;- function(file_name, path_to_tweet_data = tweet_data_path) {\n  \n  out &lt;- \n    read_rds(file = paste0(path_to_tweet_data, file_name)) %&gt;% \n    select(id, author_id, created_at, text, public_metrics) %&gt;% \n    unnest_wider(public_metrics)\n  \n  cat(\"Data file was read.\\n\")\n  \n  return(out)\n}\n\nTesten:\n\nd1 &lt;- read_and_select(tweet_data_files_names[1])\n\nhead(d1)\n\nDie Funktion read_and_select mappen wir auf alle Datendateien:\n\ntic()\nds &lt;-\n  tweet_data_files_names %&gt;% \n  map_dfr(read_and_select, .id = \"dataset\")\ntoc()\n\n214.531 sec elapsed\nDa wir den Elementen von tweet_data_files_names Namen gegeben haben, finden wir diese Namen praktischerweise wieder in ds.\nVielleicht ist es zum Entwickeln besser, mit einem kleineren Datensatz einstweilen zu arbeiten:\n\nds_short &lt;- read_rds(file = \"/Users/sebastiansaueruser/datasets/Twitter/hate-classific/ds_short.rds\")\n\nds &lt;- ds_short\n\n\n10.6.3 Vokabular erstellen\n\nds_long &lt;-\n  ds %&gt;% \n  select(text) %&gt;% \n  unnest_tweets(input = text, output = word)\n\nPuh, das hat gedauert!\nSpeichern wir uns diese Daten daher auf die Festplatte:\n\n#write_rds(ds_long, file = paste0(tweet_data_path, \"ds_long.rds\"))\n\nEntfernen wir daraus die Duplikate, um uns ein Vokabular zu erstellen:\n\nds_voc &lt;-\n  ds_long %&gt;% \n  distinct(word)\n\nUnd das resultierende Objekt speichern wir wieder ab:\n\n#write_rds(ds_voc, file = paste0(\"objects/\", \"ds_voc.rds\"))"
  },
  {
    "objectID": "080-klassifikation.html#worteinbettungen-erstellen",
    "href": "080-klassifikation.html#worteinbettungen-erstellen",
    "title": "\n10Â  Klassifikation von Hatespeech\n",
    "section": "\n10.7 Worteinbettungen erstellen",
    "text": "10.7 Worteinbettungen erstellen\n\n10.7.1 FastText-Modell\nDefiniere die Konstanten fÃ¼r das fastText-Modell:\n\ntexts &lt;- ds %&gt;% pull(text)\ntexts &lt;- tolower(texts)\n\n\nout_file_txt &lt;- \"/Users/sebastiansaueruser/datasets/Twitter/twitter-polit-model.vec\"\nout_file_model &lt;- \"/Users/sebastiansaueruser/datasets/Twitter/twitter-polit-model.bin\"\n\nfile.exists(out_file_txt)\n## [1] TRUE\nfile.exists(out_file_model)\n## [1] TRUE\n\n\n#writeLines(text = texts, con = out_file_txt)\n#execute(commands = c(\"skipgram\", \"-input\", tmp_file_txt, \"-output\", out_file_model, \"-verbose\", 1))\n\nRead 22M words\nNumber of words:  130328\nNumber of labels: 0\nProgress: 100.0% words/sec/thread:   49218 lr:  0.000000 avg.loss:  1.720812 ETA:   0h 0m 0s\nJetzt laden wir das Modell von der Festplatte:\n\ntwitter_fasttext_model &lt;- load_model(out_file_model)\ndict &lt;- get_dictionary(twitter_fasttext_model)\n\nSchauen wir uns einige Begriffe aus dem Vokabular an:\n\nprint(head(dict, 10))\n##  [1] \"&lt;/s&gt;\"            \"die\"             \"und\"             \"der\"            \n##  [5] \"sie\"             \"das\"             \"nicht\"           \"in\"             \n##  [9] \"ist\"             \"@_friedrichmerz\"\n\nHier sind die ersten paar Elemente des Vektors fÃ¼r menschen:\n\nget_word_vectors(twitter_fasttext_model, c(\"menschen\")) %&gt;% `[`(1:10)\n\n [1]  0.14156282  0.44875699  0.23911817 -0.02580349  0.29811972  0.03870077\n [7]  0.06518744  0.22527063  0.28198120  0.39931887\nErstellen wir uns einen Tibble, der als erste Spalte das Vokabular und in den Ã¼brigen 100 Spalten die Dimensionen enthÃ¤lt:\n\nword_embedding_twitter &lt;-\n  tibble(\n    word = dict\n  )\n\n\nwords_vecs_twitter &lt;-\n  get_word_vectors(twitter_fasttext_model)\n\n\nword_embedding_twitter &lt;-\n  word_embedding_twitter %&gt;% \n  bind_cols(words_vecs_twitter)\n\nnames(word_embedding_twitter) &lt;- c(\"word\", paste0(\"v\", sprintf(\"%03d\", 1:100)))  # Namen verschÃ¶nern\n\nUnd als Worteinbettungs-Datei abspeichern:\n\n#write_rds(word_embedding_twitter, file = paste0(tweet_data_path, \"word_embedding_twitter.rds\"))\n\n\n10.7.2 Aufbereiten\nAm besten nur die Spalten behalten, die wir zum Modellieren nutzen:\n\nds_short2 &lt;-\n  ds_short %&gt;% \n  select(text, id)\n\nDann backen wir die Daten mit dem vorhandenen Rezept:\n\nds_baked &lt;- bake(rec1_prepped, new_data = ds_short2)\n\nIst das nicht komfortabel? Das Textrezept Ã¼bernimmt die Arbeit fÃ¼r uns, mit den richtigen Features zu arbeiten, die tf-idfs fÃ¼r die richtigen Tokens zu berechnen.\nWer dem Frieden nicht traut, dem sei geraten, nachzuprÃ¼fen :-)"
  },
  {
    "objectID": "080-klassifikation.html#workflow-3-rezept-2-lasso",
    "href": "080-klassifikation.html#workflow-3-rezept-2-lasso",
    "title": "\n10Â  Klassifikation von Hatespeech\n",
    "section": "\n10.8 Workflow 3: Rezept 2 + Lasso",
    "text": "10.8 Workflow 3: Rezept 2 + Lasso\n\n10.8.1 Daten aufteilen\n\nd_split &lt;- initial_split(d2, strata = c1)\n\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\n\n10.8.2 Hilfsfunktionen\n\nsource(\"funs/helper-funs-recipes.R\")\n\nTesten wir die Funktionen:\n\ndummy &lt;- c(\"hallo\", \"baby\", \"fatal\")\n\ncount_profane(dummy) \n## [1] 1\n\ncount_emo_words(dummy)\n## [1] 1\n\ndummy &lt;- c(\"baby\", \"und\", \"ğŸ†—\", \"ğŸ–•\")\n\ncount_emojis(dummy)\n## [1] 0\n\ncount_wild_emojis(dummy) \n## [1] 0\n\n\n10.8.3 Rezept mit Worteinbettungen\n\nrec2 &lt;- \n  recipe(c1 ~ ., data = select(d_train, text, c1, id)) %&gt;% \n  update_role(id, new_role = \"id\") %&gt;% \n  step_text_normalization(text) %&gt;% \n  step_mutate(text_copy = text,\n              profane_n = map_int(text, count_profane),\n              emo_words_n = map_int(text, count_emo_words),\n              emojis_n = map_int(text, count_emojis),\n              wild_emojis_n = map_int(text, count_wild_emojis)\n  ) %&gt;% \n  step_textfeature(text_copy) %&gt;% \n  step_tokenize(text, token = \"words\") %&gt;% \n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %&gt;% \n  step_word_embeddings(text, embeddings = word_embedding_twitter)\n \nrec2\n## \n## â”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n## \n## â”€â”€ Inputs\n## Number of variables by role\n## outcome:   1\n## predictor: 1\n## id:        1\n## \n## â”€â”€ Operations\n## â€¢ Text Normalization for: text\n## â€¢ Variable mutation for: text, map_int(text, count_profane), ...\n## â€¢ Text feature extraction for: text_copy\n## â€¢ Tokenization for: text\n## â€¢ Stop word removal for: text\n## â€¢ Word embeddings aggregated from: text\n\nJetzt preppen:\n\nrec2_prepped &lt;- prep(rec2)\n\nVielleicht macht es Sinn, sich das Objekt zur spÃ¤teren Verwendung abzuspeichern.1 Feather verarbeitet nur Dataframes, daher nutzen wir hier RDS.\n\n#write_rds(rec2_prepped, file = \"~/datasets/Twitter/klassifik-rec2-prepped.rds\")\n\nDas Element rec2_prepped ist recht groÃŸ:\n\nformat(object.size(rec2_prepped), units  = \"Mb\")\n## [1] \"113.8 Mb\"\n\nJetzt kÃ¶nnen wir das prÃ¤parierte (â€œgepreppteâ€) Rezept â€œbackenâ€:\n\nrec2_baked &lt;- bake(rec2_prepped, new_data = NULL)\n\n\nrec2_baked %&gt;% \n  select(1:15) %&gt;% \n  glimpse()\n## Rows: 3,756\n## Columns: 15\n## $ id                                  &lt;fct&gt; 5, 7, 10, 12, 17, 27, 33, 42, 44, â€¦\n## $ c1                                  &lt;fct&gt; OFFENSE, OFFENSE, OFFENSE, OFFENSEâ€¦\n## $ profane_n                           &lt;int&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0â€¦\n## $ emo_words_n                         &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1â€¦\n## $ emojis_n                            &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0â€¦\n## $ wild_emojis_n                       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0â€¦\n## $ textfeature_text_copy_n_words       &lt;int&gt; 16, 32, 15, 26, 19, 15, 33, 30, 31â€¦\n## $ textfeature_text_copy_n_uq_words    &lt;int&gt; 16, 28, 15, 25, 17, 15, 32, 29, 29â€¦\n## $ textfeature_text_copy_n_charS       &lt;int&gt; 121, 145, 119, 134, 112, 101, 196,â€¦\n## $ textfeature_text_copy_n_uq_charS    &lt;int&gt; 31, 29, 30, 39, 36, 31, 35, 42, 35â€¦\n## $ textfeature_text_copy_n_digits      &lt;int&gt; 0, 4, 0, 2, 4, 0, 0, 0, 1, 0, 2, 0â€¦\n## $ textfeature_text_copy_n_hashtags    &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0â€¦\n## $ textfeature_text_copy_n_uq_hashtags &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0â€¦\n## $ textfeature_text_copy_n_mentions    &lt;int&gt; 1, 1, 0, 1, 0, 1, 1, 5, 1, 0, 1, 1â€¦\n## $ textfeature_text_copy_n_uq_mentions &lt;int&gt; 1, 1, 0, 1, 0, 1, 1, 5, 1, 0, 1, 1â€¦\n\n\n10.8.4 Fitting 3\n\nwf3 &lt;-\n  workflow() %&gt;% \n  add_recipe(rec2) %&gt;% \n  add_model(lasso_spec)\n\nwf3\n## â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n## Preprocessor: Recipe\n## Model: logistic_reg()\n## \n## â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n## 6 Recipe Steps\n## \n## â€¢ step_text_normalization()\n## â€¢ step_mutate()\n## â€¢ step_textfeature()\n## â€¢ step_tokenize()\n## â€¢ step_stopwords()\n## â€¢ step_word_embeddings()\n## \n## â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n## Logistic Regression Model Specification (classification)\n## \n## Main Arguments:\n##   penalty = tune()\n##   mixture = 1\n## \n## Computational engine: glmnet\n\nTunen und Fitten:\n\nset.seed(42)\n\ntic()\nfit3 &lt;-\n  tune_grid(\n    wf3,\n    folds1,\n    grid = lambda_grid,\n    control = control_resamples(save_pred = TRUE)\n  )\n(toc)\nfit3\n\n\n#write_rds(fit3, \"objects/chap_classific_fit3.rds\")\n\nHier ist die Performanz:\n\ncollect_metrics(fit3)\n\n\nautoplot(fit3)\n\n\n\n\n\nfit3 %&gt;% \n  show_best(\"roc_auc\")\n\n\n\n  \n\n\n\n\nchosen_auc_fit3 &lt;- \n  fit3 %&gt;%\n  select_by_one_std_err(metric = \"roc_auc\", -penalty)\n\nFinalisieren:\n\nwf3_final &lt;-\n  finalize_workflow(wf3, chosen_auc_fit3)\n\nwf3_final\n## â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n## Preprocessor: Recipe\n## Model: logistic_reg()\n## \n## â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n## 6 Recipe Steps\n## \n## â€¢ step_text_normalization()\n## â€¢ step_mutate()\n## â€¢ step_textfeature()\n## â€¢ step_tokenize()\n## â€¢ step_stopwords()\n## â€¢ step_word_embeddings()\n## \n## â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n## Logistic Regression Model Specification (classification)\n## \n## Main Arguments:\n##   penalty = 0.00853167852417281\n##   mixture = 1\n## \n## Computational engine: glmnet\n\n\n#fit3_final_train &lt;-  # die Berechnung kann dauern ...\n  fit(wf3_final, d_train)\n\n\nfit3_final_train %&gt;% \n  extract_fit_parsnip() %&gt;% \n  tidy() %&gt;% \n  arrange(-abs(estimate)) %&gt;% \n  head()\n\n\n\n  \n\n\n\n\nfit3_final_test &lt;-\n  last_fit(wf3_final, d_split)  # dauert etwas\n\nUnd endlich: Wie gut ist die Performanz?\n\ncollect_metrics(fit3_final_test)\n\n\n\n  \n\n\n\nAm Ende so eines Arbeitsganges, bei dem man wieder (und wieder) die gleichen Funktionen kopiert, und nur aufpassen muss, aus fit2 an der richtigen Stelle fit3 zu machen: Da blickt man jedem Umbau dieses Codes zu einer Funktion freudig ins Gesicht.\nEin anderes Problem, fÃ¼r das hier keine elegante LÃ¶sung prÃ¤sentiert wird, sind die langen Berechnungszeiten, die, wenn man Pech hat, auch noch mehrfach wiederholt werden mÃ¼ssen.\nDie Gefahr mit dem Abspeichern via write_rds ist klar: Man riskiert, spÃ¤ter ein veraltetes Objekt zu laden.\nZu diesen Punkten spÃ¤ter mehr.\n\n\n\n\nRemus, Robert, Uwe Quasthoff, und Gerhard Heyer. 2010. â€SentiWS - a Publicly Available German-Language Resource for Sentiment Analysisâ€œ. Proceedings of the 7th International Language Ressources and Evaluation (LRECâ€™10), 1168â€“71."
  },
  {
    "objectID": "080-klassifikation.html#footnotes",
    "href": "080-klassifikation.html#footnotes",
    "title": "\n10Â  Klassifikation von Hatespeech\n",
    "section": "",
    "text": "Aber Vorsicht beim Abspeichern, man kÃ¶nnte versehentlich mit einer veralteten Version weiterarbeiten.â†©ï¸"
  },
  {
    "objectID": "070-hatespeech2.html#vorab",
    "href": "070-hatespeech2.html#vorab",
    "title": "\n11Â  Fallstudie Hatespeech\n",
    "section": "\n11.1 Vorab",
    "text": "11.1 Vorab\n\n11.1.1 Lernziele\n\nSie kÃ¶nnen grundlegende Verfahren zur Klassifikation von Hatespeech einsetzen und erklÃ¤ren\nSie kÃ¶nnen mit echten Daten umgehen im Sinne eines Projektmanagement von Data Science\n\n11.1.2 BenÃ¶tigte R-Pakete\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(easystats)\nlibrary(tidytext)\nlibrary(textrecipes)\nlibrary(tictoc)  # Zeitmessung\nlibrary(beepr)  # piebt, wenn fertig\nlibrary(remoji)  # Emojis\nlibrary(feather)  # Daten speichern\nlibrary(pradadata)  # Hilfsdaten wie Schimpfwoerter\nlibrary(lubridate)  # Datum und Zeit\nlibrary(tokenizers)\nlibrary(feather)  # feather data\nlibrary(pradadata)  # helper data\nlibrary(remoji)  # processing emojis"
  },
  {
    "objectID": "070-hatespeech2.html#daten",
    "href": "070-hatespeech2.html#daten",
    "title": "\n11Â  Fallstudie Hatespeech\n",
    "section": "\n11.2 Daten",
    "text": "11.2 Daten\n\n11.2.1 Train- und Testdaten\n\nd1 &lt;- read_rds(\"objects/d1.rds\")  # Traindaten einlesen\n\nIn Train- und Test-Datensatz aufsplitten:\n\nd_split &lt;- initial_split(d1, strata = c1)\n\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\n\n11.2.2 Vorhersagedaten\nWir importieren die Tweets fÃ¼hrender deutscher Politikis.\nFÃ¼r diese Daten haben wir keine Werte der Zielvariablen. Wir kÃ¶nnen nur vorhersagen, aber nicht unsere ModellgÃ¼te berechnen. Diese Daten bezeichnen wir als Vorhersagedaten.\nPfad zu den Daten:\n\ntweet_data_path &lt;- \"/Users/sebastiansaueruser/github-repos/hate-speech/data-raw/tweets-small\"\n\nfile.exists(tweet_data_path)\n## [1] TRUE\n\nDie Nutzungsrechte von Twitter erlauben nicht, diese Daten Ã¶ffentlich zu teilen.\n\ntweet_data_files_names &lt;-\n  list.files(\n    path = tweet_data_path,\n    full.names = TRUE,\n    pattern = \".rds\")\n\n\nnames(tweet_data_files_names) &lt;-  \n  list.files(\n    path = tweet_data_path,\n    full.names = FALSE,\n    pattern = \".rds\") %&gt;% \n  str_remove(\".rds$\") %&gt;% \n  str_remove(\"^tweets-to-\")\n\ntweet_data_files_names\n##                                                                                                    BMWK_2021 \n##           \"/Users/sebastiansaueruser/github-repos/hate-speech/data-raw/tweets-small/tweets-to-BMWK_2021.rds\" \n##                                                                                          Janine_Wissler_2021 \n## \"/Users/sebastiansaueruser/github-repos/hate-speech/data-raw/tweets-small/tweets-to-Janine_Wissler_2021.rds\" \n##                                                                                          Janine_Wissler_2022 \n## \"/Users/sebastiansaueruser/github-repos/hate-speech/data-raw/tweets-small/tweets-to-Janine_Wissler_2022.rds\"\n\nSo lesen wir alle Dateien aus diesem Ordner ein. ZunÃ¤chst erstellen wir uns eine Helper-Funktion:\n\nsource(\"funs/read-and-select.R\")\n\nDie Funktion read_and_select mappen wir auf alle Datendateien:\n\ntic()\nds &lt;-\n  tweet_data_files_names %&gt;% \n  map_dfr(read_and_select, .id = \"dataset\")\n## Data file was read.\n## Data file was read.\n## Data file was read.\ntoc()\n## 2.381 sec elapsed\n\nEin Blick zur Probe:\n\nds %&gt;% \n  glimpse()\n## Rows: 10,310\n## Columns: 9\n## $ dataset       &lt;chr&gt; \"BMWK_2021\", \"BMWK_2021\", \"BMWK_2021\", \"BMWK_2021\", \"BMWâ€¦\n## $ id            &lt;chr&gt; \"1476982045268185091\", \"1476948509706407942\", \"147694476â€¦\n## $ author_id     &lt;chr&gt; \"749510675811139585\", \"146337393\", \"841768687245918208\",â€¦\n## $ created_at    &lt;chr&gt; \"2021-12-31T18:22:15.000Z\", \"2021-12-31T16:08:59.000Z\", â€¦\n## $ text          &lt;chr&gt; \"@BMWi_Bund @twittlik @Pendolino70 @nextmove_de Richtig.â€¦\n## $ retweet_count &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,â€¦\n## $ reply_count   &lt;int&gt; 0, 2, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,â€¦\n## $ like_count    &lt;int&gt; 0, 0, 1, 0, 0, 1, 1, 3, 3, 0, 3, 0, 0, 1, 0, 0, 1, 2, 1,â€¦\n## $ quote_count   &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,â€¦\n\nDa wir den Elementen von tweet_data_files_names Namen gegeben haben, finden wir diese Namen praktischerweise wieder in ds.\nEine Alternative zum Format RDS besteht im Format Feather:\n\nFeather: fast, interoperable data frame storage Feather provides binary columnar serialization for data frames. It is designed to make reading and writing data frames efficient, and to make sharing data across data analysis languages easy.\n\n\n11.2.3 Worteinbettungen\nWie in KapitelÂ 10.7.1 dargestellt, importieren wir unser FastText-Modell.\n\nword_embedding_twitter &lt;- read_rds(file = \"/Users/sebastiansaueruser/datasets/Twitter/word_embedding_twitter.rds\")\n\nWie viel Speicher benÃ¶tigt das Worteinbettungsobjekt?\n\nformat(object.size(word_embedding_twitter), units = \"Mb\")\n## [1] \"108.3 Mb\"\n\n\n11.2.4 Hilfsdaten\n\ndata(\"schimpwoerter\")\n## Warning in data(\"schimpwoerter\"): data set 'schimpwoerter' not found\ndata(\"sentiws\")\ndata(\"wild_emojis\")"
  },
  {
    "objectID": "070-hatespeech2.html#aufbereiten-der-vorhersagedaten",
    "href": "070-hatespeech2.html#aufbereiten-der-vorhersagedaten",
    "title": "\n11Â  Fallstudie Hatespeech\n",
    "section": "\n11.3 Aufbereiten der Vorhersagedaten",
    "text": "11.3 Aufbereiten der Vorhersagedaten\n\n11.3.1 Hilfsfunktionen\n\nsource(\"funs/helper-funs-recipes.R\")"
  },
  {
    "objectID": "070-hatespeech2.html#rezept",
    "href": "070-hatespeech2.html#rezept",
    "title": "\n11Â  Fallstudie Hatespeech\n",
    "section": "\n11.4 Rezept",
    "text": "11.4 Rezept\nDa wir schon ein Rezept â€œtrainiertâ€ haben, kÃ¶nnen wir die Test-Daten einfach mit dem Rezept â€œbackenâ€.\nStreng genommen mÃ¼ssten wir nicht mal das tun, denn tidymodels wÃ¼rde das beim Vorhersagen fÃ¼r uns Ã¼bernehmen. Aber es ist nÃ¼tzlich, die Daten in aufbereiteter Form zu sehen, bzw. sie direkt zugÃ¤nglich zu haben.\n\nrec2 &lt;- \n  recipe(c1 ~ ., data = select(d_train, text, c1, id)) %&gt;% \n  update_role(id, new_role = \"id\") %&gt;% \n  step_text_normalization(text) %&gt;% \n  step_mutate(text_copy = text,\n              profane_n = map_int(text_copy, count_profane, profane_list = schimpfwoerter$word),\n              emo_words_n = map_int(text_copy, count_emo_words, emo_list = sentiws$word),\n              emojis_n = map_int(text_copy, count_emojis, emoji_list = emoji(list_emoji(), pad = FALSE)),\n              wild_emojis_n = map_int(text_copy, count_wild_emojis, wild_emoji_list = wild_emojis$emojis)\n  ) %&gt;% \n  step_textfeature(text_copy) %&gt;% \n  step_tokenize(text, token = \"tweets\") %&gt;% \n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %&gt;% \n  step_word_embeddings(text, embeddings = word_embedding_twitter)\n \nrec2\n\n\n11.4.1 Preppen und Backen\nPreppen:\n\ntic()\nrec2_prepped &lt;- prep(rec2)\ntoc()\n\n29.377 sec elapsed\nBraucht ganz schÃ¶n Zeit â€¦\nZur Sicherheit speichern wir auch dieses Objekt ab.\n\n# write_rds(rec2_prepped, \"objects/rec2_prepped.rds\")\nrec2_prepped &lt;- read_rds(\"/Users/sebastiansaueruser/datasets/Twitter/hate-classific/rec2_prepped.rds\")\n\nAls nÃ¤chstes kommt das Backen der Vorhersagedaten. Das ist die Stelle, an der zum ersten Mal die neuen Daten (die Vorhersagedaten) ins Spiel kommen.\n\ntic()\nd_predict_baken &lt;-\n  bake(rec2_prepped, new_data = ds)\n\nd_predict_baken$id &lt;- ds$id\ntoc()\nbeepr::beep()\n\nPuh, das Backen dauert - bei groÃŸen DatensÃ¤tzen - gefÃ¼hlt ewig! Daher ist das beepen praktisch: Es klingelt, wenn die Berechnung fertig ist.\nZur Erinnerung: d_predict_baken ist der â€œgebackeneâ€ Testdatensatz. Der Testdatensatz also, auf dem die ganzen Operationen der Vorverarbeitung angewandt wurden.\n\n11.4.2 Git Large File System\nWenn Sie Ihre Arbeit mit einem Versionierungssystem schÃ¼tzen - und Sie sollten es tun - dann verwenden Sie vermutlich Git. Git ist fÃ¼r Textdateien ausgelegt - was bei Quellcode ja auch Sinn macht, und fÃ¼r Quellcode ist Git gemacht. Allerdings will man manchmal auch binÃ¤re Dateien sichern, etwa Daten im RDS-Format. Solche binÃ¤ren Formante funktionieren nicht wirklich aus der Sicht von Git, sie lassen sich nicht zeilenweise nachverfolgen. Kurz gesagt sollte man sie aus diesem Grund nicht in Git nachverfolgen. Eine bequeme LÃ¶sung ist dasLarge File System von Github (git lfs), das diese groÃŸen Dateien auÃŸerhalb des Git-Index verwaltet. Trotzdem sieht es fÃ¼r Nutzis aus wie immer, ist also sehr komfortabel. Dazu ist es nÃ¶tig, git lfs zu installieren.\n\n11.4.3 Metadaten\nMetadaten wieder hinzufÃ¼gen:\n\nd_predict2 &lt;-\n  d_predict_baken %&gt;% \n  left_join(ds, by = \"id\") %&gt;% \n  relocate(dataset, id, author_id, created_at, text, retweet_count, reply_count, quote_count, .after = id) %&gt;% \n  mutate(id = as.integer(id))\n## Warning: There was 1 warning in `mutate()`.\n## â„¹ In argument: `id = as.integer(id)`.\n## Caused by warning:\n## ! NAs introduced by coercion to integer range\n\nLeider mÃ¼ssen wir id in Integer umwandeln, das wir dies im Rezept auch so gemacht hatten. Dabei geht die Spalte kaputt, bzw. die Daten werden NA, da die resultierende Integerzahl zu groÃŸ fÃ¼r R ist. Aber nicht so schlimm: Wir fÃ¼gen sie spÃ¤ter wieder hinzu.\nSpaltennamen mal anschauen:\n\nnames(d_predict2)[1:33]\n##  [1] \"dataset\"                             \"id\"                                 \n##  [3] \"author_id\"                           \"created_at\"                         \n##  [5] \"text\"                                \"retweet_count\"                      \n##  [7] \"reply_count\"                         \"quote_count\"                        \n##  [9] \"profane_n\"                           \"emo_words_n\"                        \n## [11] \"emojis_n\"                            \"wild_emojis_n\"                      \n## [13] \"textfeature_text_copy_n_words\"       \"textfeature_text_copy_n_uq_words\"   \n## [15] \"textfeature_text_copy_n_charS\"       \"textfeature_text_copy_n_uq_charS\"   \n## [17] \"textfeature_text_copy_n_digits\"      \"textfeature_text_copy_n_hashtags\"   \n## [19] \"textfeature_text_copy_n_uq_hashtags\" \"textfeature_text_copy_n_mentions\"   \n## [21] \"textfeature_text_copy_n_uq_mentions\" \"textfeature_text_copy_n_commas\"     \n## [23] \"textfeature_text_copy_n_periods\"     \"textfeature_text_copy_n_exclaims\"   \n## [25] \"textfeature_text_copy_n_extraspaces\" \"textfeature_text_copy_n_caps\"       \n## [27] \"textfeature_text_copy_n_lowers\"      \"textfeature_text_copy_n_urls\"       \n## [29] \"textfeature_text_copy_n_uq_urls\"     \"textfeature_text_copy_n_nonasciis\"  \n## [31] \"textfeature_text_copy_n_puncts\"      \"textfeature_text_copy_politeness\"   \n## [33] \"textfeature_text_copy_first_person\""
  },
  {
    "objectID": "070-hatespeech2.html#vorhersagen",
    "href": "070-hatespeech2.html#vorhersagen",
    "title": "\n11Â  Fallstudie Hatespeech\n",
    "section": "\n11.5 Vorhersagen",
    "text": "11.5 Vorhersagen\nWir beziehen uns auf das Modell von KapitelÂ 10.8.4.\n\nfit3 &lt;- read_rds(\"/Users/sebastiansaueruser/github-repos/datascience-text/objects/chap_classific_fit3.rds\")\n\nfit3_final_train &lt;- read_rds(\"/Users/sebastiansaueruser/datasets/Twitter/hate-classific/fit3_final_train.rds\")\n\nUnd nutzen dann die predict-Methode von tidymodels:\n\ntic()\nd_predicted_values &lt;- predict(fit3_final_train, d_predict2)\ntoc()\nbeep()\n\nPuh, hier ist mein Rechner abgestÃ¼rzt, als ich es mit ca. 2 Millionen Tweets versucht habe!\nBesser, wir probieren erstmal mit einem winzigen Teil der Daten, ob unsere Funktion â€œim Prinzipâ€ oder â€œgrundsÃ¤tzlichâ€ funktioniert:\n\nd_predicted_values_tiny &lt;- predict(fit3_final_train, head(d_predict2))\n## Error:\n## ! Can't convert `data$id` &lt;integer&gt; to match type of `id` &lt;character&gt;.\n\nd_predicted_values_tiny\n## Error in eval(expr, envir, enclos): object 'd_predicted_values_tiny' not found\n\nFunktioniert! Gut! Also weiter.\nPasst!"
  },
  {
    "objectID": "070-hatespeech2.html#ergebnisse",
    "href": "070-hatespeech2.html#ergebnisse",
    "title": "\n11Â  Fallstudie Hatespeech\n",
    "section": "\n11.6 Ergebnisse",
    "text": "11.6 Ergebnisse\n\n11.6.1 Hass-Proxis pro Politiki insgesamt\n\nres_summary1 &lt;- \nd_predict2 %&gt;% \n  group_by(dataset) %&gt;% \n  summarise(emo_words_n_mean = mean(emo_words_n),\n            profane_words_count_mean = mean(profane_n),\n            wild_emojis_n_mean = mean(wild_emojis_n),\n            exclaims_n_mean = mean(textfeature_text_copy_n_exclaims))\n\n\nres_summary1_long &lt;-\n  res_summary1 %&gt;% \n    pivot_longer(-dataset, names_to = \"hate_proxy\", values_to = \"prop\")\n\n\nres_summary1_long %&gt;% \n  ggplot(aes(x = prop, y = hate_proxy)) +\n  geom_col() +\n  facet_wrap(~ dataset)\n\n\n\n\n\n11.6.2 Hass-Proxis pro Politiki im Zeitverlauf\n\nres_summary2 &lt;- \nd_predict2 %&gt;%\n  select(created_at, profane_n, dataset, emo_words_n, wild_emojis_n, textfeature_text_copy_n_exclaims) %&gt;% \n  mutate(month = ymd_hms(created_at) %&gt;% round_date(unit = \"month\")) %&gt;% \n  group_by(month, dataset) %&gt;% \n  summarise(emo_words_n_mean = mean(emo_words_n),\n            profane_words_count_mean = mean(profane_n),\n            wild_emojis_n_mean = mean(wild_emojis_n),\n            exclaims_n_mean = mean(textfeature_text_copy_n_exclaims)) %&gt;% \n  rowwise() %&gt;% \n  mutate(hate_proxy = mean(c_across(emo_words_n_mean:exclaims_n_mean))) %&gt;% \n  ungroup()\n## `summarise()` has grouped output by 'month'. You can override using the\n## `.groups` argument.\n  \nres_summary2 %&gt;% \n  head()\n\n\n\n  \n\n\n\nLangifizieren fÃ¼rs Plotten:\n\nres_summary2_long &lt;- \n  res_summary2 %&gt;% \n  pivot_longer(emo_words_n_mean:hate_proxy)\n\nres_summary2_long %&gt;% \n  head()\n\n\n\n  \n\n\n\n\nres_summary2_long %&gt;% \n  count(month)\n\n\n\n  \n\n\n\n\nres_summary2_long %&gt;% \n  ggplot() +\n  aes(x = month, y = value) +\n  facet_grid(dataset  ~ name) +\n  geom_point() +\n  geom_line(group=1, alpha = .7)\n## Warning: Removed 5 rows containing missing values (`geom_point()`).\n## Warning: Removed 5 rows containing missing values (`geom_line()`)."
  },
  {
    "objectID": "100-nn-quickstart.html#quick-start",
    "href": "100-nn-quickstart.html#quick-start",
    "title": "\n12Â  Einstieg in Neuronale Netze\n",
    "section": "\n12.1 Quick-Start",
    "text": "12.1 Quick-Start\n\n12.1.1 Quick Start mit R\nWir halten uns an das Tutorial von TensforFlow for R, â€œHello, World!â€.\n\n12.1.1.1 Setup\nWir starten die benÃ¶tigten Pakete:\n\nlibrary(keras)  # TensorFlow API\nlibrary(tensorflow)  # TensorFlow pur\nlibrary(tidyverse)  # Datenjudo\nlibrary(tictoc)  # Zeitmessung\n\nDas Installieren von TensorFlow bzw. Keras kann Schwierigkeiten bereiten. Tipp: Stellen Sie in RStudio sicher, dass Sie die richtige Python-Version verwenden.\n\nmnist &lt;- dataset_mnist()\nX_train &lt;- mnist$train$x\nX_test &lt;- mnist$test$x\ny_train &lt;- mnist$train$y\ny_test &lt;- mnist$test$y\n\nIn Kurzform kann man synonym schreiben:\n\nc(c(x_train, y_train), c(x_test, y_test)) %&lt;-% keras::dataset_mnist()\n\n\n12.1.1.2 Visualisieren\nWÃ¤hlen wir ein Bild aus; das schauen wir uns nÃ¤her an Quelle.\n\nimage_id &lt;- 2\nmy_image &lt;- mnist$train$x[image_id, 1:28, 1:28] %&gt;%\n                as_tibble()\n## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n## `.name_repair` is omitted as of tibble 2.0.0.\n## â„¹ Using compatibility `.name_repair`.\n\nmy_image\n\n\n\n  \n\n\n\n\nmy_image_prepared &lt;- \n  my_image |&gt; \n  rownames_to_column(var = 'y') %&gt;% \n  pivot_longer(names_to = \"x\", values_to = \"val\", V1:V28) %&gt;%\n  mutate(x = str_replace(x, 'V', '')) %&gt;% \n  mutate(x = as.numeric(x),\n         y = as.numeric(y)) %&gt;% \n  mutate(y = 28-y) \n\n\nhead(my_image_prepared)\n\n\n\n  \n\n\n\nSo, genug der Vorarbeiten, jetzt plotten:\n\nmy_image_prepared %&gt;%\n  ggplot(aes(x, y))+\n  geom_tile(aes(fill = val + 1))+\n  coord_fixed()+\n  theme_void()+\n  theme(legend.position=\"none\") +\n  scale_fill_viridis_c()\n\n\n\n\n\n12.1.1.3 Neuronales Netz 1\nFÃ¼r unser Netzwerk wollen wir Werte zwischen 0 und 1, daher teilen wir durch den Max-Wert, d.i. 255:\n\nx_train &lt;- X_train / 255\nx_test &lt;-  X_test / 255\n\n\nmodel &lt;- keras_model_sequential(input_shape = c(28, 28)) %&gt;%\n  layer_flatten() %&gt;%\n  layer_dense(128, activation = \"relu\") %&gt;%\n  layer_dropout(0.2) %&gt;%\n  layer_dense(10)\n\nHier ist eine Beschreibung des Modells:\n\nmodel\n## Model: \"sequential\"\n## ________________________________________________________________________________\n##  Layer (type)                       Output Shape                    Param #     \n## ================================================================================\n##  flatten (Flatten)                  (None, 784)                     0           \n##  dense_1 (Dense)                    (None, 128)                     100480      \n##  dropout (Dropout)                  (None, 128)                     0           \n##  dense (Dense)                      (None, 10)                      1290        \n## ================================================================================\n## Total params: 101770 (397.54 KB)\n## Trainable params: 101770 (397.54 KB)\n## Non-trainable params: 0 (0.00 Byte)\n## ________________________________________________________________________________\n\nDann definieren wir eine Fehlerfunktion:\n\nloss_fn &lt;- loss_sparse_categorical_crossentropy(from_logits = TRUE)\n\nBevor wir das Modul trainieren, konfigurieren wir es und kompilieren wir es in Maschinencode:\n\nmodel %&gt;% compile(\n  optimizer = \"adam\",\n  loss = loss_fn,\n  metrics = \"accuracy\"\n)\n\nJetzt ist Trainingszeit, das besorgt die fit-Methode:\n\ntic()\nmodel %&gt;% fit(x_train, y_train, epochs = 5)\n## Epoch 1/5\n## 1875/1875 - 6s - loss: 0.2933 - accuracy: 0.9140 - 6s/epoch - 3ms/step\n## Epoch 2/5\n## 1875/1875 - 5s - loss: 0.1415 - accuracy: 0.9579 - 5s/epoch - 3ms/step\n## Epoch 3/5\n## 1875/1875 - 5s - loss: 0.1065 - accuracy: 0.9680 - 5s/epoch - 3ms/step\n## Epoch 4/5\n## 1875/1875 - 5s - loss: 0.0869 - accuracy: 0.9737 - 5s/epoch - 3ms/step\n## Epoch 5/5\n## 1875/1875 - 5s - loss: 0.0763 - accuracy: 0.9751 - 5s/epoch - 3ms/step\ntoc()\n## 27.897 sec elapsed\n\nDie ModellgÃ¼te Ã¼berprÃ¼fen wir natÃ¼rlich im Test-Set:\n\nmodel %&gt;% evaluate(x_test,  y_test, verbose = 2)\n## 313/313 - 1s - loss: 0.0700 - accuracy: 0.9785 - 646ms/epoch - 2ms/step\n##       loss   accuracy \n## 0.06995549 0.97850001\n\nNicht schlecht: Knapp 98% Trefferquote.\nUnd hier sind die Vorhersagen fÃ¼r die ersten zwei Bilder:\n\npredictions &lt;- predict(model, x_test[1:2, , ])\n## 1/1 - 0s - 92ms/epoch - 92ms/step\n\npredictions\n##           [,1]       [,2]      [,3]      [,4]      [,5]       [,6]       [,7]\n## [1,] -3.905925 -10.611533 -1.400373  4.933351 -16.82393 -8.1285248 -18.969732\n## [2,] -7.280436   5.272494 11.744059 -3.961411 -19.05402 -0.2386339  -2.736672\n##            [,8]      [,9]     [,10]\n## [1,]   9.691772 -4.551620  -1.77038\n## [2,] -20.078672 -0.959926 -14.45026\n\nHm, das sind ja keine Wahrscheinlichkeiten? Stimmt! Es sind Logits. Daher mÃ¼ssen wir noch konvertieren:\n\ntf$nn$softmax(predictions)\n## tf.Tensor(\n## [[1.23274579e-06 1.50891550e-09 1.51015303e-05 8.50593665e-03\n##   3.02450936e-12 1.80726467e-08 3.53789682e-13 9.91466632e-01\n##   6.46325897e-07 1.04310670e-05]\n##  [5.45872712e-09 1.54439918e-03 9.98445655e-01 1.50843240e-07\n##   4.20620588e-14 6.24176197e-06 5.13361230e-07 1.50969060e-14\n##   3.03426854e-06 4.20024149e-12]], shape=(2, 10), dtype=float64)\n\nOb ihr wirklich richtig steht, seht ihr, wenn das Licht angeht:\n\ny_test[1:2]\n## [1] 7 2\n\nSieht gut aus!\n\n12.1.1.4 Best-Bet-Digit\nMÃ¶chte man ein Modell, das gleich die â€œBest-Bet-Digitâ€ nennt, kann man das so machen:\n\npredictions &lt;- model %&gt;%\n  predict(X_test[1:5, , ]) %&gt;%  # nur die ersten 5\n  k_argmax()\n## 1/1 - 0s - 65ms/epoch - 65ms/step\n\npredictions$numpy()\n## [1] 7 2 1 0 4\n\nOder ein eigenes, dazu passendes Modell bauen:\n\nprobability_model &lt;- \n  keras_model_sequential() %&gt;%\n  model() %&gt;%\n  layer_activation_softmax() %&gt;%\n  layer_lambda(tf$argmax)\n\nHier sind die Vorhersagen:\n\nprobability_model(x_test[1:5, , ])\n## tf.Tensor([3 2 1 0 4 2 2 0 2 4], shape=(10), dtype=int64)\n\n\ny_test[1:5]\n## [1] 7 2 1 0 4\n\n\n\n\n\n12.1.2 Quick-Start mit Python und Colab\nAm einfachsten ist der Einstieg mit Google Colab, wo Python voreingestellt ist. Beginnen Sie mit dem [MNIST-Tutorial](https://www.tensorflow.org/tutorials/quickstart/beginner."
  },
  {
    "objectID": "100-nn-quickstart.html#vertiefung",
    "href": "100-nn-quickstart.html#vertiefung",
    "title": "\n12Â  Einstieg in Neuronale Netze\n",
    "section": "\n12.2 Vertiefung",
    "text": "12.2 Vertiefung\nDie TensforFlow-Docs bieten einen guten Einstieg in Keras und TensorFlow."
  },
  {
    "objectID": "100-nn-quickstart.html#fallstudien",
    "href": "100-nn-quickstart.html#fallstudien",
    "title": "\n12Â  Einstieg in Neuronale Netze\n",
    "section": "\n12.3 Fallstudien",
    "text": "12.3 Fallstudien\n\nMNIST einen Schritt weiter\nFashion-MNIST"
  },
  {
    "objectID": "200-projektmgt.html#pipeline-management",
    "href": "200-projektmgt.html#pipeline-management",
    "title": "\n13Â  Projektmanagement\n",
    "section": "\n13.1 Pipeline-Management",
    "text": "13.1 Pipeline-Management\n\n13.1.1 Am Anfang\nSie haben GroÃŸes vor! Naja, zumindest planen Sie ein neues Data-Science-Projekt.\nUnd, schlau wie Sie sind, stÃ¼rzen Sie nicht sofort an die Tastatur, um sich einige Modelle berechnen zu lassen. Nein! Sie denken erst einmal nach. Zum Beispiel, wie die einzelnen Analyseschritte aussehen, worin sie bestehen, und in welcher Abfolge sie zu berechnen sind, s. AbbildungÂ 13.1.\n\n\nAbbildungÂ 13.1: So kÃ¶nnte Ihr Projektplan am Anfang aussehen, man spricht auch von einer Pipeline\n\n\n\n\n\n\n\nHinweis\n\n\n\nDen Graph der einzelnen Analyseschritte in ihrer AbhÃ¤ngigkeit bezeichnet man als *Pipeline.\n\n\n\n13.1.2 Sie trÃ¤umen von einem Werkzeug\nNach einiger Zeit Ã¼berlegen Sie sich, dass Sie ein System brÃ¤uchten, das Ihre Skizze umsetzt in tatsÃ¤chliche Berechnungen. Und zwar suchen Sie ein Projektmanagement-System das folgendes Desiderata erfÃ¼llt:\n\nEs fÃ¼hrt die einzelnen Schritte Ihres Projekt, die â€œPipelineâ€ in der richtigen Reihenfolge\nEs aktualisiert veraltete Objekte, aber es berechnet nicht Modelle neu, die unverÃ¤ndert sind\nEs ist gut zu debuggen\n\nJa, von so einem Werkzeug trÃ¤umen Sie.\nUnd tatsÃ¤chlich, Ihr Traum geht in ErfÃ¼llung. Dieses System existiert. Genau genommen gibt es viele Systeme, die sich anschicken, Ihre WÃ¼nsche zu erfÃ¼llen. Wir schauen uns eines nÃ¤her an, das speziell fÃ¼r R gemacht ist. Das R-Paket targets.\n\n13.1.3 Targets\nEs lohnt sich, an dieser Stelle den â€œWalkthroughâ€ aus dem Benutzerhandbuch von Targets durchzuarbeiten.\nFÃ¼r ein Projekt Ã¤hnlich zu den, die wir in diesem Buch bearbeiten, ist folgende _targets.R-Datei ein guter Start.\n\nlibrary(targets)\n\n\n# Funktionen einlesen:\n#purrr::walk(list.files(path = \"funs\", pattern = \".R\", full.names = TRUE), source)\nsource(\"funs/def-recipe.R\")\nsource(\"funs/read-train-data.R\")\nsource(\"funs/read-test-data.R\")\n\n# Optionen, z.B. allgemein verfÃ¼gbare Pakete in den Targets:tar_option_set(packages = c(\"readr\", \n                            \"dplyr\", \n                            \"ggplot2\", \n                            \"purrr\", \n                            \"easystats\", \n                            \"tidymodels\", \n                            \"textrecipes\"))\n\n# Definition der Pipeline:\nlist(\n  tar_target(data_train, read_train_data()),\n  tar_target(data_test, read_test_data()),\n  tar_target(recipe1, def_recipe(data_train)\n  ),\n  tar_target(model1,\n             logistic_reg(penalty = tune(), mixture = 1) %&gt;%\n               set_mode(\"classification\") %&gt;%\n               set_engine(\"glmnet\")\n             ),\n  tar_target(workflow1,\n             workflow() %&gt;% add_recipe(recipe1) %&gt;% add_model(model1)\n             ),\n  tar_target(grid1,\n             grid_regular(penalty(), levels = 3)\n             ),\n  tar_target(grid_fitted,\n             tune_grid(workflow1, \n                       resamples = vfold_cv(data_train, v = 2),\n                       grid = grid1)\n  ),\n  tar_target(best_hyperparams,\n             select_by_one_std_err(grid_fitted, metric = \"roc_auc\", penalty)\n             ),\n  tar_target(fit1,\n             workflow1 %&gt;% finalize_workflow(best_hyperparams) %&gt;% fit(data_train)),\n  tar_target(preds,\n             fit1 %&gt;% \n               predict(data_test) %&gt;% \n               bind_cols(data_test) %&gt;% \n               mutate(c1 = factor(c1))),\n  tar_target(metrics1,\n             preds %&gt;% metrics(truth = c1, .pred_class))\n)\n\nDann kann man auf den Play-Button drÃ¼cken und die ganze Pipeline wird berechnet:\n\ntar_make()\n\nWenn die Pipeline aktuell ist, und nichts berechnet werden muss (und daher auch schon fehlerfrei durchgelaufen ist), sieht die Ausgabe so aus:\nâœ” skip target grid1\nâœ” skip target model1\nâœ” skip target data_train\nâœ” skip target data_test\nâœ” skip target recipe1\nâœ” skip target workflow1\nâœ” skip target grid_fitted\nâœ” skip target best_hyperparams\nâœ” skip target fit1\nâœ” skip target preds\nâœ” skip target metrics1\nâœ” skip pipeline [0.121 seconds]\nDie Pipeline kann man sich als DAG bzw. als AbhÃ¤ngigkeitsgraph visualisieren lassen:\n\ntar_visnetwork()\n\n\n\nAbhÃ¤ngigkeitsgraph der Pipeline\n\nEinzelne Objekte kann man sich komfortabel anschauen mit tar_load(objekt), z.B. tar_load(fit1) usw.\n\n13.1.4 Eine Pipeline als Spielwiese\nDieses Github-Repo stellt Ihnen eine â€œSpielwieseâ€ zur VerfÃ¼gung, wo Sie sich mit Pipleines Ã  la Targets vertraut machen kÃ¶nnen."
  },
  {
    "objectID": "200-projektmgt.html#zeit-sparen",
    "href": "200-projektmgt.html#zeit-sparen",
    "title": "\n13Â  Projektmanagement\n",
    "section": "\n13.2 Zeit sparen",
    "text": "13.2 Zeit sparen\nEiner Maschine etwas beizubringen kann dauern â€¦ Ein einfaches Rechenbeispiel dazu:\n\nSie haben eine Kreuzvalidierung mit 10 Faltungen\nund 3 Wiederholungen\nund 3 Tuningparameter\nmit je 10 Werten\n\nDas sind 1033*10=900 Wiederholungen.\nLeider haben Sie noch in den ersten 10 Versuchen jeweils einen Bug, so dass sich die Rechenzeit noch einmal um den Faktor 10 erhÃ¶htâ€¦\nDie Rechenzeit kann also schnell ins astronomische steigen. Es braucht also Methoden, um Rechenzeit zu sparen.1 Einige Methoden zum Rechenzeit sparen sind:\n\n\nCloud: Cloud-Dienste in Anspruch nehmen (faktisch mietet man damit schnelle Rechner)\n\nParallelisierung: Mehrere Kerne des eigenen Computers nutzen\n\nUpgrade: Kaufen Sie sich einen schnelleren Rechnerâ€¦\n\nCleveres Grid-Search: Methoden wie ANOVA Racing kÃ¶nnen die Rechenzeit - was das Tuning - betrifft - deutlich verringern.\n\nDieser Post gibt einen Ãœberblick zu Rechenzeiten bei verschiedenen Tuningparameter-Optionen mit Tidymodels.\nNatÃ¼rlich ist die (mit Abstand) beste Methode: guten Code schreiben. Denn â€œguter Codeâ€ verringert die Wahrscheinlichkeit von Bugs, und damit die Gefahr, dass die ganze schÃ¶ne Rechenzeit fÃ¼r die Katz war.\nâ€œGuter Codeâ€ ist vielleicht primÃ¤r von zwei Dingen abhÃ¤ngig: erstens einen guten Plan zu haben bevor man das Programmieren anfÃ¤ngt und zweitens gute Methoden des Projektmanagements. Hunt und Thomas (2000) prÃ¤sentieren eine weithin anerkannte Umsetzung, was â€œguterâ€ Code bedeuten kÃ¶nnte."
  },
  {
    "objectID": "200-projektmgt.html#publizieren",
    "href": "200-projektmgt.html#publizieren",
    "title": "\n13Â  Projektmanagement\n",
    "section": "\n13.3 Publizieren",
    "text": "13.3 Publizieren\nSie haben eine super Analyse geschrieben, eine schicke Pipeline, und jetzt soll die Welt davon erfahren? Es gibt einige komfortable MÃ¶glichkeiten, Ihre Arbeit zu publizieren, z.B. als Blog mit Quarto.\nDieses Video zeigt Ihnen wie man einen Quarto-Blog in RStudio erstellt und ihn bei Netlify publiziert.\n\nDas Hosten bzw. Deployen bei Netlify ist kostenlos (in der Basis-Variante).\nSie kÃ¶nnen alternativ Github Pages als Hosting-Dienst verwenden. Dieses Video gibt dazu eine Anleitung."
  },
  {
    "objectID": "200-projektmgt.html#komplexitÃ¤tsmanagement",
    "href": "200-projektmgt.html#komplexitÃ¤tsmanagement",
    "title": "\n13Â  Projektmanagement\n",
    "section": "\n13.4 KomplexitÃ¤tsmanagement",
    "text": "13.4 KomplexitÃ¤tsmanagement\nProgrammieren ist faszinierend. Vor allem, wenn das Programm funktioniert. Genau genommen ist es eigentlich nur dann faszinierend, ansonsten wird es anstrengend? aufregend? sÃ¼chtig? faszinierend? nervig? Wie auch immer: Bugs treten auf und mit steigender KomplexitÃ¤t Ihrer Software steigen die Bugs nicht linear, sondern eher quadratisch oder gar exponentiell an.\nEs gibt viele AnsÃ¤tze, sich gegen die KomplexitÃ¤t zu â€œwehrenâ€. Der beste ist vielleicht: Die Software so einfach wie mÃ¶glich zu halten - und nur so komplex wie nÃ¶tig. Sozusagen: Das beste Feature ist das, das Sie nicht implementieren.\n\n13.4.1 Geben Sie gute Namen\nDaraus leitet sich ab, dass die zentralen Methoden, um der Fehler Herr zu werden im KomplexitÃ¤tsmanagement liegen. Den Variablen (Objekten) gute, â€œsprechendeâ€ aber prÃ¤gnante Namen zu geben, ist in diesem Lichte auch als KomplexitÃ¤tsmanagement (Reduktion) zu verstehen.\nEin typischer Fehler, der mir immer mal wieder passiert, ist: Ich Ã¤ndere den Namen eines Objekts, aber vergesse, an allen Stellen im Code den Namen anzupassen. GlÃ¼cklicherweise gibt es hier eine einfache Abhilfe: Replace-All.\n\n13.4.2 Portionieren\nEine andere, zentrale MaÃŸnahme ist es, den Code in handlichen â€œHÃ¤ppchenâ€ zu verpacken. Statt einer Skriptdatei mit zich Tausend Zeilen, wÃ¼nschen Sie sich doch sicher ein Skript der Art:\nmache_1()\nmache_2()\nmache_3()\ngratuliere_fertig()\nSchaut man dann in mache_1() rein, sieht man wiederum Ã¼bersichtlichen Code.\nFunktionales Programmieren ist eine Umsetzung davon: Jedes HÃ¤ppchen, jeder Schritt ist eine Funktion. Eine Funktion hat Input und Output; der Output ist dann der Input fÃ¼r die Funktion des nÃ¤chsten Schrittes. targets ist eine Umsetzung dieser Idee.\n\n13.4.3 Debugging mit einem Logger\nWenn das Kind in dem Brunnen gefallen ist, hilft nur Heulen und Wehklagen Das Problem finden und lÃ¶sen. Mit einem Logger kann man sich das Entwanzen, das Finden der Fehler, erleichtern. Ein Logger schreibt Zwischenschritte in eine Log-Datei.\nHier ist ein Beispiel mit dem futile Logger:. Mein Problem war, dass ich eine dynamische Aufgabe fÃ¼r eine Statistik-Klausur programmiert hatte, aber leider gab es einen Bug, den ich nicht gefunden habe2.\nDie LÃ¶sung brachte ein Logger, mit dem ich den Wert zentraler Variablen im Verlauf des Durchlaufens des Codes - bis eben der Laufzeitfehler aufkam3.\nHier ist ein Ausschnitt der Syntax. Zuerst initialisiert man den Logger mit einer Datei, hier exams.log. Neue Logging-Inhalte sollen an die bestehenden Logs angehÃ¤ngt werden (appender).\n\nlibrary(futile.logger)\nflog.appender(appender.file(\"/Users/sebastiansaueruser/github-repos/rexams-exams/exams.log\"))\n\nDann gebe ich eine Loggings vom Typ â€œInfoâ€ zum Protokoll:\n\nflog.info(paste0(\"Ex: post-uncertainty1\"))\nflog.info(msg = paste0(\"Data set: \", d_name))\nflog.info(paste0(\"Preds chosen: \", stringr::str_c(preds_chosen, collapse = \", \")))\nflog.info(paste0(\"Output var: \", av))\n\nDie Ergebnisse kann man dann in der Logging-Datei anschauen:\nNFO [2023-01-05 11:27:51] Rhats: 1.004503053029\nINFO [2023-01-05 11:27:51] Sol: 0.18\nINFO [2023-01-05 11:27:51] Sol typeof: double\nINFO [2023-01-05 11:27:52] Ex: post-uncertainty1\nINFO [2023-01-05 11:27:52] Data set: tips\nINFO [2023-01-05 11:27:52] Preds chosen: size, total_bill\nINFO [2023-01-05 11:27:52] Output var: tip\nINFO [2023-01-05 11:27:53] Rhats: 0.999004883794722\nINFO [2023-01-05 11:27:53] Rhats: 1.00021605674421\nINFO [2023-01-05 11:27:53] Rhats: 1.00091357638756\nINFO [2023-01-05 11:27:53] Sol: 0.32\nINFO [2023-01-05 11:27:53] Sol typeof: double\nINFO [2023-01-05 11:27:54] Ex: post-uncertainty1\nINFO [2023-01-05 11:27:54] Data set: TeachingRatings\nINFO [2023-01-05 11:27:54] Preds chosen: prof, beauty\nINFO [2023-01-05 11:27:54] Output var: eval\nINFO [2023-01-05 11:27:55] Rhats: 0.999060308710712\nINFO [2023-01-05 11:27:55] Rhats: 0.999032305267221\nINFO [2023-01-05 11:27:55] Rhats: 0.999229003550072\nINFO [2023-01-05 11:27:55] Sol: 0\nINFO [2023-01-05 11:27:55] Sol typeof: double\nINFO [2023-01-05 11:27:56] Ex: post-uncertainty1\nINFO [2023-01-05 11:27:56] Data set: gtcars\nINFO [2023-01-05 11:27:56] Preds chosen: mpg_c, year\nINFO [2023-01-05 11:27:56] Output var: msrp\nINFO [2023-01-05 11:28:00] Rhats: 0.99913061005524\nINFO [2023-01-05 11:28:00] Rhats: 0.998999786100339\nINFO [2023-01-05 11:28:00] Rhats: 0.999130286784586\nINFO [2023-01-05 11:28:01] Sol: 21959.35\nINFO [2023-01-05 11:28:01] Sol typeof: double\nJa, das sieht nicht schÃ¶n aus. Aber es brachte mir die LÃ¶sung: Mir fiel auf, dass der Fehler nur auftrat, wenn sol einen groÃŸen Wert hatte (1000 oder mehr). Danke, Logger!\n\n\n\n\nHunt, Andrew, und David Thomas. 2000. The Pragmatic Programmer from Journeyman to Master. Reading, Mass.: Addison-Wesley."
  },
  {
    "objectID": "200-projektmgt.html#footnotes",
    "href": "200-projektmgt.html#footnotes",
    "title": "\n13Â  Projektmanagement\n",
    "section": "",
    "text": "Allerdings haben lange Rechenzeiten auch Vorteile, wie dieses XKCD-Cartoon zeigt.â†©ï¸\nStackOverflow hat mich dann gerettetâ†©ï¸\nERROR!â†©ï¸"
  },
  {
    "objectID": "300-fallstudien.html#quellen-fÃ¼r-textdaten",
    "href": "300-fallstudien.html#quellen-fÃ¼r-textdaten",
    "title": "14Â  Fallstudien",
    "section": "14.1 Quellen fÃ¼r Textdaten",
    "text": "14.1 Quellen fÃ¼r Textdaten\nDer MonkeyLearn Blog liefert eine Reihe von Quellen zu API, die Textdaten bereitstellen."
  }
]