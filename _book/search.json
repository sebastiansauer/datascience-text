[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "datascience2: Prädiktive Modelle auf Basis von Textdaten",
    "section": "",
    "text": "falls Sie die Pakete schon installiert haben, könnten Sie mal in RStudio auf “update.packages” klicken↩︎"
  },
  {
    "objectID": "pruefung.html",
    "href": "pruefung.html",
    "title": "1  Prüfung",
    "section": "",
    "text": "Text als Datenbasis prädiktiver Modelle\nBild von mcmurryjulie auf Pixabay"
  },
  {
    "objectID": "pruefung.html#prüfungsform-datenanalyse-als-quarto-blog-post",
    "href": "pruefung.html#prüfungsform-datenanalyse-als-quarto-blog-post",
    "title": "1  Prüfung",
    "section": "1.1 Prüfungsform: Datenanalyse als Quarto-Blog-Post",
    "text": "1.1 Prüfungsform: Datenanalyse als Quarto-Blog-Post\nAls Prüfungsleistung ist ein Corpus an Twitter-Daten deutscher aktueller Politiker auf Hate Speech hin zu untersuchen.\n\nDer Bericht der Analyse ist als Quarto Blog-Posts zu formatieren.\nEinzureichen ist die URL des Posts.\nDer Post muss während des gesamten Prüfungszeitraums online sein, gehostet von einem beliebigen Provider (z.B. Netlify oder Github).\nNach Einreichen des Posts dürfen keine Änderungen mehr vorgenommen werden.\nZu Dokumentationszwecken soll ein PDF-Print des Posts in die Abgabe mit hochgeladen werden. Das PDF-Print des Posts muss identisch (exakt gleich) sein zum Post, der über die URL verfügbar ist.\nDer Quelltext des Posts soll bei Github vorliegen.\n\n\n\nAlle folgenden Hinweise gelten nur insoweit Ihre Lehrkraft Ihnen keine anders lautenden Hinweise gegeben hat (schriftlich).\n\n1.2 Allgemeines\n\nGegenstand dieser Prüfungsform ist die Analyse eines Datensatzes nach einer Forschungsfrage und die Dokumentation dieser Analyse.\nSchreiben Sie Ihre Datenanalyse in Form eines Berichts, der sich an den Gliederungspunkten wie unten dargestellt orientiert.\nWenden Sie die passenden, im Unterricht eingeführten, statistischen Verfahren an. Es steht Ihnen frei, andere (nicht im Unterricht behandelte) Verfahren zur Analyse der Daten anzuwenden, nach Maßgabe der fachlichen Angemessenheit.\nWerten Sie die Daten mit R aus.\nDie R-Syntax soll im Hauptteil des Berichts dokumentiert werden. R-Output darf ggf. gekürzt wiedergegeben werden.\nFügen Sie keine Erklärungen oder Definitionen von statistischen Verfahren an.\nBeschreiben und interpretieren Sie jede Analyse bzw. jeden R-Code bzw. jedes Ergebnis (jede R-Ausgabe).\nVon hoher Bedeutung ist die Korrektheit der Beschreibung und Interpretation der statistischen Modellierung (z.B. mit der Regressionsanalyse).\nEs hat keinen Einfluss auf Ihre Note, ob sich ein (erwarteter) Effekt zeigt und wie stark dieser Effekt ggf. ist.\n\n\n\n1.3 Formatierung des Berichts\n\nDer Bericht ist nur elektronisch, nicht ausgedruckt einzureichen.\nDer Bericht kann als paginiertes Dokument oder als nicht-paginiertes Dokument (d.h. als HTML-Dokument) eingereicht werden.\nPaginierte Formate sind als PDF-Datei einzureichen. HTML-Formate sind entweder in HTML-Form oder (nur bei Bedarf) als PDF-Druck eines HTML-Dokuments einzureichen.\nDie Wahl eines bestimmten Stylesheets ist nicht von Bedeutung. Lesbarkeit und Übersichtlichkeit in der Formatierung sind unabhängig davon anzustreben.\nIm Kopfbereich (oder auf einem Deckblatt) sind die relevanten Metadatan anzugeben wie Name (Nachname, Vorname) der Autorin/des Autors, Abgabedatum, Titel der Arbeit, Modul.\n\n\n\n1.4 Formalia\n\nRichtlinien einer Wortzahl gibt es nicht. Entscheidend ist, dass relevante Analysen durchgeführt und beschrieben wurden.\nDer Anspruch richtet sich nach dem Inhalt und Niveau der auf dieses Modul vorbereitenden Module. Oft sind das Module in quantitativer Datenanalyse (und wissenschaftliches Arbeiten). Deren Inhalte sollen im Rahmen dieser Prüfungsleistung als selbständig und flüssig verfügbare Kompetenz von den Studentis demonstriert werden.\nDie Gliederung der Arbeit kann sich an den PPDAC-Zyklus und am Data Science Model von Wickham und Grolemund orientieren.\n\n\n\n1.5 Beurteilungskriterien\nDie Arbeit wird im Hinblick auf drei Kriterien bewertet:\n\nFormalia (z. B. Vollständigkeit der Abarbeitung, Angemessenheit der äußeren Gestaltung, Fokus auf Wesentliches)\nMethodik (z. B. Richtige Auswahl und Anwendung der Verfahren)\nInhalt (z. B. Verständlichkeit, Breite und Tiefe der Problemlösung, Korrektheit der Interpretation)\n\nSie erhalten für jedes der drei Kriterien eine Teilnote sowie eine Gesamtnote. Außerdem erhalten Sie ggf. für die Kriterien noch ausformulierte Hinweise.\nDie Gesamtnote muss sich nicht als Mittelwert der Teilnoten ergeben.\nInsbesondere kann eine Fünf in einem der Kriterien zum Durchfallen führen, auch wenn die anderen beiden Kriterien gut oder sehr gut beurteilt wurden.\n\n\n1.6 Beispiele für Aspekte der Beurteilungskriterien\n\nWurden deskriptive Statistiken (an angemessenen Ort) berichtet?\nWurden Diagramme und Tabellen angemessen eingesetzt?\nWurde Inferenzstatistik (angemessen) eingesetzt?\nWurden Effektstärkemaße (idealerweise mit Konfidenzintervallen dazu) berichtet?\nWurden alle relevanten Informationen für ein statistisches Verfahren angegeben (z.B. zum gewählten Prior)?\nWurde die Aussagekraft von Modellergebnissen richtig eingeschätzt?\nWaren die Schlussfolgerungen, die aus den statistischen Ergebnissen gezogen wurden, angemessen (z. B. wurde erkannt, dass ein Nicht-Verwerfeen einer Hypothese nicht automatisch ein Bestätigen derselben bedeutet)?\nWurde angemessen gerundet (inkl. konsistente Anzahl von Nachkommastellen)?\nPassen die statistischen Verfahren zu den Hypothesen?\nWurden die Voraussetzungen der statistischen Verfahren geprüft?\nSind die Ergebnisse reproduzierbar (Daten und Syntax eingereicht)?\n\n\n\n1.7 Beispiele für Fehler\nSchwere Fehler, die zum Durchfallen oder deutlichem Abwerten der Note führen können, sind z.B.:\n\nfehlende Inferenzstatistik (oder adäquatem Ersatz)\nfalsche Interpretation von Posteriori-Verteilungen oder p-Werten\nkeine Angabe von Konfidenzintervallen\nfalsche Interpretation von Konfidenzintervallen\nWahl des falschen Intervalls (Vorhersageintervall vs. Perzentilintervall vs. HDI)\nfalsche Entscheidung zum Hypothesentest auf Basis entsprechender Kennwerte (wie ROPE-Wahrscheinlichkeit oder p-Wert)\nfalsche Wahl des statistischen Verfahrens\nfehlende Deskriptivstatistik\n\nHäufige kleinere Mängel sind z. B.\n\npixelige Abbildungen\nR-Ausgaben oder R-Syntax als Screenshot\nfehlende Seitenzahlen (nur bei paginierten Formaten, nicht bei HTML)\nunübersichtliche Diagramme\nkein (verlinktes) Inhaltsverzeichnis ︎\nfehlende oder unverständliche Achsenbeschriftung bei Diagrammen\nfehlende oder falsche Beschreibung der/des Skalenniveau(s) der untersuchten Variablen"
  },
  {
    "objectID": "pruefung.html#allgemeines",
    "href": "pruefung.html#allgemeines",
    "title": "1  Prüfung",
    "section": "1.2 Allgemeines",
    "text": "1.2 Allgemeines\n\nGegenstand dieser Prüfungsform ist die Analyse eines Datensatzes nach einer Forschungsfrage und die Dokumentation dieser Analyse.\nSchreiben Sie Ihre Datenanalyse in Form eines Berichts, der sich an den Gliederungspunkten wie unten dargestellt orientiert.\nWenden Sie die passenden, im Unterricht eingeführten, statistischen Verfahren an. Es steht Ihnen frei, andere (nicht im Unterricht behandelte) Verfahren zur Analyse der Daten anzuwenden, nach Maßgabe der fachlichen Angemessenheit.\nWerten Sie die Daten mit R aus.\nDie R-Syntax soll im Hauptteil des Berichts dokumentiert werden. R-Output darf ggf. gekürzt wiedergegeben werden.\nFügen Sie keine Erklärungen oder Definitionen von statistischen Verfahren an.\nBeschreiben und interpretieren Sie jede Analyse bzw. jeden R-Code bzw. jedes Ergebnis (jede R-Ausgabe).\nVon hoher Bedeutung ist die Korrektheit der Beschreibung und Interpretation der statistischen Modellierung (z.B. mit der Regressionsanalyse).\nEs hat keinen Einfluss auf Ihre Note, ob sich ein (erwarteter) Effekt zeigt und wie stark dieser Effekt ggf. ist."
  },
  {
    "objectID": "pruefung.html#formatierung-des-berichts",
    "href": "pruefung.html#formatierung-des-berichts",
    "title": "1  Prüfung",
    "section": "1.3 Formatierung des Berichts",
    "text": "1.3 Formatierung des Berichts\n\nDer Bericht ist nur elektronisch, nicht ausgedruckt einzureichen.\nDer Bericht kann als paginiertes Dokument oder als nicht-paginiertes Dokument (d.h. als HTML-Dokument) eingereicht werden.\nPaginierte Formate sind als PDF-Datei einzureichen. HTML-Formate sind entweder in HTML-Form oder (nur bei Bedarf) als PDF-Druck eines HTML-Dokuments einzureichen.\nDie Wahl eines bestimmten Stylesheets ist nicht von Bedeutung. Lesbarkeit und Übersichtlichkeit in der Formatierung sind unabhängig davon anzustreben.\nIm Kopfbereich (oder auf einem Deckblatt) sind die relevanten Metadatan anzugeben wie Name (Nachname, Vorname) der Autorin/des Autors, Abgabedatum, Titel der Arbeit, Modul."
  },
  {
    "objectID": "pruefung.html#formalia",
    "href": "pruefung.html#formalia",
    "title": "1  Prüfung",
    "section": "1.4 Formalia",
    "text": "1.4 Formalia\n\nRichtlinien einer Wortzahl gibt es nicht. Entscheidend ist, dass relevante Analysen durchgeführt und beschrieben wurden.\nDer Anspruch richtet sich nach dem Inhalt und Niveau der auf dieses Modul vorbereitenden Module. Oft sind das Module in quantitativer Datenanalyse (und wissenschaftliches Arbeiten). Deren Inhalte sollen im Rahmen dieser Prüfungsleistung als selbständig und flüssig verfügbare Kompetenz von den Studentis demonstriert werden.\nDie Gliederung der Arbeit kann sich an den PPDAC-Zyklus und am Data Science Model von Wickham und Grolemund orientieren."
  },
  {
    "objectID": "pruefung.html#beurteilungskriterien",
    "href": "pruefung.html#beurteilungskriterien",
    "title": "1  Prüfung",
    "section": "1.5 Beurteilungskriterien",
    "text": "1.5 Beurteilungskriterien\nDie Arbeit wird im Hinblick auf drei Kriterien bewertet:\n\nFormalia (z. B. Vollständigkeit der Abarbeitung, Angemessenheit der äußeren Gestaltung, Fokus auf Wesentliches)\nMethodik (z. B. Richtige Auswahl und Anwendung der Verfahren)\nInhalt (z. B. Verständlichkeit, Breite und Tiefe der Problemlösung, Korrektheit der Interpretation)\n\nSie erhalten für jedes der drei Kriterien eine Teilnote sowie eine Gesamtnote. Außerdem erhalten Sie ggf. für die Kriterien noch ausformulierte Hinweise.\nDie Gesamtnote muss sich nicht als Mittelwert der Teilnoten ergeben.\nInsbesondere kann eine Fünf in einem der Kriterien zum Durchfallen führen, auch wenn die anderen beiden Kriterien gut oder sehr gut beurteilt wurden."
  },
  {
    "objectID": "pruefung.html#beispiele-für-aspekte-der-beurteilungskriterien",
    "href": "pruefung.html#beispiele-für-aspekte-der-beurteilungskriterien",
    "title": "1  Prüfung",
    "section": "1.6 Beispiele für Aspekte der Beurteilungskriterien",
    "text": "1.6 Beispiele für Aspekte der Beurteilungskriterien\n\nWurden deskriptive Statistiken (an angemessenen Ort) berichtet?\nWurden Diagramme und Tabellen angemessen eingesetzt?\nWurde Inferenzstatistik (angemessen) eingesetzt?\nWurden Effektstärkemaße (idealerweise mit Konfidenzintervallen dazu) berichtet?\nWurden alle relevanten Informationen für ein statistisches Verfahren angegeben (z.B. zum gewählten Prior)?\nWurde die Aussagekraft von Modellergebnissen richtig eingeschätzt?\nWaren die Schlussfolgerungen, die aus den statistischen Ergebnissen gezogen wurden, angemessen (z. B. wurde erkannt, dass ein Nicht-Verwerfeen einer Hypothese nicht automatisch ein Bestätigen derselben bedeutet)?\nWurde angemessen gerundet (inkl. konsistente Anzahl von Nachkommastellen)?\nPassen die statistischen Verfahren zu den Hypothesen?\nWurden die Voraussetzungen der statistischen Verfahren geprüft?\nSind die Ergebnisse reproduzierbar (Daten und Syntax eingereicht)?"
  },
  {
    "objectID": "pruefung.html#beispiele-für-fehler",
    "href": "pruefung.html#beispiele-für-fehler",
    "title": "1  Prüfung",
    "section": "1.7 Beispiele für Fehler",
    "text": "1.7 Beispiele für Fehler\nSchwere Fehler, die zum Durchfallen oder deutlichem Abwerten der Note führen können, sind z.B.:\n\nfehlende Inferenzstatistik (oder adäquatem Ersatz)\nfalsche Interpretation von Posteriori-Verteilungen oder p-Werten\nkeine Angabe von Konfidenzintervallen\nfalsche Interpretation von Konfidenzintervallen\nWahl des falschen Intervalls (Vorhersageintervall vs. Perzentilintervall vs. HDI)\nfalsche Entscheidung zum Hypothesentest auf Basis entsprechender Kennwerte (wie ROPE-Wahrscheinlichkeit oder p-Wert)\nfalsche Wahl des statistischen Verfahrens\nfehlende Deskriptivstatistik\n\nHäufige kleinere Mängel sind z. B.\n\npixelige Abbildungen\nR-Ausgaben oder R-Syntax als Screenshot\nfehlende Seitenzahlen (nur bei paginierten Formaten, nicht bei HTML)\nunübersichtliche Diagramme\nkein (verlinktes) Inhaltsverzeichnis ︎\nfehlende oder unverständliche Achsenbeschriftung bei Diagrammen\nfehlende oder falsche Beschreibung der/des Skalenniveau(s) der untersuchten Variablen"
  },
  {
    "objectID": "twittermining.html",
    "href": "twittermining.html",
    "title": "2  Twitter Mining",
    "section": "",
    "text": "Text als Datenbasis prädiktiver Modelle\nBild von mcmurryjulie auf Pixabay"
  },
  {
    "objectID": "twittermining.html#vorab",
    "href": "twittermining.html#vorab",
    "title": "2  Twitter Mining",
    "section": "2.1 Vorab",
    "text": "2.1 Vorab\n\n2.1.1 Lernziele\n\nTwitterdaten via API von Twitter auslesen\n\n\n\n2.1.2 Vorbereitung\n\nLesen Sie in Hvitfeldt und Silge (2022) Kap. 1.\nLegen Sie sich ein Konto bei Github an.\nLegen Sie sich ein Konto bei Twitter an.\nLesen Sie diesen Artikel zur Anmeldung bei der Twitter API1\n\n\n\n2.1.3 Benötigte R-Pakete\n\nlibrary(tidyverse)\nlibrary(rtweet)\n\n\n\n\nR-Paket {rtweet}\n\n\nEinen Überblick über die Funktionen des Pakets (function reference) findet sich hier."
  },
  {
    "objectID": "twittermining.html#anmelden-bei-twitter",
    "href": "twittermining.html#anmelden-bei-twitter",
    "title": "2  Twitter Mining",
    "section": "2.2 Anmelden bei Twitter",
    "text": "2.2 Anmelden bei Twitter\n\n2.2.1 Welche Accounts interessieren uns?\nHier ist eine (subjektive) Auswahl von deutschen Politikern, die einen Startpunkt gibt zur Analyse von Art und Ausmaß von Hate Speech gerichtet an deutsche Politiker:innen.\n\nd_path <- \"data/twitter-german-politicians.csv\"\n\nd <- read_csv(d_path)\nd\n\n\n\n\n\n\n\n\n\n\n\nname\nparty\nscreenname\ncomment\n\n\n\n\nKarl Lauterbach\nSPD\nKarl_Lauterbach\nNA\n\n\nOlaf Scholz\nSPD\nOlafScholz\nNA\n\n\nAnnalena Baerback\nGruene\nABaerbock\nNA\n\n\nBundesministerium für Wirtschaft und Klimaschutz\nGruene\nBMWK\nRobert Habeck ist der Minister im BMWK\n\n\nFriedrich Merz\nCDU\n_FriedrichMerz\nCDU-Chef\n\n\nMarkus Söder\nCSU\nMarkus_Soeder\nCSU-Chef\n\n\nCem Özdemir\nGruene\ncem_oezdemir\nBMEL\n\n\nJanine Wissler\nLinke\nJanine_Wissler\nLinke-Chefin\n\n\nMartin Schirdewan\nLinke\nschirdewan\nLinke-Chef\n\n\nChristian Lindner\nFDP\nc_lindner\nFDP-Chef\n\n\nMarie-Agnes Strack-Zimmermann\nFDP\nMAStrackZi\nVorsitzende Verteidigungsausschuss\n\n\nTino Chrupalla\nAFD\nTino_Chrupalla\nAFD-Bundessprecher\n\n\nAlice Weidel\nAFD\nAlice_Weidel\nAFD-Bundessprecherin\n\n\n\n\n\n\n\n\n2.2.2 Twitter App erstellen\nTutorial\n\n\n2.2.3 Intro\nDie Seite von rtweet gibt eine gute Starthilfe in die Benutzung des Pakets.\n\n\n2.2.4 Zugangsdaten\nZugangsdaten sollte man geschützt speichern, also z.B. nicht in einem geteilten Ordner.\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\n\nAnmelden:\n\n#auth_setup_default() \n\nauth <- rtweet_bot(api_key = api_key,\n                   api_secret = api_secret,\n                   access_token = access_token,\n                   access_secret = access_secret)"
  },
  {
    "objectID": "twittermining.html#tweets-einlesen",
    "href": "twittermining.html#tweets-einlesen",
    "title": "2  Twitter Mining",
    "section": "2.3 Tweets einlesen",
    "text": "2.3 Tweets einlesen\nZu beachten ist, dass es Limits gibt, wie viele Informationen (pro Zeiteinheit) man über die Twitter-API auslesen darf. Informationen dazu findet man z.B. hier oder auch mit rate_limit().\nEin gängiges Limit der Twitter-API sind 900 Anfragen (z.B. Tweets auslesen) pro 15 Minuten.\n\n2.3.1 Timeline einlesen einzelner Accounts\nMal ein paar Tweets zur Probe:\n\nsesa_test <- get_timeline(user = \"sauer_sebastian\", n = 3) %>% \n  select(full_text)\n\n\n\nRT @pia_lamberty: Ein Ansatz, der sich beim Debunking wissenschaftlich als erfolgreich herausgestellt hat, ist das sog. Faktensandwich: htt…\nRT @ianbremmer: sure, it’s the hottest summer europe has ever had in history \n\nbut look at the upside\n\nit’s one of the coolest summers euro…\nRT @twisteddoodles: Balanced news reporting https://t.co/O1iiItEQrs\n\n\n\ntweets <- get_timeline(user = d$screenname)\nsaveRDS(tweets, file = \"tweets/tweets01.rds\")\n\nMichael Kearney rät uns:\n\nPRO TIP #4: (for developer accounts only) Use bearer_token() to increase rate limit to 45,000 per fifteen minutes.\n\n\n\n2.3.2 Retweets einlesen\n\noptions(rtweet.retryonratelimit = TRUE)\n\n\ntweets01_retweets <- \n  tweets$id_str %>% \n  head(3) %>% \n  map_dfr( ~ get_retweets(status_id = .x, retryonratelimit = TRUE))\n\nDa die meisten Retweets aber nix sagen, sondern nur auf das einen Tweet wiederholen, ist das Sammeln der Retweets ziemlich langweilig.\n\n\n2.3.3 EPINetz Twitter Politicians 2021\nKönig u. a. (2022) Volltext hier haben einen Datensatz mit knapp 2500 Twitter Accounts deutscher Politiker zusammengestellt, zum Stand 2021.\nDer Datensatz kann über Gesis bezogen werden.\nAuf der gleichen Seite findet sich auch eine Dokumentation des Vorgehens.\nNachdem wir den Datensatz heruntergeladen haben, können wir ihn einlesen:\n\npoliticians_path <- \"data/EPINetz_TwitterPoliticians_2021.RDs\"\npoliticians_twitter <- read_rds(politicians_path)\n\nhead(politicians_twitter)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nofficial_name\nparty\nregion\ninstitution\noffice\nuser_id\ntwitter_name\ntwitter_handle\nfrom\nuntil\nyear_of_birth\nabgeordnetenwatch_id\ngender\nwikidata_id\n\n\n\n\n535\nManja Schüle\nSPD\nBrandenburg\nState Parliament\nParliamentarian\n827090742162100224\nManja Schüle\nManjaSchuele\n2019-09-25\nNA\n1976\n146790\nfemale\nQ40974942\n\n\n962\nPetra Pau\nDIE LINKE\nFederal\nFederal Parliament\nParliamentarian\n1683845126\nTeam PetraPau\nTeamPetraPau\n2017-10-24\nNA\n1963\n79091\nfemale\nQ77195\n\n\n864\nDagmar Schmidt\nSPD\nFederal\nFederal Parliament\nParliamentarian\n1377117206\nTeam #dieschmidt\nTeamDieSchmidt\n2017-10-24\nNA\n1973\n79036\nfemale\nQ15433815\n\n\n2517\nBernd Buchholz\nFDP\nSchleswig-Holstein\nState Parliament\nParliamentarian\n1073605033\nBernd Buchholz\nBerndBuchholz\n2017-06-06\nNA\n1961\n121092\nmale\nQ823715\n\n\n1378\nIngrid Remmers\nDIE LINKE\nFederal\nFederal Parliament\nParliamentarian\n551802475\nIngrid Remmers MdB\ningrid_remmers\n2017-10-24\nNA\n1965\n120775\nfemale\nQ1652660\n\n\n1116\nReinhard Brandl\nCSU\nFederal\nFederal Parliament\nParliamentarian\n262730721\nReinhard Brandl\nreinhardbrandl\n2017-10-24\nNA\n1977\n79427\nmale\nQ111160\n\n\n\n\n\n\nDann lesen wir die Timelines (die Tweets) dieser Konten aus; in diesem Beispiel nur 10 Tweets pro Account:\n\nepi_tweets <- get_timeline(user = head(politicians_twitter$twitter_name), n = 10)\nhead(epi_tweets)\n\nNatürlich könnte man auch mehr als 10 Tweets pro Konto einsammeln, braucht nur seine Zeit.\n\n\n2.3.4 Followers suchen\n\nfollowers01 <-\n  d$screenname %>% \n map_dfr( ~ get_followers(user = .x, retryonratelimit = TRUE))\n\nsaveRDS(followers01, file = \"tweets/followers01.rds\")\n\nDamit haben wir eine Liste an Followers, deren Tweets wir einlesen und analysieren können, z.B. nach Hate Speech.\nIm Gegensatz zu Followers heißen bei Twitter die Accounts, denen ei Nutzi folgt “Friends”.\n\n\n2.3.5 Follower Tweets einlesen\n\nfollowers_tweets <- get_timeline(user = head(followers01$from_id), n = 10)"
  },
  {
    "objectID": "twittermining.html#cron-jobs",
    "href": "twittermining.html#cron-jobs",
    "title": "2  Twitter Mining",
    "section": "2.4 Cron Jobs",
    "text": "2.4 Cron Jobs\n\n2.4.1 Was ist ein Cron Job?\nCron ist ein Programm auf Unix-artigen Betriebssystemen, das Skripte zu eingestellten Zeiten (wiederholt) ausführt, das sind dann “Cron Jobs”. Auf Windows gibt es aber analoge Funktionen. Cron Jobs sind praktisch, da man nicht jedes Mal selber z.B. Tweets, die heute zu einem Thema getweetet wurden, herunterladen muss. Das wird dann vom Cron Job übernommen.\nIn R gibt es eine API zum Programm Cron mit dem Paket {cronR}, s. Anleitung hier.\nDas analoge R-Paket für Windows heißt {taskscheduleR}.\n\n\n2.4.2 Beispiel für einen Cron Job\n\nlibrary(cronR)\n\nscrape_script <- cron_rscript(\"scrape_tweets.R\")\n\n# Cron Job hinzufügen:\ncron_add(command = scrape_script, \n         frequency = 'daily', \n         at = \"10AM\",\n         id = 'Hate Speech')  # Name des Cron Jobs\n\ncron_clear(ask = FALSE)  # Alle Cron Jobs löschen\ncron_ls()  # Liste aller Cron Jobs\n\nIm obigen Beispiel wird das R-Skript scrape_tweets.R täglich um 10h ausgeführt.\nDer Inhalt von scrape_tweets.R könnte dann, in Grundzügen, so aussehen:\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(rtweet)\nfollowers_lauterbach <-\n  followers01 %>% \n  filter(to_id == \"Karl_Lauterbach\")\n\nfollowers_lauterbach_tweets <- \n  get_timeline(user = followers_lauterbach$from_id[1:10], n = 10, retryonratelimit = TRUE, verbose = FALSE)\n\n\npath_output <- \"/Users/sebastiansaueruser/Google Drive/RRRing/Scrape-Tweets/tweets/\"\n\nwrite_csv(x = followers_lauterbach_tweets,\n          file = paste0(path_output, \"followers_lauterbach_tweets.csv\"),\n          append = TRUE)\n\nWir schreiben nicht jedes Mal (jeden Tag) eine neue CSV-Datei, sondern wir hängen hier die neu ausgelesenen Daten an die Datei an.\nLeider ist es mit rtweet nicht möglich, ein Datum anzugeben, ab dem man Tweets auslesen möchte2"
  },
  {
    "objectID": "twittermining.html#workshop-zu-rtweet",
    "href": "twittermining.html#workshop-zu-rtweet",
    "title": "2  Twitter Mining",
    "section": "2.5 Workshop zu rtweet",
    "text": "2.5 Workshop zu rtweet\nErarbeiten Sie die Folien zu diesem rtweet-Workshop. Eine Menge guter Tipps!"
  },
  {
    "objectID": "twittermining.html#aufgaben",
    "href": "twittermining.html#aufgaben",
    "title": "2  Twitter Mining",
    "section": "2.6 Aufgaben",
    "text": "2.6 Aufgaben\n\nÜberlegen Sie, wie Sie das Ausmaß an Hate Speech, dem deutsche Politikerinnen und Politiker konfrontiert sind, messen können.\nArgumentieren Sie die Vorteile und Nachteile Ihres Ansatzes. Außerdem, auf welches Ergebnis dieser Analyse sie gespannt sind bzw. wären.\nÜberlegen Sie Korrelate, oder besser noch: (mögliche) Ursachen, des Hasses in den Tweets, gerichtet auf Polikter:innen. Sie können auch Gruppen von Ursachen bilden, etwas personengebundene Variablen der Politiker:innen (z.B. Alter? Geschlecht? Migrationshintergrund?).\nErstellen Sie sich eine Liste an Personen, deren Tweets sich lohnen (könnten), auf Hate Speech hin analysiert zu werden. Laden Sie deren Tweets (ggf. in Auszügen) herunter.\nDas Skript zu scrape_tweets.R könnte man noch verbessern, in dem man jeden Tag nur die neuesten Tweets herunterlädt. Dazu kann man bei get_timeline() mit dem Argument since_id eine Untergrenze der ID festlegen, so dass nur neuere Tweets (d.h. mit größerem Wert bei ID) ausgelesen werden. Ändern Sie das Skript entsprechend, so dass nur neuerer Tweets gelesen werden.\n\n\n\n\n\nHvitfeldt, Emil, und Julia Silge. 2022. Supervised Machine Learning for Text Analysis in R. 1. Aufl. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nKönig, Tim, Wolf J. Schünemann, Alexander Brand, Julian Freyberg, und Michael Gertz. 2022. „The EPINetz Twitter Politicians Dataset 2021. A New Resource for the Study of the German Twittersphere and Its Application for the 2021 Federal Elections“. Politische Vierteljahresschrift 63 (3): 529–47. https://doi.org/10.1007/s11615-022-00405-7."
  },
  {
    "objectID": "textmining1.html",
    "href": "textmining1.html",
    "title": "3  Textmining1",
    "section": "",
    "text": "Bild von mcmurryjulie auf Pixabay"
  },
  {
    "objectID": "textmining1.html#vorab",
    "href": "textmining1.html#vorab",
    "title": "3  Textmining1",
    "section": "3.1 Vorab",
    "text": "3.1 Vorab\n\n3.1.1 Lernziele\n\nDie vorgestellten Techniken des Textminings mit R anwenden können\n\n\n\n3.1.2 Vorbereitung\n\nLesen Sie in Hvitfeldt und Silge (2022) Kap. 2.\n\n\n\n3.1.3 Benötigte R-Pakete\n\nlibrary(tidyverse)\nlibrary(tokenizers)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(hcandersenr)\nlibrary(SnowballC)  # Stemming\nlibrary(lsa)  # Stopwörter\nlibrary(easystats)  # Komfort für deskriptive Statistiken, wie `describe_distribution`"
  },
  {
    "objectID": "textmining1.html#einfache-methoden-des-textminings",
    "href": "textmining1.html#einfache-methoden-des-textminings",
    "title": "3  Textmining1",
    "section": "3.2 Einfache Methoden des Textminings",
    "text": "3.2 Einfache Methoden des Textminings\nArbeiten Sie die folgenden grundlegenden Methoden des Textminigs durch.\n\n3.2.1 Tokenisierung\nErarbeiten Sie dieses Kapitel: Hvitfeldt und Silge (2022), Kap. 2\nWie viele Zeilen hat das Märchen “The Fir tree” (in der englischen Fassung?)\n\nhcandersen_en %>% \n  filter(book == \"The fir tree\") %>% \n  nrow()\n\n[1] 253\n\n\n\n\n3.2.2 Stopwörter entfernen\nErarbeiten Sie dieses Kapitel: s. Hvitfeldt und Silge (2022), Kap. 3\n\n\n3.2.3 Stemming (Wortstamm finden)\nErarbeiten Sie dieses Kapitel: Hvitfeldt und Silge (2022), Kap. 4\nVertiefende Hinweise zum UpSet plot finden Sie hier, Lex u. a. (2014).\nFür welche Sprachen gibt es Stemming im Paket SnowballC?\n\nlibrary(SnowballC)\ngetStemLanguages()\n\n [1] \"arabic\"     \"basque\"     \"catalan\"    \"danish\"     \"dutch\"     \n [6] \"english\"    \"finnish\"    \"french\"     \"german\"     \"greek\"     \n[11] \"hindi\"      \"hungarian\"  \"indonesian\" \"irish\"      \"italian\"   \n[16] \"lithuanian\" \"nepali\"     \"norwegian\"  \"porter\"     \"portuguese\"\n[21] \"romanian\"   \"russian\"    \"spanish\"    \"swedish\"    \"tamil\"     \n[26] \"turkish\"   \n\n\nEinfacher Test: Suchen wir den Wordstamm für das Wort “wissensdurstigen”, wie in “die wissensdurstigen Studentis löcherten dis armi Professi”1.\n\nwordStem(\"wissensdurstigen\", language = \"german\")\n\n[1] \"wissensdurst\"\n\n\nWerfen Sie mal einen Blick in das Handbuch von SnowballC.\n\n\n3.2.4 Fallstudie AfD-Parteiprogramm\nDaten einlesen:\n\nd_link <- \"https://raw.githubusercontent.com/sebastiansauer/pradadata/master/data-raw/afd_2022.csv\"\nafd <- read_csv(d_link, show_col_types = FALSE)\n\nWie viele Seiten hat das Dokument?\n\nnrow(afd)\n\n[1] 190\n\n\nUnd wie viele Wörter?\n\nstr_count(afd$text, pattern = \"\\\\w\") %>% sum(na.rm = TRUE)\n\n[1] 179375\n\n\nAus breit mach lang, oder: wir tokenisieren (nach Wörtern):\n\nafd %>% \n  unnest_tokens(output = token, input = text) %>% \n  filter(str_detect(token, \"[a-z]\")) -> afd_long\n\nStopwörter entfernen:\n\ndata(stopwords_de, package = \"lsa\")\n\nstopwords_de <- tibble(word = stopwords_de)\n\n# Für das Joinen werden gleiche Spaltennamen benötigt:\nstopwords_de <- stopwords_de %>% \n  rename(token = word)  \n\nafd_long %>% \n  anti_join(stopwords_de) -> afd_no_stop\n\nJoining, by = \"token\"\n\n\nWörter zählen:\n\nafd_no_stop %>% \n  count(token, sort = TRUE) -> afd_count\n\nhead(afd_count)\n\n\n\n\n\ntoken\nn\n\n\n\n\nafd\n174\n\n\ndeutschland\n113\n\n\nwollen\n66\n\n\neuro\n60\n\n\nbürger\n57\n\n\neu\n54\n\n\n\n\n\n\nWörter trunkieren:\n\nafd_no_stop %>% \n  mutate(token_stem = wordStem(token, language = \"de\")) %>% \n  count(token_stem, sort = TRUE) -> afd_count_stemmed\n\nhead(afd_no_stop)\n\n\n\n\n\npage\ntoken\n\n\n\n\n1\nprogramm\n\n\n1\ndeutschland\n\n\n1\ngrundsatzprogramm\n\n\n1\nalternative\n\n\n1\ndeutschland\n\n\n2\ninhaltsverzeichnis\n\n\n\n\n\n\n\n\n3.2.5 Stringverarbeitung\nErarbeiten Sie dieses Kapitel: Wickham und Grolemund (2018), Kap. 14\n\n3.2.5.1 Regulärausdrücke\nDas \"[a-z]\" in der Syntax oben steht für “alle Buchstaben von a-z”. D iese flexible Art von “String-Verarbeitung mit Jokern” nennt man Regulärausdrücke (regular expressions; regex). Es gibt eine ganze Reihe von diesen Regulärausdrücken, die die Verarbeitung von Texten erleichert. Mit dem Paket stringr geht das - mit etwas Übung - gut von der Hand. Nehmen wir als Beispiel den Text eines Tweets:\n\nstring <- \"Correlation of unemployment and #AfD votes at #btw17: ***r = 0.18***\\n\\nhttps://t.co/YHyqTguVWx\"  \n\nMöchte man Ziffern identifizieren, so hilft der Reulärausdruck [:digit:]:\n“Gibt es mindestens eine Ziffer in dem String?”\n\nstr_detect(string, \"[:digit:]\")\n\n[1] TRUE\n\n\n“Finde die Position der ersten Ziffer! Welche Ziffer ist es?”\n\nstr_locate(string, \"[:digit:]\")\n\n     start end\n[1,]    51  51\n\nstr_extract(string, \"[:digit:]\")\n\n[1] \"1\"\n\n\n“Finde alle Ziffern!”\n\nstr_extract_all(string, \"[:digit:]\")\n\n[[1]]\n[1] \"1\" \"7\" \"0\" \"1\" \"8\"\n\n\n“Finde alle Stellen an denen genau 2 Ziffern hintereinander folgen!”\n\nstr_extract_all(string, \"[:digit:]{2}\")\n\n[[1]]\n[1] \"17\" \"18\"\n\n\nDer Quantitätsoperator {n} findet alle Stellen, in der der der gesuchte Ausdruck genau \\(n\\) mal auftaucht.\n“Zeig die Hashtags!”\n\nstr_extract_all(string, \"#[:alnum:]+\")\n\n[[1]]\n[1] \"#AfD\"   \"#btw17\"\n\n\nDer Operator [:alnum:] steht für “alphanumerischer Charakter” - also eine Ziffer oder ein Buchstabe; synonym hätte man auch \\\\w schreiben können (w wie word). Warum werden zwei Backslashes gebraucht? Mit \\\\w wird signalisiert, dass nicht der Buchstabe w, sondern etwas Besonderes, eben der Regex-Operator \\w gesucht wird.\n“Zeig die URLs!”\n\nstr_extract_all(string, \"https?://[:graph:]+\")\n\n[[1]]\n[1] \"https://t.co/YHyqTguVWx\"\n\n\nDas Fragezeichen ? ist eine Quantitätsoperator, der einen Treffer liefert, wenn das vorherige Zeichen (hier s) null oder einmal gefunden wird. [:graph:] ist die Summe von [:alpha:] (Buchstaben, groß und klein), [:digit:] (Ziffern) und [:punct:] (Satzzeichen u.ä.).\n“Zähle die Wörter im String!”\n\nstr_count(string, boundary(\"word\"))\n\n[1] 13\n\n\n“Liefere nur Buchstabenfolgen zurück, lösche alles übrige”\n\nstr_extract_all(string, \"[:alpha:]+\")\n\n[[1]]\n [1] \"Correlation\"  \"of\"           \"unemployment\" \"and\"          \"AfD\"         \n [6] \"votes\"        \"at\"           \"btw\"          \"r\"            \"https\"       \n[11] \"t\"            \"co\"           \"YHyqTguVWx\"  \n\n\nDer Quantitätsoperator + liefert alle Stellen zurück, in denen der gesuchte Ausdruck einmal oder häufiger vorkommt. Die Ergebnisse werden als Vektor von Wörtern zurückgegeben. Ein anderer Quantitätsoperator ist *, der für 0 oder mehr Treffer steht. Möchte man einen Vektor, der aus Stringen-Elementen besteht zu einem Strring zusammenfüngen, hilft paste(string) oder str_c(string, collapse = \" \").\n\nstr_replace_all(string, \"[^[:alpha:]+]\", \"\")\n\n[1] \"CorrelationofunemploymentandAfDvotesatbtwrhttpstcoYHyqTguVWx\"\n\n\nMit dem Negationsoperator [^x] wird der Regulärausrck x negiert; die Syntax oben heißt also “ersetze in string alles außer Buchstaben durch Nichts”. Mit “Nichts” sind hier Strings der Länge Null gemeint; ersetzt man einen belieibgen String durch einen String der Länge Null, so hat man den String gelöscht.\nDas Cheatsheet zur Strings bzw zu stringr von RStudio gibt einen guten Überblick über Regex; im Internet finden sich viele Beispiele."
  },
  {
    "objectID": "textmining1.html#regex-im-texteditor",
    "href": "textmining1.html#regex-im-texteditor",
    "title": "3  Textmining1",
    "section": "3.3 Regex im Texteditor",
    "text": "3.3 Regex im Texteditor\nEinige Texteditoren unterstützen Regex, so auch RStudio.\nDas ist eine praktische Sache. Ein Beispiel: Sie haben eine Liste mit Namen der Art:\n\nNachname1, Vorname1\nNachname2, Vorname2\nNachname3, Vorname3\n\nUnd Sie möchten jetzt aber die Liste mit Stil Vorname Nachname sortiert haben.\nRStudio mit Regex macht’s möglich, s. ?fig-regex-rstudio.\n\n\n \nAbbildung 3.1: ?(caption)\n\n\n\n3.3.1 Sentimentanalyse\n\n3.3.1.1 Einführung\nEine weitere interessante Analyse ist, die “Stimmung” oder “Emotionen” (Sentiments) eines Textes auszulesen. Die Anführungszeichen deuten an, dass hier ein Maß an Verständnis suggeriert wird, welches nicht (unbedingt) von der Analyse eingehalten wird. Jedenfalls ist das Prinzip der Sentiment-Analyse im einfachsten Fall so:\n\nSchau dir jeden Token aus dem Text an.\n\nPrüfe, ob sich das Wort im Lexikon der Sentiments wiederfindet.\n\nWenn ja, dann addiere den Sentimentswert dieses Tokens zum bestehenden Sentiments-Wert.\n\nWenn nein, dann gehe weiter zum nächsten Wort.\n\nLiefere zum Schluss die Summenwerte pro Sentiment zurück.\n\nEs gibt Sentiment-Lexika, die lediglich einen Punkt für “positive Konnotation” bzw. “negative Konnotation” geben; andere Lexiko weisen differenzierte Gefühlskonnotationen auf. Wir nutzen hier das Sentimentlexikon sentiws (Remus, Quasthoff, und Heyer 2010). Sie können das Lexikon als CSV hier herunterladen:\n\nsentiws <- read_csv(\"https://osf.io/x89wq/?action=download\")\n\nDen Volltext zum Paper finden Sie z.B. hier.\nAlternativ können Sie die Daten aus dem Paket pradadata laden. Allerdings müssen Sie dieses Paket von Github installieren:\n\ninstall.packages(\"devtools\", dep = TRUE)\ndevtools::install_github(\"sebastiansauer/pradadata\")\n\n\ndata(sentiws, package = \"pradadata\")\n\nTabelle 3.1 zeigt einen Ausschnitt aus dem Sentiment-Lexikon SentiWS.\n\n\n\n\n\nTabelle 3.1: Auszug aus SentiWS\n\n\nneg_pos\nword\nvalue\ninflections\n\n\n\n\nneg\nAbbau\n-0.0580\nAbbaus,Abbaues,Abbauen,Abbaue\n\n\nneg\nAbbruch\n-0.0048\nAbbruches,Abbrüche,Abbruchs,Abbrüchen\n\n\nneg\nAbdankung\n-0.0048\nAbdankungen\n\n\nneg\nAbdämpfung\n-0.0048\nAbdämpfungen\n\n\nneg\nAbfall\n-0.0048\nAbfalles,Abfälle,Abfalls,Abfällen\n\n\nneg\nAbfuhr\n-0.3367\nAbfuhren\n\n\n\n\n\n\n\n\n\n3.3.1.2 Ungewichtete Sentiment-Analyse\nNun können wir jedes Token des Textes mit dem Sentiment-Lexikon abgleichen; dabei zählen wir die Treffer für positive bzw. negative Terme. Zuvor müssen wir aber noch die Daten (afd_long) mit dem Sentimentlexikon zusammenführen (joinen). Das geht nach bewährter Manier mit inner_join; “inner” sorgt dabei dafür, dass nur Zeilen behalten werden, die in beiden Dataframes vorkommen. Tabelle Tabelle 3.2 zeigt Summe, Anzahl und Anteil der Emotionswerte.\nWir nutzen die Tabelle afd_long, die wir oben definiert haben.\n\nafd_long %>% \n  inner_join(sentiws, by = c(\"token\" = \"word\")) %>% \n  select(-inflections) -> afd_senti  # die Spalte brauchen wir nicht\n\nafd_senti %>% \n  group_by(neg_pos) %>% \n  summarise(polarity_sum = sum(value),\n            polarity_count = n()) %>% \n  mutate(polarity_prop = (polarity_count / sum(polarity_count)) %>% round(2)) ->\n  afd_senti_tab\n\n\n\n\n\n\nTabelle 3.2: Zusammenfassung von SentiWS\n\n\nneg_pos\npolarity_sum\npolarity_count\npolarity_prop\n\n\n\n\nneg\n-48.5307\n210\n0.27\n\n\npos\n30.6595\n578\n0.73\n\n\n\n\n\n\n\nDie Analyse zeigt, dass die emotionale Bauart des Textes durchaus interessant ist: Es gibt viel mehr positiv getönte Wörter als negativ getönte. Allerdings sind die negativen Wörter offenbar deutlich stärker emotional aufgeladen, denn die Summe an Emotionswert der negativen Wörter ist (überraschenderweise?) deutlich größer als die der positiven.\nBetrachten wir also die intensivsten negativ und positive konnotierten Wörter näher.\n\nafd_senti %>% \n  distinct(token, .keep_all = TRUE) %>% \n  mutate(value_abs = abs(value)) %>% \n  top_n(20, value_abs) %>% \n  pull(token)\n\n [1] \"ungerecht\"    \"besonders\"    \"gefährlich\"   \"überflüssig\"  \"behindern\"   \n [6] \"gelungen\"     \"brechen\"      \"unzureichend\" \"gemein\"       \"verletzt\"    \n[11] \"zerstören\"    \"trennen\"      \"falsch\"       \"vermeiden\"    \"zerstört\"    \n[16] \"schwach\"      \"belasten\"     \"schädlich\"    \"töten\"        \"verbieten\"   \n\n\nDiese “Hitliste” wird zumeist (19/20) von negativ polarisierten Begriffen aufgefüllt, wobei “besonders” ein Intensivierwort ist, welches das Bezugswort verstärt (“besonders gefährlich”). Das Argument keep_all = TRUE sorgt dafür, dass alle Spalten zurückgegeben werden, nicht nur die durchsuchte Spalte token. Mit pull haben wir aus dem Dataframe, der von den dplyr-Verben übergeben wird, die Spalte pull “herausgezogen”; hier nur um Platz zu sparen bzw. der Übersichtlichkeit halber.\nNun könnte man noch den erzielten “Netto-Sentimentswert” des Corpus ins Verhältnis setzen Sentimentswert des Lexikons: Wenn es insgesamt im Sentiment-Lexikon sehr negativ zuginge, wäre ein negativer Sentimentwer in einem beliebigen Corpus nicht überraschend. describe_distribution aus {easystats} gibt uns einen Überblick der üblichen deskriptiven Statistiken.\n\nsentiws %>% \n  select(value, neg_pos) %>% \n  #group_by(neg_pos) %>% \n  describe_distribution()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nMean\nSD\nIQR\nRange\nSkewness\nKurtosis\nn\nn_Missing\n\n\n\n\nvalue\n-0.05\n0.20\n0.05\n(-1.00, 1.00)\n-0.68\n2.36\n3468\n0\n\n\n\n\n\nInsgesamt ist das Lexikon ziemlich ausgewogen; negative Werte sind leicht in der Überzahl im Lexikon. Unser Corpus hat eine ähnliche mittlere emotionale Konnotation wie das Lexikon:\n\nafd_senti %>% \n  summarise(senti_sum = mean(value) %>% round(2))\n\n\n\n\n\nsenti_sum\n\n\n\n\n-0.02"
  },
  {
    "objectID": "textmining1.html#aufgaben",
    "href": "textmining1.html#aufgaben",
    "title": "3  Textmining1",
    "section": "3.4 Aufgaben",
    "text": "3.4 Aufgaben\n\npurrr-map01\npurrr-map02\npurrr-map03\npurrr-map04\nRegex-Übungen"
  },
  {
    "objectID": "textmining1.html#fallstudie-hate-speech",
    "href": "textmining1.html#fallstudie-hate-speech",
    "title": "3  Textmining1",
    "section": "3.5 Fallstudie Hate-Speech",
    "text": "3.5 Fallstudie Hate-Speech\n\n3.5.1 Daten\nEs finden sich mehrere Datensätze zum Thema Hate-Speech im öffentlichen Internet, eine Quelle ist Hate Speech Data, ein Repositorium, das mehrere Datensätze beinhaltet.\n\nKaggle Hate Speech and Offensive Language Dataset\nBretschneider and Peters Prejudice on Facebook Dataset\nDaten zum Fachartikel”Large Scale Crowdsourcing and Characterization of Twitter Abusive Behavior”\n\nTwitterdaten dürfen nur in “dehydrierter” Form weitergegeben werden, so dass kein Rückschluss von ID zum Inhalt des Tweets möglich ist. Daher werden öffentlich nur die IDs der Tweets, als einzige Information zum Tweet, a lso ohne den eigentlichen Inhalt des Tweets, bereitgestellt.\nÜber die Twitter-API kann man sich, wie oben dargestellt, dann die Tweets wieder “rehydrieren”, also wieder mit dem zugehörigen Tweet-Text (und sonstigen Infos des Tweets) zu versehen.\n\n\n3.5.2 Grundlegendes Text Mining\nWenden Sie die oben aufgeführten Techniken des grundlegenden Textminings auf einen der oben dargestellten Hate-Speech-Datensätze an. Erstellen Sie ein (HTML-Dokument) mit Ihren Ergebnissen. Stellen Sie die Ergebnisse auf dem Github-Repo dieses Kurses ein. Vergleichen Sie Ihre Lösung mit den Lösungen der anderen Kursmitglieder.\nWir nutzen noch nicht eigene Daten, die wir von Twitter ausgelesen haben, das heben wir uns für später auf.\n\n\n\n\nHvitfeldt, Emil, und Julia Silge. 2022. Supervised Machine Learning for Text Analysis in R. 1. Aufl. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nLex, Alexander, Nils Gehlenborg, Hendrik Strobelt, Romain Vuillemot, und Hanspeter Pfister. 2014. „UpSet: Visualization of Intersecting Sets“. IEEE Transactions on Visualization and Computer Graphics 20 (12): 1983–92. https://doi.org/10.1109/TVCG.2014.2346248.\n\n\nRemus, Robert, Uwe Quasthoff, und Gerhard Heyer. 2010. „SentiWS - a Publicly Available German-language Resource for Sentiment Analysis“. Proceedings of the 7th International Language Ressources and Evaluation (LREC’10), 1168–71.\n\n\nWickham, Hadley, und Garrett Grolemund. 2018. R für Data Science: Daten importieren, bereinigen, umformen, modellieren und visualisieren. Übersetzt von Frank Langenau. 1. Auflage. Heidelberg: O’Reilly. https://r4ds.had.co.nz/index.html."
  },
  {
    "objectID": "populismus.html",
    "href": "populismus.html",
    "title": "4  Fallstudie Populismus",
    "section": "",
    "text": "Bild von mcmurryjulie auf Pixabay"
  },
  {
    "objectID": "populismus.html#wie-populistisch-tweeten-unsere-politikerinnen",
    "href": "populismus.html#wie-populistisch-tweeten-unsere-politikerinnen",
    "title": "4  Fallstudie Populismus",
    "section": "4.1 Wie populistisch tweeten unsere Politiker:innen?",
    "text": "4.1 Wie populistisch tweeten unsere Politiker:innen?\nVerschaffen Sie sich einen Überblick über dieses Projekt! Im Rahmen dieses Projekts vergleicht der Autor den Populismus von deutschen Politiker:innen, so wie er sich in den Tweets dieser Personen niederschlägt. Auf dieser Basis wird ein Populismuswert, bestehend aus mehreren Teilwerten, berechnet und auf Parteiebenen (als Mittel der zugehörigen Politiker:innen) berechnet. Natürlich fragt man sich, wie Populismus definiert ist und wie diese Definition in den Berechnungen umgesetzt wurde. Finden Sie es selber heraus: Im Github-Repo sind alle Details dokumentiert.\nZum Einstieg hilft ein Überblick über die Ergebnisse der Analyse, die in diesem Vortrag zusammengefasst sind.\nDieser Post stellt die Ergebnisse mit etwas Kontext dar."
  },
  {
    "objectID": "Information.html",
    "href": "Information.html",
    "title": "5  Informationstheorie",
    "section": "",
    "text": "Die grundlegenden Konzepte der Informationstheorie erklären können"
  },
  {
    "objectID": "Information.html#grundlagen",
    "href": "Information.html#grundlagen",
    "title": "5  Informationstheorie",
    "section": "5.1 Grundlagen",
    "text": "5.1 Grundlagen\nDie Informationstheorie ist eine der Sternstunden der Wissenschaft. Manche sagen dass Claude Shannon, der Autor der Theorie, auf einer Stufe mit Darwin und Einstein stehen sollte:\n\nIn this single paper, Shannon introduced this new fundamental theory. He raised the right questions, which no one else even thought of asking. This would have been enough to make this contribution earthshaking. But amazingly enough, Shannon also provided most of the right answers with class and elegance. In comparison, it took decades for a dozen of top physicists to define the basics of quantum theory. Meanwhile, Shannon constructed something equivalent, all by himself, in a single paper. Shannon’s theory has since transformed the world like no other ever had, from information technologies to telecommunications, from theoretical physics to economical globalization, from everyday life to philosophy. (…) I don’t think Shannon has had the credits he deserves. He should be right up there, near Darwin and Einstein, among the few greatest scientists mankind has ever had the chance to have.\n\nFür die Statistik ist die Informationstheorie von hoher Bedeutung. Im Folgenden schauen wir uns einige Grundlagen an.\n\n5.1.1 Shannon-Information\nMit der Shannon-Information (Information, Selbstinformation) quantifizieren wir, wie viel “Überraschung” sich in einem Ereignis verbirgt (Shannon 1948).\nEin Ereignis mit …\n\ngeringer Wahrscheinlichkeit: Viel Überraschung (Information)\nhoher Wahrscheinlichkeit: Wenig Überraschung (Information)\n\nWenn wir also erfahren, dass ein unwahrscheinliches Ereignis eingetreten ist (Schnee im Sommer), sind wir überraschter als wenn wir höhen, dass ein wahrscheinliches Ereignis eingetreten ist (Schnee im Winter).\nDie Information eines Ereignis ist also eng verbunden mit seiner Wahrscheinlichkeit oder auch mit den Odds eines Ereignisses.\nDie Shannon-Information ist die einzige Größe, die einige wünschenswerte Anforderungen1 erfüllt:\n\nStetig\nJe mehr Ereignisse in einem Zufallsexperiment möglich sind, desto höher die Information, wenn ein bestimmtes Ereignis eintritt\nAdditiv: Die Summe der Information zweier Teilereignisse ist gleich der Information des Gesamtereignis\n\n\nDefinition 5.1 (Shannon-Information) Die Information ist so definiert:\n\\[I(x) = - \\log_2 \\left( Pr(x) \\right)\\]\n\nAndere Logaritmusbasen sind möglich. Bei einem binären Logarithmus nennt man die Einheit Bit2.\nEin Münwzurf3 hat 1 Bit Information:\n\n-log(1/2, base = 2)\n\n[1] 1\n\n\nDamit gilt: \\(I = \\frac{1}{Pr(x)}\\)\nDie Information ist eng verwandt mit den Odds bzw. Log-Odds (Logits):\n\\(\\text{log-odds}(x)=\\log \\left({\\frac {p(x)}{p(\\lnot x)}}\\right)\\)\nLogits können als Differenz zweier Shannon-Infos ausgedrückt werden:\n\\(\\text{log-odds}(x)=I(\\lnot x)-I(x)\\)\nDie Information zweier unabhängiger Ereignisse ist additiv.\nDie gemeinsame Wahrscheinlichkeit zweier unabhängiger Ereignisse ist das Produkt der einzelnen Wahrscheinlichkeiten:\n\\(Pr(x,y) = Pr(x) \\cdot Pr(y)\\)\nDie gemeinsame Information ist dann\n\\[\n{\\displaystyle {\\begin{aligned}\\operatorname {I}(x,y)&=-\\log _{2}\\left[p(x,y)\\right]=-\\log _{2}\\left[p(x)p(y)\\right]\\\\[5pt]&=-\\log _{2}\\left[p{(x)}\\right]-\\log _{2}\\left[p{(y)}\\right]\\\\[5pt]&=\\operatorname {I} (x)+\\operatorname {I} (y)\\end{aligned}}}\n\\]\n\nBeispiel 5.1 (Information eines wahrscheinlichen Ereignisses) Die Information eines fast sicheren Ereignisses ist gering.\n\n-log(99/100, base = 2)\n\n[1] 0.01449957\n\n\n\n\nBeispiel 5.2 (Information eines unwahrscheinlichen Ereignisses) Die Information eines unwahrscheinlichen Ereignisses ist hoch.\n\n-log(01/100, base = 2)\n\n[1] 6.643856\n\n\n\n\nBeispiel 5.3 (Information eines Würfelwurfs) Die Wahrscheinlichkeitsfunktion eines Würfel ist\n\\({\\displaystyle Pr(k)={\\begin{cases}{\\frac {1}{6}},&k\\in \\{1,2,3,4,5,6\\}\\\\0,&{\\text{ansonsten}}\\end{cases}}}\\)\nDie Wahrscheinlichkeit, eine 6 zu würfeln, ist \\(Pr(X=6) = \\frac{1}{6}\\).\nDie Information von \\(X=6\\) beträgt also\n\\(I(X=6) = -\\log_2 \\left( Pr(X=6) \\right) = -\\log_2(1/6) \\approx 2.585 \\, \\text{bits}\\).\n\n-log(1/6, base = 2)\n\n[1] 2.584963\n\n\n\n\nBeispiel 5.4 (Information zweier Würfelwurfe) Die Wahrscheinlichkeit, mit zwei Würfeln, \\(X\\) und \\(Y\\), jeweils 6 zu würfeln, beträgt \\(Pr(X=6, Y=6) = \\frac{1}{36}\\)\nDie Information beträgt also\n\\(I(X=6, Y=6) = -\\log_2 \\left( Pr(6,6) \\right)\\)\n\n-log(1/36, base = 2)\n\n[1] 5.169925\n\n\nAufgrund der Additivität der Information gilt\n\\(I(6,6) = I(6) + I(6)\\)\n\n-log(1/6, base = 2) + -log(1/6, base = 2)\n\n[1] 5.169925\n\n\n\n\n\n5.1.2 Entropie\n(Informations)entropie ist die Summe der Informationen der Ereignisse einer Zufallsvariablen, \\(X\\).\n\nDefinition 5.2 (Informationsentropie) Informationsentropie ist so definiert:\n\\[H(p) = - \\text{E log} (p_i) = - \\sum_{i = 1}^n p_i \\text{log} (p_i) = E\\left[I(X) \\right]\\]\n\nDie Informationsentropie ist also die “mittlere” oder “erwartete Information einer Zufallsvariablen.\nDie Entropie eines Münzwurf-Versuchs (Bernoulli-Experiment) ist dann maximal (1 bit), wenn die Trefferwahrscheinlichkeit 50% beträgt: \\(Pr(X=x) = 1/2\\), s. Abb. Abbildung 5.1.\n\n\n\nAbbildung 5.1: Der Zusammenhang von Wahrscheinlichkeit und Entropie bei einem Bernoulli-Versuch, Quelle: Wikipedia, CC-BY-SA, Brona, Rubber Duck\n\n\n\n\n5.1.3 Gemeinsame Information\nDie gemeinsame Information (mutual information, MI) zweier Zufallsvariablen \\(X\\) und \\(Y\\), \\(I(X,Y)\\), quantifiziert die Informationsmenge, die man über \\(Y\\) erhält, wenn man \\(X\\) beobachtet. Mit anderen Worten: Die MI ist ein Maß des Zusammenhangs zweier (nominaler) Variablen. Im Gegensatz zur Korrelation ist die MI nicht auf lineare Abhängigkeiten beschränkt.\nDie MI quantifiziert den Unterschied zwischen der gemeinsamen Verteilung \\(Pr(X,Y)\\) und dem Produkt einer einzelnen4 Wahrscheinlichkeitsverteilungen, d.h. \\(Pr(X)\\) und \\(Pr(Y)\\).\nWenn die beiden Variablen (stochastisch) unabhängig5 sind, ist ihre gemeinsame Information Null:\n\\(I(X,Y) = 0 \\quad \\text{gdw} \\quad \\bot(X,Y)\\).\nDann gilt nämlich:\n\\(\\log \\left( \\frac{Pr(X,Y)} {Pr(X) \\cdot Pr(Y)} \\right) =\\log(1) = 0\\).\nDas macht intuitiv Sinn: Sind zwei Variablen unabhängig, so erfährt man nichts über die zweite, wenn man die erste kennt. So ist Kenntnis der Sternzeichens einer Person und ihrer Körpergröße unabhängig.\nDas Gegenteil ist auch wahr: Sind zwei Variablen voneinander komplett abhängig, so weiß man alles über die zweite, wenn man die erste kennt.\nDie gemeinsame Information kann man sich als Summe der einzelnen gemeinsamen Informationen von \\(XY\\) sehen (s. ?tbl-mi1):\n\nd <- tibble::tribble(\n     ~x1,    ~x2,    ~x3,\n  \"x1y2\", \"x2y1\", \"x3y1\",\n  \"x2y1\", \"x2y2\", \"x3y2\",\n  \"x1y3\", \"x2y3\", \"x3y3\"\n  )\n\n\n?(caption)\n\n\n\n\\(I(X,Y) = \\Sigma_Y \\Sigma_y Pr(x,y) \\underbrace{\\log \\left( \\frac{Pr(X,Y)}{Pr(X) Pr(Y)} \\right)}_\\text{punktweise MI}\\)\nDie Summanden der gemeinsamen Information bezeichnet man auch als punktweise gemeinsame Information (pointwise mutual information, PMI), entsprechend, s. Gleichung 5.1. MI ist also der Erwartungswert der PMI.\n\\[{\\displaystyle \\operatorname {PMI} (x,y)\\equiv \\log_{2}{\\frac {p(x,y)}{p(x)p(y)}}=\\log _{2}{\\frac {p(x|y)}{p(x)}}=\\log _{2}{\\frac {p(y|x)}{p(y)}}}\n\\tag{5.1}\\]\nAndere Basen als log2 sind gebräuchlich, vor allem der natürliche Logarithmus.\n\nAnmerkung. Die zwei rechten Umformungen in Gleichung 5.1 basieren auf der Umformung der gemeinsamen Wahrscheinlichkeit.\nZur Erinnerung: \\(p(x,y) = p(y)p(x|y) = p(x)p(y|x)\\)\n\n\nBeispiel 5.5 (Interpretation der PMI) Sei \\(p(x) = p(y) = 1/10\\) und \\(p(x,y) = 1/10\\). Wären \\(x\\) und \\(y\\) unabhängig, dann wäre \\(p^{\\prime}(x,y) = p(x)p(y) = 1/100\\). Das Verhältnis der Produkte der einzelnen Wahrscheinlichkeit zur gemeinsamen Wahrscheinlichkeit wäre dann 1 und der Logarithmus von 1 ist 0. Das Verhältnis von 1 entspricht also der Unabhängigkeit. Ist das Verhältnis z.B. 5, so zeigt das eine gewisse Abhängigkeit an. Im obigen Beispiel gilt: \\(\\frac{1/20}{1/100}=5\\).\n\nDie MI wird auch über die sog. Kullback-Leibler-Divergenz definiert, die die Differenz zwischen gemeinsamer Wahrscheinlichkeitsfunktion bzw. -dichte und dem Produkt der jeweiligen Randwahrscheinlichkeiten bzw. -dichte angibt.\n\n\n5.1.4 Maximumentropie\n\nDefinition 5.3 (Maximumentropie) Die Verteilungsform, für die es die meisten Möglichkeiten (Pfade im Baumdiagramm) gibt, hat die höchste Informationsentropie.\n\nAbbildung 5.2 zeigt ein Baumdiagramm für einen 3-fachen Münzwurf. In den “Blättern” (Endknoten) sind die Ergebnisse des Experiments dargestellt sowie die Zufallsvariable \\(X\\), die die Anzahl der “Treffer” (Kopf) fasst. Wie man sieht, vereinen einige Werte der Zufallsvariable mehr Pfade auf sich als andere: Der Wert \\(X=1\\) vereinigt 3 Pfade (von 8) auf sich; der Wert \\(X=3\\) nur 1 Pfad.\n\n\n\nAbbildung 5.2: Pfade im Baumdiagramm: 3-facher Münzwurf\n\n\n\n5.1.4.1 Ilustration\nSagen wir, Sie stehen vor 5 Eimern und haben 10 Kieselsteine bei sich, die mit den Nummern 1 bis 10 beschriftet sind (McElreath 2020). Weil Sie nichts besseres zu tun haben, werfen Sie die Kiesel in die Eimer und zwar so, dass die Wahrscheinlichkeit für einen Kiesel in einen bestimmten Eimer zu landen für alle Eimer gleich ist. Sie werfen also Ihre 10 Kiesel und betrachten das Ergebnis; die Kiesel sind jetzt in einem bestimmten (zufälligen) Arrangement auf die Eimer verteilt. Jede Aufteilung (der 10 Kiesel in den 5 Eimern) ist gleich wahrscheinlich6 – die Wahrscheinlichkeit, dass alle 10 Kiesel in Eimer 1 landen ist also gleich hoch wie die Wahrscheinlichkeit, dass jeder Eimer einen Kiesel abkriegt. Jetzt kommt’s: Manche Arrangements können auf mehrere Arten erzielt werden als andere. So gibt es nur eine Aufteilung für alle 10 Kiesel in einem Eimer (Teildiagramm a, in Abbildung 5.3). Aber es gibt 90 Möglichkeiten, die Kiesel so aufzuteilen, dass 2 in Eimer 2 landen, 8 in Eimer 4 und 2 in Eimer 4, s. Teildiagramm b in Abbildung 5.3. Teildiagramme c bis e zeigen, dass die Anzahl der Aufteilungen schnell astronomisch hoch wird, wenn sich die Kiesel “gleichmäßiger” auf die Eimer verteilen. Die gleichmäßigste Aufteilung (Diagramm e) hat die größte Zahl an möglichen Anordnungen. Eine Aufteilung der Kiesel auch als Pfad durch ein Baumdiagramm beschrieben werden.\nHier sind ein paar verschiedene Arrangements, deren Anzahl von Aufteilungen wir hier anschauen:\n\nd <-\n  tibble(a = c(0, 0, 10, 0, 0),\n         b = c(0, 1, 8, 1, 0),\n         c = c(0, 2, 6, 2, 0),\n         d = c(1, 2, 4, 2, 1),\n         e = 2) \nd\n\n\n\n\n\na\nb\nc\nd\ne\n\n\n\n\n0\n0\n0\n1\n2\n\n\n0\n1\n2\n2\n2\n\n\n10\n8\n6\n4\n2\n\n\n0\n1\n2\n2\n2\n\n\n0\n0\n0\n1\n2\n\n\n\n\n\n\n\n\n\n\n\nAbbildung 5.3: Anzahl von Aufteilungen der 10 Kiesel in 5 Eimer\n\n\n\n\nHier sind die Wahrscheinlichkeitsverteilungen der 5 Arrangements7:\n\nd %>% \n  mutate_all(~. / sum(.))\n\n\n\n\n\na\nb\nc\nd\ne\n\n\n\n\n0\n0.0\n0.0\n0.1\n0.2\n\n\n0\n0.1\n0.2\n0.2\n0.2\n\n\n1\n0.8\n0.6\n0.4\n0.2\n\n\n0\n0.1\n0.2\n0.2\n0.2\n\n\n0\n0.0\n0.0\n0.1\n0.2\n\n\n\n\n\n\nDann kann man die Wahrscheinlichkeit einfach in Entropie umrechnen8:\n\nd %>% \n  mutate_all(~ . / sum(.)) %>% \n  gather() %>% \n  group_by(key) %>% \n  summarise(h = -sum(ifelse(value == 0, 0, value * log(value))))\n\n\n\n\n\nkey\nh\n\n\n\n\na\n0.0000000\n\n\nb\n0.6390319\n\n\nc\n0.9502705\n\n\nd\n1.4708085\n\n\ne\n1.6094379\n\n\n\n\n\n\nDas ifelse dient nur dazu, eine Wahrscheinlichkeit von 0 eine Entropie von 0 zu verpassen9, denn sonst würden wir ein Problem rennen, wenn wir \\(log(0)\\) ausrechnen.\n\nlog(0)\n\n[1] -Inf"
  },
  {
    "objectID": "Information.html#zufallstext-erkennen",
    "href": "Information.html#zufallstext-erkennen",
    "title": "5  Informationstheorie",
    "section": "5.2 Zufallstext erkennen",
    "text": "5.2 Zufallstext erkennen\n\n5.2.1 Entropie von Zufallstext\nKann man wohl Zufallstext maschinell erkennen? Sicher gibt es viele Ansätze, um das Problem anzugehen. Lassen Sie uns einen Ansatz erforschen. Erforschen heißt, wir erforschen für uns, es handelt sich um eine didaktische Übung, das Ziel ist nicht, Neuland für die Menschheit zu betreten.\nAber zuerst müssen wir überlegen, was “Zufallstext” bedeuten soll.\nNehmen wir uns dazu zuerst einen richtigen Text, ein Märchen von H.C. Andersen zum Beispiel. Nehmen wir das Erste aus der Liste in dem Tibble hcandersen_de, “das Feuerzeug”.\n\ndas_feuerzeug <-\n  hcandersen_de  %>% \n  filter(book == \"Das Feuerzeug\") %>% \n  unnest_tokens(input = text, output = word) %>% \n  pull(word) \n\nhead(das_feuerzeug)\n\n[1] \"es\"         \"kam\"        \"ein\"        \"soldat\"     \"die\"       \n[6] \"landstraße\"\n\n\nDas Märchen ist 2688 Wörter lang.\n\nwortliste <- \nhcandersen_de  %>% \n  filter(book == \"Das Feuerzeug\") %>% \n  unnest_tokens(output = word, input = text) %>% \n  pull(word) %>% \n  unique()\n\nhead(wortliste)\n\n[1] \"es\"         \"kam\"        \"ein\"        \"soldat\"     \"die\"       \n[6] \"landstraße\"\n\n\nJetzt ziehen wir Stichproben (mit Zurücklegen) aus dieser Liste und erstellen damit eine Art Zufallstext.\n\nzufallstext <- \n  sample(x = wortliste, size = length(das_feuerzeug)*10^2, replace = TRUE)\nhead(zufallstext)\n\n[1] \"oben\"      \"offiziere\" \"morgen\"    \"leg\"       \"soldaten\"  \"aber\"     \n\n\nZählen wir, wie häufig jedes Wort vorkommt:\n\nzufallstext_count <-\ntibble(zufallstext = zufallstext) %>% \n  count(zufallstext)\n\nhead(zufallstext_count)\n\n\n\n\n\nzufallstext\nn\n\n\n\n\nab\n397\n\n\nabend\n357\n\n\naber\n402\n\n\nabflog\n382\n\n\nabschlagen\n376\n\n\nacht\n390\n\n\n\n\n\n\nDer Häufigkeitsvektor von wortliste besteht nur aus Einsen, so haben wir ja gerade die Wortliste definiert:\n\nwortliste_count <-\ntibble(wortliste = wortliste) %>% \n  count(wortliste)\n\nhead(wortliste_count)\n\n\n\n\n\nwortliste\nn\n\n\n\n\nab\n1\n\n\nabend\n1\n\n\naber\n1\n\n\nabflog\n1\n\n\nabschlagen\n1\n\n\nacht\n1\n\n\n\n\n\n\nDaher ist ihre Informationsentropy maximal.\n\nentropy(wortliste_count$n, unit = \"log2\")\n\n[1] 9.47978\n\n\nDie Häufigkeiten der Wörter in zufallstext hat eine hohe Entropie.\n\nentropy(zufallstext_count$n, unit = \"log2\")\n\n[1] 9.477938\n\n\nZählen wir die Häufigkeiten in der Geschichte “Das Feuerzeug”.\n\ndas_feuerzeug_count <-\n  tibble(text = das_feuerzeug) %>% \n  count(text)\n\nhead(das_feuerzeug_count)\n\n\n\n\n\ntext\nn\n\n\n\n\nab\n2\n\n\nabend\n3\n\n\naber\n21\n\n\nabflog\n1\n\n\nabschlagen\n1\n\n\nacht\n1\n\n\n\n\n\n\nUnd berechnen dann die Entropie:\n\nentropy(das_feuerzeug_count$n, unit = \"log2\")\n\n[1] 8.075194\n\n\nDer Zufallstext hat also eine höhere Entropie als der echte Märchentext. Der Zufallstext ist also gleichverteilter in den Worthäufigkeiten.\nPro Bit weniger Entropie halbiert sich die Anzahl der Möglichkeiten einer Häufigkeitsverteilung.\n\n\n5.2.2 MI von Zufallstext\nLeft as an exercises for the reader10 🥳."
  },
  {
    "objectID": "Information.html#literatur",
    "href": "Information.html#literatur",
    "title": "5  Informationstheorie",
    "section": "5.3 Literatur",
    "text": "5.3 Literatur\n\n5.3.1 Wikipedia\nEs gibt eine Reihe nützlicher (und recht informationsdichter) Wikipedia-Einträge zum Thema Informationstheorie.\n\nWikipedia: Pointwise Mutual Information\nWikipedia: Mutual information\nWikipedia: Information theory\n\n\n\n\n\nKurz, A. Solomon. 2021. Statistical rethinking with brms, ggplot2, and the tidyverse: Second edition. https://bookdown.org/content/4857/.\n\n\nMcElreath, Richard. 2020. Statistical rethinking: a Bayesian course with examples in R and Stan. 2. Aufl. CRC texts in statistical science. Boca Raton: Taylor; Francis, CRC Press.\n\n\nShannon, C. E. 1948. „A Mathematical Theory of Communication“. Bell System Technical Journal 27 (3): 379–423. https://doi.org/10.1002/j.1538-7305.1948.tb01338.x."
  },
  {
    "objectID": "word-embedding.html",
    "href": "word-embedding.html",
    "title": "6  Word Embedding",
    "section": "",
    "text": "Bild von mcmurryjulie auf Pixabay"
  },
  {
    "objectID": "word-embedding.html#vorab",
    "href": "word-embedding.html#vorab",
    "title": "6  Word Embedding",
    "section": "6.1 Vorab",
    "text": "6.1 Vorab\n\n6.1.1 Lernziele\n\nDie vorgestellten Techniken des Textminings mit R anwenden können\n\n\n\n6.1.2 Vorbereitung\n\nArbeiten Sie Hvitfeldt und Silge (2022), Kap. 5 durch.\n\n\n\n6.1.3 Benötigte R-Pakete\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(slider)  # slide\nlibrary(widyr)  # pairwise_pmi\nlibrary(furrr)  # mehrere Kerne gleichzeitig nutzen\nlibrary(textdata)  # Worteinbettungen, vorgekocht"
  },
  {
    "objectID": "word-embedding.html#daten",
    "href": "word-embedding.html#daten",
    "title": "6  Word Embedding",
    "section": "6.2 Daten",
    "text": "6.2 Daten\n\n6.2.1 Complaints-Datensatz\nDer Datensatz complaints stammt aus dieser Quelle.\nDen Datensatz complaints kann man hier herunterladen. Im Buch ist die Webseite nicht (direkt?) angegeben. Die Datei ist mit gz gepackt; read_csv sollte das automatisch entpacken. Achtung: Die Datei ist recht groß.\n\nd_path <- \"https://github.com/EmilHvitfeldt/smltar/raw/master/data/complaints.csv.gz\"\ncomplaints <- read_csv(d_path)\n\nGeschickter als jedes Mal die Datei herunterzuladen, ist es, sie einmal herunterzuladen, und dann lokal zu speichern, etwa im Unterordner data des RStudio-Projektordners.\nNach dem Importieren wird der Datensatz in eine Tidy-Form gebracht (mit unnest_tokens) und dann verschachtelt, mit nest.\n\n\n6.2.2 Complaints verkürzt und geschachtelt\nUm ein Herumprobieren zu erleichtern, ist hier der Datensatz complaints in zwei verkürzten Formen bereitgestellt:\n\nnested_words2_path <- \"https://github.com/sebastiansauer/datascience-text/raw/main/data/nested_words2.rds\"\nnested_words3_path <- \"https://github.com/sebastiansauer/datascience-text/raw/main/data/nested_words3.rds\"\n\nnested_words2 enthält die ersten 10% des Datensatz nested_wordsund ist gut 4 MB groß (mit gz gezippt); er besteht aus ca. 11 Tausend Beschwerden. nested_words3 enthält nur die ersten 10 Beschwerden, was ihn gut zum Handhaben macht.\nBeide sind verschachtelt und aus tidy_complaints (s. Kap. 5.1) hervorgegangen.\n\nnested_words3 <- read_rds(nested_words3_path)\n\nDas sieht dann so aus:\n\nnested_words3 %>% \n  head(3)\n\n\n\n\n\n\n\n\n\ncomplaint_id\nwords\n\n\n\n\n3384392\nsystems , inc , is , trying , to , collect , a , debt , that , is , not , mine , not , owed , and , is , inaccurate\n\n\n3417821\ni , would , like , to , request , the , of , the , following , items , from , my , credit , report , which , are , the , result , of , my , victim , to , identity , theft , this , information , does , not , to , transactions , that , i , have , made , accounts , that , i , have , opened , as , the , attached , supporting , documentation, can , as , such , it , should , be , blocked , from , on , my , credit , report , pursuant , to , section , of , the , fair , credit , reporting , act\n\n\n3433198\nover , the , past , 2 , weeks , i , have , been , receiving , amounts , of , telephone , calls , from , the , company , listed , in , this , complaint , the , calls , between , xxxx , xxxx , and , xxxx , xxxx , to , my , cell , and , at , my , job , the , company , does , not , have , the , right , to , me , at , work , and , i , want , this , to , stop , it , is , extremely , to , be , told , 5 , times , a , day , that , i , have , a , call , from , this , collection, agency , while , at , work\n\n\n\n\n\n\nWerfen wir einen Blick in den TExt der ersten Beschwerde des Datensatzes mit der ID nested_words3_path$complaint_id[1].\n\nbeschwerde1_text <- nested_words3$words[[1]]\n\nDas ist ein Tibble mit einer Spalte und 17 Wörtern; da wir schon auf Unigramme aufgeteilt haben, ist jedes Wort ein Element des Vektors word:\n\nbeschwerde1_text %>% \n  head()\n\n\n\n\n\nword\n\n\n\n\nsystems\n\n\ninc\n\n\nis\n\n\ntrying\n\n\nto\n\n\ncollect\n\n\n\n\n\n\n\nbeschwerde1_text$word\n\n [1] \"systems\"    \"inc\"        \"is\"         \"trying\"     \"to\"        \n [6] \"collect\"    \"a\"          \"debt\"       \"that\"       \"is\"        \n[11] \"not\"        \"mine\"       \"not\"        \"owed\"       \"and\"       \n[16] \"is\"         \"inaccurate\""
  },
  {
    "objectID": "word-embedding.html#kommentare-und-hilfestellungen",
    "href": "word-embedding.html#kommentare-und-hilfestellungen",
    "title": "6  Word Embedding",
    "section": "6.3 Kommentare und Hilfestellungen",
    "text": "6.3 Kommentare und Hilfestellungen\n\n6.3.1 PMI berechnen\nRufen Sie sich die Definition der PMI ins Gedächtnis, s. Gleichung 5.1.\nMit R kann man die PMI z.B. so berechnen, s. ? pairwise_pmi aus dem Paket {widyr}.\nZum Paket widyr von Robinson und Silge:\n\nThis package wraps the pattern of un-tidying data into a wide matrix, performing some processing, then turning it back into a tidy form. This is useful for several mathematical operations such as co-occurrence counts, correlations, or clustering that are best done on a wide matrix.\n\nQuelle\nErzeugen wir uns Dummy-Daten:\n\ndat <- tibble(feature = rep(1:5, each = 2),\n              item = c(\"a\", \"b\",\n                       \"a\", \"c\",\n                       \"a\", \"c\",\n                       \"b\", \"e\",\n                       \"b\", \"f\"))\n\ndat\n\n\n\n\n\nfeature\nitem\n\n\n\n\n1\na\n\n\n1\nb\n\n\n2\na\n\n\n2\nc\n\n\n3\na\n\n\n3\nc\n\n\n4\nb\n\n\n4\ne\n\n\n5\nb\n\n\n5\nf\n\n\n\n\n\n\nAus der Hilfe der Funktion:\n\nFind pointwise mutual information of pairs of items in a column, based on a “feature” column that links them together. This is an example of the spread-operate-retidy pattern.\n\nDie Argumente der Funktion sind:\nitem\nItem to compare; will end up in item1 and item2 columns\nfeature\nColumn describing the feature that links one item to others\nManche Berechnungen (Operationen) lassen sich vielleicht leichter nicht in der Tidy-Form (Langform), sondern in der “breiten” oder Matrixform ausführen. Wandeln wir mal dat von der Langform in die Breitform um:\n\ntable(dat$item, dat$feature)\n\n   \n    1 2 3 4 5\n  a 1 1 1 0 0\n  b 1 0 0 1 1\n  c 0 1 1 0 0\n  e 0 0 0 1 0\n  f 0 0 0 0 1\n\n\nSilge und Robinson verdeutlichen das Prinzip von widyr so, s. Abbildung 6.1.\n\n\n\nAbbildung 6.1: Die Funktionsweise von widyr, Quelle: Silge und Robinson\n\n\n(Vgl. auch die Erklärung hier.)\nBauen wir das mal von Hand nach.\nRandwahrscheinlichkeiten von a und c sowie deren Produkt, p_a_p_c:\n\np_a <- 3/5\np_c <- 2/5\n\np_a_p_c <- p_a * p_c\np_a_p_c\n\n[1] 0.24\n\n\nGemeinsame Wahrscheinlichkeit von a und c:\n\np_ac <- 2/5\n\nPMI von Hand berechnet:\n\nlog(p_ac/p_a_p_c)\n\n[1] 0.5108256\n\n\nMan beachte, dass hier als Basis \\(e\\), der natürliche Logarithmus, verwendet wurde (nicht 2).\nJetzt berechnen wir die PMI mit pairwise_pmi.\n\npairwise_pmi(dat, item = item, feature = feature)\n\n\n\n\n\nitem1\nitem2\npmi\n\n\n\n\nb\na\n-0.5877867\n\n\nc\na\n0.5108256\n\n\na\nb\n-0.5877867\n\n\ne\nb\n0.5108256\n\n\nf\nb\n0.5108256\n\n\na\nc\n0.5108256\n\n\nb\ne\n0.5108256\n\n\nb\nf\n0.5108256\n\n\n\n\n\n\nWie man sieht, entspricht unserer Berechnung von Hand der Berechnung mit pairwise_pmi.\n\n\n6.3.2 Sliding\nSliding ist ein interessantes Konzept, aber man braucht vielleicht etwas Zeit, um sein Hirn um das Konzept zu wickeln…\nHier eine Illustration:\n\ntxt_vec <- \"Das ist ein Test, von dem nicht viel zu erwarten ist\"\n\nslider::slide(txt_vec, ~ .x, .before = 2)\n\n[[1]]\n[1] \"Das ist ein Test, von dem nicht viel zu erwarten ist\"\n\n\nOh, da passiert nichts?! Kaputt? Nein, wir müssen jedes Wort als ein Element des Vektors auffassen.\n\ntxt_df <-\n  tibble(txt = txt_vec) %>% \n  unnest_tokens(input = txt, output = word)\n\nhead(txt_df)\n\n\n\n\n\nword\n\n\n\n\ndas\n\n\nist\n\n\nein\n\n\ntest\n\n\nvon\n\n\ndem\n\n\n\n\n\n\n\nslider::slide(txt_df$word, ~ .x, .before = 2)\n\n[[1]]\n[1] \"das\"\n\n[[2]]\n[1] \"das\" \"ist\"\n\n[[3]]\n[1] \"das\" \"ist\" \"ein\"\n\n[[4]]\n[1] \"ist\"  \"ein\"  \"test\"\n\n[[5]]\n[1] \"ein\"  \"test\" \"von\" \n\n[[6]]\n[1] \"test\" \"von\"  \"dem\" \n\n[[7]]\n[1] \"von\"   \"dem\"   \"nicht\"\n\n[[8]]\n[1] \"dem\"   \"nicht\" \"viel\" \n\n[[9]]\n[1] \"nicht\" \"viel\"  \"zu\"   \n\n[[10]]\n[1] \"viel\"     \"zu\"       \"erwarten\"\n\n[[11]]\n[1] \"zu\"       \"erwarten\" \"ist\"     \n\n\nAh!\nDas Aufteilen in einzelne Wörter pro Element des Vektors könnte man auch so erreichen:\n\ntxt_vec2 <- str_split(txt_vec, pattern = boundary(\"word\")) %>% \n  simplify() \n\nslide(txt_vec2, .f = ~.x, .before = 2)\n\n[[1]]\n[1] \"Das\"\n\n[[2]]\n[1] \"Das\" \"ist\"\n\n[[3]]\n[1] \"Das\" \"ist\" \"ein\"\n\n[[4]]\n[1] \"ist\"  \"ein\"  \"Test\"\n\n[[5]]\n[1] \"ein\"  \"Test\" \"von\" \n\n[[6]]\n[1] \"Test\" \"von\"  \"dem\" \n\n[[7]]\n[1] \"von\"   \"dem\"   \"nicht\"\n\n[[8]]\n[1] \"dem\"   \"nicht\" \"viel\" \n\n[[9]]\n[1] \"nicht\" \"viel\"  \"zu\"   \n\n[[10]]\n[1] \"viel\"     \"zu\"       \"erwarten\"\n\n[[11]]\n[1] \"zu\"       \"erwarten\" \"ist\"     \n\n\nIn unserem Beispiel mit den Beschwerden:\n\nslide(beschwerde1_text$word,  ~.x, .before = 2)\n\n[[1]]\n[1] \"systems\"\n\n[[2]]\n[1] \"systems\" \"inc\"    \n\n[[3]]\n[1] \"systems\" \"inc\"     \"is\"     \n\n[[4]]\n[1] \"inc\"    \"is\"     \"trying\"\n\n[[5]]\n[1] \"is\"     \"trying\" \"to\"    \n\n[[6]]\n[1] \"trying\"  \"to\"      \"collect\"\n\n[[7]]\n[1] \"to\"      \"collect\" \"a\"      \n\n[[8]]\n[1] \"collect\" \"a\"       \"debt\"   \n\n[[9]]\n[1] \"a\"    \"debt\" \"that\"\n\n[[10]]\n[1] \"debt\" \"that\" \"is\"  \n\n[[11]]\n[1] \"that\" \"is\"   \"not\" \n\n[[12]]\n[1] \"is\"   \"not\"  \"mine\"\n\n[[13]]\n[1] \"not\"  \"mine\" \"not\" \n\n[[14]]\n[1] \"mine\" \"not\"  \"owed\"\n\n[[15]]\n[1] \"not\"  \"owed\" \"and\" \n\n[[16]]\n[1] \"owed\" \"and\"  \"is\"  \n\n[[17]]\n[1] \"and\"        \"is\"         \"inaccurate\"\n\n\n\n\n6.3.3 Funktion slide_windows\nDie Funktion slide_windows im Kapitel 5.2 ist recht kompliziert. In solchen Fällen ist es hilfreich, sich jeden Schritt einzeln ausführen zu lassen. Sind die Schritte in einer Funktion zusammengefasst, kann man mit debug(fun) sich die Schritte innerhalb der Funktion einzeln ausführen lassen.\n\nslide_windows <- function(tbl, window_size) {\n  skipgrams <- slider::slide(\n    tbl, \n    ~.x, \n    .after = window_size - 1, \n    .step = 1, \n    .complete = TRUE\n  )\n  \n  safe_mutate <- safely(mutate)\n  \n  out <- map2(skipgrams,\n              1:length(skipgrams),\n              ~ safe_mutate(.x, window_id = .y))\n  \n  out %>%\n    transpose() %>%\n    pluck(\"result\") %>%\n    compact() %>%\n    bind_rows()\n}\n\nErschwerend kommt eine große Datenmenge und eine lange Berechnungszeit dazu, was das Debuggen (Nachvollziehen und Durchdenken) der Schritte zusätzlich erschwert. In solchen Fällen hilft die goldende Regel: Mach es dir so einfach wie möglich (aber nicht einfacher). Wir nutzen also den stark verkleinerten Datensatz nested_words3, den wir oben importiert haben.\nHier ist der Syntax-Auszug:\n\nlibrary(widyr)\nlibrary(furrr)\n\nplan(multisession)  ## for parallel processing\n\n#debug(slide_windows)  # um sich die Schritte in `slide_windows` einzeln anzuschauen.\n\ntidy_pmi <- nested_words3 %>%  # <--- Kleiner Datensatz!\n  mutate(words = future_map(words, slide_windows, 4L)) %>%\n  unnest(words) %>%\n  unite(window_id, complaint_id, window_id) %>%\n  pairwise_pmi(word, window_id)\n\ntidy_pmi %>% \n  head()\n\n\n\n\n\nitem1\nitem2\npmi\n\n\n\n\ninc\nsystems\n5.728498\n\n\nis\nsystems\n2.838126\n\n\ntrying\nsystems\n5.035351\n\n\nsystems\ninc\n5.728498\n\n\nis\ninc\n2.838126\n\n\ntrying\ninc\n5.035351\n\n\n\n\n\n\n\n\n6.3.4 SVD\nDie Singulärwertzerlegung (singular value decomposition, SVD) ist ein Verfahren zur Verringerung der Spaltenzahl (Dimension), vergleichbar zur Faktorenanalyse. Zur Anschaulichkeit - und ohne substanziellen Hintergrund - sei folgendes Beispiel genannt: Die Verben “gehen”, “rennen”, “laufen”, “schwimmen”, “fahren”, “rutschen” könnten zu einer gemeinsamen Dimension, etwa “fortbewegen” reduziert werden. Jedes einzelne der eingehenden Verben erhält eine Zahl von 0 bis 1, das die konzeptionelle Nähe des Verbs zur “dahinterliegenden” Dimension (fortbewegen) quantifiziert; die Zahl nennt man auch die “Ladung” des Items (Worts) auf die Dimension. Sagen wir, wir identifizieren 10 Dimensionen. Man erhält dann für jedes unique Wort im Corpus einen Vektor mit den Ladungen auf die Dimensionsen. Im genannten Beispiel wäre es ein 10-stelliger Vektor. So wie ein 3-stelliger Vektor die Position einer Biene im Raum beschreibt1, beschreibt hier unser 10-stelliger Vektor die “Position” eines Worts in unserem Einbettungsvektor.\nDie Syntax dazu ist dieses Mal einfach:\n\ntidy_word_vectors <- \n  tidy_pmi %>%\n  widely_svd(\n    item1, item2, pmi,\n    nv = 100\n  )\n\ntidy_word_vectors %>% \n  (head)\n\n\n\n\n\nitem1\ndimension\nvalue\n\n\n\n\ninc\n1\n-0.0378963\n\n\nis\n1\n-0.1132069\n\n\ntrying\n1\n-0.0512764\n\n\nsystems\n1\n-0.0333332\n\n\nto\n1\n-0.1203434\n\n\ncollect\n1\n-0.0554211\n\n\n\n\n\n\nMit nv = 100 haben wir die Anzahl (n) der Dimensionen (Variablen, v) auf 100 bestimmt.\n\n\n6.3.5 Wortähnlichkeit\nJetzt, da wir mit der SVD jedes Wort in einem Koordinatensystem verortet haben, können wir die Abstände der Wörter im Koordinatensystem bestimmen. Das geht mit Hilfe des alten Pythagoras, s. Abbildung 6.2. Der Abstand, den man mit Hilfe des Satz des Pythagoras berechnet, nennt man auch euklidische Distanz.\n\n\n\nAbbildung 6.2: Euklidische Distanz in 2D, Quelle: Wikipedia, CC BY4.0, Kmhkmh\n\n\nOkay, wir sind in einem Raum mit vielen Dimensionen, was mein Hirn nicht mitmacht, aber der Algebra ist das egal. Pythagoras’ Satz lässt sich genauso anwenden, wenn es mehr als Dimensionen sind.\nDie Autoren basteln sich selber eine Funktion in Kap. 5.3, aber der Einfachheit halber nehme ich (erstmal) die entsprechende Funktion aus widyr:\n\nword_neighbors <- \ntidy_word_vectors %>% \n  pairwise_dist(item1, dimension, value)\n\nword_neighbors %>% \n  head()\n\n\n\n\n\nitem1\nitem2\ndistance\n\n\n\n\nis\ninc\n1.0220141\n\n\ntrying\ninc\n0.9332851\n\n\nsystems\ninc\n0.4161215\n\n\nto\ninc\n1.0913872\n\n\ncollect\ninc\n0.5221759\n\n\na\ninc\n1.0309566\n\n\n\n\n\n\nWas sind die Nachbarn von “inaccurate”?\n\nword_neighbors %>% \n  filter(item1 == \"inaccurate\") %>% \n  arrange(distance) %>% \n  head()\n\n\n\n\n\nitem1\nitem2\ndistance\n\n\n\n\ninaccurate\nmine\n0.5248868\n\n\ninaccurate\nscore\n0.5310116\n\n\ninaccurate\noh\n0.5400913\n\n\ninaccurate\nny\n0.5400913\n\n\ninaccurate\ndob\n0.5801281\n\n\ninaccurate\ncell\n0.6093670\n\n\n\n\n\n\nHier ist die Datenmenge zu klein, um vernünftige Schlüsse zu ziehen. Aber “incorrectly”, “correct”, “balance” sind wohl plausible Nachbarn von “inaccurate”.\n\n\n6.3.6 Word-Embeddings vorgekocht: Glove6B\nIn Kap. 5.4 schreiben die Autoren:\n\nIf your data set is too small, you typically cannot train reliable word embeddings.\n\nEin paar Millionen Wörter sollte der Corpus schon enthalten, so die Autoren. Da solche “Worteinbettungen” (word embedings) aufwändig zu erstellen sind, kann man fertige, “vorgekochte” Produkte nutzen.\nGlove6B wurde anhand von Wikipedia und anderen Datenquellen erstellt (Pennington, Socher, und Manning 2014).\n\n\n\n\n\n\nHinweis\n\n\n\nDie zugehörigen Daten sind recht groß; für glove6b (Pennington, Socher, und Manning 2014) ist fast ein Gigabyte fällig. Sie sollten sich die Daten in einem ruhigen Moment (mit stabiler Internetverbindung) herunterladen und in einem Verzeichnis meiner Wahl abgespeichert (datasets). Da bei mir Download abbrach, als ich embedding_glove6b(dimensions = 100) aufrief, habe ich die Daten manuell heruntergeladen, s.u.\n\n\n\nglove6b <- embedding_glove6b(dir = \"~/datasets\", dimensions = 10, manual_download = TRUE)\nglove6b %>% \n  head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntoken\nd1\nd2\nd3\nd4\nd5\nd6\nd7\nd8\nd9\nd10\nd11\nd12\nd13\nd14\nd15\nd16\nd17\nd18\nd19\nd20\nd21\nd22\nd23\nd24\nd25\nd26\nd27\nd28\nd29\nd30\nd31\nd32\nd33\nd34\nd35\nd36\nd37\nd38\nd39\nd40\nd41\nd42\nd43\nd44\nd45\nd46\nd47\nd48\nd49\nd50\nd51\nd52\nd53\nd54\nd55\nd56\nd57\nd58\nd59\nd60\nd61\nd62\nd63\nd64\nd65\nd66\nd67\nd68\nd69\nd70\nd71\nd72\nd73\nd74\nd75\nd76\nd77\nd78\nd79\nd80\nd81\nd82\nd83\nd84\nd85\nd86\nd87\nd88\nd89\nd90\nd91\nd92\nd93\nd94\nd95\nd96\nd97\nd98\nd99\nd100\n\n\n\n\nthe\n-0.038194\n-0.244870\n0.728120\n-0.399610\n0.083172\n0.043953\n-0.391410\n0.334400\n-0.57545\n0.087459\n0.287870\n-0.06731\n0.30906\n-0.26384\n-0.13231\n-0.20757\n0.33395\n-0.338480\n-0.31743\n-0.48336\n0.146400\n-0.373040\n0.34577\n0.052041\n0.4494600\n-0.469710\n0.026280\n-0.54155\n-0.155180\n-0.141070\n-0.039722\n0.28277\n0.14393\n0.234640\n-0.31021\n0.086173\n0.20397\n0.52624\n0.171640\n-0.082378\n-0.71787\n-0.41531\n0.203350\n-0.12763\n0.413670\n0.551870\n0.579080\n-0.33477\n-0.36559\n-0.54857\n-0.0628920\n0.26584\n0.30205\n0.99775\n-0.804810\n-3.0243\n0.012540\n-0.369420\n2.2167\n0.72201\n-0.24978\n0.92136\n0.034514\n0.46745\n1.10790\n-0.193580\n-0.074575\n0.233530\n-0.052062\n-0.220440\n0.057162\n-0.15806\n-0.307980\n-0.416250\n0.379720\n0.1500600\n-0.532120\n-0.205500\n-1.25260\n0.071624\n0.70565\n0.497440\n-0.42063\n0.26148\n-1.5380\n-0.30223\n-0.073438\n-0.283120\n0.371040\n-0.25217\n0.016215\n-0.017099\n-0.389840\n0.87424\n-0.72569\n-0.51058\n-0.520280\n-0.14590\n0.82780\n0.270620\n\n\n,\n-0.107670\n0.110530\n0.598120\n-0.543610\n0.673960\n0.106630\n0.038867\n0.354810\n0.06351\n-0.094189\n0.157860\n-0.81665\n0.14172\n0.21939\n0.58505\n-0.52158\n0.22783\n-0.166420\n-0.68228\n0.35870\n0.425680\n0.190210\n0.91963\n0.575550\n0.4618500\n0.423630\n-0.095399\n-0.42749\n-0.165670\n-0.056842\n-0.295950\n0.26037\n-0.26606\n-0.070404\n-0.27662\n0.158210\n0.69825\n0.43081\n0.279520\n-0.454370\n-0.33801\n-0.58184\n0.223640\n-0.57780\n-0.268620\n-0.204250\n0.563940\n-0.58524\n-0.14365\n-0.64218\n0.0054697\n-0.35248\n0.16162\n1.17960\n-0.476740\n-2.7553\n-0.132100\n-0.047729\n1.0655\n1.10340\n-0.22080\n0.18669\n0.131770\n0.15117\n0.71310\n-0.352150\n0.913480\n0.617830\n0.709920\n0.239550\n-0.145710\n-0.37859\n-0.045959\n-0.473680\n0.238500\n0.2053600\n-0.189960\n0.325070\n-1.11120\n-0.363410\n0.98679\n-0.084776\n-0.54008\n0.11726\n-1.0194\n-0.24424\n0.127710\n0.013884\n0.080374\n-0.35414\n0.349510\n-0.722600\n0.375490\n0.44410\n-0.99059\n0.61214\n-0.351110\n-0.83155\n0.45293\n0.082577\n\n\n.\n-0.339790\n0.209410\n0.463480\n-0.647920\n-0.383770\n0.038034\n0.171270\n0.159780\n0.46619\n-0.019169\n0.414790\n-0.34349\n0.26872\n0.04464\n0.42131\n-0.41032\n0.15459\n0.022239\n-0.64653\n0.25256\n0.043136\n-0.194450\n0.46516\n0.456510\n0.6858800\n0.091295\n0.218750\n-0.70351\n0.167850\n-0.350790\n-0.126340\n0.66384\n-0.25820\n0.036542\n-0.13605\n0.402530\n0.14289\n0.38132\n-0.122830\n-0.458860\n-0.25282\n-0.30432\n-0.112150\n-0.26182\n-0.224820\n-0.445540\n0.299100\n-0.85612\n-0.14503\n-0.49086\n0.0082973\n-0.17491\n0.27524\n1.44010\n-0.212390\n-2.8435\n-0.279580\n-0.457220\n1.6386\n0.78808\n-0.55262\n0.65000\n0.086426\n0.39012\n1.06320\n-0.353790\n0.483280\n0.346000\n0.841740\n0.098707\n-0.242130\n-0.27053\n0.045287\n-0.401470\n0.113950\n0.0062226\n0.036673\n0.018518\n-1.02130\n-0.208060\n0.64072\n-0.068763\n-0.58635\n0.33476\n-1.1432\n-0.11480\n-0.250910\n-0.459070\n-0.096819\n-0.17946\n-0.063351\n-0.674120\n-0.068895\n0.53604\n-0.87773\n0.31802\n-0.392420\n-0.23394\n0.47298\n-0.028803\n\n\nof\n-0.152900\n-0.242790\n0.898370\n0.169960\n0.535160\n0.487840\n-0.588260\n-0.179820\n-1.35810\n0.425410\n0.153770\n0.24215\n0.13474\n0.41193\n0.67043\n-0.56418\n0.42985\n-0.012183\n-0.11677\n0.31781\n0.054177\n-0.054273\n0.35516\n-0.302410\n0.3143400\n-0.338460\n0.717150\n-0.26855\n-0.158370\n-0.474670\n0.051581\n-0.33252\n0.15003\n-0.129900\n-0.54617\n-0.378430\n0.64261\n0.82187\n-0.080006\n0.078479\n-0.96976\n-0.57741\n0.564910\n-0.39873\n-0.057099\n0.197430\n0.065706\n-0.48092\n-0.20125\n-0.40834\n0.3945600\n-0.02642\n-0.11838\n1.01200\n-0.531710\n-2.7474\n-0.042981\n-0.748490\n1.7574\n0.59085\n0.04885\n0.78267\n0.384970\n0.42097\n0.67882\n0.103370\n0.632800\n-0.026595\n0.586470\n-0.443320\n0.330570\n-0.12022\n-0.556450\n0.073611\n0.209150\n0.4339500\n-0.012761\n0.089874\n-1.79910\n0.084808\n0.77112\n0.631050\n-0.90685\n0.60326\n-1.7515\n0.18596\n-0.506870\n-0.702030\n0.665780\n-0.81304\n0.187120\n-0.018488\n-0.267570\n0.72700\n-0.59363\n-0.34839\n-0.560940\n-0.59100\n1.00390\n0.206640\n\n\nto\n-0.189700\n0.050024\n0.190840\n-0.049184\n-0.089737\n0.210060\n-0.549520\n0.098377\n-0.20135\n0.342410\n-0.092677\n0.16100\n-0.13268\n-0.28160\n0.18737\n-0.42959\n0.96039\n0.139720\n-1.07810\n0.40518\n0.505390\n-0.550640\n0.48440\n0.380440\n-0.0029055\n-0.349420\n-0.099696\n-0.78368\n1.036300\n-0.231400\n-0.471210\n0.57126\n-0.21454\n0.359580\n-0.48319\n1.087500\n0.28524\n0.12447\n-0.039248\n-0.076732\n-0.76343\n-0.32409\n-0.574900\n-1.08930\n-0.418110\n0.451200\n0.121120\n-0.51367\n-0.13349\n-1.13780\n-0.2876800\n0.16774\n0.55804\n1.53870\n0.018859\n-2.9721\n-0.242160\n-0.924950\n2.1992\n0.28234\n-0.34780\n0.51621\n-0.433870\n0.36852\n0.74573\n0.072102\n0.279310\n0.925690\n-0.050336\n-0.858560\n-0.135800\n-0.92551\n-0.339910\n-1.039400\n-0.067203\n-0.2137900\n-0.476900\n0.213770\n-0.84008\n0.052536\n0.59298\n0.296040\n-0.67644\n0.13916\n-1.5504\n-0.20765\n0.722200\n0.520560\n-0.076221\n-0.15194\n-0.131340\n0.058617\n-0.318690\n-0.61419\n-0.62393\n-0.41548\n-0.038175\n-0.39804\n0.47647\n-0.159830\n\n\nand\n-0.071953\n0.231270\n0.023731\n-0.506380\n0.339230\n0.195900\n-0.329430\n0.183640\n-0.18057\n0.289630\n0.204480\n-0.54960\n0.27399\n0.58327\n0.20468\n-0.49228\n0.19974\n-0.070237\n-0.88049\n0.29485\n0.140710\n-0.100900\n0.99449\n0.369730\n0.4455400\n0.289980\n-0.137600\n-0.56365\n-0.029365\n-0.412200\n-0.252690\n0.63181\n-0.44767\n0.243630\n-0.10813\n0.251640\n0.46967\n0.37550\n-0.236130\n-0.141290\n-0.44537\n-0.65737\n-0.042421\n-0.28636\n-0.288110\n0.063766\n0.202810\n-0.53542\n0.41307\n-0.59722\n-0.3861400\n0.19389\n-0.17809\n1.66180\n-0.011819\n-2.3737\n0.058427\n-0.269800\n1.2823\n0.81925\n-0.22322\n0.72932\n-0.053211\n0.43507\n0.85011\n-0.429350\n0.926640\n0.390510\n1.058500\n-0.245610\n-0.182650\n-0.53280\n0.059518\n-0.660190\n0.189910\n0.2883600\n-0.243400\n0.527840\n-0.65762\n-0.140810\n1.04910\n0.513400\n-0.23816\n0.69895\n-1.4813\n-0.24870\n-0.179360\n-0.059137\n-0.080560\n-0.48782\n0.014487\n-0.625900\n-0.323670\n0.41862\n-1.08070\n0.46742\n-0.499310\n-0.71895\n0.86894\n0.195390\n\n\n\n\n\n\nIn eine Tidyform bringen:\n\ntidy_glove <- \n  glove6b %>%\n  pivot_longer(contains(\"d\"),\n               names_to = \"dimension\") %>%\n  rename(item1 = token)\n\nhead(tidy_glove)\n\n\n\n\n\nitem1\ndimension\nvalue\n\n\n\n\nthe\nd1\n-0.038194\n\n\nthe\nd2\n-0.244870\n\n\nthe\nd3\n0.728120\n\n\nthe\nd4\n-0.399610\n\n\nthe\nd5\n0.083172\n\n\nthe\nd6\n0.043953\n\n\n\n\n\n\nGanz schön groß:\n\nobject.size(tidy_glove)\n\n983837536 bytes\n\n\nIn Megabyte2\n\nobject.size(tidy_glove) / 2^20\n\n938.3 bytes\n\n\nEinfacher und genauer geht es so:\n\npryr::object_size(tidy_glove)\n\n983.83 MB\n\n\n\npryr::mem_used()\n\n1.48 GB\n\n\nUm Speicher zu sparen, könnte man glove6b wieder direkt löschen, wenn man nur mit der Tidyform weiterarbeitet.\n\nrm(glove6b)\n\nJetzt können wir wieder nach Nachbarn fragen, im euklidischen Sinne, wie oben. Probieren wir aus, welche Wörter nah zu “inaccurate” stehen.\n\n\n\n\n\n\nHinweis\n\n\n\nWie wir oben gesehen haben, ist der Datensatz riesig3, was die Berechnungen (zeitaufwändig) und damit nervig machen können. Darüber hinaus kann es nötig sein, dass Sie mehr Speicher auf Ihrem Computer zur Verfügung stellen müssen4. Wir müssen noch maximum_size = NULL, um das Jonglieren mit riesigen Matrixen zu erlauben. Möge der Gott der RAMs und Arbeitsspeicher uns gnädig sein!\n\n\nMit pairwise_dist dauert die Berechnung sehr lange und braucht wohl sehr viel Speicher. Mitunter kam folgender Fehler auf: “R error: vector memory exhausted (limit reached?)”.\n\nword_neighbors_glove6b <- \ntidy_glove %>% \n  slice_head(prop = .1) %>% \n  pairwise_dist(item1, dimension, value, maximum_size = NULL)\n\nhead(word_neighbors_glove6b)\n\ntidy_glove %>% \n  filter(item1 == \"inaccurate\") %>% \n  arrange(-value) %>% \n  slice_head(n = 5)\n\nDeswegen probieren wir doch die Funktion nearest_neighbors, so wies im Buch vorgeschlagen wird.\n\nnearest_neighbors <- function(df, token) {\n  df %>%\n    widely(\n      ~ {\n        y <- .[rep(token, nrow(.)), ]\n        res <- rowSums(. * y) / \n          (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))\n        matrix(res, ncol = 1, dimnames = list(x = names(res)))\n      },\n      sort = TRUE,\n      maximum_size = NULL\n    )(item1, dimension, value) %>%\n    select(-item2)\n}\n\n\ntidy_glove %>%\n  # slice_head(prob = .1) %>% \n  nearest_neighbors(\"error\") %>% \n  head()\n\n\n\n\n\nitem1\nvalue\n\n\n\n\nerror\n1.0000000\n\n\nerrors\n0.7916719\n\n\nmistake\n0.6641135\n\n\ncorrect\n0.6205814\n\n\nincorrect\n0.6132556\n\n\nfault\n0.6068035\n\n\n\n\n\n\nEntschachteln wir unsere Daten zu complaints:\n\ntidy_complaints3 <-\n  nested_words3 %>% \n  unnest(words)\n\nDann erstellen wir uns eine Tabelle, in der nur die Schnittmenge der Wörter aus den Beschwerden und Glove vorkommen. Dazu nutzen winr einen inneren Join\n\n\n\nInner Join, Quelle: Garrick Adenbuie\n\n\nQuelle\n\ncomplaints_glove <- \ntidy_complaints3 %>% \n  inner_join(by = \"word\", \n  tidy_glove %>% \n  distinct(item1) %>% \n  rename(word = item1)) \n\nhead(complaints_glove)\n\n\n\n\n\ncomplaint_id\nword\n\n\n\n\n3384392\nsystems\n\n\n3384392\ninc\n\n\n3384392\nis\n\n\n3384392\ntrying\n\n\n3384392\nto\n\n\n3384392\ncollect\n\n\n\n\n\n\nWie viele unique (distinkte) Wörter gibt es in unserem Corpus?\n\ntidy_complaints3_distinct_words_n <- \ntidy_complaints3 %>% \n  distinct(word) %>% \n  nrow()\n\ntidy_complaints3_distinct_words_n\n\n[1] 222\n\n\nIn tidy_complaints gibt es übrigens 222 verschiedene Wörter.\n\nword_matrix <- tidy_complaints3 %>%\n  inner_join(by = \"word\",\n             tidy_glove %>%\n               distinct(item1) %>%\n               rename(word = item1)) %>%\n  count(complaint_id, word) %>%\n  cast_sparse(complaint_id, word, n)\n\n#word_matrix\n\nword_matrix zählt für jede der 10 Beschwerden, welche Wörter (und wie häufig) vorkommen.\n\ndim(word_matrix)\n\n[1]  10 222\n\n\n10 Beschwerden (Dokumente) und 222 unique Wörter.\n\nglove_matrix <- tidy_glove %>%\n  inner_join(by = \"item1\",\n             tidy_complaints3 %>%\n               distinct(word) %>%\n               rename(item1 = word)) %>%\n  cast_sparse(item1, dimension, value)\n\n#glove_matrix\n\nglove_matrix gibt für jedes unique Wort den Einbettungsvektor an.\n\ndim(glove_matrix)\n\n[1] 222 100\n\n\nDas sind 222 unique Wörter und 100 Dimensionen des Einbettungsvektors.\nJetzt können wir noch pro Dokument (10 in diesem Beispiel) die mittlere “Position” jedes Dokuments im Einbettungsvektor ausrechnen. Bildlich gesprochen: Was ist der mittlere Raumpunkt (Zentroid) des Bienenschwarms, wobei eine Biene ein Dokument darstellt und die Raumachsen die Dimensionen des Einbettungsvektors.\nDazu gewichten wir jedes Wort eines Dokuments mit den Ladungen des Einbettungsvektor und summieren diese Terme. Es resultiert eine Matrix mit einem Einbettungsvektor pro Dokument. Diese Matrix können wir jetzt als Prädiktorenmatrix hernehmen.\n\ndoc_matrix <- word_matrix %*% glove_matrix\n#doc_matrix %>% head()\n\n\ndim(doc_matrix)\n\n[1]  10 100\n\n\nDie Anzahl der Dokumente ist 10 und die Anzahl der Dimensionen (des Einbettungsvektors) ist 100."
  },
  {
    "objectID": "word-embedding.html#fazit",
    "href": "word-embedding.html#fazit",
    "title": "6  Word Embedding",
    "section": "6.4 Fazit",
    "text": "6.4 Fazit\nWorteinbettungen sind eine aufwändige Angelegenheit. Positiv gesprochen kann ein Analysti die Muskeln spielen lassen und zeigen, was sie oder er so alles drauf hat. Ist ja schon cooles Zeugs, die Word Embeddings. Es besteht die Chance, dass man mit dieser Methode bessere Vorhersagen erreicht, als mit anderen, einfachen Ansätzen wir Worthäufigkeiten oder tf-idf. Auf der anderen Seite ist es oft sinnvoll, mit einfachen Ansätzen zu starten, und zu sehen, wie weit man kommt. Vielleicht ja weit genug.\n\n\n\n\nHvitfeldt, Emil, und Julia Silge. 2022. Supervised Machine Learning for Text Analysis in R. 1. Aufl. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nPennington, Jeffrey, Richard Socher, und Christopher Manning. 2014. „GloVe: Global Vectors for Word Representation“. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1532–43. Doha, Qatar: Association for Computational Linguistics. https://doi.org/10.3115/v1/D14-1162."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Hvitfeldt, Emil, and Julia Silge. 2022. Supervised Machine Learning\nfor Text Analysis in r. 1st ed. Boca Raton: Chapman;\nHall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nKönig, Tim, Wolf J. Schünemann, Alexander Brand, Julian Freyberg, and\nMichael Gertz. 2022. “The EPINetz Twitter Politicians\nDataset 2021. A New Resource for the Study of the German Twittersphere\nand Its Application for the 2021 Federal Elections.”\nPolitische Vierteljahresschrift 63 (3): 529–47. https://doi.org/10.1007/s11615-022-00405-7.\n\n\nKurz, A. Solomon. 2021. Statistical Rethinking with Brms, Ggplot2,\nand the Tidyverse: Second Edition. https://bookdown.org/content/4857/.\n\n\nLex, Alexander, Nils Gehlenborg, Hendrik Strobelt, Romain Vuillemot, and\nHanspeter Pfister. 2014. “UpSet: Visualization of\nIntersecting Sets.” IEEE Transactions on\nVisualization and Computer Graphics 20 (12): 1983–92. https://doi.org/10.1109/TVCG.2014.2346248.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course\nwith Examples in r and Stan. 2nd ed. CRC Texts in\nStatistical Science. Boca Raton: Taylor; Francis, CRC\nPress.\n\n\nPennington, Jeffrey, Richard Socher, and Christopher Manning. 2014.\n“GloVe: Global Vectors for Word\nRepresentation.” In Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), 1532–43. Doha, Qatar: Association for\nComputational Linguistics. https://doi.org/10.3115/v1/D14-1162.\n\n\nRemus, Robert, Uwe Quasthoff, and Gerhard Heyer. 2010.\n“SentiWS - a Publicly Available German-Language\nResource for Sentiment Analysis.” Proceedings of the 7th\nInternational Language Ressources and Evaluation\n(LREC’10), 1168–71.\n\n\nShannon, C. E. 1948. “A Mathematical Theory of\nCommunication.” Bell System Technical Journal 27 (3):\n379–423. https://doi.org/10.1002/j.1538-7305.1948.tb01338.x.\n\n\nWickham, Hadley, and Garrett Grolemund. 2018. R Für Data Science:\nDaten Importieren, Bereinigen, Umformen, Modellieren Und\nVisualisieren. Translated by Frank Langenau. 1. Auflage.\nHeidelberg: O’Reilly. https://r4ds.had.co.nz/index.html."
  }
]