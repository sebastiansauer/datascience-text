[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science 2: Textdaten als Grundlage pr√§diktiver Modelle",
    "section": "",
    "text": "falls Sie die Pakete schon installiert haben, k√∂nnten Sie mal in RStudio auf ‚Äúupdate.packages‚Äù klicken‚Ü©Ô∏é"
  },
  {
    "objectID": "pruefung.html",
    "href": "pruefung.html",
    "title": "1¬† Pr√ºfung",
    "section": "",
    "text": "Text als Datenbasis pr√§diktiver Modelle\nBild von mcmurryjulie auf Pixabay"
  },
  {
    "objectID": "pruefung.html#pr√ºfungsform-datenanalyse-als-quarto-blog-post",
    "href": "pruefung.html#pr√ºfungsform-datenanalyse-als-quarto-blog-post",
    "title": "1¬† Pr√ºfung",
    "section": "1.1 Pr√ºfungsform: Datenanalyse als Quarto-Blog-Post",
    "text": "1.1 Pr√ºfungsform: Datenanalyse als Quarto-Blog-Post\nAls Pr√ºfungsleistung ist ein Corpus an Twitter-Daten, die an deutsche, aktuelle Politiker gerichtet sind, auf Hate Speech hin zu untersuchen.\n\nDer Dozent wei√üt jedis Studenti einen deutschen Politiker (bzw. dessen Twitter-Account) zu.\nDer Bericht der Analyse ist als Quarto Blog-Posts zu formatieren.\nEinzureichen ist die URL des Posts.\nDer Post muss w√§hrend des gesamten Pr√ºfungszeitraums online sein, gehostet von einem beliebigen Provider (z.B. Netlify oder Github).\nNach Einreichen des Posts d√ºrfen keine √Ñnderungen mehr vorgenommen werden.\nZu Dokumentationszwecken soll ein PDF-Print des Posts in die Abgabe mit hochgeladen werden. Das PDF-Print des Posts muss identisch (exakt gleich) sein zum Post, der √ºber die URL verf√ºgbar ist.\nDer Quelltext des Posts soll bei Github vorliegen.\nDie Methoden des Textminings aus dem Unterricht sollen angewendet werden\nZus√§tzlich d√ºrfen sonstige Techniken des Textminings (die nicht im Unterricht behandelt wurden), angewendet werden\nDar√ºber hinaus sollen pr√§diktive Modelle zur Klassifikation von Hate-Speech (ja/nein) berechnet werden.\nEin Trainingsdatensatz wird gemeinsam erstellt.\nMethoden der Inferenzstatistik (wie Bayes) sind nicht n√∂tig.\nEs soll eine mittlere vierstellige Zahl an Tweets verarbeitet werden oder wenigstens so viele Tweets wie verf√ºgbar."
  },
  {
    "objectID": "pruefung.html#politiker-accounts",
    "href": "pruefung.html#politiker-accounts",
    "title": "1¬† Pr√ºfung",
    "section": "1.2 Politiker-Accounts",
    "text": "1.2 Politiker-Accounts\nFolgende Politiker-Accounts k√∂nnten als Pr√ºfungsgegenstand verwendet werden (nach Hinweisen des Dozenten):\n\nOlaf Scholz\nAnnalena Baerbock\nChristian Lindner\nRobert Habeck (bzw. der Account seines Ministeriums)\nCem √ñzdemir\nVolker Wissing\nNancy Faeser\nFriedrich Merz\nBj√∂rn H√∂cke\nSarah Wagenknecht\n\n\n\nAlle folgenden Hinweise gelten nur insoweit Ihre Lehrkraft Ihnen keine anders lautenden Hinweise gegeben hat (schriftlich).\n\n1.3 Allgemeines\n\nGegenstand dieser Pr√ºfungsform ist die Analyse eines Datensatzes nach einer Forschungsfrage und die Dokumentation dieser Analyse.\nSchreiben Sie Ihre Datenanalyse in Form eines Berichts, der sich an den Gliederungspunkten wie unten dargestellt orientiert.\nWenden Sie die passenden, im Unterricht eingef√ºhrten, statistischen Verfahren an. Es steht Ihnen frei, andere (nicht im Unterricht behandelte) Verfahren zur Analyse der Daten anzuwenden, nach Ma√ügabe der fachlichen Angemessenheit.\nWerten Sie die Daten mit R aus.\nDie R-Syntax soll im Hauptteil des Berichts dokumentiert werden. R-Output darf ggf. gek√ºrzt wiedergegeben werden.\nF√ºgen Sie keine Erkl√§rungen oder Definitionen von statistischen Verfahren an.\nBeschreiben und interpretieren Sie jede Analyse bzw. jeden R-Code bzw. jedes Ergebnis (jede R-Ausgabe).\nVon hoher Bedeutung ist die Korrektheit der Beschreibung und Interpretation der statistischen Modellierung (z.B. mit der Regressionsanalyse).\nEs hat keinen Einfluss auf Ihre Note, ob sich ein (erwarteter) Effekt zeigt und wie stark dieser Effekt ggf. ist.\n\n\n\n1.4 Formatierung des Berichts\n\nDer Bericht ist nur elektronisch, nicht ausgedruckt einzureichen.\nDer Bericht kann als paginiertes Dokument oder als nicht-paginiertes Dokument (d.h. als HTML-Dokument) eingereicht werden.\nPaginierte Formate sind als PDF-Datei einzureichen. HTML-Formate sind entweder in HTML-Form oder (nur bei Bedarf) als PDF-Druck eines HTML-Dokuments einzureichen.\nDie Wahl eines bestimmten Stylesheets ist nicht von Bedeutung. Lesbarkeit und √úbersichtlichkeit in der Formatierung sind unabh√§ngig davon anzustreben.\nIm Kopfbereich (oder auf einem Deckblatt) sind die relevanten Metadatan anzugeben wie Name (Nachname, Vorname) der Autorin/des Autors, Abgabedatum, Titel der Arbeit, Modul.\n\n\n\n1.5 Formalia\n\nRichtlinien einer Wortzahl gibt es nicht. Entscheidend ist, dass relevante Analysen durchgef√ºhrt und beschrieben wurden.\nDer Anspruch richtet sich nach dem Inhalt und Niveau der auf dieses Modul vorbereitenden Module. Oft sind das Module in quantitativer Datenanalyse (und wissenschaftliches Arbeiten). Deren Inhalte sollen im Rahmen dieser Pr√ºfungsleistung als selbst√§ndig und fl√ºssig verf√ºgbare Kompetenz von den Studentis demonstriert werden.\nDie Gliederung der Arbeit kann sich an den PPDAC-Zyklus und am Data Science Model von Wickham und Grolemund orientieren.\n\n\n\n1.6 Beurteilungskriterien\nDie Arbeit wird im Hinblick auf drei Kriterien bewertet:\n\nFormalia (z. B. Vollst√§ndigkeit der Abarbeitung, Angemessenheit der √§u√üeren Gestaltung, Fokus auf Wesentliches)\nMethodik (z. B. Richtige Auswahl und Anwendung der Verfahren)\nInhalt (z. B. Verst√§ndlichkeit, Breite und Tiefe der Probleml√∂sung, Korrektheit der Interpretation)\n\nSie erhalten f√ºr jedes der drei Kriterien eine Teilnote sowie eine Gesamtnote. Au√üerdem erhalten Sie ggf. f√ºr die Kriterien noch ausformulierte Hinweise.\nDie Gesamtnote muss sich nicht als Mittelwert der Teilnoten ergeben.\nInsbesondere kann eine F√ºnf in einem der Kriterien zum Durchfallen f√ºhren, auch wenn die anderen beiden Kriterien gut oder sehr gut beurteilt wurden.\n\n\n1.7 Beispiele f√ºr Aspekte der Beurteilungskriterien\n\nWurden deskriptive Statistiken (an angemessenen Ort) berichtet?\nWurden Diagramme und Tabellen angemessen eingesetzt?\nWurde Inferenzstatistik (angemessen) eingesetzt?\nWurden Effektst√§rkema√üe (idealerweise mit Konfidenzintervallen dazu) berichtet?\nWurden alle relevanten Informationen f√ºr ein statistisches Verfahren angegeben (z.B. zum gew√§hlten Prior)?\nWurde die Aussagekraft von Modellergebnissen richtig eingesch√§tzt?\nWaren die Schlussfolgerungen, die aus den statistischen Ergebnissen gezogen wurden, angemessen (z. B. wurde erkannt, dass ein Nicht-Verwerfeen einer Hypothese nicht automatisch ein Best√§tigen derselben bedeutet)?\nWurde angemessen gerundet (inkl. konsistente Anzahl von Nachkommastellen)?\nPassen die statistischen Verfahren zu den Hypothesen?\nWurden die Voraussetzungen der statistischen Verfahren gepr√ºft?\nSind die Ergebnisse reproduzierbar (Daten und Syntax eingereicht)?\n\n\n\n1.8 Beispiele f√ºr Fehler\nSchwere Fehler, die zum Durchfallen oder deutlichem Abwerten der Note f√ºhren k√∂nnen, sind z.B.:\n\nfehlende Inferenzstatistik (oder ad√§quatem Ersatz)\nfalsche Interpretation von Posteriori-Verteilungen oder p-Werten\nkeine Angabe von Konfidenzintervallen\nfalsche Interpretation von Konfidenzintervallen\nWahl des falschen Intervalls (Vorhersageintervall vs.¬†Perzentilintervall vs.¬†HDI)\nfalsche Entscheidung zum Hypothesentest auf Basis entsprechender Kennwerte (wie ROPE-Wahrscheinlichkeit oder p-Wert)\nfalsche Wahl des statistischen Verfahrens\nfehlende Deskriptivstatistik\n\nH√§ufige kleinere M√§ngel sind z. B.\n\npixelige Abbildungen\nR-Ausgaben oder R-Syntax als Screenshot\nfehlende Seitenzahlen (nur bei paginierten Formaten, nicht bei HTML)\nun√ºbersichtliche Diagramme\nkein (verlinktes) Inhaltsverzeichnis Ô∏é\nfehlende oder unverst√§ndliche Achsenbeschriftung bei Diagrammen\nfehlende oder falsche Beschreibung der/des Skalenniveau(s) der untersuchten Variablen"
  },
  {
    "objectID": "pruefung.html#allgemeines",
    "href": "pruefung.html#allgemeines",
    "title": "1¬† Pr√ºfung",
    "section": "1.3 Allgemeines",
    "text": "1.3 Allgemeines\n\nGegenstand dieser Pr√ºfungsform ist die Analyse eines Datensatzes nach einer Forschungsfrage und die Dokumentation dieser Analyse.\nSchreiben Sie Ihre Datenanalyse in Form eines Berichts, der sich an den Gliederungspunkten wie unten dargestellt orientiert.\nWenden Sie die passenden, im Unterricht eingef√ºhrten, statistischen Verfahren an. Es steht Ihnen frei, andere (nicht im Unterricht behandelte) Verfahren zur Analyse der Daten anzuwenden, nach Ma√ügabe der fachlichen Angemessenheit.\nWerten Sie die Daten mit R aus.\nDie R-Syntax soll im Hauptteil des Berichts dokumentiert werden. R-Output darf ggf. gek√ºrzt wiedergegeben werden.\nF√ºgen Sie keine Erkl√§rungen oder Definitionen von statistischen Verfahren an.\nBeschreiben und interpretieren Sie jede Analyse bzw. jeden R-Code bzw. jedes Ergebnis (jede R-Ausgabe).\nVon hoher Bedeutung ist die Korrektheit der Beschreibung und Interpretation der statistischen Modellierung (z.B. mit der Regressionsanalyse).\nEs hat keinen Einfluss auf Ihre Note, ob sich ein (erwarteter) Effekt zeigt und wie stark dieser Effekt ggf. ist."
  },
  {
    "objectID": "pruefung.html#formatierung-des-berichts",
    "href": "pruefung.html#formatierung-des-berichts",
    "title": "1¬† Pr√ºfung",
    "section": "1.4 Formatierung des Berichts",
    "text": "1.4 Formatierung des Berichts\n\nDer Bericht ist nur elektronisch, nicht ausgedruckt einzureichen.\nDer Bericht kann als paginiertes Dokument oder als nicht-paginiertes Dokument (d.h. als HTML-Dokument) eingereicht werden.\nPaginierte Formate sind als PDF-Datei einzureichen. HTML-Formate sind entweder in HTML-Form oder (nur bei Bedarf) als PDF-Druck eines HTML-Dokuments einzureichen.\nDie Wahl eines bestimmten Stylesheets ist nicht von Bedeutung. Lesbarkeit und √úbersichtlichkeit in der Formatierung sind unabh√§ngig davon anzustreben.\nIm Kopfbereich (oder auf einem Deckblatt) sind die relevanten Metadatan anzugeben wie Name (Nachname, Vorname) der Autorin/des Autors, Abgabedatum, Titel der Arbeit, Modul."
  },
  {
    "objectID": "pruefung.html#formalia",
    "href": "pruefung.html#formalia",
    "title": "1¬† Pr√ºfung",
    "section": "1.5 Formalia",
    "text": "1.5 Formalia\n\nRichtlinien einer Wortzahl gibt es nicht. Entscheidend ist, dass relevante Analysen durchgef√ºhrt und beschrieben wurden.\nDer Anspruch richtet sich nach dem Inhalt und Niveau der auf dieses Modul vorbereitenden Module. Oft sind das Module in quantitativer Datenanalyse (und wissenschaftliches Arbeiten). Deren Inhalte sollen im Rahmen dieser Pr√ºfungsleistung als selbst√§ndig und fl√ºssig verf√ºgbare Kompetenz von den Studentis demonstriert werden.\nDie Gliederung der Arbeit kann sich an den PPDAC-Zyklus und am Data Science Model von Wickham und Grolemund orientieren."
  },
  {
    "objectID": "pruefung.html#beurteilungskriterien",
    "href": "pruefung.html#beurteilungskriterien",
    "title": "1¬† Pr√ºfung",
    "section": "1.6 Beurteilungskriterien",
    "text": "1.6 Beurteilungskriterien\nDie Arbeit wird im Hinblick auf drei Kriterien bewertet:\n\nFormalia (z. B. Vollst√§ndigkeit der Abarbeitung, Angemessenheit der √§u√üeren Gestaltung, Fokus auf Wesentliches)\nMethodik (z. B. Richtige Auswahl und Anwendung der Verfahren)\nInhalt (z. B. Verst√§ndlichkeit, Breite und Tiefe der Probleml√∂sung, Korrektheit der Interpretation)\n\nSie erhalten f√ºr jedes der drei Kriterien eine Teilnote sowie eine Gesamtnote. Au√üerdem erhalten Sie ggf. f√ºr die Kriterien noch ausformulierte Hinweise.\nDie Gesamtnote muss sich nicht als Mittelwert der Teilnoten ergeben.\nInsbesondere kann eine F√ºnf in einem der Kriterien zum Durchfallen f√ºhren, auch wenn die anderen beiden Kriterien gut oder sehr gut beurteilt wurden."
  },
  {
    "objectID": "pruefung.html#beispiele-f√ºr-aspekte-der-beurteilungskriterien",
    "href": "pruefung.html#beispiele-f√ºr-aspekte-der-beurteilungskriterien",
    "title": "1¬† Pr√ºfung",
    "section": "1.7 Beispiele f√ºr Aspekte der Beurteilungskriterien",
    "text": "1.7 Beispiele f√ºr Aspekte der Beurteilungskriterien\n\nWurden deskriptive Statistiken (an angemessenen Ort) berichtet?\nWurden Diagramme und Tabellen angemessen eingesetzt?\nWurde Inferenzstatistik (angemessen) eingesetzt?\nWurden Effektst√§rkema√üe (idealerweise mit Konfidenzintervallen dazu) berichtet?\nWurden alle relevanten Informationen f√ºr ein statistisches Verfahren angegeben (z.B. zum gew√§hlten Prior)?\nWurde die Aussagekraft von Modellergebnissen richtig eingesch√§tzt?\nWaren die Schlussfolgerungen, die aus den statistischen Ergebnissen gezogen wurden, angemessen (z. B. wurde erkannt, dass ein Nicht-Verwerfeen einer Hypothese nicht automatisch ein Best√§tigen derselben bedeutet)?\nWurde angemessen gerundet (inkl. konsistente Anzahl von Nachkommastellen)?\nPassen die statistischen Verfahren zu den Hypothesen?\nWurden die Voraussetzungen der statistischen Verfahren gepr√ºft?\nSind die Ergebnisse reproduzierbar (Daten und Syntax eingereicht)?"
  },
  {
    "objectID": "pruefung.html#beispiele-f√ºr-fehler",
    "href": "pruefung.html#beispiele-f√ºr-fehler",
    "title": "1¬† Pr√ºfung",
    "section": "1.8 Beispiele f√ºr Fehler",
    "text": "1.8 Beispiele f√ºr Fehler\nSchwere Fehler, die zum Durchfallen oder deutlichem Abwerten der Note f√ºhren k√∂nnen, sind z.B.:\n\nfehlende Inferenzstatistik (oder ad√§quatem Ersatz)\nfalsche Interpretation von Posteriori-Verteilungen oder p-Werten\nkeine Angabe von Konfidenzintervallen\nfalsche Interpretation von Konfidenzintervallen\nWahl des falschen Intervalls (Vorhersageintervall vs.¬†Perzentilintervall vs.¬†HDI)\nfalsche Entscheidung zum Hypothesentest auf Basis entsprechender Kennwerte (wie ROPE-Wahrscheinlichkeit oder p-Wert)\nfalsche Wahl des statistischen Verfahrens\nfehlende Deskriptivstatistik\n\nH√§ufige kleinere M√§ngel sind z. B.\n\npixelige Abbildungen\nR-Ausgaben oder R-Syntax als Screenshot\nfehlende Seitenzahlen (nur bei paginierten Formaten, nicht bei HTML)\nun√ºbersichtliche Diagramme\nkein (verlinktes) Inhaltsverzeichnis Ô∏é\nfehlende oder unverst√§ndliche Achsenbeschriftung bei Diagrammen\nfehlende oder falsche Beschreibung der/des Skalenniveau(s) der untersuchten Variablen"
  },
  {
    "objectID": "twittermining.html",
    "href": "twittermining.html",
    "title": "2¬† Twitter Mining",
    "section": "",
    "text": "Text als Datenbasis pr√§diktiver Modelle\nBild von mcmurryjulie auf Pixabay"
  },
  {
    "objectID": "twittermining.html#vorab",
    "href": "twittermining.html#vorab",
    "title": "2¬† Twitter Mining",
    "section": "2.1 Vorab",
    "text": "2.1 Vorab\n\n2.1.1 Lernziele\n\nTwitterdaten via API von Twitter auslesen\n\n\n\n2.1.2 Vorbereitung\n\nLesen Sie in Hvitfeldt und Silge (2022) Kap. 1.\nLegen Sie sich ein Konto bei Github an.\nLegen Sie sich ein Konto bei Twitter an.\nLesen Sie diesen Artikel zur Anmeldung bei der Twitter API1\n\n\n\n2.1.3 Ben√∂tigte R-Pakete\n\nlibrary(tidyverse)\nlibrary(rtweet)\nlibrary(rio)  \nlibrary(glue)\nlibrary(tweetbotornot)  # optional\nlibrary(keyring)  # optional\nlibrary(askpass)  # optional\nlibrary(academictwitteR)\n\n\n\n\nR-Paket {rtweet}\n\n\nEinen √úberblick √ºber die Funktionen des Pakets (function reference) findet sich hier."
  },
  {
    "objectID": "twittermining.html#anmelden-bei-twitter",
    "href": "twittermining.html#anmelden-bei-twitter",
    "title": "2¬† Twitter Mining",
    "section": "2.2 Anmelden bei Twitter",
    "text": "2.2 Anmelden bei Twitter\n\n2.2.1 Welche Accounts interessieren uns?\nHier ist eine (subjektive) Auswahl von deutschen Politikern2, die einen Startpunkt gibt zur Analyse von Art und Ausma√ü von Hate Speech gerichtet an deutsche Politiker:innen.\n\nd_path <- \"data/twitter-german-politicians.csv\"\n\npoliticians <- import(d_path)\npoliticians\n\n\n\n\n\n\n\n\n\n\n\nname\nparty\nscreenname\ncomment\n\n\n\n\nKarl Lauterbach\nSPD\nKarl_Lauterbach\nNA\n\n\nOlaf Scholz\nSPD\nOlafScholz\nNA\n\n\nAnnalena Baerback\nGruene\nABaerbock\nNA\n\n\nBundesministerium f√ºr Wirtschaft und Klimaschutz\nGruene\nBMWK\nRobert Habeck ist der Minister im BMWK\n\n\nFriedrich Merz\nCDU\n_FriedrichMerz\nCDU-Chef\n\n\nMarkus S√∂der\nCSU\nMarkus_Soeder\nCSU-Chef\n\n\nCem √ñzdemir\nGruene\ncem_oezdemir\nBMEL\n\n\nJanine Wissler\nLinke\nJanine_Wissler\nLinke-Chefin\n\n\nMartin Schirdewan\nLinke\nschirdewan\nLinke-Chef\n\n\nChristian Lindner\nFDP\nc_lindner\nFDP-Chef\n\n\nMarie-Agnes Strack-Zimmermann\nFDP\nMAStrackZi\nVorsitzende Verteidigungsausschuss\n\n\nTino Chrupalla\nAFD\nTino_Chrupalla\nAFD-Bundessprecher\n\n\nAlice Weidel\nAFD\nAlice_Weidel\nAFD-Bundessprecherin\n\n\n\n\n\n\n\n\n2.2.2 Twitter App erstellen\nTutorial\nAuf der Twitter Developer Seite k√∂nnen Sie sich ein Konto erstellen und dann anmelden.\n\n\n2.2.3 Intro\nDie Seite von rtweet gibt eine gute Starthilfe in die Benutzung des Pakets.\n\n\n2.2.4 Zugangsdaten\nZugangsdaten sollte man gesch√ºtzt speichern, also z.B. nicht in einem geteilten Ordner.\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\n\nAnmelden:\n\nauth <- rtweet_bot(api_key = API_Key,\n                   api_secret = API_Key_Secret,\n                   access_token = Access_Token,\n                   access_secret = Access_Token_Secret)\n\nAlternativ kann man sich auch als App anmelden, damit kann man z.B. nicht posten, aber daf√ºr mehr herunterladen Quelle.\n\nauth <- rtweet_app(bearer_token = Bearer_Token)\n\nJetzt haben wir ein Anmeldeobjekt, das wir f√ºr die weiteren Anfragen dieser Session nutzen k√∂nnen. Das sagen wir jetzt der Twitter-API:\n\nauth_as(auth)\n\nInfos √ºber Ihre aktuellen Raten kann man sich mittels rate_limit() ausgeben lassen.\n\n\n2.2.5 Sch√ºtzen Sie Ihre Zugangsdaten\nAchtung, Sicherheitshinweis ‚Ä¶ Passw√∂rter und andere sensitive (Anmelde-)Informationen muss man sch√ºtzen, das wei√ü jeder. Konkret bedeutet es, dass Sie diese Daten nicht in einem √∂ffentlichen oder geteilten Repo herumliegen lassen. Achten Sie auch darauf, dass, wenn Sie diese Information sourceen, so wie ich gerade, diese dann ungesch√ºtzt in Ihrem RStudio Environment Fenster zu sehen sind. Falls Sie also den Bildschirm teilen, oder Ihnen jemand √ºber die Schulter schaut, sind Ihre Zugangsdaten nicht gesch√ºtzt.\nEin √§hnlicher Fehler w√§re, die History-Dateien von R in ein √∂ffentliches Repo einzustellen (z.B. via Git). In der Datei .gitignore sollten daher folgende Dateien aufgef√ºhrt sein:\n.Rhistory\n.Rapp.history\nEin Rat von rtweet dazu:\n\nIt‚Äôs good practice to only provide secrets interactively, because that makes it harder to accidentally share them in either your .Rhistory or an .R file.\n\nEinen alternativen, sichereren Zugang bietet z.B. das Paket keyring. Dieses Paket bietet eine Anbindung zur Schl√ºsselbundverwaltung Ihres Betriebssystems:\n\nPlatform independent API to access the operating systems credential store.\n\nIm MacOS wird die zentrale Schl√ºsselbundverwaltung genutzt, in Windows und Linux die analoge Vorrichtungen.\nWir erstellen uns einen Schl√ºsselbund:\n\nkeyring_create(keyring = \"hate-speech-twitter\")\n\nDann k√∂nnen wir einen Eintrag im Schl√ºsselbund erstellen. Es √∂ffnet sich eine Maske, die nach einem Passwort fragt. Geben Sie dort die sensitiven Informationen ein, etwa die Client-ID. Ggf. werden Sie noch nach dem Passwort des Schl√ºsselbunds an sich gefragt3\n\nkey_set(service = \"client_id\",\n        keyring = \"hate-speech-twitter\")\n\n\nkey_set(service = \"client_secret\",\n        keyring = \"hate-speech-twitter\")\n\n\nkey_set(service = \"bearer_token\",\n        keyring = \"hate-speech-twitter\")\n\nK√ºnftig k√∂nnen wir dann die Passw√∂rter aus dem Schl√ºsselbund abrufen:\n\nkey_get(service = \"client_id\",\n        keyring = \"hate-speech-twitter\")\n\n\nkey_get(service = \"bearer_token\",\n        keyring = \"hate-speech-twitter\")"
  },
  {
    "objectID": "twittermining.html#tweets-einlesen",
    "href": "twittermining.html#tweets-einlesen",
    "title": "2¬† Twitter Mining",
    "section": "2.3 Tweets einlesen",
    "text": "2.3 Tweets einlesen\nZu beachten ist, dass es Limits gibt, wie viele Informationen (pro Zeiteinheit) man √ºber die Twitter-API auslesen darf. Informationen dazu findet man z.B. hier oder auch mit rate_limit().\nEin g√§ngiges Limit der Twitter-API sind 900 Anfragen (z.B. Tweets auslesen) pro 15 Minuten.\n\n2.3.1 Timeline einlesen einzelner Accounts\nMal ein paar Tweets zur Probe:\n\nsesa_test <- get_timeline(user = \"sauer_sebastian\", n = 3) %>% \n  select(full_text)\n\n\n\nRT @pia_lamberty: Ein Ansatz, der sich beim Debunking wissenschaftlich als erfolgreich herausgestellt hat, ist das sog. Faktensandwich: htt‚Ä¶\nRT @ianbremmer: sure, it‚Äôs the hottest summer europe has ever had in history \n\nbut look at the upside\n\nit‚Äôs one of the coolest summers euro‚Ä¶\nRT @twisteddoodles: Balanced news reporting https://t.co/O1iiItEQrs\n\n\n\ntweets <- get_timeline(user = politicians$screenname)\nsaveRDS(tweets, file = \"tweets/tweets01.rds\")\n\nMichael Kearney r√§t uns:\n\nPRO TIP #4: (for developer accounts only) Use bearer_token() to increase rate limit to 45,000 per fifteen minutes.\n\n\n\n2.3.2 Retweets einlesen\n\ntweets01_retweets <- \n  tweets$id_str %>% \n  head(3) %>% \n  map_dfr( ~ get_retweets(status_id = .x, retryonratelimit = TRUE))\n\nDa die meisten Retweets aber nix sagen, sondern nur auf das einen Tweet wiederholen, ist das Sammeln der Retweets ziemlich langweilig.\nM√∂chte man retry on rate limit im Standard auf TRUE setzen, so kann man das √ºber die Optionen von R tun.\n\noptions(rtweet.retryonratelimit = TRUE)\n\n\n\n2.3.3 EPINetz Twitter Politicians 2021\nK√∂nig u.¬†a. (2022) Volltext hier haben einen Datensatz mit knapp 2500 Twitter Accounts deutscher Politiker zusammengestellt, zum Stand 2021.\nDer Datensatz kann √ºber Gesis bezogen werden.\nAuf der gleichen Seite findet sich auch eine Dokumentation des Vorgehens.\nNachdem wir den Datensatz heruntergeladen haben, k√∂nnen wir ihn einlesen:\n\npoliticians_path <- \"data/EPINetz_TwitterPoliticians_2021.RDs\"\npoliticians_twitter <- read_rds(politicians_path)\n\nhead(politicians_twitter)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nofficial_name\nparty\nregion\ninstitution\noffice\nuser_id\ntwitter_name\ntwitter_handle\nfrom\nuntil\nyear_of_birth\nabgeordnetenwatch_id\ngender\nwikidata_id\n\n\n\n\n535\nManja Sch√ºle\nSPD\nBrandenburg\nState Parliament\nParliamentarian\n827090742162100224\nManja Sch√ºle\nManjaSchuele\n2019-09-25\nNA\n1976\n146790\nfemale\nQ40974942\n\n\n962\nPetra Pau\nDIE LINKE\nFederal\nFederal Parliament\nParliamentarian\n1683845126\nTeam PetraPau\nTeamPetraPau\n2017-10-24\nNA\n1963\n79091\nfemale\nQ77195\n\n\n864\nDagmar Schmidt\nSPD\nFederal\nFederal Parliament\nParliamentarian\n1377117206\nTeam #dieschmidt\nTeamDieSchmidt\n2017-10-24\nNA\n1973\n79036\nfemale\nQ15433815\n\n\n2517\nBernd Buchholz\nFDP\nSchleswig-Holstein\nState Parliament\nParliamentarian\n1073605033\nBernd Buchholz\nBerndBuchholz\n2017-06-06\nNA\n1961\n121092\nmale\nQ823715\n\n\n1378\nIngrid Remmers\nDIE LINKE\nFederal\nFederal Parliament\nParliamentarian\n551802475\nIngrid Remmers MdB\ningrid_remmers\n2017-10-24\nNA\n1965\n120775\nfemale\nQ1652660\n\n\n1116\nReinhard Brandl\nCSU\nFederal\nFederal Parliament\nParliamentarian\n262730721\nReinhard Brandl\nreinhardbrandl\n2017-10-24\nNA\n1977\n79427\nmale\nQ111160\n\n\n\n\n\n\nDann lesen wir die Timelines (die Tweets) dieser Konten aus; in diesem Beispiel nur 10 Tweets pro Account:\n\nepi_tweets <- get_timeline(user = head(politicians_twitter$twitter_name), n = 10)\nhead(epi_tweets)\n\nNat√ºrlich k√∂nnte man auch mehr als 10 Tweets pro Konto einsammeln, braucht nur seine Zeit.\n\n\n2.3.4 Followers suchen\n\nfollowers01 <-\n  politicians$screenname %>% \n map_dfr( ~ get_followers(user = .x, retryonratelimit = TRUE))\n\nDa es dauern kann, Daten auszulesen (wir d√ºrfen pro 15 Min. nur eine begrenzte Zahl an Information abrufen), kann es Sinn machen, die Daten lokal zu speichern.\n\nsaveRDS(followers01, file = \"tweets/followers01.rds\")\n\nUnd ggf. wieder importieren:\n\nfollowers01 <- read_rds(file = \"tweets/followers01.rds\")\n\nWie viele unique Followers haben wir identifiziert?\n\nfollowers02 <- \n  followers01 %>% \n  distinct(from_id)\n\nDie Screennames w√§ren noch n√ºtzlich:\n\nlookup_users(users = \"1690868335\")\n\nDie Anzahl der Users, die man nachschauen kann, ist begrenzt auf 180 pro 15 Minuten.\n\nfollowers03 <-\n  followers02 %>% \n  mutate(screenname = \n           list(lookup_users(users = from_id, retryonratelimit = TRUE,verbose = TRUE)))\n\nEntsprechend kann man wieder einlesen:\nDamit haben wir eine Liste an Followers, deren Tweets wir einlesen und analysieren k√∂nnen, z.B. nach Hate Speech.\nIm Gegensatz zu Followers hei√üen bei Twitter die Accounts, denen ei Nutzi folgt ‚ÄúFriends‚Äù.\nLesen wir mal die Followers von karl_lauterbach ein:\n\nkarl_followers <- get_followers(user = \"karl_lauterbach\", verbose = TRUE)\n\nUm nicht jedes Mal aufs Neue die Daten herunterzuladen, bietet es sich an, die Daten lokal zu speichern:\n\nwrite_rds(karl_followers, file = \"tweets/karl_followers.rds\",\n          compress = \"gz\")\n\nEntsprechend kann man die Daten dann auch wieder einlesen:\n\nkarl_followers <- read_rds(file = \"tweets/karl_followers.rds\")\n\n\n\n2.3.5 Follower Tweets einlesen\n\nfollowers_tweets <- get_timeline(user = head(followers01$from_id), n = 10)\n\n\n\n2.3.6 Tweets nach Stichwort suchen\nUm nach einem Stichwort, allgemeiner nach einem bestimmten Text, in einem Tweet zu suchen, kann man die Funktion search_tweets nutzen:\n\nmy_tweet <- search_tweets(\"Sebastian Sauer\", n = 1)\nmy_tweet$full_text\n\n\n\n\nSchaut man sich das zur√ºckgelieferte Objekt (einen Tibble) n√§her an, entdeckt man eine F√ºlle an Informationen. Satte 43 Spalten (teilweise Listenspalten) finden sich dort:\n\nnames(my_tweet)\n\n [1] \"created_at\"                    \"id\"                           \n [3] \"id_str\"                        \"full_text\"                    \n [5] \"truncated\"                     \"display_text_range\"           \n [7] \"entities\"                      \"metadata\"                     \n [9] \"source\"                        \"in_reply_to_status_id\"        \n[11] \"in_reply_to_status_id_str\"     \"in_reply_to_user_id\"          \n[13] \"in_reply_to_user_id_str\"       \"in_reply_to_screen_name\"      \n[15] \"geo\"                           \"coordinates\"                  \n[17] \"place\"                         \"contributors\"                 \n[19] \"retweeted_status\"              \"is_quote_status\"              \n[21] \"quoted_status_id\"              \"quoted_status_id_str\"         \n[23] \"retweet_count\"                 \"favorite_count\"               \n[25] \"favorited\"                     \"retweeted\"                    \n[27] \"lang\"                          \"possibly_sensitive\"           \n[29] \"quoted_status\"                 \"text\"                         \n[31] \"favorited_by\"                  \"scopes\"                       \n[33] \"display_text_width\"            \"quoted_status_permalink\"      \n[35] \"quote_count\"                   \"timestamp_ms\"                 \n[37] \"reply_count\"                   \"filter_level\"                 \n[39] \"query\"                         \"withheld_scope\"               \n[41] \"withheld_copyright\"            \"withheld_in_countries\"        \n[43] \"possibly_sensitive_appealable\"\n\n\nDie Tweet-ID dieses Tweets bekommen Sie, wenn Sie die Variable id_str auslesen:\n\nmy_tweet$id_str\n\n[1] \"1593598440675500032\"\n\n\n\nmy_tweet$source\n\n[1] \"<a href=\\\"https://mobile.twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web App</a>\"\n\n\nDabei ist source nicht etwa die Person, die tweetet, wie man vielleicht meinen k√∂nnte, sondern das Frontend, das dabei verwendet wurde, also z.B. die iphone-App oder die Twitter-Webseite.\nLeider sucht man den screenname zu einen Tweet vergeblich in my_tweet.\nGegeben eines Dataframes mit Tweets kann man sich aber wie folgt den Nutzernamen (screen_name) ausgeben lassen.\n\nusers_data(my_tweet)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nid_str\nname\nscreen_name\nlocation\ndescription\nurl\nprotected\nfollowers_count\nfriends_count\nlisted_count\ncreated_at\nfavourites_count\nverified\nstatuses_count\nprofile_image_url_https\nprofile_banner_url\ndefault_profile\ndefault_profile_image\nwithheld_in_countries\nderived\nwithheld_scope\nentities\n\n\n\n\n1690868335\n1690868335\nSebastian Sauer üá∫üá¶\nsauer_sebastian\nN√ºrnberg, Deutschland\nData Science\nhttps://t.co/zwh3dCw3Dc\nFALSE\n485\n346\n34\nThu Aug 22 11:49:09 +0000 2013\n7517\nFALSE\n1241\nhttps://pbs.twimg.com/profile_images/853024804676521984/VQ5YBKVu_normal.jpg\nhttps://pbs.twimg.com/profile_banners/1690868335/1453724621\nTRUE\nFALSE\nNULL\nNA\nNA\nNA , NA , NA , NA , NA , https://t.co/zwh3dCw3Dc , https://data-se.netlify.com/, data-se.netlify.com , 0 , 23 , NA\n\n\n\n\n\n\nAu√üerdem gibt es einen ‚ÄúTrick‚Äù laut dieser Quelle, vgl. auch diesen SO-Post: Gibt man in die URL eines Tweets einen beliebigen Nutzernamen - das kann ein Fantasiename sein - so wird man automatisch zum richtigen Nutzer geleitet.\nDie Rohform der URL sieht also so aus:\nhttps://twitter.com/irgendeinnutzer/status/<id_str>\nGeben Sie also z.B. Folgende URL in Ihren Browser sein:\nhttps://twitter.com/irgendeinnutzer/status/1593598440675500032\nUnd Sie werden zum Nutzer sauer_sebastian weitergeleitet bzw. zu seinem Tweet mit obiger ID.\n\n\n2.3.7 Der Volltext ist manchmal abgeschniten\nManchmal ist der Volltext abgeschnitten\n\nmy_tweet$full_text\n\nHier steht der Beginn des Tweet-Textes, aber dann endet der Text abrup...\"\nGl√ºcklicherweise - sofern man bei einer so umst√§ndlichen Darstellung von Gl√ºck reden kann - findet man den kompletten Text andernorts Quelle.\nDazu schreibt in diesem SO-Post der Nutzer Jonas:\n\nYou will need to check if the tweet is a retweet. If it is, use the retweet‚Äôs full_text. If it is not, use the tweet‚Äôs full_text. ‚Äì Jonas, Nov 13, 2017 at 15:00\n\n\nmy_tweet$retweeted_status[[1]][[\"full_text\"]]\n\nHier steht der Beginn des Tweet-Textes, aber dann endet der Text abrupt? \nNein,er geht weiter und irgendwann ist der dann wirklich aus.\"\n\n\n2.3.8 Tweets nach ID suchen\nMit lookup_tweets(id_des_tweets) k√∂nnen Sie sich die Informationen zu einen Tweet ausgeben lassen. Das ist nat√ºrlich prim√§r der Volltext:\n\ntweet_example <- lookup_tweets(\"1593598440675500032\")\ntweet_example$full_text\n\nAber auch die √ºbrigen Informationen k√∂nnen interessant sein."
  },
  {
    "objectID": "twittermining.html#tweets-verarbeiten",
    "href": "twittermining.html#tweets-verarbeiten",
    "title": "2¬† Twitter Mining",
    "section": "2.4 Tweets verarbeiten",
    "text": "2.4 Tweets verarbeiten\n\n2.4.1 Grundlegende Verarbeitung\nSind die Tweets eingelesen, kann man z.B. eine Sentimentanalyse, s. Kapitel¬†3.2.10, durchf√ºhren, oder schlicht vergleichen, welche Personen welche W√∂rter h√§ufig verwenden, s. Kapitel¬†3.2.3.\n\n\n2.4.2 Bot or not?\nEine interessante Methode, Tweets zu verarbeiten, bietet das R-Paket tweetbotornot von M. Kearney.\nAus der Readme:\n\nDue to Twitter‚Äôs REST API rate limits, users are limited to only 180 estimates per every 15 minutes. To maximize the number of estimates per 15 minutes (at the cost of being less accurate), use the fast = TRUE argument. This method uses only users-level data, which increases the maximum number of estimates per 15 minutes to 90,000! Due to losses in accuracy, this method should be used with caution!\n\n\nusers <- c(\"sauer_sebastian\")\nbot01 <-\n  tweetbotornot(users)\n\n\n\n\n\n\n\nWichtig\n\n\n\nIch habe ein Fehlermeldung bekommen bei tweetbotornot. Da k√∂nnte ein technisches Problem in der Funktion vorliegen."
  },
  {
    "objectID": "twittermining.html#cron-jobs",
    "href": "twittermining.html#cron-jobs",
    "title": "2¬† Twitter Mining",
    "section": "2.5 Cron Jobs",
    "text": "2.5 Cron Jobs\n\n2.5.1 Was ist ein Cron Job?\nCron ist ein Programm auf Unix-artigen Betriebssystemen, das Skripte zu eingestellten Zeiten (wiederholt) ausf√ºhrt, das sind dann ‚ÄúCron Jobs‚Äù. Auf Windows gibt es aber analoge Funktionen. Cron Jobs sind praktisch, da man nicht jedes Mal selber z.B. Tweets, die heute zu einem Thema getweetet wurden, herunterladen muss. Das wird dann vom Cron Job √ºbernommen.\nIn R gibt es eine API zum Programm Cron mit dem Paket {cronR}, s. Anleitung hier.\nDas analoge R-Paket f√ºr Windows hei√üt {taskscheduleR}.\n\n\n2.5.2 Beispiel f√ºr einen Cron Job\n\nlibrary(cronR)\n\nscrape_script <- cron_rscript(\"scrape_tweets.R\")\n\n# Cron Job hinzuf√ºgen:\ncron_add(command = scrape_script, \n         frequency = 'daily', \n         at = \"10AM\",\n         id = 'Hate Speech')  # Name des Cron Jobs\n\ncron_clear(ask = FALSE)  # Alle Cron Jobs l√∂schen\ncron_ls()  # Liste aller Cron Jobs\n\nIm obigen Beispiel wird das R-Skript scrape_tweets.R t√§glich um 10h ausgef√ºhrt.\nDer Inhalt von scrape_tweets.R k√∂nnte dann, in Grundz√ºgen, so aussehen:\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(rtweet)\nfollowers_lauterbach <-\n  followers01 %>% \n  filter(to_id == \"Karl_Lauterbach\")\n\nfollowers_lauterbach_tweets <- \n  get_timeline(user = followers_lauterbach$from_id[1:10], n = 10, retryonratelimit = TRUE, verbose = FALSE)\n\n\npath_output <- \"/Users/sebastiansaueruser/Google Drive/RRRing/Scrape-Tweets/tweets/\"\n\nwrite_csv(x = followers_lauterbach_tweets,\n          file = paste0(path_output, \"followers_lauterbach_tweets.csv\"),\n          append = TRUE)\n\nWir schreiben nicht jedes Mal (jeden Tag) eine neue CSV-Datei, sondern wir h√§ngen hier die neu ausgelesenen Daten an die Datei an.\nLeider ist es mit rtweet nicht m√∂glich, ein Datum anzugeben, ab dem man Tweets auslesen m√∂chte4"
  },
  {
    "objectID": "twittermining.html#datenbank-an-tweets-aufbauen",
    "href": "twittermining.html#datenbank-an-tweets-aufbauen",
    "title": "2¬† Twitter Mining",
    "section": "2.6 Datenbank an Tweets aufbauen",
    "text": "2.6 Datenbank an Tweets aufbauen\n\n2.6.1 Stamm an bisherigen Tweets\nIn diesem Abschnitt k√ºmmern wir uns in gr√∂√üerem Detail um das Aufbauen einer Tweets-Datenbank.\nDiese Pakete ben√∂tigen wir:\n\nlibrary(rtweet)\nlibrary(tidyverse)\nlibrary(rio)  # R Data import/export\n\nDann melden wir uns an:\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\nauth <- rtweet_app(bearer_token = Bearer_Token)\n\nDann brauchen wir eine Liste an Twitterkonten, die uns interessieren. Im Kontext von Hate Speech soll uns hier interessieren, welche Tweets an deutsche Spitzenpolitikis5 gesendet werden. Wir suchen also nach Tweets mit dem Text @karl_lauterbach, um ein Beispiel f√ºr einen Spitzenpolitiker zu nennen, der vermutlich von Hate Speech in h√∂herem Ma√üe betroffen ist.\n\npoliticians_twitter_path <- \"/Users/sebastiansaueruser/github-repos/datascience-text/data/twitter-german-politicians.csv\"\n\npoliticians_twitter <- rio::import(file = politicians_twitter_path)\n\nIn der Liste befinden sich 13 Politiker. Es macht die Sache vielleicht einfacher, wenn wir die Rate nicht √ºberziehen. Bleiben wir daher bei 1000 Tweets pro Politiki:\n\nn_tweets_per_politician <- 1e3\n\nDie R-Syntax, die die Arbeit leistet, ist in Funktionen ausgelagert, der √úbersichtlichkeit halber.\n\nsource(\"funs/filter_recent_tweets.R\")\nsource(\"funs/download_recent_tweets.R\")\nsource(\"funs/add_tweets_to_tweet_db.R\")\nsource(\"funs/sanitize_tweets.R\")\n\n\n\n\nJetzt laden wir einfach die aktuellsten 1000 Tweets pro Konto herunter, daher brauchen wir keine Tweet-ID angeben, die ein Mindest- oder Maximum-Datum (bzw. ID) f√ºr einen Tweet angibt:\n\ntweets_older <-\n  download_recent_tweets(screenname = politicians_twitter$screenname,\n                         max_or_since_id_str = NULL,\n                         n = n_tweets_per_politician,\n                         strip_columns = TRUE,\n                         reverse = TRUE)\n\nWie weit in die Vergangenheit reicht unsere Tweet-Sammlung?\n\noldest_tweets <- filter_recent_tweets(tweets_older, max_or_min_id_str = is_min_id_str)\noldest_tweets\n\n\n\n\n\n\n\n\n\n\n\n\nid_str\nscreenname\ncreated_at\nis_min_id_str\nis_max_id_str\n\n\n\n\n1590754620649115648\nKarl_Lauterbach\n2022-11-10 18:13:51\nTRUE\nFALSE\n\n\n1589837879693352960\nOlafScholz\n2022-11-08 05:31:02\nTRUE\nFALSE\n\n\n1590485330742083585\nABaerbock\n2022-11-10 00:23:47\nTRUE\nFALSE\n\n\n1589983737621942272\nBMWK\n2022-11-08 15:10:38\nTRUE\nFALSE\n\n\n1590646264433373184\n_FriedrichMerz\n2022-11-10 11:03:16\nTRUE\nFALSE\n\n\n1588595577360875520\nMarkus_Soeder\n2022-11-04 19:14:34\nTRUE\nFALSE\n\n\n1590097264613425152\ncem_oezdemir\n2022-11-08 22:41:45\nTRUE\nFALSE\n\n\n1588082031656898560\nJanine_Wissler\n2022-11-03 09:13:56\nTRUE\nFALSE\n\n\n1588082031656898560\nschirdewan\n2022-11-03 09:13:56\nTRUE\nFALSE\n\n\n1590628128791007233\nc_lindner\n2022-11-10 09:51:13\nTRUE\nFALSE\n\n\n1589277825152208898\nMAStrackZi\n2022-11-06 16:25:35\nTRUE\nFALSE\n\n\n1587964349993422848\nTino_Chrupalla\n2022-11-03 01:26:18\nTRUE\nFALSE\n\n\n1589696708447186945\nAlice_Weidel\n2022-11-07 20:10:05\nTRUE\nFALSE\n\n\n\n\n\n\nWas sind die neuesten Tweets, die wir habven?\n\nmost_recent_tweets <- filter_recent_tweets(oldest_tweets)\nmost_recent_tweets\n\n\n\n\n\n\n\n\n\n\n\n\nid_str\nscreenname\ncreated_at\nis_min_id_str\nis_max_id_str\n\n\n\n\n1590754620649115648\nKarl_Lauterbach\n2022-11-10 18:13:51\nTRUE\nTRUE\n\n\n1589837879693352960\nOlafScholz\n2022-11-08 05:31:02\nTRUE\nTRUE\n\n\n1590485330742083585\nABaerbock\n2022-11-10 00:23:47\nTRUE\nTRUE\n\n\n1589983737621942272\nBMWK\n2022-11-08 15:10:38\nTRUE\nTRUE\n\n\n1590646264433373184\n_FriedrichMerz\n2022-11-10 11:03:16\nTRUE\nTRUE\n\n\n1588595577360875520\nMarkus_Soeder\n2022-11-04 19:14:34\nTRUE\nTRUE\n\n\n1590097264613425152\ncem_oezdemir\n2022-11-08 22:41:45\nTRUE\nTRUE\n\n\n1588082031656898560\nJanine_Wissler\n2022-11-03 09:13:56\nTRUE\nTRUE\n\n\n1588082031656898560\nschirdewan\n2022-11-03 09:13:56\nTRUE\nTRUE\n\n\n1590628128791007233\nc_lindner\n2022-11-10 09:51:13\nTRUE\nTRUE\n\n\n1589277825152208898\nMAStrackZi\n2022-11-06 16:25:35\nTRUE\nTRUE\n\n\n1587964349993422848\nTino_Chrupalla\n2022-11-03 01:26:18\nTRUE\nTRUE\n\n\n1589696708447186945\nAlice_Weidel\n2022-11-07 20:10:05\nTRUE\nTRUE\n\n\n\n\n\n\nJetzt laden wir die neueren Tweets herunter, also mit einer ID gr√∂√üer als die gr√∂√üte in unserer Sammlung:\n\n\n\n\ntweets_new <- \n  download_recent_tweets(screenname = most_recent_tweets$screenname,\n                         max_or_since_id_str = most_recent_tweets$id_str)\n\ntweets_new %>% \n  select(screenname, created_at, id_str) %>% \n  head()\n\nJetzt - und jedes Mal, wenn wir Tweets herunterladen - f√ºgen wir diese einer Datenbank (oder zumindest einer ‚ÄúGesamt-Tabelle‚Äù) hinzu:\n\ntweets_db <- add_tweets_to_tweets_db(tweets_new, tweets_older)\n\nnrow(tweets_db)\n\n[1] 10969\n\n\nSchlie√ülich sollten wir nicht vergessen diese in einer Datei zu speichern:\n\nwrite_rds(tweets_db, file = \"~/datasets/Twitter/tweets-db-2022-11-11.rds\")\n\n‚Ä¶ ‚Ä¶ So, einige Zeit ist vergangen. Laden wir noch √§ltere Tweets herunter und f√ºgen Sie unserer Datenbank hinzu:\n\ntweets_older2 <-\n  download_recent_tweets(screenname = politicians_twitter$screenname,\n                         max_or_since_id_str = oldest_tweets$id_str,\n                         n = 1e3,\n                         strip_columns = TRUE,\n                         reverse = TRUE)\n\n\n\n\n\ntweets_db <- add_tweets_to_tweets_db(tweets_new, tweets_older2)\n\nnrow(tweets_db)\n\n[1] 10011\n\n\nUnd wieder speichern wir die vergr√∂√üerte Datenbasis auf der Festplatte:\n\nwrite_rds(tweets_db, file = \"~/datasets/Twitter/hate-speech-twitter.rds\")\n\nLeider ist die Datenbasis nicht mehr deutlich gewachsen. Eine plausible Ursache ist, dass Twitter den Zugriff auf alte Tweets einschr√§nkt.\nAus der Hilfe von search_tweets:\n\nReturns Twitter statuses matching a user provided search query. ONLY RETURNS DATA FROM THE PAST 6-9 DAYS.\n\nMit Hilfe des Academic Research Access sind deutlich h√∂here Raten m√∂glich.\n\n\n2.6.2 Neue Tweets per Cron Job\nWie oben schon ausprobiert, legen wir uns einen Cron Job an.\nDas ist √ºbrigens auch eine komfortable L√∂sung.\n\nlibrary(cronR)\n\nscrape_script <- cron_rscript(\"/Users/sebastiansaueruser/github-repos/datascience-text/funs/get_tweets_politicians.R\")\n\n# Cron Job hinzuf√ºgen:\ncron_add(command = scrape_script, \n         frequency = 'daily', \n         at = \"10AM\",\n         id = 'Hate Speech')  # Name des Cron Jobs\n\nDas Skript get_tweets_politicians.R birgt die Schritte, die wir in diesem Abschnitt ausprobiert haben, hier liegt es. Kurz gesagt sucht es nach neuen Tweets, die also noch nicht in Ihrer ‚ÄúDatenbank‚Äù vorhanden sind, und l√§dt diese herunter. Dabei werden maximal 1000 Tweets pro Konto (derer sind es 13) heruntergeladen.\nBei einem Cronjob sollten absolute Pfade angegeben werden, da der Cronjob nicht aus dem aktuellen Projekt-Repo startet.\nDie Ergebnisse eines Cronjob-Durchlaufs werden in einer Log-Datei abgelegt, und zwar in dem Ordner, in dem auch das Skript liegt, das im Rahmen des Cronjobs durchgef√ºhrt wird.\n\n\n\n\n\n\nHinweis\n\n\n\nSchauen Sie sich die Funktionen im Ordner /funs einmal in Ruhe an. Hier geht es zu dem Ordner im Github-Repo. Es ist alles keine Zauberei, aber im Detail gibt es immer wieder Schwierigkeiten. Am meisten lernt man, wenn man selber Hand anlegt.\n\n\nM√∂chte man den Cron Job wieder l√∂schen, so kann man das so tun:\n\ncron_clear(ask = FALSE)  # Alle Cron Jobs l√∂schen\ncron_ls()  # Liste aller Cron Jobs\n\nUm die Tweets ‚Äúh√§ndisch‚Äù herunterzuladen, kann man get_tweets_politicians() aufrufen:\n\nsource(\"funs/get_tweets_politicians.R\")\nget_tweets_politicians()\n\n\n\n2.6.3 Tweets in Excel exportieren\nUm pr√§diktive Modelle zu erstellen, braucht man ein Trainingsset, Tweets also, die schon vorklassifiziert sind, z.B. im Hinblick auf Hassrede mit ja oder nein. Technisch bietet sich ein 1 vs.¬†0 an.\nDazu laden wir einen Datensatz mit Tweets, z.B. diesen hier:\n\ntweets_to_kl <- import(\"/Users/sebastiansaueruser/datasets/Twitter/tweets_to_karl_lauterbach.rds\")\n\nDa es viele Spalten gibt, die teilweise Listenspalten sind, also komplex, begrenzen wir uns auf das Wesentliche, den Tweet-Text und die ID des Tweets.\n\ntweets_to_kl2 <-\n  tweets_to_kl %>% \n  select(id_str, full_text) \n\n\n\n\n\n\n\nHinweis\n\n\n\nDie Tweet-ID wird einmal als String und einmal als Integer gespeichert. Allerdings √ºbersteigt die Anzahl der Ziffern die Speichergr√∂√üe von (normalen) Integer-Formaten in R. Daher ist die Twitter-ID als Integer nicht zuverl√§ssig; als Text hingegen schon.\n\n\nUnd schlie√ülich k√∂nnen wir die Excel-Datei importieren.\n\nexport(tweets_to_kl2, file = \"~/datasets/Twitter/tweets_to_kl.xlsx\")\n\nDie Excel-Tabelle k√∂nnen wir dann bequem hernehmen, um Tweets manuell zu klassifizieren.\n\n\n2.6.4 Twitterkonten f√ºr Wissenschaftler\nTwitter stellt spezielle Konten f√ºr Wissentschaftlis bereit, die √ºber h√∂here Raten und mehr Funktionen verf√ºgen, also mehr Tweets herunterladen k√∂nnen, z.B. 10 Millionen Tweets pro Monat pro Projekt.\n\nauth_academic <- rtweet_app(bearer_token = askpass::askpass(\"bearer token\"))\nauth_academic\n\nDas R-Paket {askpass} stellt eine weitere M√∂glichkeit bereit, um Zugangsdaten zu sch√ºtzen. Es √∂ffnet eine Maske, die interaktiv und als Punkte gesch√ºtzte Buchstaben nach einem Passwort fragt, in diesem Fall nach dem Bearer-Token.\nAus der Hilfe:\n\nPrompt the user for a password to authenticate or read a protected key. By default, this function automatically uses the most appropriate method based on the user platform and front-end. Users or IDEs can override this and set a custom password entry function via the askpass option.\n\nTwitter bietet in diesem Repo einen n√ºtzlichen Kurs an, um sich mit der API vertraut zu machen.\n\ntweets_to__FriedrichMerz_2022 <-\n  get_all_tweets(query = \"to:_FriedrichMerz -is:retweet\",\n                 start_tweets = \"2022-01-01T00:00:00Z\",\n                 end_tweets = \"2022-11-23T23:59:59Z\",\n                 bearer_token = askpass(\"Bearer token\"),\n                 file = \"~/datasets/Twitter/tweets-to-_FriedrichMerz_2022.rds\",\n                 n = 1e5)\n\nOder als Funktion, das ist praktischer, wenn man die Syntax mehrfach verwendet:\n\nget_all_tweets_politicians <- function(screenname, bearer_token, n = 1e5) {\n  get_all_tweets(query = paste0(\"to:\", screenname, \" -is:retweet\"),\n                 start_tweets = \"2021-01-01T00:00:00Z\",\n                 end_tweets = \"2021-12-31T23:59:59Z\",\n                 bearer_token = bearer_token,\n                 file = glue::glue(\"~/datasets/Twitter/tweets_to_{screenname}_2021.rds\"),\n                 data_path = glue::glue(\"~/datasets/Twitter/{screenname}\"),\n                 n = n)\n}\n\n\n#debug(get_all_tweets_politicians)\nget_all_tweets_politicians(screenname = politicians$screenname[5],\n                           bearer_token = askpass(\"Bearer token\"),\n                           n = 1e05)\n\nDann kann man die Objekte abespeichern, etwas als RDS-Datei oder als Feather-Datei.\nDen Datensatz politicians hatten wir oben angelegt, s. Kapitel¬†2.2.1. Er beinhaltet die Kontonamen (screennames) einiger deutscher Politikis.\nWichtig ist, mit den Lizenzregeln in Einklang zu bleiben.\nZentral ist dabei sicherlich die Frage, ob und wie man Tweets weitergeben darf. Dazu:\n\nAcademic researchers are permitted to distribute an unlimited number of Tweet IDs and/or User IDs if they are doing so for the purposes of non-commercial research and to validate or replicate previous academic studies. You should not share the entire Tweet text directly. Instead, you can build a list of Tweet IDs and share those. The researchers who you share this set of Tweet IDs with, can then use the Twitter API to hydrate and get the full Tweet objects from the Tweet IDs.\n\nQuelle\nMehr Details finden sich den Entwicklerrichtlinien von Twitter.\nTwitter stellt eine Reihe von Lehrmaterialien f√ºr die wissenschaftliche Nutzung von Tweets bereit."
  },
  {
    "objectID": "twittermining.html#aufgaben",
    "href": "twittermining.html#aufgaben",
    "title": "2¬† Twitter Mining",
    "section": "2.7 Aufgaben",
    "text": "2.7 Aufgaben\n\n√úberlegen Sie, wie Sie das Ausma√ü an Hate Speech, dem deutsche Politikerinnen und Politiker konfrontiert sind, messen k√∂nnen.\nArgumentieren Sie die Vorteile und Nachteile Ihres Ansatzes. Au√üerdem, auf welches Ergebnis dieser Analyse sie gespannt sind bzw. w√§ren.\n√úberlegen Sie Korrelate, oder besser noch: (m√∂gliche) Ursachen, des Hasses in den Tweets, gerichtet auf Polikter:innen. Sie k√∂nnen auch Gruppen von Ursachen bilden, etwas personengebundene Variablen der Politiker:innen (z.B. Alter? Geschlecht? Migrationshintergrund?).\nErstellen Sie sich eine Liste an Personen, deren Tweets sich lohnen (k√∂nnten), auf Hate Speech hin analysiert zu werden. Laden Sie deren Tweets (ggf. in Ausz√ºgen) herunter.\nDas Skript zu scrape_tweets.R k√∂nnte man noch verbessern, in dem man jeden Tag nur die neuesten Tweets herunterl√§dt. Dazu kann man bei get_timeline() mit dem Argument since_id eine Untergrenze der ID festlegen, so dass nur neuere Tweets (d.h. mit gr√∂√üerem Wert bei ID) ausgelesen werden. √Ñndern Sie das Skript entsprechend, so dass nur neuerer Tweets gelesen werden.\nErarbeiten Sie die Folien zu diesem rtweet-Workshop. Eine Menge guter Tipps!\n\n\n\n\n\nHvitfeldt, Emil, und Julia Silge. 2022. Supervised Machine Learning for Text Analysis in R. 1. Aufl. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nK√∂nig, Tim, Wolf J. Sch√ºnemann, Alexander Brand, Julian Freyberg, und Michael Gertz. 2022. ‚ÄûThe EPINetz Twitter Politicians Dataset 2021. A¬†New Resource for the Study of the German Twittersphere and Its Application for the 2021 Federal Elections‚Äú. Politische Vierteljahresschrift 63 (3): 529‚Äì47. https://doi.org/10.1007/s11615-022-00405-7."
  },
  {
    "objectID": "textmining1.html",
    "href": "textmining1.html",
    "title": "3¬† Textmining1",
    "section": "",
    "text": "Bild von mcmurryjulie auf Pixabay"
  },
  {
    "objectID": "textmining1.html#vorab",
    "href": "textmining1.html#vorab",
    "title": "3¬† Textmining1",
    "section": "3.1 Vorab",
    "text": "3.1 Vorab\n\n3.1.1 Lernziele\n\nDie vorgestellten Techniken des Textminings mit R anwenden k√∂nnen\n\n\n\n3.1.2 Vorbereitung\n\nLesen Sie in Hvitfeldt und Silge (2022) Kap. 2.\n\n\n\n3.1.3 Ben√∂tigte R-Pakete\n\nlibrary(tidyverse)\nlibrary(tokenizers)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(hcandersenr)\nlibrary(SnowballC)  # Stemming\nlibrary(lsa)  # Stopw√∂rter\nlibrary(easystats)  # Komfort f√ºr deskriptive Statistiken, wie `describe_distribution`\nlibrary(textclean)  # Emojis ersetzen\nlibrary(wordcloud)"
  },
  {
    "objectID": "textmining1.html#einfache-methoden-des-textminings",
    "href": "textmining1.html#einfache-methoden-des-textminings",
    "title": "3¬† Textmining1",
    "section": "3.2 Einfache Methoden des Textminings",
    "text": "3.2 Einfache Methoden des Textminings\nArbeiten Sie die folgenden grundlegenden Methoden des Textminigs durch.\n\n3.2.1 Tokenisierung\nErarbeiten Sie dieses Kapitel: Hvitfeldt und Silge (2022), Kap. 2\nWie viele Zeilen hat das M√§rchen ‚ÄúThe Fir tree‚Äù (in der englischen Fassung?)\n\nhcandersen_en %>% \n  filter(book == \"The fir tree\") %>% \n  nrow()\n\n[1] 253\n\n\n\n\n3.2.2 Stopw√∂rter entfernen\nErarbeiten Sie dieses Kapitel: s. Hvitfeldt und Silge (2022), Kap. 3\nEine alternative Quelle von Stopw√∂rtern - in verschiedenen Sprachen - biwetet das Paket quanteda:\n\nstop2 <-\n  tibble(word = quanteda::stopwords(\"german\"))\n\nhead(stop2)\n\n\n\n\n\nword\n\n\n\n\naber\n\n\nalle\n\n\nallem\n\n\nallen\n\n\naller\n\n\nalles\n\n\n\n\n\n\nEs bestehst (in der deutschen Version) aus 231 W√∂rtern.\n\n\n3.2.3 W√∂rter z√§hlen\nIst der Text tokenisiert, kann man einfach mit ‚ÄúBordmitteln‚Äù die W√∂rter z√§hlen.\n\nhc_andersen_count <- \n  hcandersen_de %>% \n  filter(book == \"Das Feuerzeug\") %>% \n  unnest_tokens(output = word, input = text) %>% \n  anti_join(stop2) %>% \n  count(word, sort = TRUE) \n\nJoining, by = \"word\"\n\nhc_andersen_count %>% \n  head()\n\n\n\n\n\nword\nn\n\n\n\n\nsoldat\n35\n\n\nsagte\n28\n\n\nhund\n23\n\n\nprinzessin\n17\n\n\nhexe\n16\n\n\nfeuerzeug\n14\n\n\n\n\n\n\nZur Visualisierung eignen sich Balkendiagramme, s. ?fig-hcandersen-count.\n\nhc_andersen_count %>% \n  slice_max(order_by = n, n = 10) %>% \n  mutate(word = factor(word)) %>% \n  ggplot() +\n  aes(y = reorder(word, n), x = n) +\n  geom_col()\n\n\n\n\nAbbildung¬†3.1: Die h√§ufigsten W√∂rter in H.C. Anderssens Feuerzeug\n\n\n\n\nDabei macht es Sinn, aus word einen Faktor zu machen, denn Faktorstufen kann man sortieren, zumindest ist das die einfachste L√∂sung in ggplot2 (wenn auch nicht super komfortabel).\nEine (beliebite?) Methode, um Worth√§ufigkeiten in Corpora darzustellen, sind Wortwolken, s. Abbildung¬†3.2. Es sei hinzugef√ºgt, dass solche Wortwolken nicht gerade optimale perzeptorische Qualit√§ten aufweisen.\n\nwordcloud(words = hc_andersen_count$word,\n          freq = hc_andersen_count$n,\n          max.words = 50,\n          rot.per = 0.35,\n          colors = brewer.pal(8, \"Dark2\"))\n\n\n\n\nAbbildung¬†3.2: Eine Wortwolke zu den h√§ufigsten W√∂rtern in H.C. Andersens Feuerzeug\n\n\n\n\n\n\n3.2.4 Stemming (Wortstamm finden)\nErarbeiten Sie dieses Kapitel: Hvitfeldt und Silge (2022), Kap. 4\nVertiefende Hinweise zum UpSet plot finden Sie hier, Lex u.¬†a. (2014).\nF√ºr welche Sprachen gibt es Stemming im Paket SnowballC?\n\nlibrary(SnowballC)\ngetStemLanguages()\n\n [1] \"arabic\"     \"basque\"     \"catalan\"    \"danish\"     \"dutch\"     \n [6] \"english\"    \"finnish\"    \"french\"     \"german\"     \"greek\"     \n[11] \"hindi\"      \"hungarian\"  \"indonesian\" \"irish\"      \"italian\"   \n[16] \"lithuanian\" \"nepali\"     \"norwegian\"  \"porter\"     \"portuguese\"\n[21] \"romanian\"   \"russian\"    \"spanish\"    \"swedish\"    \"tamil\"     \n[26] \"turkish\"   \n\n\nEinfacher Test: Suchen wir den Wordstamm f√ºr das Wort ‚Äúwissensdurstigen‚Äù, wie in ‚Äúdie wissensdurstigen Studentis l√∂cherten dis armi Professi‚Äù1.\n\nwordStem(\"wissensdurstigen\", language = \"german\")\n\n[1] \"wissensdurst\"\n\n\nWerfen Sie mal einen Blick in das Handbuch von SnowballC.\n\n\n3.2.5 Fallstudie AfD-Parteiprogramm\nDaten einlesen:\n\nd_link <- \"https://raw.githubusercontent.com/sebastiansauer/pradadata/master/data-raw/afd_2022.csv\"\nafd <- read_csv(d_link, show_col_types = FALSE)\n\nWie viele Seiten hat das Dokument?\n\nnrow(afd)\n\n[1] 190\n\n\nUnd wie viele W√∂rter?\n\nstr_count(afd$text, pattern = \"\\\\w\") %>% sum(na.rm = TRUE)\n\n[1] 179375\n\n\nAus breit mach lang, oder: wir tokenisieren (nach W√∂rtern):\n\nafd %>% \n  unnest_tokens(output = token, input = text) %>% \n  filter(str_detect(token, \"[a-z]\")) -> afd_long\n\nStopw√∂rter entfernen:\n\ndata(stopwords_de, package = \"lsa\")\n\nstopwords_de <- tibble(word = stopwords_de)\n\n# F√ºr das Joinen werden gleiche Spaltennamen ben√∂tigt:\nstopwords_de <- stopwords_de %>% \n  rename(token = word)  \n\nafd_long %>% \n  anti_join(stopwords_de) -> afd_no_stop\n\nJoining, by = \"token\"\n\n\nW√∂rter z√§hlen:\n\nafd_no_stop %>% \n  count(token, sort = TRUE) -> afd_count\n\nhead(afd_count)\n\n\n\n\n\ntoken\nn\n\n\n\n\nafd\n174\n\n\ndeutschland\n113\n\n\nwollen\n66\n\n\neuro\n60\n\n\nb√ºrger\n57\n\n\neu\n54\n\n\n\n\n\n\nW√∂rter trunkieren:\n\nafd_no_stop %>% \n  mutate(token_stem = wordStem(token, language = \"de\")) %>% \n  count(token_stem, sort = TRUE) -> afd_count_stemmed\n\nhead(afd_no_stop)\n\n\n\n\n\npage\ntoken\n\n\n\n\n1\nprogramm\n\n\n1\ndeutschland\n\n\n1\ngrundsatzprogramm\n\n\n1\nalternative\n\n\n1\ndeutschland\n\n\n2\ninhaltsverzeichnis\n\n\n\n\n\n\n\n\n3.2.6 Stringverarbeitung\nErarbeiten Sie dieses Kapitel: Wickham und Grolemund (2018), Kap. 14\n\n3.2.6.1 Regul√§rausdr√ºcke\nDas \"[a-z]\" in der Syntax oben steht f√ºr ‚Äúalle Buchstaben von a-z‚Äù. D iese flexible Art von ‚ÄúString-Verarbeitung mit Jokern‚Äù nennt man Regul√§rausdr√ºcke (regular expressions; regex). Es gibt eine ganze Reihe von diesen Regul√§rausdr√ºcken, die die Verarbeitung von Texten erleichert. Mit dem Paket stringr geht das - mit etwas √úbung - gut von der Hand. Nehmen wir als Beispiel den Text eines Tweets:\n\nstring <- \"Correlation of unemployment and #AfD votes at #btw17: ***r = 0.18***\\n\\nhttps://t.co/YHyqTguVWx\"  \n\nM√∂chte man Ziffern identifizieren, so hilft der Reul√§rausdruck [:digit:]:\n‚ÄúGibt es mindestens eine Ziffer in dem String?‚Äù\n\nstr_detect(string, \"[:digit:]\")\n\n[1] TRUE\n\n\n‚ÄúFinde die Position der ersten Ziffer! Welche Ziffer ist es?‚Äù\n\nstr_locate(string, \"[:digit:]\")\n\n     start end\n[1,]    51  51\n\nstr_extract(string, \"[:digit:]\")\n\n[1] \"1\"\n\n\n‚ÄúFinde alle Ziffern!‚Äù\n\nstr_extract_all(string, \"[:digit:]\")\n\n[[1]]\n[1] \"1\" \"7\" \"0\" \"1\" \"8\"\n\n\n‚ÄúFinde alle Stellen an denen genau 2 Ziffern hintereinander folgen!‚Äù\n\nstr_extract_all(string, \"[:digit:]{2}\")\n\n[[1]]\n[1] \"17\" \"18\"\n\n\nDer Quantit√§tsoperator {n} findet alle Stellen, in der der der gesuchte Ausdruck genau \\(n\\) mal auftaucht.\n‚ÄúZeig die Hashtags!‚Äù\n\nstr_extract_all(string, \"#[:alnum:]+\")\n\n[[1]]\n[1] \"#AfD\"   \"#btw17\"\n\n\nDer Operator [:alnum:] steht f√ºr ‚Äúalphanumerischer Charakter‚Äù - also eine Ziffer oder ein Buchstabe; synonym h√§tte man auch \\\\w schreiben k√∂nnen (w wie word). Warum werden zwei Backslashes gebraucht? Mit \\\\w wird signalisiert, dass nicht der Buchstabe w, sondern etwas Besonderes, eben der Regex-Operator \\w gesucht wird.\n‚ÄúZeig die URLs!‚Äù\n\nstr_extract_all(string, \"https?://[:graph:]+\")\n\n[[1]]\n[1] \"https://t.co/YHyqTguVWx\"\n\n\nDas Fragezeichen ? ist eine Quantit√§tsoperator, der einen Treffer liefert, wenn das vorherige Zeichen (hier s) null oder einmal gefunden wird. [:graph:] ist die Summe von [:alpha:] (Buchstaben, gro√ü und klein), [:digit:] (Ziffern) und [:punct:] (Satzzeichen u.√§.).\n‚ÄúZ√§hle die W√∂rter im String!‚Äù\n\nstr_count(string, boundary(\"word\"))\n\n[1] 13\n\n\n‚ÄúLiefere nur Buchstabenfolgen zur√ºck, l√∂sche alles √ºbrige‚Äù\n\nstr_extract_all(string, \"[:alpha:]+\")\n\n[[1]]\n [1] \"Correlation\"  \"of\"           \"unemployment\" \"and\"          \"AfD\"         \n [6] \"votes\"        \"at\"           \"btw\"          \"r\"            \"https\"       \n[11] \"t\"            \"co\"           \"YHyqTguVWx\"  \n\n\nDer Quantit√§tsoperator + liefert alle Stellen zur√ºck, in denen der gesuchte Ausdruck einmal oder h√§ufiger vorkommt. Die Ergebnisse werden als Vektor von W√∂rtern zur√ºckgegeben. Ein anderer Quantit√§tsoperator ist *, der f√ºr 0 oder mehr Treffer steht. M√∂chte man einen Vektor, der aus Stringen-Elementen besteht zu einem Strring zusammenf√ºngen, hilft paste(string) oder str_c(string, collapse = \" \").\n\nstr_replace_all(string, \"[^[:alpha:]+]\", \"\")\n\n[1] \"CorrelationofunemploymentandAfDvotesatbtwrhttpstcoYHyqTguVWx\"\n\n\nMit dem Negationsoperator [^x] wird der Regul√§rausrck x negiert; die Syntax oben hei√üt also ‚Äúersetze in string alles au√üer Buchstaben durch Nichts‚Äù. Mit ‚ÄúNichts‚Äù sind hier Strings der L√§nge Null gemeint; ersetzt man einen belieibgen String durch einen String der L√§nge Null, so hat man den String gel√∂scht.\nDas Cheatsheet zur Strings bzw zu stringr von RStudio gibt einen guten √úberblick √ºber Regex; im Internet finden sich viele Beispiele.\n\n\n3.2.6.2 Regex im Texteditor\nEinige Texteditoren unterst√ºtzen Regex, so auch RStudio.\nDas ist eine praktische Sache. Ein Beispiel: Sie haben eine Liste mit Namen der Art:\n\nNachname1, Vorname1\nNachname2, Vorname2\nNachname3, Vorname3\n\nUnd Sie m√∂chten jetzt aber die Liste mit Stil Vorname Nachname sortiert haben.\nRStudio mit Regex macht‚Äôs m√∂glich, s. ?fig-vorher-regex.\n\n\n \nAbbildung¬†3.3: ?(caption)\n\n\n\n\n\n3.2.7 Emoji-Analyse\nEine einfache Art, Emojis in einer Textmining-Analyse zu verarbeiten, bietet das Paket textclean:\n\nfls <- system.file(\"docs/emoji_sample.txt\", package = \"textclean\")\nx <- readLines(fls)[1]\nx\n\n[1] \"Proin üòç ut maecenas üòè condimentum üòî purus eget. Erat, üòÇvitae nunc elit. Condimentum üò¢ semper iaculis bibendum sed tellus. Ut suscipit interdumüòë in. Faucibüòû us nunc quis a vitae posuere. üòõ Eget amet sit condimentum non. Nascetur vitae ‚òπ et. Auctor ornare ‚ò∫ vestibulum primis justo congue üòÄurna ac magna. Quam üò• pharetra üòü eros üòífacilisis ac lectus nibh est üòôvehicula üòê ornare! Vitae, malesuada üòé erat sociosqu urna, üòè nec sed ad aliquet üòÆ .\"\n\n\n\nreplace_emoji(x)\n\n[1] \"Proin smiling face with heart-eyes ut maecenas smirking face condimentum pensive face purus eget. Erat, face with tears of joy vitae nunc elit. Condimentum crying face semper iaculis bibendum sed tellus. Ut suscipit interdum expressionless face in. Faucib disappointed face us nunc quis a vitae posuere. face with tongue Eget amet sit condimentum non. Nascetur vitae frowning face et. Auctor ornare smiling face vestibulum primis justo congue grinning face urna ac magna. Quam sad but relieved face pharetra worried face eros unamused face facilisis ac lectus nibh est kissing face with smiling eyes vehicula neutral face ornare! Vitae, malesuada smiling face with sunglasses erat sociosqu urna, smirking face nec sed ad aliquet face with open mouth .\"\n\nreplace_emoji_identifier(x)\n\n[1] \"Proin lexiconwiutsdotskrupggpgmhm ut maecenas lexiconwizbukzesopzflfinotj condimentum lexiconwlnxqescoesytfatoevi purus eget. Erat, lexiconwcaiviebiytolowkanmb vitae nunc elit. Condimentum lexiconwpujksvgujncexktvyrn semper iaculis bibendum sed tellus. Ut suscipit interdum lexiconwknnasgueiicggptyzbx in. Faucib lexiconwoxfeslcareuqfkbyjgy us nunc quis a vitae posuere. lexiconwobmhqdrrzgygdexhnkk Eget amet sit condimentum non. Nascetur vitae lexiconbfalxvockmnmtmycmwyq et. Auctor ornare lexiconbgmujofaalvxqrklfqgd vestibulum primis justo congue lexiconvygwtlyrpywfarytvfis urna ac magna. Quam lexiconwurhpvewhizayynmfxqo pharetra lexiconwpmuduwgbxxrxeltrueb eros lexiconwkrvakxddtqckcjxeksl facilisis ac lectus nibh est lexiconwmsjgfnelqfeyhgudmfj vehicula lexiconwjfhkpcsgcjtotwlapxa ornare! Vitae, malesuada lexiconwivnupleicqgksianinp erat sociosqu urna, lexiconwizbukzesopzflfinotj nec sed ad aliquet lexiconxbwhfeflxbuupjezgdwl .\"\n\n\n\n\n3.2.8 Text aufr√§umen\nEine Reihe generischer Tests bietet das Paket textclean von Tyler Rinker:\nHier ist ein ‚Äúunaufger√§umeter‚Äù Text:\n\nx <- c(\"i like\", \"<p>i want. </p>. thet them ther .\", \"I am ! that|\", \"\", NA, \n    \"&quot;they&quot; they,were there\", \".\", \"   \", \"?\", \"3;\", \"I like goud eggs!\", \n    \"bi\\xdfchen Z\\xfcrcher\", \"i 4like...\", \"\\\\tgreat\",  \"She said \\\"yes\\\"\")\n\nLassen wir uns dazu ein paar Diagnostiken ausgeben.\n\nEncoding(x) <- \"latin1\"\nx <- as.factor(x)\ncheck_text(x)\n\n\n=============\nNON CHARACTER\n=============\n\nThe text variable is not a character column (likely `factor`):\n\n\n*Suggestion: Consider using `as.character` or `stringsAsFactors = FALSE` when reading in\n             Also, consider rerunning `check_text` after fixing\n\n\n=====\nDIGIT\n=====\n\nThe following observations contain digits/numbers:\n\n10, 13\n\nThis issue affected the following text:\n\n10: 3;\n13: i 4like...\n\n*Suggestion: Consider using `replace_number`\n\n\n========\nEMOTICON\n========\n\nThe following observations contain emoticons:\n\n6\n\nThis issue affected the following text:\n\n6: &quot;they&quot; they,were there\n\n*Suggestion: Consider using `replace_emoticons`\n\n\n=====\nEMPTY\n=====\n\nThe following observations contain empty text cells (all white space):\n\n1\n\nThis issue affected the following text:\n\n1: i like\n\n*Suggestion: Consider running `drop_empty_row`\n\n\n=======\nESCAPED\n=======\n\nThe following observations contain escaped back spaced characters:\n\n14\n\nThis issue affected the following text:\n\n14: \\tgreat\n\n*Suggestion: Consider using `replace_white`\n\n\n====\nHTML\n====\n\nThe following observations contain HTML markup:\n\n2, 6\n\nThis issue affected the following text:\n\n2: <p>i want. </p>. thet them ther .\n6: &quot;they&quot; they,were there\n\n*Suggestion: Consider running `replace_html`\n\n\n==========\nINCOMPLETE\n==========\n\nThe following observations contain incomplete sentences (e.g., uses ending punctuation like '...'):\n\n13\n\nThis issue affected the following text:\n\n13: i 4like...\n\n*Suggestion: Consider using `replace_incomplete`\n\n\n=============\nMISSING VALUE\n=============\n\nThe following observations contain missing values:\n\n5\n\n*Suggestion: Consider running `drop_NA`\n\n\n========\nNO ALPHA\n========\n\nThe following observations contain elements with no alphabetic (a-z) letters:\n\n4, 7, 8, 9, 10\n\nThis issue affected the following text:\n\n4: \n7: .\n8:    \n9: ?\n10: 3;\n\n*Suggestion: Consider cleaning the raw text or running `filter_row`\n\n\n==========\nNO ENDMARK\n==========\n\nThe following observations contain elements with missing ending punctuation:\n\n1, 3, 4, 6, 8, 10, 12, 14, 15\n\nThis issue affected the following text:\n\n1: i like\n3: I am ! that|\n4: \n6: &quot;they&quot; they,were there\n8:    \n10: 3;\n12: bi√üchen Z√ºrcher\n14: \\tgreat\n15: She said \"yes\"\n\n*Suggestion: Consider cleaning the raw text or running `add_missing_endmark`\n\n\n====================\nNO SPACE AFTER COMMA\n====================\n\nThe following observations contain commas with no space afterwards:\n\n6\n\nThis issue affected the following text:\n\n6: &quot;they&quot; they,were there\n\n*Suggestion: Consider running `add_comma_space`\n\n\n=========\nNON ASCII\n=========\n\nThe following observations contain non-ASCII text:\n\n12\n\nThis issue affected the following text:\n\n12: bi√üchen Z√ºrcher\n\n*Suggestion: Consider running `replace_non_ascii`\n\n\n==================\nNON SPLIT SENTENCE\n==================\n\nThe following observations contain unsplit sentences (more than one sentence per element):\n\n2, 3\n\nThis issue affected the following text:\n\n2: <p>i want. </p>. thet them ther .\n3: I am ! that|\n\n*Suggestion: Consider running `textshape::split_sentence`\n\n\n\n\n3.2.9 Diverse Wortlisten\nTyler Rinker stellt mit dem Paket lexicon eine Zusammenstellung von Wortlisten zu diversen Zwecken zur Verf√ºgung. Allerding nur f√ºr die englische Sprache.\n\n\n3.2.10 Sentimentanalyse\n\n3.2.10.1 Einf√ºhrung\nEine weitere interessante Analyse ist, die ‚ÄúStimmung‚Äù oder ‚ÄúEmotionen‚Äù (Sentiments) eines Textes auszulesen. Die Anf√ºhrungszeichen deuten an, dass hier ein Ma√ü an Verst√§ndnis suggeriert wird, welches nicht (unbedingt) von der Analyse eingehalten wird. Jedenfalls ist das Prinzip der Sentiment-Analyse im einfachsten Fall so:\n\nSchau dir jeden Token aus dem Text an.\n\nPr√ºfe, ob sich das Wort im Lexikon der Sentiments wiederfindet.\n\nWenn ja, dann addiere den Sentimentswert dieses Tokens zum bestehenden Sentiments-Wert.\n\nWenn nein, dann gehe weiter zum n√§chsten Wort.\n\nLiefere zum Schluss die Summenwerte pro Sentiment zur√ºck.\n\nEs gibt Sentiment-Lexika, die lediglich einen Punkt f√ºr ‚Äúpositive Konnotation‚Äù bzw. ‚Äúnegative Konnotation‚Äù geben; andere Lexiko weisen differenzierte Gef√ºhlskonnotationen auf. Wir nutzen hier das deutsche Sentimentlexikon sentiws (Remus, Quasthoff, und Heyer 2010). Sie k√∂nnen das Lexikon als CSV hier herunterladen:\n\nsentiws <- read_csv(\"https://osf.io/x89wq/?action=download\")\n\nDen Volltext zum Paper finden Sie z.B. hier.\nAlternativ k√∂nnen Sie die Daten aus dem Paket pradadata laden. Allerdings m√ºssen Sie dieses Paket von Github installieren:\n\ninstall.packages(\"devtools\", dep = TRUE)\ndevtools::install_github(\"sebastiansauer/pradadata\")\n\n\ndata(sentiws, package = \"pradadata\")\n\nTabelle¬†3.1 zeigt einen Ausschnitt aus dem Sentiment-Lexikon SentiWS.\n\n\n\n\n\nTabelle¬†3.1: Auszug aus SentiWS\n\n\nneg_pos\nword\nvalue\ninflections\n\n\n\n\nneg\nAbbau\n-0.0580\nAbbaus,Abbaues,Abbauen,Abbaue\n\n\nneg\nAbbruch\n-0.0048\nAbbruches,Abbr√ºche,Abbruchs,Abbr√ºchen\n\n\nneg\nAbdankung\n-0.0048\nAbdankungen\n\n\nneg\nAbd√§mpfung\n-0.0048\nAbd√§mpfungen\n\n\nneg\nAbfall\n-0.0048\nAbfalles,Abf√§lle,Abfalls,Abf√§llen\n\n\nneg\nAbfuhr\n-0.3367\nAbfuhren\n\n\n\n\n\n\n\n\n\n3.2.10.2 Ungewichtete Sentiment-Analyse\nNun k√∂nnen wir jedes Token des Textes mit dem Sentiment-Lexikon abgleichen; dabei z√§hlen wir die Treffer f√ºr positive bzw. negative Terme. Zuvor m√ºssen wir aber noch die Daten (afd_long) mit dem Sentimentlexikon zusammenf√ºhren (joinen). Das geht nach bew√§hrter Manier mit inner_join; ‚Äúinner‚Äù sorgt dabei daf√ºr, dass nur Zeilen behalten werden, die in beiden Dataframes vorkommen. Tabelle Tabelle¬†3.2 zeigt Summe, Anzahl und Anteil der Emotionswerte.\nWir nutzen die Tabelle afd_long, die wir oben definiert haben.\n\nafd_long %>% \n  inner_join(sentiws, by = c(\"token\" = \"word\")) %>% \n  select(-inflections) -> afd_senti  # die Spalte brauchen wir nicht\n\nafd_senti %>% \n  group_by(neg_pos) %>% \n  summarise(polarity_sum = sum(value),\n            polarity_count = n()) %>% \n  mutate(polarity_prop = (polarity_count / sum(polarity_count)) %>% round(2)) ->\n  afd_senti_tab\n\n\n\n\n\n\nTabelle¬†3.2: Zusammenfassung von SentiWS\n\n\nneg_pos\npolarity_sum\npolarity_count\npolarity_prop\n\n\n\n\nneg\n-48.5307\n210\n0.27\n\n\npos\n30.6595\n578\n0.73\n\n\n\n\n\n\n\nDie Analyse zeigt, dass die emotionale Bauart des Textes durchaus interessant ist: Es gibt viel mehr positiv get√∂nte W√∂rter als negativ get√∂nte. Allerdings sind die negativen W√∂rter offenbar deutlich st√§rker emotional aufgeladen, denn die Summe an Emotionswert der negativen W√∂rter ist (√ºberraschenderweise?) deutlich gr√∂√üer als die der positiven.\nBetrachten wir also die intensivsten negativ und positive konnotierten W√∂rter n√§her.\n\nafd_senti %>% \n  distinct(token, .keep_all = TRUE) %>% \n  mutate(value_abs = abs(value)) %>% \n  top_n(20, value_abs) %>% \n  pull(token)\n\n [1] \"ungerecht\"    \"besonders\"    \"gef√§hrlich\"   \"√ºberfl√ºssig\"  \"behindern\"   \n [6] \"gelungen\"     \"brechen\"      \"unzureichend\" \"gemein\"       \"verletzt\"    \n[11] \"zerst√∂ren\"    \"trennen\"      \"falsch\"       \"vermeiden\"    \"zerst√∂rt\"    \n[16] \"schwach\"      \"belasten\"     \"sch√§dlich\"    \"t√∂ten\"        \"verbieten\"   \n\n\nDiese ‚ÄúHitliste‚Äù wird zumeist (19/20) von negativ polarisierten Begriffen aufgef√ºllt, wobei ‚Äúbesonders‚Äù ein Intensivierwort ist, welches das Bezugswort verst√§rt (‚Äúbesonders gef√§hrlich‚Äù). Das Argument keep_all = TRUE sorgt daf√ºr, dass alle Spalten zur√ºckgegeben werden, nicht nur die durchsuchte Spalte token. Mit pull haben wir aus dem Dataframe, der von den dplyr-Verben √ºbergeben wird, die Spalte pull ‚Äúherausgezogen‚Äù; hier nur um Platz zu sparen bzw. der √úbersichtlichkeit halber.\nNun k√∂nnte man noch den erzielten ‚ÄúNetto-Sentimentswert‚Äù des Corpus ins Verh√§ltnis setzen Sentimentswert des Lexikons: Wenn es insgesamt im Sentiment-Lexikon sehr negativ zuginge, w√§re ein negativer Sentimentwer in einem beliebigen Corpus nicht √ºberraschend. describe_distribution aus {easystats} gibt uns einen √úberblick der √ºblichen deskriptiven Statistiken.\n\nsentiws %>% \n  select(value, neg_pos) %>% \n  #group_by(neg_pos) %>% \n  describe_distribution()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nMean\nSD\nIQR\nRange\nSkewness\nKurtosis\nn\nn_Missing\n\n\n\n\nvalue\n-0.05\n0.20\n0.05\n(-1.00, 1.00)\n-0.68\n2.36\n3468\n0\n\n\n\n\n\nInsgesamt ist das Lexikon ziemlich ausgewogen; negative Werte sind leicht in der √úberzahl im Lexikon. Unser Corpus hat eine √§hnliche mittlere emotionale Konnotation wie das Lexikon:\n\nafd_senti %>% \n  summarise(senti_sum = mean(value) %>% round(2))\n\n\n\n\n\nsenti_sum\n\n\n\n\n-0.02\n\n\n\n\n\n\n\n\n\n3.2.11 Weitere Sentiment-Lexika\nTyler Rinker stellt das Paket sentimentr zur Verf√ºgung. Matthew Jockers stellt das Paket Syushet zur Verf√ºgung.\n\n\n3.2.12 Google Trends\nEine weitere M√∂glichkeit, ‚ÄúWorth√§ufigkeiten‚Äù zu identifizieren ist Google Trends. Dieser Post zeigt Ihnen eine Einsatzm√∂glichkeit."
  },
  {
    "objectID": "textmining1.html#aufgaben",
    "href": "textmining1.html#aufgaben",
    "title": "3¬† Textmining1",
    "section": "3.3 Aufgaben",
    "text": "3.3 Aufgaben\n\npurrr-map01\npurrr-map02\npurrr-map03\npurrr-map04\nRegex-√úbungen\nAufgaben zum Textmining von Tweets"
  },
  {
    "objectID": "textmining1.html#fallstudie-hate-speech",
    "href": "textmining1.html#fallstudie-hate-speech",
    "title": "3¬† Textmining1",
    "section": "3.4 Fallstudie Hate-Speech",
    "text": "3.4 Fallstudie Hate-Speech\n\n3.4.1 Daten\nEs finden sich mehrere Datens√§tze zum Thema Hate-Speech im √∂ffentlichen Internet, eine Quelle ist Hate Speech Data, ein Repositorium, das mehrere Datens√§tze beinhaltet.\n\nKaggle Hate Speech and Offensive Language Dataset\nBretschneider and Peters Prejudice on Facebook Dataset\nDaten zum Fachartikel‚ÄùLarge Scale Crowdsourcing and Characterization of Twitter Abusive Behavior‚Äù\n\nF√ºr Textmining kann eine Liste mit anst√∂√üigen (obsz√∂nen) W√∂rten n√ºtzlich sein, auch wenn man solche Dinge ungern anf√§sst, verst√§ndlicherweise. Jenyay bietet solche Listen in verschiedenen Sprachen an. Die Liste von KDNOOBW sieht sehr √§hnlich aus (zumindest die deutsche Version). Eine lange Sammlung deutscher Schimpfw√∂rter findet sich im insult.wiki; √§hnlich bei Hyperhero.\nTwitterdaten d√ºrfen nur in ‚Äúdehydrierter‚Äù Form weitergegeben werden, so dass kein R√ºckschluss von ID zum Inhalt des Tweets m√∂glich ist. Daher werden √∂ffentlich nur die IDs der Tweets, als einzige Information zum Tweet, also ohne den eigentlichen Inhalt des Tweets, bereitgestellt.\n√úber die Twitter-API kann man sich, wie oben dargestellt, dann die Tweets wieder ‚Äúrehydrieren‚Äù, also wieder mit dem zugeh√∂rigen Tweet-Text (und sonstigen Infos des Tweets) zu versehen.\n\n\n3.4.2 Grundlegendes Text Mining\nWenden Sie die oben aufgef√ºhrten Techniken des grundlegenden Textminings auf einen der oben dargestellten Hate-Speech-Datens√§tze an. Erstellen Sie ein (HTML-Dokument) mit Ihren Ergebnissen. Stellen Sie die Ergebnisse auf dem Github-Repo dieses Kurses ein. Vergleichen Sie Ihre L√∂sung mit den L√∂sungen der anderen Kursmitglieder.\nWir nutzen noch nicht eigene Daten, die wir von Twitter ausgelesen haben, das heben wir uns f√ºr sp√§ter auf.\n\n\n\n\nHvitfeldt, Emil, und Julia Silge. 2022. Supervised Machine Learning for Text Analysis in R. 1. Aufl. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nLex, Alexander, Nils Gehlenborg, Hendrik Strobelt, Romain Vuillemot, und Hanspeter Pfister. 2014. ‚ÄûUpSet: Visualization of Intersecting Sets‚Äú. IEEE Transactions on Visualization and Computer Graphics 20 (12): 1983‚Äì92. https://doi.org/10.1109/TVCG.2014.2346248.\n\n\nRemus, Robert, Uwe Quasthoff, und Gerhard Heyer. 2010. ‚ÄûSentiWS - a Publicly Available German-language Resource for Sentiment Analysis‚Äú. Proceedings of the 7th International Language Ressources and Evaluation (LREC‚Äô10), 1168‚Äì71.\n\n\nWickham, Hadley, und Garrett Grolemund. 2018. R f√ºr Data Science: Daten importieren, bereinigen, umformen, modellieren und visualisieren. √úbersetzt von Frank Langenau. 1. Auflage. Heidelberg: O‚ÄôReilly. https://r4ds.had.co.nz/index.html."
  },
  {
    "objectID": "populismus.html",
    "href": "populismus.html",
    "title": "4¬† Fallstudie Populismus",
    "section": "",
    "text": "Bild von mcmurryjulie auf Pixabay"
  },
  {
    "objectID": "populismus.html#vorab",
    "href": "populismus.html#vorab",
    "title": "4¬† Fallstudie Populismus",
    "section": "4.1 Vorab",
    "text": "4.1 Vorab\n\n4.1.1 Lernziele\n\nDie Fallstudie erkl√§ren k√∂nnen\n\n\n\n4.1.2 Vorbereitung\n\nClonen Sie das Projekt-Repositorium oder laden Sie es herunter1.\nArbeiten Sie die Syntax zu dem Projekt durch.\n\n\n\n4.1.3 Ben√∂tigte R-Pakete\nIn dem vorgestellten Projekt werden die folgenden R-Pakete verwendet.\n\nlibrary(tidyverse)\nlibrary(twitteR)\nlibrary(tidytext)\nlibrary(lubridate)\nlibrary(magrittr)\nlibrary(tidytext)\nlibrary(stringr)\nlibrary(viridis)\nlibrary(wordcloud)\nlibrary(SnowballC)\nlibrary(knitr)\nlibrary(testthat)"
  },
  {
    "objectID": "populismus.html#wie-populistisch-tweeten-unsere-politikerinnen",
    "href": "populismus.html#wie-populistisch-tweeten-unsere-politikerinnen",
    "title": "4¬† Fallstudie Populismus",
    "section": "4.2 Wie populistisch tweeten unsere Politiker:innen?",
    "text": "4.2 Wie populistisch tweeten unsere Politiker:innen?\nVerschaffen Sie sich einen √úberblick √ºber dieses Projekt! Im Rahmen dieses Projekts vergleicht der Autor den Populismus von deutschen Politiker:innen, so wie er sich in den Tweets dieser Personen niederschl√§gt. Auf dieser Basis wird ein Populismuswert, bestehend aus mehreren Teilwerten, berechnet und auf Parteiebenen (als Mittel der zugeh√∂rigen Politiker:innen) berechnet. Nat√ºrlich fragt man sich, wie Populismus definiert ist und wie diese Definition in den Berechnungen umgesetzt wurde. Finden Sie es selber heraus: Im Github-Repo sind alle Details dokumentiert.\nZum Einstieg hilft ein √úberblick √ºber die Ergebnisse der Analyse, die in diesem Vortrag zusammengefasst sind.\nDieser Post stellt die Ergebnisse mit etwas Kontext dar."
  },
  {
    "objectID": "word-embedding.html",
    "href": "word-embedding.html",
    "title": "5¬† Word Embedding",
    "section": "",
    "text": "Bild von mcmurryjulie auf Pixabay"
  },
  {
    "objectID": "word-embedding.html#vorab",
    "href": "word-embedding.html#vorab",
    "title": "5¬† Word Embedding",
    "section": "5.1 Vorab",
    "text": "5.1 Vorab\n\n5.1.1 Lernziele\n\nDie grundlegenden Konzepte der Informationstheorie erkl√§ren k√∂nnen\nDie vorgestellten Techniken des Textminings mit R anwenden k√∂nnen\n\n\n\n5.1.2 Vorbereitung\n\nLesen Sie diesen Text als Vorbereitung\nArbeiten Sie Hvitfeldt und Silge (2022), Kap. 5 durch.\n\n\n\n5.1.3 Ben√∂tigte R-Pakete\n\nlibrary(tidyverse)\nlibrary(easystats)  # Komfort f√ºr deskriptive Statistiken, wie `describe_distribution`\nlibrary(tidytext)\nlibrary(hcandersenr)   # Textdaten\nlibrary(slider)  # slide\nlibrary(widyr)  # pairwise_pmi\nlibrary(furrr)  # mehrere Kerne gleichzeitig nutzen\nlibrary(textdata)  # Worteinbettungen, vorgekocht\nlibrary(entropy)  # Entropie berechnen\nlibrary(widyr)  # √Ñhnlichkeit berechnen mit widyr_svd\nlibrary(furrr)  # Mehrere Kerne gleichzeitig"
  },
  {
    "objectID": "word-embedding.html#informationstheorie",
    "href": "word-embedding.html#informationstheorie",
    "title": "5¬† Word Embedding",
    "section": "5.2 Informationstheorie",
    "text": "5.2 Informationstheorie\n\n5.2.1 Einf√ºhrung\nDie Informationstheorie ist eine der Sternstunden der Wissenschaft. Manche sagen dass Claude Shannon, der Autor der Theorie, auf einer Stufe mit Darwin und Einstein stehen sollte:\n\nIn this single paper, Shannon introduced this new fundamental theory. He raised the right questions, which no one else even thought of asking. This would have been enough to make this contribution earthshaking. But amazingly enough, Shannon also provided most of the right answers with class and elegance. In comparison, it took decades for a dozen of top physicists to define the basics of quantum theory. Meanwhile, Shannon constructed something equivalent, all by himself, in a single paper. Shannon‚Äôs theory has since transformed the world like no other ever had, from information technologies to telecommunications, from theoretical physics to economical globalization, from everyday life to philosophy. (‚Ä¶) I don‚Äôt think Shannon has had the credits he deserves. He should be right up there, near Darwin and Einstein, among the few greatest scientists mankind has ever had the chance to have.\n\nF√ºr die Statistik ist die Informationstheorie von hoher Bedeutung. Im Folgenden schauen wir uns einige Grundlagen an.\n\n\n5.2.2 Wozu ist das gut?\nBevor man sich mit einem Thema wie der Informationtheorie (Informationsentropie mit verwandten Konstrukten oder kurz Entropie) besch√§ftigt, sollte die Frage gekl√§rt sein, wozu das Thema gut ist. Hier sind drei Antworten dazu:\n\nIm Maschinenlernen wird die Informationtheorie verwendet, um die G√ºte von Klassifikationsmodellen zu berechnen.\nSpeziell im Textmining wird die Entropie verwendet, um den Zusammenhang von W√∂rtern zu quantifizieren.\nWenige Theorien haben so viel neue Forschung initiert, wie Shannon (1948) ber√ºhmtes Paper. Es ist also auf jeden Fall eine Perle der Geistesgeschichte.\n\n\n\n5.2.3 Shannon-Information\nMit der Shannon-Information (Information, Selbstinformation) quantifizieren wir, wie viel ‚Äú√úberraschung‚Äù sich in einem Ereignis verbirgt (Shannon 1948).\nEin Ereignis mit ‚Ä¶\n\ngeringer Wahrscheinlichkeit: Viel √úberraschung (Information)\nhoher Wahrscheinlichkeit: Wenig √úberraschung (Information)\n\nWenn wir also erfahren, dass ein unwahrscheinliches Ereignis eingetreten ist (Schnee im Sommer), sind wir √ºberraschter als wenn wir h√∂hen, dass ein wahrscheinliches Ereignis eingetreten ist (Schnee im Winter).\nDie Information eines Ereignis ist also eng verbunden mit seiner Wahrscheinlichkeit oder auch mit den Odds eines Ereignisses.\nDie Shannon-Information ist die einzige Gr√∂√üe, die einige w√ºnschenswerte Anforderungen1 erf√ºllt:\n\nStetig\nJe mehr Ereignisse in einem Zufallsexperiment m√∂glich sind, desto h√∂her die Information, wenn ein bestimmtes Ereignis eintritt\nAdditiv: Die Summe der Information zweier Teilereignisse ist gleich der Information des Gesamtereignis\n\n\nDefinition 5.1 (Shannon-Information) Die Information ist so definiert:\n\\[I(x) = - \\log_2 \\left( Pr(x) \\right)\\]\n\nAndere Logaritmusbasen sind m√∂glich. Bei einem bin√§ren Logarithmus nennt man die Einheit Bit2.\nEin M√ºnwzurf3 hat 1 Bit Information:\n\n-log(1/2, base = 2)\n\n[1] 1\n\n\nDamit gilt: \\(I = \\frac{1}{Pr(x)}\\)\nDie Information ist eng verwandt mit den Odds bzw. Log-Odds (Logits):\n\\(\\text{log-odds}(x)=\\log \\left({\\frac {p(x)}{p(\\lnot x)}}\\right)\\)\nLogits k√∂nnen als Differenz zweier Shannon-Infos ausgedr√ºckt werden:\n\\(\\text{log-odds}(x)=I(\\lnot x)-I(x)\\)\nDie Information zweier unabh√§ngiger Ereignisse ist additiv.\nDie gemeinsame Wahrscheinlichkeit zweier unabh√§ngiger Ereignisse ist das Produkt der einzelnen Wahrscheinlichkeiten:\n\\(Pr(x,y) = Pr(x) \\cdot Pr(y)\\)\nDie gemeinsame Information ist dann\n\\[\n{\\displaystyle {\\begin{aligned}\\operatorname {I}(x,y)&=-\\log _{2}\\left[p(x,y)\\right]=-\\log _{2}\\left[p(x)p(y)\\right]\\\\[5pt]&=-\\log _{2}\\left[p{(x)}\\right]-\\log _{2}\\left[p{(y)}\\right]\\\\[5pt]&=\\operatorname {I} (x)+\\operatorname {I} (y)\\end{aligned}}}\n\\]\n\nBeispiel 5.1 (Information eines wahrscheinlichen Ereignisses) Die Information eines fast sicheren Ereignisses ist gering.\n\n-log(99/100, base = 2)\n\n[1] 0.01449957\n\n\n\n\nBeispiel 5.2 (Information eines unwahrscheinlichen Ereignisses) Die Information eines unwahrscheinlichen Ereignisses ist hoch.\n\n-log(01/100, base = 2)\n\n[1] 6.643856\n\n\n\n\nBeispiel 5.3 (Information eines W√ºrfelwurfs) Die Wahrscheinlichkeitsfunktion eines W√ºrfel ist\n\\({\\displaystyle Pr(k)={\\begin{cases}{\\frac {1}{6}},&k\\in \\{1,2,3,4,5,6\\}\\\\0,&{\\text{ansonsten}}\\end{cases}}}\\)\nDie Wahrscheinlichkeit, eine 6 zu w√ºrfeln, ist \\(Pr(X=6) = \\frac{1}{6}\\).\nDie Information von \\(X=6\\) betr√§gt also\n\\(I(X=6) = -\\log_2 \\left( Pr(X=6) \\right) = -\\log_2(1/6) \\approx 2.585 \\, \\text{bits}\\).\n\n-log(1/6, base = 2)\n\n[1] 2.584963\n\n\n\n\nBeispiel 5.4 (Information zweier W√ºrfelwurfe) Die Wahrscheinlichkeit, mit zwei W√ºrfeln, \\(X\\) und \\(Y\\), jeweils 6 zu w√ºrfeln, betr√§gt \\(Pr(X=6, Y=6) = \\frac{1}{36}\\)\nDie Information betr√§gt also\n\\(I(X=6, Y=6) = -\\log_2 \\left( Pr(6,6) \\right)\\)\n\n-log(1/36, base = 2)\n\n[1] 5.169925\n\n\nAufgrund der Additivit√§t der Information gilt\n\\(I(6,6) = I(6) + I(6)\\)\n\n-log(1/6, base = 2) + -log(1/6, base = 2)\n\n[1] 5.169925\n\n\n\n\n\n5.2.4 Entropie\n(Informations)entropie ist die Summe der Informationen der Ereignisse einer Zufallsvariablen, \\(X\\).\n\nDefinition 5.2 (Informationsentropie) Informationsentropie ist so definiert:\n\\[H(p) = - \\text{E log} (p_i) = - \\sum_{i = 1}^n p_i \\text{log} (p_i) = E\\left[I(X) \\right]\\]\n\nDie Informationsentropie ist also die ‚Äúmittlere‚Äù oder ‚Äúerwartete Information einer Zufallsvariablen.\nDie Entropie eines M√ºnzwurf-Versuchs (Bernoulli-Experiment) ist dann maximal (1 bit), wenn die Trefferwahrscheinlichkeit 50% betr√§gt: \\(Pr(X=x) = 1/2\\), s. Abb. Abbildung¬†5.1.\n\n\n\nAbbildung¬†5.1: Der Zusammenhang von Wahrscheinlichkeit und Entropie bei einem Bernoulli-Versuch, Quelle: Wikipedia, CC-BY-SA, Brona, Rubber Duck\n\n\n\n\n5.2.5 Gemeinsame Information\nDie gemeinsame Information (mutual information, MI) zweier Zufallsvariablen \\(X\\) und \\(Y\\), \\(I(X,Y)\\), quantifiziert die Informationsmenge, die man √ºber \\(Y\\) erh√§lt, wenn man \\(X\\) beobachtet. Mit anderen Worten: Die MI ist ein Ma√ü des Zusammenhangs zweier (nominaler) Variablen. Im Gegensatz zur Korrelation ist die MI nicht auf lineare Abh√§ngigkeiten beschr√§nkt.\nDie MI quantifiziert den Unterschied zwischen der gemeinsamen Verteilung \\(Pr(X,Y)\\) und dem Produkt einer einzelnen4 Wahrscheinlichkeitsverteilungen, d.h. \\(Pr(X)\\) und \\(Pr(Y)\\).\nWenn die beiden Variablen (stochastisch) unabh√§ngig5 sind, ist ihre gemeinsame Information Null:\n\\(I(X,Y) = 0 \\quad \\text{gdw} \\quad \\bot(X,Y)\\).\nDann gilt n√§mlich:\n\\(\\log \\left( \\frac{Pr(X,Y)} {Pr(X) \\cdot Pr(Y)} \\right) =\\log(1) = 0\\).\nDas macht intuitiv Sinn: Sind zwei Variablen unabh√§ngig, so erf√§hrt man nichts √ºber die zweite, wenn man die erste kennt. So ist Kenntnis der Sternzeichens einer Person und ihrer K√∂rpergr√∂√üe unabh√§ngig.\nDas Gegenteil ist auch wahr: Sind zwei Variablen voneinander komplett abh√§ngig, so wei√ü man alles √ºber die zweite, wenn man die erste kennt.\nDie gemeinsame Information kann man sich als Summe der einzelnen gemeinsamen Informationen von \\(XY\\) sehen (s. Tabelle¬†5.1):\n\nd <- tibble::tribble(\n     ~x1,    ~x2,    ~x3,\n  \"x1y2\", \"x2y1\", \"x3y1\",\n  \"x2y1\", \"x2y2\", \"x3y2\",\n  \"x1y3\", \"x2y3\", \"x3y3\"\n  )\nd\n\n\n\n\nTabelle¬†5.1: Summe der punktweisen gemeinsamen Informationen\n\n\nx1\nx2\nx3\n\n\n\n\nx1y2\nx2y1\nx3y1\n\n\nx2y1\nx2y2\nx3y2\n\n\nx1y3\nx2y3\nx3y3\n\n\n\n\n\n\n\n\\(I(X,Y) = \\Sigma_Y \\Sigma_y Pr(x,y) \\underbrace{\\log \\left( \\frac{Pr(X,Y)}{Pr(X) Pr(Y)} \\right)}_\\text{punktweise MI}\\)\nDie Summanden der gemeinsamen Information bezeichnet man auch als punktweise gemeinsame Information (pointwise mutual information, PMI), entsprechend, s. Gleichung¬†5.1. MI ist also der Erwartungswert der PMI.\n\\[{\\displaystyle \\operatorname {PMI} (x,y)\\equiv \\log_{2}{\\frac {p(x,y)}{p(x)p(y)}}=\\log _{2}{\\frac {p(x|y)}{p(x)}}=\\log _{2}{\\frac {p(y|x)}{p(y)}}}\n\\tag{5.1}\\]\nAndere Basen als log2 sind gebr√§uchlich, vor allem der nat√ºrliche Logarithmus.\n\nAnmerkung. Die zwei rechten Umformungen in Gleichung¬†5.1 basieren auf der Umformung der gemeinsamen Wahrscheinlichkeit.\nZur Erinnerung: \\(p(x,y) = p(y)p(x|y) = p(x)p(y|x)\\)\n\n\nBeispiel 5.5 (Interpretation der PMI) Sei \\(p(x) = p(y) = 1/10\\) und \\(p(x,y) = 1/10\\). W√§ren \\(x\\) und \\(y\\) unabh√§ngig, dann w√§re \\(p^{\\prime}(x,y) = p(x)p(y) = 1/100\\). Das Verh√§ltnis der Produkte der einzelnen Wahrscheinlichkeit zur gemeinsamen Wahrscheinlichkeit w√§re dann 1 und der Logarithmus von 1 ist 0. Das Verh√§ltnis von 1 entspricht also der Unabh√§ngigkeit. Ist das Verh√§ltnis z.B. 5, so zeigt das eine gewisse Abh√§ngigkeit an. Im obigen Beispiel gilt: \\(\\frac{1/20}{1/100}=5\\).\n\nDie MI wird auch √ºber die sog. Kullback-Leibler-Divergenz definiert, die die Differenz zwischen gemeinsamer Wahrscheinlichkeitsfunktion bzw. -dichte und dem Produkt der jeweiligen Randwahrscheinlichkeiten bzw. -dichte angibt.\n\n\n5.2.6 Maximumentropie\n\nDefinition 5.3 (Maximumentropie) Die Verteilungsform, f√ºr die es die meisten M√∂glichkeiten (Pfade im Baumdiagramm) gibt, hat die h√∂chste Informationsentropie.\n\nAbbildung¬†5.2 zeigt ein Baumdiagramm f√ºr einen 3-fachen M√ºnzwurf. In den ‚ÄúBl√§ttern‚Äù (Endknoten) sind die Ergebnisse des Experiments dargestellt sowie die Zufallsvariable \\(X\\), die die Anzahl der ‚ÄúTreffer‚Äù (Kopf) fasst. Wie man sieht, vereinen einige Werte der Zufallsvariable mehr Pfade auf sich als andere: Der Wert \\(X=1\\) vereinigt 3 Pfade (von 8) auf sich; der Wert \\(X=3\\) nur 1 Pfad.\n\n\n\nAbbildung¬†5.2: Pfade im Baumdiagramm: 3-facher M√ºnzwurf\n\n\n\n\n5.2.7 Ilustration\nSagen wir, Sie stehen vor 5 Eimern und haben 10 Kieselsteine bei sich, die mit den Nummern 1 bis 10 beschriftet sind (McElreath 2020). Weil Sie nichts besseres zu tun haben, werfen Sie die Kiesel in die Eimer und zwar so, dass die Wahrscheinlichkeit f√ºr einen Kiesel in einen bestimmten Eimer zu landen f√ºr alle Eimer gleich ist. Sie werfen also Ihre 10 Kiesel und betrachten das Ergebnis; die Kiesel sind jetzt in einem bestimmten (zuf√§lligen) Arrangement auf die Eimer verteilt. Jede Aufteilung (der 10 Kiesel in den 5 Eimern) ist gleich wahrscheinlich6 ‚Äì die Wahrscheinlichkeit, dass alle 10 Kiesel in Eimer 1 landen ist also gleich hoch wie die Wahrscheinlichkeit, dass jeder Eimer einen Kiesel abkriegt. Jetzt kommt‚Äôs: Manche Arrangements k√∂nnen auf mehrere Arten erzielt werden als andere. So gibt es nur eine Aufteilung f√ºr alle 10 Kiesel in einem Eimer (Teildiagramm a, in Abbildung¬†5.3). Aber es gibt 90 M√∂glichkeiten, die Kiesel so aufzuteilen, dass 2 in Eimer 2 landen, 8 in Eimer 4 und 2 in Eimer 4, s. Teildiagramm b in Abbildung¬†5.3. Teildiagramme c bis e zeigen, dass die Anzahl der Aufteilungen schnell astronomisch hoch wird, wenn sich die Kiesel ‚Äúgleichm√§√üiger‚Äù auf die Eimer verteilen. Die gleichm√§√üigste Aufteilung (Diagramm e) hat die gr√∂√üte Zahl an m√∂glichen Anordnungen. Eine Aufteilung der Kiesel auch als Pfad durch ein Baumdiagramm beschrieben werden.\nHier sind ein paar verschiedene Arrangements, deren Anzahl von Aufteilungen wir hier anschauen, s. Tabelle¬†5.2:\n\nd <-\n  tibble(a = c(0, 0, 10, 0, 0),\n         b = c(0, 1, 8, 1, 0),\n         c = c(0, 2, 6, 2, 0),\n         d = c(1, 2, 4, 2, 1),\n         e = 2) \nd\n\n\n\n\nTabelle¬†5.2: Ein paar verschiedene Arrangements (a-e) der Kiesel in den f√ºnf Eimern\n\n\na\nb\nc\nd\ne\n\n\n\n\n0\n0\n0\n1\n2\n\n\n0\n1\n2\n2\n2\n\n\n10\n8\n6\n4\n2\n\n\n0\n1\n2\n2\n2\n\n\n0\n0\n0\n1\n2\n\n\n\n\n\n\n\n\n\n\n\n\nAbbildung¬†5.3: Anzahl von Aufteilungen der 10 Kiesel in 5 Eimer\n\n\n\n\nHier sind die Wahrscheinlichkeitsverteilungen der 5 Arrangements7:\n\nd %>% \n  mutate_all(~. / sum(.))\n\n\n\n\n\na\nb\nc\nd\ne\n\n\n\n\n0\n0.0\n0.0\n0.1\n0.2\n\n\n0\n0.1\n0.2\n0.2\n0.2\n\n\n1\n0.8\n0.6\n0.4\n0.2\n\n\n0\n0.1\n0.2\n0.2\n0.2\n\n\n0\n0.0\n0.0\n0.1\n0.2\n\n\n\n\n\n\nDann kann man die Wahrscheinlichkeit einfach in Entropie umrechnen8:\n\nd %>% \n  mutate_all(~ . / sum(.)) %>% \n  gather() %>% \n  group_by(key) %>% \n  summarise(h = -sum(ifelse(value == 0, 0, value * log(value))))\n\n\n\n\n\nkey\nh\n\n\n\n\na\n0.0000000\n\n\nb\n0.6390319\n\n\nc\n0.9502705\n\n\nd\n1.4708085\n\n\ne\n1.6094379\n\n\n\n\n\n\nDas ifelse dient nur dazu, eine Wahrscheinlichkeit von 0 eine Entropie von 0 zu verpassen9, denn sonst w√ºrden wir ein Problem rennen, wenn wir \\(log(0)\\) ausrechnen.\n\nlog(0)\n\n[1] -Inf"
  },
  {
    "objectID": "word-embedding.html#zufallstext-erkennen",
    "href": "word-embedding.html#zufallstext-erkennen",
    "title": "5¬† Word Embedding",
    "section": "5.3 Zufallstext erkennen",
    "text": "5.3 Zufallstext erkennen\n\n5.3.1 Entropie von Zufallstext\nKann man wohl Zufallstext maschinell erkennen? Sicher gibt es viele Ans√§tze, um das Problem anzugehen. Lassen Sie uns einen Ansatz erforschen. Erforschen hei√üt, wir erforschen f√ºr uns, es handelt sich um eine didaktische √úbung, das Ziel ist nicht, Neuland f√ºr die Menschheit zu betreten.\nAber zuerst m√ºssen wir √ºberlegen, was ‚ÄúZufallstext‚Äù bedeuten soll.\nNehmen wir uns dazu zuerst einen richtigen Text, ein M√§rchen von H.C. Andersen zum Beispiel. Nehmen wir das Erste aus der Liste in dem Tibble hcandersen_de, ‚Äúdas Feuerzeug‚Äù.\n\ndas_feuerzeug <-\n  hcandersen_de  %>% \n  filter(book == \"Das Feuerzeug\") %>% \n  unnest_tokens(input = text, output = word) %>% \n  pull(word) \n\nhead(das_feuerzeug)\n\n[1] \"es\"         \"kam\"        \"ein\"        \"soldat\"     \"die\"       \n[6] \"landstra√üe\"\n\n\nDas M√§rchen ist 2688 W√∂rter lang.\n\nwortliste <- \nhcandersen_de  %>% \n  filter(book == \"Das Feuerzeug\") %>% \n  unnest_tokens(output = word, input = text) %>% \n  pull(word) %>% \n  unique()\n\nhead(wortliste)\n\n[1] \"es\"         \"kam\"        \"ein\"        \"soldat\"     \"die\"       \n[6] \"landstra√üe\"\n\n\nJetzt ziehen wir Stichproben (mit Zur√ºcklegen) aus dieser Liste und erstellen damit eine Art Zufallstext.\n\nzufallstext <- \n  sample(x = wortliste, size = length(das_feuerzeug)*10^2, replace = TRUE)\nhead(zufallstext)\n\n[1] \"hat\"    \"kreide\" \"ins\"    \"komme\"  \"seinen\" \"dort\"  \n\n\nZ√§hlen wir, wie h√§ufig jedes Wort vorkommt:\n\nzufallstext_count <-\ntibble(zufallstext = zufallstext) %>% \n  count(zufallstext)\n\nhead(zufallstext_count)\n\n\n\n\n\nzufallstext\nn\n\n\n\n\nab\n356\n\n\nabend\n386\n\n\naber\n347\n\n\nabflog\n350\n\n\nabschlagen\n388\n\n\nacht\n379\n\n\n\n\n\n\nDer H√§ufigkeitsvektor von wortliste besteht nur aus Einsen, so haben wir ja gerade die Wortliste definiert:\n\nwortliste_count <-\ntibble(wortliste = wortliste) %>% \n  count(wortliste)\n\nhead(wortliste_count)\n\n\n\n\n\nwortliste\nn\n\n\n\n\nab\n1\n\n\nabend\n1\n\n\naber\n1\n\n\nabflog\n1\n\n\nabschlagen\n1\n\n\nacht\n1\n\n\n\n\n\n\nDaher ist ihre Informationsentropy maximal.\n\nentropy(wortliste_count$n, unit = \"log2\")\n\n[1] 9.47978\n\n\nDie H√§ufigkeiten der W√∂rter in zufallstext hat eine hohe Entropie.\n\nentropy(zufallstext_count$n, unit = \"log2\")\n\n[1] 9.47792\n\n\nZ√§hlen wir die H√§ufigkeiten in der Geschichte ‚ÄúDas Feuerzeug‚Äù.\n\ndas_feuerzeug_count <-\n  tibble(text = das_feuerzeug) %>% \n  count(text)\n\nhead(das_feuerzeug_count)\n\n\n\n\n\ntext\nn\n\n\n\n\nab\n2\n\n\nabend\n3\n\n\naber\n21\n\n\nabflog\n1\n\n\nabschlagen\n1\n\n\nacht\n1\n\n\n\n\n\n\nUnd berechnen dann die Entropie:\n\nentropy(das_feuerzeug_count$n, unit = \"log2\")\n\n[1] 8.075194\n\n\nDer Zufallstext hat also eine h√∂here Entropie als der echte M√§rchentext. Der Zufallstext ist also gleichverteilter in den Worth√§ufigkeiten.\nPro Bit weniger Entropie halbiert sich die Anzahl der M√∂glichkeiten einer H√§ufigkeitsverteilung.\n\n\n5.3.2 MI von Zufallstext\nLeft as an exercises for the reader10 ü•≥."
  },
  {
    "objectID": "word-embedding.html#daten",
    "href": "word-embedding.html#daten",
    "title": "5¬† Word Embedding",
    "section": "5.4 Daten",
    "text": "5.4 Daten\n\n5.4.1 Complaints-Datensatz\nDer Datensatz complaints stammt aus dieser Quelle.\nDen Datensatz complaints kann man hier herunterladen. Im Buch ist die Webseite nicht (direkt?) angegeben. Die Datei ist mit gz gepackt; read_csv sollte das automatisch entpacken. Achtung: Die Datei ist recht gro√ü.\n\nd_path <- \"https://github.com/EmilHvitfeldt/smltar/raw/master/data/complaints.csv.gz\"\ncomplaints <- read_csv(d_path)\n\nGeschickter als jedes Mal die Datei herunterzuladen, ist es, sie einmal herunterzuladen, und dann lokal zu speichern, etwa im Unterordner data des RStudio-Projektordners.\nNach dem Importieren wird der Datensatz in eine Tidy-Form gebracht (mit unnest_tokens) und dann verschachtelt, mit nest.\n\n\n5.4.2 Complaints verk√ºrzt und geschachtelt\nUm ein Herumprobieren zu erleichtern, ist hier der Datensatz complaints in zwei verk√ºrzten Formen bereitgestellt:\n\nnested_words2_path <- \"https://github.com/sebastiansauer/datascience-text/raw/main/data/nested_words2.rds\"\nnested_words3_path <- \"https://github.com/sebastiansauer/datascience-text/raw/main/data/nested_words3.rds\"\n\nnested_words2 enth√§lt die ersten 10% des Datensatz nested_wordsund ist gut 4 MB gro√ü (mit gz gezippt); er besteht aus ca. 11 Tausend Beschwerden. nested_words3 enth√§lt nur die ersten 10 Beschwerden, was ihn gut zum Handhaben macht.\nBeide sind verschachtelt und aus tidy_complaints (s. Kap. 5.1) hervorgegangen.\n\nnested_words3 <- read_rds(nested_words3_path)\n\nDas sieht dann so aus:\n\nnested_words3 %>% \n  head(3)\n\n\n\n\n\n\n\n\n\ncomplaint_id\nwords\n\n\n\n\n3384392\nsystems , inc , is , trying , to , collect , a , debt , that , is , not , mine , not , owed , and , is , inaccurate\n\n\n3417821\ni , would , like , to , request , the , of , the , following , items , from , my , credit , report , which , are , the , result , of , my , victim , to , identity , theft , this , information , does , not , to , transactions , that , i , have , made , accounts , that , i , have , opened , as , the , attached , supporting , documentation, can , as , such , it , should , be , blocked , from , on , my , credit , report , pursuant , to , section , of , the , fair , credit , reporting , act\n\n\n3433198\nover , the , past , 2 , weeks , i , have , been , receiving , amounts , of , telephone , calls , from , the , company , listed , in , this , complaint , the , calls , between , xxxx , xxxx , and , xxxx , xxxx , to , my , cell , and , at , my , job , the , company , does , not , have , the , right , to , me , at , work , and , i , want , this , to , stop , it , is , extremely , to , be , told , 5 , times , a , day , that , i , have , a , call , from , this , collection, agency , while , at , work\n\n\n\n\n\n\nWerfen wir einen Blick in den TExt der ersten Beschwerde des Datensatzes mit der ID nested_words3_path$complaint_id[1].\n\nbeschwerde1_text <- nested_words3$words[[1]]\n\nDas ist ein Tibble mit einer Spalte und 17 W√∂rtern; da wir schon auf Unigramme aufgeteilt haben, ist jedes Wort ein Element des Vektors word:\n\nbeschwerde1_text %>% \n  head()\n\n\n\n\n\nword\n\n\n\n\nsystems\n\n\ninc\n\n\nis\n\n\ntrying\n\n\nto\n\n\ncollect\n\n\n\n\n\n\n\nbeschwerde1_text$word\n\n [1] \"systems\"    \"inc\"        \"is\"         \"trying\"     \"to\"        \n [6] \"collect\"    \"a\"          \"debt\"       \"that\"       \"is\"        \n[11] \"not\"        \"mine\"       \"not\"        \"owed\"       \"and\"       \n[16] \"is\"         \"inaccurate\""
  },
  {
    "objectID": "word-embedding.html#wordembeddings-selber-erstellen",
    "href": "word-embedding.html#wordembeddings-selber-erstellen",
    "title": "5¬† Word Embedding",
    "section": "5.5 Wordembeddings selber erstellen",
    "text": "5.5 Wordembeddings selber erstellen\n\n5.5.1 PMI berechnen\nRufen Sie sich die Definition der PMI ins Ged√§chtnis, s. Gleichung¬†5.1.\nMit R kann man die PMI z.B. so berechnen, s. ? pairwise_pmi aus dem Paket {widyr}.\nZum Paket widyr von Robinson und Silge:\n\nThis package wraps the pattern of un-tidying data into a wide matrix, performing some processing, then turning it back into a tidy form. This is useful for several mathematical operations such as co-occurrence counts, correlations, or clustering that are best done on a wide matrix.\n\nQuelle\nErzeugen wir uns Dummy-Daten:\n\ndat <- tibble(feature = rep(1:5, each = 2),\n              item = c(\"a\", \"b\",\n                       \"a\", \"c\",\n                       \"a\", \"c\",\n                       \"b\", \"e\",\n                       \"b\", \"f\"))\n\ndat\n\n\n\n\n\nfeature\nitem\n\n\n\n\n1\na\n\n\n1\nb\n\n\n2\na\n\n\n2\nc\n\n\n3\na\n\n\n3\nc\n\n\n4\nb\n\n\n4\ne\n\n\n5\nb\n\n\n5\nf\n\n\n\n\n\n\nAus der Hilfe der Funktion:\n\nFind pointwise mutual information of pairs of items in a column, based on a ‚Äúfeature‚Äù column that links them together. This is an example of the spread-operate-retidy pattern.\n\nDie Argumente der Funktion sind:\nitem\nItem to compare; will end up in item1 and item2 columns\nfeature\nColumn describing the feature that links one item to others\nManche Berechnungen (Operationen) lassen sich vielleicht leichter nicht in der Tidy-Form (Langform), sondern in der ‚Äúbreiten‚Äù oder Matrixform ausf√ºhren. Wandeln wir mal dat von der Langform in die Breitform um:\n\ntable(dat$item, dat$feature)\n\n   \n    1 2 3 4 5\n  a 1 1 1 0 0\n  b 1 0 0 1 1\n  c 0 1 1 0 0\n  e 0 0 0 1 0\n  f 0 0 0 0 1\n\n\nSilge und Robinson verdeutlichen das Prinzip von widyr so, s. Abbildung¬†5.4.\n\n\n\nAbbildung¬†5.4: Die Funktionsweise von widyr, Quelle: Silge und Robinson\n\n\n(Vgl. auch die Erkl√§rung hier.)\nBauen wir das mal von Hand nach.\nRandwahrscheinlichkeiten von a und c sowie deren Produkt, p_a_p_c:\n\np_a <- 3/5\np_c <- 2/5\n\np_a_p_c <- p_a * p_c\np_a_p_c\n\n[1] 0.24\n\n\nGemeinsame Wahrscheinlichkeit von a und c:\n\np_ac <- 2/5\n\nPMI von Hand berechnet:\n\nlog(p_ac/p_a_p_c)\n\n[1] 0.5108256\n\n\nMan beachte, dass hier als Basis \\(e\\), der nat√ºrliche Logarithmus, verwendet wurde (nicht 2).\nJetzt berechnen wir die PMI mit pairwise_pmi.\n\npairwise_pmi(dat, item = item, feature = feature)\n\n\n\n\n\nitem1\nitem2\npmi\n\n\n\n\nb\na\n-0.5877867\n\n\nc\na\n0.5108256\n\n\na\nb\n-0.5877867\n\n\ne\nb\n0.5108256\n\n\nf\nb\n0.5108256\n\n\na\nc\n0.5108256\n\n\nb\ne\n0.5108256\n\n\nb\nf\n0.5108256\n\n\n\n\n\n\nWie man sieht, entspricht unserer Berechnung von Hand der Berechnung mit pairwise_pmi.\n\n\n5.5.2 Sliding\nSliding ist ein interessantes Konzept, aber man braucht vielleicht etwas Zeit, um sein Hirn um das Konzept zu wickeln‚Ä¶\nHier eine Illustration:\n\ntxt_vec <- \"Das ist ein Test, von dem nicht viel zu erwarten ist\"\n\nslider::slide(txt_vec, ~ .x, .before = 2)\n\n[[1]]\n[1] \"Das ist ein Test, von dem nicht viel zu erwarten ist\"\n\n\nOh, da passiert nichts?! Kaputt? Nein, wir m√ºssen jedes Wort als ein Element des Vektors auffassen.\n\ntxt_df <-\n  tibble(txt = txt_vec) %>% \n  unnest_tokens(input = txt, output = word)\n\nhead(txt_df)\n\n\n\n\n\nword\n\n\n\n\ndas\n\n\nist\n\n\nein\n\n\ntest\n\n\nvon\n\n\ndem\n\n\n\n\n\n\n\nslider::slide(txt_df$word, ~ .x, .before = 2)\n\n[[1]]\n[1] \"das\"\n\n[[2]]\n[1] \"das\" \"ist\"\n\n[[3]]\n[1] \"das\" \"ist\" \"ein\"\n\n[[4]]\n[1] \"ist\"  \"ein\"  \"test\"\n\n[[5]]\n[1] \"ein\"  \"test\" \"von\" \n\n[[6]]\n[1] \"test\" \"von\"  \"dem\" \n\n[[7]]\n[1] \"von\"   \"dem\"   \"nicht\"\n\n[[8]]\n[1] \"dem\"   \"nicht\" \"viel\" \n\n[[9]]\n[1] \"nicht\" \"viel\"  \"zu\"   \n\n[[10]]\n[1] \"viel\"     \"zu\"       \"erwarten\"\n\n[[11]]\n[1] \"zu\"       \"erwarten\" \"ist\"     \n\n\nAh!\nDas Aufteilen in einzelne W√∂rter pro Element des Vektors k√∂nnte man auch so erreichen:\n\ntxt_vec2 <- str_split(txt_vec, pattern = boundary(\"word\")) %>% \n  simplify() \n\nslide(txt_vec2, .f = ~.x, .before = 2)\n\nShifting non-numeric variables is not possible.\n  Try using 'to_numeric()' and specify the 'lowest' argument.\n\n\n [1] \"Das\"      \"ist\"      \"ein\"      \"Test\"     \"von\"      \"dem\"     \n [7] \"nicht\"    \"viel\"     \"zu\"       \"erwarten\" \"ist\"     \n\n\nIn unserem Beispiel mit den Beschwerden:\n\nslide(beschwerde1_text$word,  ~.x, .before = 2)\n\nShifting non-numeric variables is not possible.\n  Try using 'to_numeric()' and specify the 'lowest' argument.\n\n\n [1] \"systems\"    \"inc\"        \"is\"         \"trying\"     \"to\"        \n [6] \"collect\"    \"a\"          \"debt\"       \"that\"       \"is\"        \n[11] \"not\"        \"mine\"       \"not\"        \"owed\"       \"and\"       \n[16] \"is\"         \"inaccurate\"\n\n\n\n\n5.5.3 Funktion slide_windows\nDie Funktion slide_windows im Kapitel 5.2 ist recht kompliziert. In solchen F√§llen ist es hilfreich, sich jeden Schritt einzeln ausf√ºhren zu lassen. Das machen wir jetzt mal.\nHier ist die Syntax der Funktion slide_windows:\n\nslide_windows <- function(tbl, window_size) {\n  skipgrams <- slider::slide(\n    tbl, \n    ~.x,  # Syntax √§hnlich zu purrr::map()\n    .after = window_size - 1, \n    .step = 1, \n    .complete = TRUE\n  )\n  \n  safe_mutate <- safely(mutate)\n  \n  out <- map2(skipgrams,\n              1:length(skipgrams),\n              ~ safe_mutate(.x, window_id = .y))\n  \n  out %>%\n    transpose() %>%\n    pluck(\"result\") %>%\n    compact() %>%\n    bind_rows()\n}\n\nErschwerend kommt eine gro√üe Datenmenge und eine lange Berechnungszeit dazu, was das Debuggen (Nachvollziehen und Durchdenken) der Schritte zus√§tzlich erschwert. In solchen F√§llen hilft die goldene Regel: Mach es dir so einfach wie m√∂glich (aber nicht einfacher). Wir nutzen also den stark verkleinerten Datensatz nested_words3, den wir oben importiert haben.\nZuerst erlauben wir mal, dasss unsere R-Session mehrere Kerne benutzen darf.\n\nplan(multisession)  ## for parallel processing\n\nDie Funktion slide_windows ist recht kompliziert. Es hilft oft, sich mit debug(fun) eine Funktion Schritt f√ºr Schritt anzuschauen.\nGehen wir Schritt f√ºr Schritt durch die Syntax von slide_windows.\nWerfen wir einen Blick in words, erstes Element (ein Tibble mit einer Spalte). Denn die einzelnen Elemente vonwordswerden an die Funktionslide_windows` als ‚ÄúFutter‚Äù √ºbergeben.\n\nfutter1 <- nested_words3[[\"words\"]][[1]]\nfutter1\n\n\n\n\n\nword\n\n\n\n\nsystems\n\n\ninc\n\n\nis\n\n\ntrying\n\n\nto\n\n\ncollect\n\n\na\n\n\ndebt\n\n\nthat\n\n\nis\n\n\nnot\n\n\nmine\n\n\nnot\n\n\nowed\n\n\nand\n\n\nis\n\n\ninaccurate\n\n\n\n\n\n\nDas ist der Text der ersten Beschwerde.\nOkay, also dann geht‚Äôs los durch die einzelnen Schritte der Funktion slide_windows.\nZun√§chst holen wir uns die ‚ÄúFenster‚Äù oder ‚ÄúSkipgrams‚Äù:\n\nskipgrams1 <- slider::slide(\n   futter1, \n    ~.x, \n    .after = 3, \n    .step = 1, \n    .complete = TRUE\n  )\n\nBei slide(tbl, ~.x) geben wir die Funktion an, die auf tbl angewendet werden soll. Daher auch die Tilde, die uns von purrr::map() her bekannt ist. In unserem Fall wollen wir nur die Elemente auslesen; Elemente auslesen erreicht man, in dem man sie mit Namen anspricht, in diesem Fall mit dem Platzhalter .x.\nJedes Element von skipgrams1 ist ein 4*1-Tibble und ist ein Skripgram.\n\nskipgrams1 %>% str()\n\nList of 17\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"systems\" \"inc\" \"is\" \"trying\"\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"inc\" \"is\" \"trying\" \"to\"\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"is\" \"trying\" \"to\" \"collect\"\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"trying\" \"to\" \"collect\" \"a\"\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"to\" \"collect\" \"a\" \"debt\"\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"collect\" \"a\" \"debt\" \"that\"\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"a\" \"debt\" \"that\" \"is\"\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"debt\" \"that\" \"is\" \"not\"\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"that\" \"is\" \"not\" \"mine\"\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"is\" \"not\" \"mine\" \"not\"\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"not\" \"mine\" \"not\" \"owed\"\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"mine\" \"not\" \"owed\" \"and\"\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"not\" \"owed\" \"and\" \"is\"\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"owed\" \"and\" \"is\" \"inaccurate\"\n $ : NULL\n $ : NULL\n $ : NULL\n\n\nDas zweite Skipgram von skipgrams1 enth√§lt, naja, das zweite Skipgram.\n\nskipgrams1[[2]] %>% str()\n\ntibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n $ word: chr [1:4] \"inc\" \"is\" \"trying\" \"to\"\n\n\nUnd so weiter.\nOkay, weiter im Programm. Jetzt mappen wir das Erstellen der Skipgrams\n\nsafe_mutate <- safely(mutate)\n  \nout1 <- map2(skipgrams1,\n             1:length(skipgrams1),\n             ~ safe_mutate(.x, window_id = .y))\n  \nout1 %>% \n  head(2) %>% \n  str()\n\nList of 2\n $ :List of 2\n  ..$ result: tibble [4 √ó 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ word     : chr [1:4] \"systems\" \"inc\" \"is\" \"trying\"\n  .. ..$ window_id: int [1:4] 1 1 1 1\n  ..$ error : NULL\n $ :List of 2\n  ..$ result: tibble [4 √ó 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ word     : chr [1:4] \"inc\" \"is\" \"trying\" \"to\"\n  .. ..$ window_id: int [1:4] 2 2 2 2\n  ..$ error : NULL\n\n\nout1 ist eine Liste mit 17 Elementen; jedes Element mit jeweils zwei Elementen: Den Ergebnissen und ob es einen Fehler gab bei safe_mutate. Die 10 Elemente entsprechen den 10 Skipgrams. Wir k√∂nnen aber out1 auch ‚Äúdrehen‚Äù, transponieren genauer gesagt. so dass wir eine Liste mit zwei Elementen bekommen: das erste Element hat die (zehn) Ergebnisse (n√§mlich die Skipgrams) und das zweite Elemente, ob es Fehler gab.\nDas Prinzip des Transponierens ist in Abbildung¬†5.5 dargestellt.\n\n\n\nAbbildung¬†5.5: Transponieren einer Matrix (‚ÄúTabelle‚Äù)\n\n\n\nout2 <-\nout1 %>%\n  transpose() \n\nPuh, das ist schon anstrengendes Datenyoga‚Ä¶\nAber jetzt ist es einfach. Wir ziehen das erste der beiden Elemente, die Ergebnisse heraus (pluck), entfernen leere Elemente (compact) und machen einen Tibble daraus (bind_rows):\n\nout2 %>% \n  pluck(\"result\") %>%\n  compact() %>%\n  bind_rows() %>% \n  head()\n\n\n\n\n\nword\nwindow_id\n\n\n\n\nsystems\n1\n\n\ninc\n1\n\n\nis\n1\n\n\ntrying\n1\n\n\ninc\n2\n\n\nis\n2\n\n\n\n\n\n\nGeschafft!\n\n\n5.5.4 √Ñhnlichkeit berechnen\nNachdem wir jetzt slide_windows kennen, schauen wir uns die n√§chsten Schritte an:\n\ntidy_pmi1 <- nested_words3 %>%  # <--- Kleiner Datensatz!\n  mutate(words = future_map(words, slide_windows, 4L))\n\nWir werden slide_windows auf die Liste words an, die die Beschwerden enth√§lt. F√ºr jede Beschwerde erstellen wir die Skipgrams; diese Schleife wird realisiert √ºber map bzw. future_map, die uns erlaubt, mehrere Kerne des Rechners gleichzeitig zu nutzen, damit es schneller geht.\nHier sehen wir z.B. die Skipgram-IDs der ersten Beschwerde.\n\ntidy_pmi1[[\"words\"]][[1]] %>% \n  head()\n\n\n\n\n\nword\nwindow_id\n\n\n\n\nsystems\n1\n\n\ninc\n1\n\n\nis\n1\n\n\ntrying\n1\n\n\ninc\n2\n\n\nis\n2\n\n\n\n\n\n\nGenestet siehst es so aus:\n\ntidy_pmi1 %>% \n  head(1)\n\n\n\n\n\n\n\n\n\ncomplaint_id\nwords\n\n\n\n\n3384392\nsystems , inc , is , trying , inc , is , trying , to , is , trying , to , collect , trying , to , collect , a , to , collect , a , debt , collect , a , debt , that , a , debt , that , is , debt , that , is , not , that , is , not , mine , is , not , mine , not , not , mine , not , owed , mine , not , owed , and , not , owed , and , is , owed , and , is , inaccurate, 1 , 1 , 1 , 1 , 2 , 2 , 2 , 2 , 3 , 3 , 3 , 3 , 4 , 4 , 4 , 4 , 5 , 5 , 5 , 5 , 6 , 6 , 6 , 6 , 7 , 7 , 7 , 7 , 8 , 8 , 8 , 8 , 9 , 9 , 9 , 9 , 10 , 10 , 10 , 10 , 11 , 11 , 11 , 11 , 12 , 12 , 12 , 12 , 13 , 13 , 13 , 13 , 14 , 14 , 14 , 14\n\n\n\n\n\n\nDie Listenspalte entschachteln wir mal:\n\ntidy_pmi2 <- tidy_pmi1 %>% \n  unnest(words)  # entschachtele\n\ntidy_pmi2 %>% \n  head()\n\n\n\n\n\ncomplaint_id\nword\nwindow_id\n\n\n\n\n3384392\nsystems\n1\n\n\n3384392\ninc\n1\n\n\n3384392\nis\n1\n\n\n3384392\ntrying\n1\n\n\n3384392\ninc\n2\n\n\n3384392\nis\n2\n\n\n\n\n\n\nZum Berechnen der √Ñhnlichkeit brauchen wir eineindeutige IDs, nach dem Prinzip ‚Äú1. Skipgram der 1. Beschwerde‚Äù etc:\n\ntidy_pmi3 <- tidy_pmi2 %>% \n  unite(window_id, complaint_id, window_id)  # f√ºhre Spalten zusammen\n\ntidy_pmi3 %>% \n  head()\n\n\n\n\n\nwindow_id\nword\n\n\n\n\n3384392_1\nsystems\n\n\n3384392_1\ninc\n\n\n3384392_1\nis\n\n\n3384392_1\ntrying\n\n\n3384392_2\ninc\n\n\n3384392_2\nis\n\n\n\n\n\n\nSchlie√ülich berechnen wir die √Ñhnlichkeit mit pairwise_pmi, das hatten wir uns oben schon mal n√§her angeschaut:\n\ntidy_pmi4 <- tidy_pmi3 %>% \n  pairwise_pmi(word, window_id)  # berechne √Ñhnlichkeit\n\ntidy_pmi <- tidy_pmi4  # mit dem Objekt arbeiten wir dann weiter\n\ntidy_pmi %>% \n  head()\n\n\n\n\n\nitem1\nitem2\npmi\n\n\n\n\ninc\nsystems\n5.728498\n\n\nis\nsystems\n2.838126\n\n\ntrying\nsystems\n5.035351\n\n\nsystems\ninc\n5.728498\n\n\nis\ninc\n2.838126\n\n\ntrying\ninc\n5.035351\n\n\n\n\n\n\n\n\n5.5.5 SVD\nDie Singul√§rwertzerlegung (singular value decomposition, SVD) ist ein Verfahren zur Verringerung der Spaltenzahl (Dimension), vergleichbar zur Faktorenanalyse. Zur Anschaulichkeit - und ohne substanziellen Hintergrund - sei folgendes Beispiel genannt: Die Verben ‚Äúgehen‚Äù, ‚Äúrennen‚Äù, ‚Äúlaufen‚Äù, ‚Äúschwimmen‚Äù, ‚Äúfahren‚Äù, ‚Äúrutschen‚Äù k√∂nnten zu einer gemeinsamen Dimension, etwa ‚Äúfortbewegen‚Äù reduziert werden. Jedes einzelne der eingehenden Verben erh√§lt eine Zahl von 0 bis 1, das die konzeptionelle N√§he des Verbs zur ‚Äúdahinterliegenden‚Äù Dimension (fortbewegen) quantifiziert; die Zahl nennt man auch die ‚ÄúLadung‚Äù des Items (Worts) auf die Dimension. Sagen wir, wir identifizieren 10 Dimensionen. Man erh√§lt dann f√ºr jedes unique Wort im Corpus einen Vektor mit den Ladungen auf die Dimensionen. Im genannten Beispiel w√§re es ein 10-stelliger Vektor. So wie ein 3-stelliger Vektor die Position einer Biene im Raum beschreibt11, beschreibt hier unser 10-stelliger Vektor die ‚ÄúPosition‚Äù eines Worts in unserem Einbettungsvektor.\nDie Syntax dazu ist dieses Mal einfach:\n\ntidy_word_vectors <- \n  tidy_pmi %>%\n  widely_svd(\n    item1, item2, pmi,\n    nv = 100\n  )\n\ntidy_word_vectors %>% \n  (head)\n\n\n\n\n\nitem1\ndimension\nvalue\n\n\n\n\ninc\n1\n-0.0378963\n\n\nis\n1\n-0.1132069\n\n\ntrying\n1\n-0.0512764\n\n\nsystems\n1\n-0.0333332\n\n\nto\n1\n-0.1203434\n\n\ncollect\n1\n-0.0554211\n\n\n\n\n\n\nMit nv = 100 haben wir die Anzahl (n) der Dimensionen (Variablen, v) auf 100 bestimmt.\n\n\n5.5.6 Wort√§hnlichkeit\nJetzt, da wir mit der SVD jedes Wort in einem Koordinatensystem verortet haben, k√∂nnen wir die Abst√§nde der W√∂rter im Koordinatensystem bestimmen. Das geht mit Hilfe des alten Pythagoras, s. Abbildung¬†5.6. Der Abstand, den man mit Hilfe des Satz des Pythagoras berechnet, nennt man auch euklidische Distanz.\n\n\n\nAbbildung¬†5.6: Euklidische Distanz in 2D, Quelle: Wikipedia, CC BY4.0, Kmhkmh\n\n\nOkay, wir sind in einem Raum mit vielen Dimensionen, was mein Hirn nicht mitmacht, aber der Algebra ist das egal. Pythagoras‚Äô Satz l√§sst sich genauso anwenden, wenn es mehr als Dimensionen sind.\nDie Autoren basteln sich selber eine Funktion in Kap. 5.3, aber der Einfachheit halber nehme ich (erstmal) die entsprechende Funktion aus widyr:\n\nword_neighbors <- \ntidy_word_vectors %>% \n  pairwise_dist(item1, dimension, value)\n\nword_neighbors %>% \n  head()\n\n\n\n\n\nitem1\nitem2\ndistance\n\n\n\n\nis\ninc\n1.0220141\n\n\ntrying\ninc\n0.9332851\n\n\nsystems\ninc\n0.4161215\n\n\nto\ninc\n1.0913872\n\n\ncollect\ninc\n0.5221759\n\n\na\ninc\n1.0309566\n\n\n\n\n\n\nSchauen wir uns ein Beispiel an. Was sind die Nachbarn von ‚Äúinaccurate‚Äù?\n\nword_neighbors %>% \n  filter(item1 == \"inaccurate\") %>% \n  arrange(distance) %>% \n  head()\n\n\n\n\n\nitem1\nitem2\ndistance\n\n\n\n\ninaccurate\nmine\n0.5248868\n\n\ninaccurate\nscore\n0.5310116\n\n\ninaccurate\noh\n0.5400913\n\n\ninaccurate\nny\n0.5400913\n\n\ninaccurate\ndob\n0.5801281\n\n\ninaccurate\ncell\n0.6093670\n\n\n\n\n\n\nHier ist die Datenmenge zu klein, um vern√ºnftige Schl√ºsse zu ziehen. Aber ‚Äúincorrectly‚Äù, ‚Äúcorrect‚Äù, ‚Äúbalance‚Äù sind wohl plausible Nachbarn von ‚Äúinaccurate‚Äù.\n\n\n5.5.7 Cosinus-√Ñhnlichkeit\nDie N√§he zweier Vektoren l√§sst sich, neben der euklidischen Distanz, auch z.B. √ºber die Cosinus-√Ñhnlichkeit (Cosine similarity) berechnen, vgl. auch Abbildung¬†5.7:\n\n\n\nAbbildung¬†5.7: Die Cosinus-√Ñhnlichkeit zweier Vektoren\n\n\nQuelle: Mazin07, Lizenz: PD\n\\[{\\displaystyle {\\text{Cosinus-√Ñhnlichkeit}}=S_{C}(A,B):=\\cos(\\theta )={\\mathbf {A} \\cdot \\mathbf {B}  \\over \\|\\mathbf {A} \\|\\|\\mathbf {B} \\|}={\\frac {\\sum \\limits _{i=1}^{n}{A_{i}B_{i}}}{{\\sqrt {\\sum \\limits _{i=1}^{n}{A_{i}^{2}}}}{\\sqrt {\\sum \\limits _{i=1}^{n}{B_{i}^{2}}}}}},}\\]\nwobei \\(A\\) und \\(B\\) zwei Vektoren sind und \\(\\|\\mathbf {A} \\|\\) das Skalarprodukt von A (und B genauso). Das Skalarprodukt von \\(\\color {red} {a = {\\displaystyle [a_{1},a_{2},\\cdots ,a_{n}]}}\\) und \\(\\color {blue} {b = {\\displaystyle [b_{1},b_{2},\\cdots ,b_{n}]}}\\) ist so definiert:\n\\[{\\displaystyle \\mathbf {\\color {red}a} \\cdot \\mathbf {\\color {blue}b} =\\sum _{i=1}^{n}{\\color {red}a}_{i}{\\color {blue}b}_{i}={\\color {red}a}_{1}{\\color {blue}b}_{1}+{\\color {red}a}_{2}{\\color {blue}b}_{2}+\\cdots +{\\color {red}a}_{n}{\\color {blue}b}_{n}}\\]\nEntsprechend ist die Funktion nearest_neighbors zu verstehen aus Kap. 5.3:\n\nnearest_neighbors <- function(df, token) {\n  df %>%\n    widely(\n      ~ {\n        y <- .[rep(token, nrow(.)), ]\n        res <- rowSums(. * y) / \n          (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))\n        \n        matrix(res, ncol = 1, dimnames = list(x = names(res)))\n      },\n      sort = TRUE\n    )(item1, dimension, value) %>%\n    select(-item2)\n}\n\nWobei mit widely zuerst noch von der Langform in die Breitform umformatiert wird, da die Breitform von der Formel verlangt wird bzw. das Rechnen einfacher macht.\nDer eine Vektor ist das Embedding des Tokens, der andere Vektor ist das mittlere Embedding √ºber alle Tokens des Corpus. Wenn die Anzahl der Elemente konstant bleibt, kann man sich das Teilen durch \\(n\\) schenken, wenn man einen Mittelwert berechnen; so h√§lt es auch die Syntax von nearest_neighbors.\nEin n√ºtzlicher Post zur Cosinus-√Ñhnlichkeit findet sich hier. Dieses Bild zeigt das Konzept der Cosinus-√Ñhnlichkeit anschaulich.\nZur Erinnerung: Der Cosinus eines Winkels ist definiert als Verh√§ltnis der L√§nge der Ankathete (das ist jene Kathete, die einen Schenkel des Winkels bildet) zur L√§nge der Hypotenuse12 in einem rechtwinkligen, vgl. Abbildung¬†5.8.\n\n\n\nAbbildung¬†5.8: Ein rechtwinkliges Dreieck als Grundlage der trigonometrischen Funktionen\n\n\nAlso: \\({\\displaystyle \\cos \\alpha ={\\frac {b}{c}}}\\)\nQuelle: PaterSigmund, Wikipedia,dre CC-BY-SA 2.5\nHilfreich ist auch die Visualisierung von Sinus und Cosinus am Einheitskreis; gerne animiert betrachten."
  },
  {
    "objectID": "word-embedding.html#word-embeddings-vorgekocht",
    "href": "word-embedding.html#word-embeddings-vorgekocht",
    "title": "5¬† Word Embedding",
    "section": "5.6 Word-Embeddings vorgekocht",
    "text": "5.6 Word-Embeddings vorgekocht\n\n5.6.1 Glove6B\nIn Kap. 5.4 schreiben die Autoren:\n\nIf your data set is too small, you typically cannot train reliable word embeddings.\n\nEin paar Millionen W√∂rter sollte der Corpus schon enthalten, so die Autoren. Da solche ‚ÄúWorteinbettungen‚Äù (word embedings) aufw√§ndig zu erstellen sind, kann man fertige, ‚Äúvorgekochte‚Äù Produkte nutzen.\nGlove6B wurde anhand von Wikipedia und anderen Datenquellen erstellt (Pennington, Socher, und Manning 2014).\n\n\n\n\n\n\nHinweis\n\n\n\nDie zugeh√∂rigen Daten sind recht gro√ü; f√ºr glove6b (Pennington, Socher, und Manning 2014) ist fast ein Gigabyte f√§llig. Sie sollten sich die Daten in einem ruhigen Moment (mit stabiler Internetverbindung) herunterladen und in einem Verzeichnis meiner Wahl abgespeichert (datasets). Da bei mir Download abbrach, als ich embedding_glove6b(dimensions = 100) aufrief, habe ich die Daten manuell heruntergeladen, s.u.\n\n\n\nglove6b <- \n  embedding_glove6b(dir = \"~/datasets\", dimensions = 50, manual_download = TRUE)\n\nglove6b %>% \n  select(1:5) %>% \n  head()\n\n\n\n\n\ntoken\nd1\nd2\nd3\nd4\n\n\n\n\nthe\n0.418000\n0.249680\n-0.41242\n0.121700\n\n\n,\n0.013441\n0.236820\n-0.16899\n0.409510\n\n\n.\n0.151640\n0.301770\n-0.16763\n0.176840\n\n\nof\n0.708530\n0.570880\n-0.47160\n0.180480\n\n\nto\n0.680470\n-0.039263\n0.30186\n-0.177920\n\n\nand\n0.268180\n0.143460\n-0.27877\n0.016257\n\n\n\n\n\n\nDie ersten paar Tokens sind:\n\nglove6b$token %>% head(20)\n\n [1] \"the\"  \",\"    \".\"    \"of\"   \"to\"   \"and\"  \"in\"   \"a\"    \"\\\"\"   \"'s\"  \n[11] \"for\"  \"-\"    \"that\" \"on\"   \"is\"   \"was\"  \"said\" \"with\" \"he\"   \"as\"  \n\n\nIn eine Tidyform bringen:\n\ntidy_glove <- \n  glove6b %>%\n  pivot_longer(contains(\"d\"),\n               names_to = \"dimension\") %>%\n  rename(item1 = token)\n\nhead(tidy_glove)\n\n\n\n\n\nitem1\ndimension\nvalue\n\n\n\n\nthe\nd1\n0.418000\n\n\nthe\nd2\n0.249680\n\n\nthe\nd3\n-0.412420\n\n\nthe\nd4\n0.121700\n\n\nthe\nd5\n0.345270\n\n\nthe\nd6\n-0.044457\n\n\n\n\n\n\nGanz sch√∂n gro√ü:\n\ndim(glove6b)\n\n[1] 400000     51\n\n\n\nobject.size(tidy_glove)\n\n503834736 bytes\n\n\nIn Megabyte13\n\nobject.size(tidy_glove) / 2^20\n\n480.5 bytes\n\n\nEinfacher und genauer geht es so:\n\npryr::object_size(tidy_glove)\n\n503.83 MB\n\n\n\npryr::mem_used()\n\n850 MB\n\n\nUm Speicher zu sparen, k√∂nnte man glove6b wieder direkt l√∂schen, wenn man nur mit der Tidyform weiterarbeitet.\n\nrm(glove6b)\n\nJetzt k√∂nnen wir wieder nach Nachbarn fragen, im euklidischen Sinne, wie oben. Probieren wir aus, welche W√∂rter nah zu ‚Äúinaccurate‚Äù stehen.\n\n\n\n\n\n\nHinweis\n\n\n\nWie wir oben gesehen haben, ist der Datensatz riesig14, was die Berechnungen (zeitaufw√§ndig) und damit nervig machen k√∂nnen. Dar√ºber hinaus kann es n√∂tig sein, dass Sie mehr Speicher auf Ihrem Computer zur Verf√ºgung stellen m√ºssen15. Wir m√ºssen noch maximum_size = NULL, um das Jonglieren mit riesigen Matrixen zu erlauben. M√∂ge der Gott der RAMs und Arbeitsspeicher uns gn√§dig sein!\n\n\nMit pairwise_dist dauert die Berechnung sehr lange und braucht wohl sehr viel Speicher. Mitunter kam folgender Fehler auf: ‚ÄúR error: vector memory exhausted (limit reached?)‚Äù.\n\nword_neighbors_glove6b <- \ntidy_glove %>% \n  slice_head(prop = .1) %>% \n  pairwise_dist(item1, dimension, value, maximum_size = NULL)\n\nhead(word_neighbors_glove6b)\n\ntidy_glove %>% \n  filter(item1 == \"inaccurate\") %>% \n  arrange(-value) %>% \n  slice_head(n = 5)\n\nDeswegen probieren wir doch die Funktion nearest_neighbors, so wie es im Buch vorgeschlagen wird, s. Kap 5.3.\n\nnearest_neighbors <- function(df, token) {\n  df %>%\n    widely(\n      ~ {\n        y <- .[rep(token, nrow(.)), ]\n        res <- rowSums(. * y) / \n          (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))\n        matrix(res, ncol = 1, dimnames = list(x = names(res)))\n      },\n      sort = TRUE,\n      maximum_size = NULL\n    )(item1, dimension, value) %>%\n    select(-item2)\n}\n\n\ntidy_glove %>%\n  # slice_head(prob = .1) %>% \n  nearest_neighbors(\"error\") %>% \n  head()\n\n\n\n\n\nitem1\nvalue\n\n\n\n\nerror\n1.0000000\n\n\nerrors\n0.7916719\n\n\nmistake\n0.6641135\n\n\ncorrect\n0.6205814\n\n\nincorrect\n0.6132556\n\n\nfault\n0.6068035\n\n\n\n\n\n\nEntschachteln wir unsere Daten zu complaints:\n\ntidy_complaints3 <-\n  nested_words3 %>% \n  unnest(words)\n\nDann erstellen wir uns eine Tabelle, in der nur die Schnittmenge der W√∂rter aus den Beschwerden und Glove vorkommen. Dazu nutzen winr einen inneren Join\n\n\n\nInner Join, Quelle: Garrick Adenbuie\n\n\nQuelle\n\ncomplaints_glove <- \ntidy_complaints3 %>% \n  inner_join(by = \"word\", \n  tidy_glove %>% \n  distinct(item1) %>% \n  rename(word = item1)) \n\nhead(complaints_glove)\n\n\n\n\n\ncomplaint_id\nword\n\n\n\n\n3384392\nsystems\n\n\n3384392\ninc\n\n\n3384392\nis\n\n\n3384392\ntrying\n\n\n3384392\nto\n\n\n3384392\ncollect\n\n\n\n\n\n\nWie viele unique (distinkte) W√∂rter gibt es in unserem Corpus?\n\ntidy_complaints3_distinct_words_n <- \ntidy_complaints3 %>% \n  distinct(word) %>% \n  nrow()\n\ntidy_complaints3_distinct_words_n\n\n[1] 222\n\n\nIn tidy_complaints gibt es √ºbrigens 222 verschiedene W√∂rter.\n\nword_matrix <- tidy_complaints3 %>%\n  inner_join(by = \"word\",\n             tidy_glove %>%\n               distinct(item1) %>%\n               rename(word = item1)) %>%\n  count(complaint_id, word) %>%\n  cast_sparse(complaint_id, word, n)\n\n#word_matrix\n\nword_matrix z√§hlt f√ºr jede der 10 Beschwerden, welche W√∂rter (und wie h√§ufig) vorkommen.\n\ndim(word_matrix)\n\n[1]  10 222\n\n\n10 Beschwerden (Dokumente) und 222 unique W√∂rter.\n\nglove_matrix <- tidy_glove %>%\n  inner_join(by = \"item1\",\n             tidy_complaints3 %>%\n               distinct(word) %>%\n               rename(item1 = word)) %>%\n  cast_sparse(item1, dimension, value)\n\n#glove_matrix\n\nglove_matrix gibt f√ºr jedes unique Wort den Einbettungsvektor an.\n\ndim(glove_matrix)\n\n[1] 222 100\n\n\nDas sind 222 unique W√∂rter und 100 Dimensionen des Einbettungsvektors.\nJetzt k√∂nnen wir noch pro Dokument (10 in diesem Beispiel) die mittlere ‚ÄúPosition‚Äù jedes Dokuments im Einbettungsvektor ausrechnen. Bildlich gesprochen: Was ist der mittlere Raumpunkt (Zentroid) des Bienenschwarms, wobei eine Biene ein Dokument darstellt und die Raumachsen die Dimensionen des Einbettungsvektors.\nDazu gewichten wir jedes Wort eines Dokuments mit den Ladungen des Einbettungsvektor und summieren diese Terme. Es resultiert eine Matrix mit einem Einbettungsvektor pro Dokument. Diese Matrix k√∂nnen wir jetzt als Pr√§diktorenmatrix hernehmen.\n\ndoc_matrix <- word_matrix %*% glove_matrix\n#doc_matrix %>% head()\n\n\ndim(doc_matrix)\n\n[1]  10 100\n\n\nDie Anzahl der Dokumente ist 10 und die Anzahl der Dimensionen (des Einbettungsvektors) ist 100.\n\n\n5.6.2 Wordembeddings f√ºr die deutsche Sprache\nIn diesem Github-Projekt finden sich die Materialien f√ºr ein deutsches Wordembedding (M√ºller 2015)."
  },
  {
    "objectID": "word-embedding.html#fazit",
    "href": "word-embedding.html#fazit",
    "title": "5¬† Word Embedding",
    "section": "5.7 Fazit",
    "text": "5.7 Fazit\nWorteinbettungen sind eine aufw√§ndige Angelegenheit. Positiv gesprochen kann ein Analysti die Muskeln spielen lassen und zeigen, was sie oder er so alles drauf hat. Ist ja schon cooles Zeugs, die Word Embeddings. Es besteht die Chance, dass man mit dieser Methode bessere Vorhersagen erreicht, als mit anderen, einfachen Ans√§tzen wir Worth√§ufigkeiten oder tf-idf. Auf der anderen Seite ist es oft sinnvoll, mit einfachen Ans√§tzen zu starten, und zu sehen, wie weit man kommt. Vielleicht ja weit genug."
  },
  {
    "objectID": "word-embedding.html#literatur",
    "href": "word-embedding.html#literatur",
    "title": "5¬† Word Embedding",
    "section": "5.8 Literatur",
    "text": "5.8 Literatur\n\n5.8.1 Wikipedia\nEs gibt eine Reihe n√ºtzlicher (und recht informationsdichter) Wikipedia-Eintr√§ge zum Thema Informationstheorie.\n\nWikipedia: Pointwise Mutual Information\nWikipedia: Mutual information\nWikipedia: Information theory\n\n\n\n\n\nHvitfeldt, Emil, und Julia Silge. 2022. Supervised Machine Learning for Text Analysis in R. 1. Aufl. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nKurz, A. Solomon. 2021. Statistical rethinking with brms, ggplot2, and the tidyverse: Second edition. https://bookdown.org/content/4857/.\n\n\nMcElreath, Richard. 2020. Statistical rethinking: a Bayesian course with examples in R and Stan. 2. Aufl. CRC texts in statistical science. Boca Raton: Taylor; Francis, CRC Press.\n\n\nM√ºller, Andreas. 2015. ‚ÄûAnalyse von Wort-Vektoren deutscher Textkorpora‚Äú. Bachelor's Thesis, Technische Universit√§t Berlin. https://devmount.github.io/GermanWordEmbeddings.\n\n\nPennington, Jeffrey, Richard Socher, und Christopher Manning. 2014. ‚ÄûGloVe: Global Vectors for Word Representation‚Äú. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1532‚Äì43. Doha, Qatar: Association for Computational Linguistics. https://doi.org/10.3115/v1/D14-1162.\n\n\nShannon, C. E. 1948. ‚ÄûA Mathematical Theory of Communication‚Äú. Bell System Technical Journal 27 (3): 379‚Äì423. https://doi.org/10.1002/j.1538-7305.1948.tb01338.x."
  },
  {
    "objectID": "hassrede.html",
    "href": "hassrede.html",
    "title": "6¬† Hassrede",
    "section": "",
    "text": "Finden Sie eine operationale Definition f√ºr Hassrede (engl. hate speech) bzw. Hatespeech1!\n\n\n\n\n\nLesen Sie die unten aufgef√ºhrte Literatur\n\n\n\n\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "hassrede.html#worum-gehts",
    "href": "hassrede.html#worum-gehts",
    "title": "6¬† Hassrede",
    "section": "6.2 Worum geht‚Äôs?",
    "text": "6.2 Worum geht‚Äôs?\nWir m√∂chten eine treffende und praktikable Definition, um zu erkennen, wann eis deutschis Politiki Hass entgegenschl√§gt.\nTreffend meint, dass Hassrede als Hassrede erkannt wird von unserer Definition, und Nicht-Hassrede als Nicht-Hassrede erkannt wird. Mit anderen Worten: Wir verlangen, dass die Sensitivit√§t und Spezifit√§t unserer Definition hoch ist.\nPraktikabel meint, dass wir diese Definition in der Praxis gut umsetzen k√∂nnen. Wir denken dabei an die Schwierigkeiten, einer (tumben) Maschine unsere Regeln beizubringen. Insbesondere muss die Definition objektiv sein in dem Sinne, dass mehrere Gutachtis zur gleichen Einsch√§tzung kommen w√ºrden."
  },
  {
    "objectID": "hassrede.html#einstieg",
    "href": "hassrede.html#einstieg",
    "title": "6¬† Hassrede",
    "section": "6.3 Einstieg",
    "text": "6.3 Einstieg\n\n6.3.1 Einstiegsdefinition\nHier ist eine Definition als Startpunkt f√ºr Ihre √úberlegungen.\nHassrede liegt vor, wenn eine oder mehrere der folgenden Inhalte in einem Text verwendet werden:\n\nSchimpfw√∂rter (‚ÄúVollpfosten‚Äù)\nRassismus, Sexismus, Antisemitismus oder andere Formen von gruppenbezogener Menschenfeindlichkeit (‚ÄúDer Schwarze schnackselt gerne‚Äù)\nAufruf oder Androhung zur Gewalt, auch in indirekter Form (‚ÄúDa k√∂nnte mal jemand mit der Pistole bei dir vorbeikommen‚Äù)\nHerabsetzung (‚ÄúVolksverr√§ter‚Äù)\n\nDabei sollten wir uns mit Blick auf das Ziel, Hass gegen einzelne Personen zu erkennen, nicht auf gruppenbezogene Menschenfeindlichkeit begrenzen, sondern auch Hass auf Individuen einbeziehen. Vielleicht ist daher der Begriff Cybermobbing passender als Hatespeech.\n\n\n6.3.2 Einstiegsliteratur\nDer Artikel zu Hatespeech der Stanford-Enzyklop√§die birgt (am Anfang) gute Hinweise; im weiteren Verlauf geht der Text mehr in die Tiefe.\nIn dieser Zotero-Gruppe finden Sie empfehlenswerte (und √∂ffentlich zug√§ngliche) Artikel zum Thema Hatespeech und Hate-Speech-Erkennung.\n\n\n6.3.3 Trainingsdaten\nDie Universit√§t Heidelberg ver√∂ffentlicht Daten, die Tweets (oder √§hnliche Kurztexte) nach Hatespeech hin untersucht (Wiegand 2019). Nutzen Sie dieser Ressource.\n\n\n6.3.4 Los geht‚Äôs!\nLesen Sie diese und weitere Literatur, um zu einer Arbeitsdefinition von Hassrede zu kommen.\n\n\n\n\nWiegand, Michael. 2019. ‚ÄûGermEval-2018 Corpus (DE)‚Äú. heiDATA. https://doi.org/10.11588/DATA/0B5VML."
  },
  {
    "objectID": "klassifikation.html",
    "href": "klassifikation.html",
    "title": "7¬† Klassifikation von Hatespeech",
    "section": "",
    "text": "Sie k√∂nnen grundlegende Verfahren zur Klassifikation von Hatespeech einsetzen und erkl√§ren\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(tidymodels)\nlibrary(tidytext)\nlibrary(textrecipes)\nlibrary(lsa)  # stopwords\nlibrary(discrim)  # naive bayes classification\nlibrary(naivebayes)\nlibrary(tictoc)  # Zeitmessung\nlibrary(fastrtext)  # Worteinbettungen\nlibrary(remoji)  # Emojis\nlibrary(tokenizers)  # Vektoren tokenisieren"
  },
  {
    "objectID": "klassifikation.html#daten",
    "href": "klassifikation.html#daten",
    "title": "7¬† Klassifikation von Hatespeech",
    "section": "7.2 Daten",
    "text": "7.2 Daten\nF√ºr Maschinenlernen brauchen wir Trainingsdaten, Daten also, bei denen wir pro Beobachtung der Wert der Zielvariablen kennen. Man spricht auch von ‚Äúgelabelten‚Äù Daten.\nWir nutzen die Daten von Wiegand (2019a) bzw. Wiegand (2019b). Die Daten sind unter CC-By-4.0 Int. lizensiert.\n\nd_raw <- \n  data_read(\"data/germeval2018.training.txt\",\n         header = FALSE)\n\nWarning in data.table::fread(input = path, ...): Found and resolved improper\nquoting out-of-sample. First healed line 111: <<\"Edel sei der Mensch, hilfreich\nund gut\" - Nicht eine dieser Charaktereigenschaften kann Merkel f√ºr sich\nbeanspruchen. OTHER OTHER>>. If the fields are not quoted (e.g. field separator\ndoes not appear within any field), try quote=\"\" to avoid this warning.\n\n\nDie Daten finden sich auch im Paket pradadata.\nDa die Daten keine Spaltenk√∂pfe haben, informieren wir die Funktion dazu mit header = FALSE.\nBenennen wir die die Spalten um:\n\nnames(d_raw) <- c(\"text\", \"c1\", \"c2\")\n\nDabei soll c1 und c2 f√ºr die 1. bzw. 2. Klassifikation stehen.\nIn c1 finden sich diese Werte:\n\nd_raw %>% \n  count(c1)\n\n\n\n\n\nc1\nn\n\n\n\n\nOFFENSE\n1688\n\n\nOTHER\n3321\n\n\n\n\n\n\nHier wurde klassifiziert, ob beleidigende Sprache (offensive language) vorlag oder nicht (isch-etal-2021-overview?):\n\nTask 1 was to decide whether a tweet includes some form of offensive language or not. The tweets had to be classiÔ¨Åed into the two classes OFFENSE and OTHER. The OFFENSE category covered abusive language, insults, as well as merely profane statements.\n\nUnd in c2 finden sich folgende Auspr√§gungen:\n\nd_raw %>% \n  count(c2)\n\n\n\n\n\nc2\nn\n\n\n\n\nABUSE\n1022\n\n\nINSULT\n595\n\n\nOTHER\n3321\n\n\nPROFANITY\n71\n\n\n\n\n\n\nIn c2 ging es um eine feinere Klassifikation beleidigender Sprache (isch-etal-2021-overview?):\n\nThe second task involved four categories, a nonoffensive OTHER class and three sub-categories of what is OFFENSE in Task 1. In the case of PROFANITY, profane words are used, however, the tweet does not want to insult anyone. This typically concerns the usage of swearwords (Schei√üe, Fuck etc.) and cursing (Zur H√∂lle! Verdammt! etc.). This can be often found in youth language. Swearwords and cursing may, but need not, co-occur with insults or abusive speech. Profane language may in fact be used in tweets with positive sentiment to express emphasis. Whenever profane words are not directed towards a speciÔ¨Åc person or group of persons and there are no separate cues of INSULT or ABUSE, then tweets are labeled as simple cases of PROFANITY.\n\nSind Texte, die als OFFENSE klassifiziert sind, auch (fast) immer als ABUSE, INSULT oder PROFANITY klassifiziert?\n\nd_raw %>% \n  filter(c1 == \"OTHER\", c2 == \"OTHER\") %>% \n  nrow() / nrow(d_raw)\n\n[1] 0.6630066\n\n\nIn ca. 2/3 der F√§lle wurden in beiden Klassifikation OTHER klassifiziert.\n\nd_raw %>% \n  filter(c1 != \"OTHER\", c2 != \"OTHER\") %>% \n  nrow() / nrow(d_raw)\n\n[1] 0.3369934\n\n\nEntsprechend in ca. 1/3 der F√§lle wurde jeweils nicht mit OTHER klassifiziert.\nWir begn√ºgen uns hier mit der ersten, gr√∂beren Klassifikation.\nF√ºgen wir abschlie√üend noch eine ID-Variable hinzu:\n\nd1 <-\n  d_raw %>% \n  mutate(id = as.character(1:nrow(.)))\n\nDie ID-Variable definieren als Text (nicht als Integer), da die Twitter-IDs zwar nat√ºrliche Zahlen sind, aber zu gro√ü, um von R als Integer verarbeitet zu werden. Faktisch sind sie f√ºr uns auch nur nominal skalierte Variablen, so dass wir keinen Informationsverlust haben.\n\n#write_rds(d1, \"objects/d1.rds\")"
  },
  {
    "objectID": "klassifikation.html#feature-engineering",
    "href": "klassifikation.html#feature-engineering",
    "title": "7¬† Klassifikation von Hatespeech",
    "section": "7.3 Feature Engineering",
    "text": "7.3 Feature Engineering\nReichern wir die Daten mit weiteren Features an, in der Hoffnung, damit eine bessere Klassifikation erzielen zu k√∂nnen.\n\n7.3.1 Textl√§nge\n\nd2 <-\n  d1 %>% \n  mutate(text_length = str_length(text))\n\nhead(d2)\n\n\n\n\n\n\n\n\n\n\n\n\ntext\nc1\nc2\nid\ntext_length\n\n\n\n\n(corinnamilborn?) Liebe Corinna, wir w√ºrden dich gerne als Moderatorin f√ºr uns gewinnen! W√§rst du begeisterbar?\nOTHER\nOTHER\n1\n109\n\n\n(Martin28a?) Sie haben ja auch Recht. Unser Tweet war etwas missverst√§ndlich. Dass das BVerfG Sachleistungen nicht ausschlie√üt, kritisieren wir.\nOTHER\nOTHER\n2\n142\n\n\n(ahrens_theo?) fr√∂hlicher gru√ü aus der sch√∂nsten stadt der welt theo ‚öìÔ∏è\nOTHER\nOTHER\n3\n69\n\n\n(dushanwegner?) Amis h√§tten alles und jeden gew√§hlt‚Ä¶nur Hillary wollten sie nicht und eine Fortsetzung von Obama-Politik erst recht nicht..!\nOTHER\nOTHER\n4\n140\n\n\n(spdde?) kein verl√§√ülicher Verhandlungspartner. Nachkarteln nach den Sondierzngsgespr√§chen - schickt diese St√ºmper #SPD in die Versenkung.\nOFFENSE\nINSULT\n5\n136\n\n\n(Dirki_M?) Ja, aber wo widersprechen die Zahlen denn denen, die im von uns verlinkten Artikel stehen? In unserem Tweet geht es rein um subs. Gesch√ºtzte. 2017 ist der gesamte Familiennachzug im Vergleich zu 2016 - die Zahlen, die Hr. Brandner bem√ºht - √ºbrigens leicht r√ºckl√§ufig gewesen.\nOTHER\nOTHER\n6\n284\n\n\n\n\n\n\n\n\n7.3.2 Sentimentanalyse\nWir nutzen dazu SentiWS (Remus, Quasthoff, und Heyer 2010).\n\nsentiws <- read_csv(\"https://osf.io/x89wq/?action=download\")\n\nRows: 3468 Columns: 4\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (3): neg_pos, word, inflections\ndbl (1): value\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nd2_long <-\n  d2 %>% \n  unnest_tokens(input = text, output = token)\n\nhead(d2_long)\n\n\n\n\n\nc1\nc2\nid\ntext_length\ntoken\n\n\n\n\nOTHER\nOTHER\n1\n109\ncorinnamilborn\n\n\nOTHER\nOTHER\n1\n109\nliebe\n\n\nOTHER\nOTHER\n1\n109\ncorinna\n\n\nOTHER\nOTHER\n1\n109\nwir\n\n\nOTHER\nOTHER\n1\n109\nw√ºrden\n\n\nOTHER\nOTHER\n1\n109\ndich\n\n\n\n\n\n\nJetzt filtern wir unsere Textdaten so, dass nur W√∂rter mit Sentimentwert √ºbrig bleiben:\n\nd2_long_senti <- \n  d2_long %>%  \n  inner_join(sentiws %>% select(-inflections), by = c(\"token\" = \"word\"))\n\nhead(d2_long)\n\n\n\n\n\nc1\nc2\nid\ntext_length\ntoken\n\n\n\n\nOTHER\nOTHER\n1\n109\ncorinnamilborn\n\n\nOTHER\nOTHER\n1\n109\nliebe\n\n\nOTHER\nOTHER\n1\n109\ncorinna\n\n\nOTHER\nOTHER\n1\n109\nwir\n\n\nOTHER\nOTHER\n1\n109\nw√ºrden\n\n\nOTHER\nOTHER\n1\n109\ndich\n\n\n\n\n\n\nSchlie√ülich berechnen wir die Sentimentwert pro Polarit√§t und pro Tweet:\n\nd2_sentis <-\n  d2_long_senti %>% \n  group_by(id, neg_pos) %>% \n  summarise(senti_avg = mean(value))\n\n`summarise()` has grouped output by 'id'. You can override using the `.groups`\nargument.\n\nhead(d2_sentis)\n\n\n\n\n\nid\nneg_pos\nsenti_avg\n\n\n\n\n1\npos\n0.0040\n\n\n1012\nneg\n-0.2087\n\n\n1013\nneg\n-0.0420\n\n\n1015\nneg\n-0.0048\n\n\n1021\npos\n0.0845\n\n\n1024\nneg\n-0.4787\n\n\n\n\n\n\nDiese Tabelle bringen wir wieder eine breitere Form, um sie dann wieder mit den Hauptdaten zu vereinigen.\n\nd2_sentis_wide <-\n  d2_sentis %>% \n  pivot_wider(names_from = \"neg_pos\", values_from = \"senti_avg\")\n\nd2_sentis_wide %>% head()\n\n\n\n\n\nid\npos\nneg\n\n\n\n\n1\n0.0040\nNA\n\n\n1012\nNA\n-0.2087\n\n\n1013\nNA\n-0.0420\n\n\n1015\nNA\n-0.0048\n\n\n1021\n0.0845\nNA\n\n\n1024\n0.0040\n-0.4787\n\n\n\n\n\n\n\nd3 <-\n  d2 %>% \n  full_join(d2_sentis_wide)\n\nJoining, by = \"id\"\n\nhead(d3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntext\nc1\nc2\nid\ntext_length\npos\nneg\n\n\n\n\n(corinnamilborn?) Liebe Corinna, wir w√ºrden dich gerne als Moderatorin f√ºr uns gewinnen! W√§rst du begeisterbar?\nOTHER\nOTHER\n1\n109\n0.004\nNA\n\n\n(Martin28a?) Sie haben ja auch Recht. Unser Tweet war etwas missverst√§ndlich. Dass das BVerfG Sachleistungen nicht ausschlie√üt, kritisieren wir.\nOTHER\nOTHER\n2\n142\nNA\n-0.3466\n\n\n(ahrens_theo?) fr√∂hlicher gru√ü aus der sch√∂nsten stadt der welt theo ‚öìÔ∏è\nOTHER\nOTHER\n3\n69\nNA\nNA\n\n\n(dushanwegner?) Amis h√§tten alles und jeden gew√§hlt‚Ä¶nur Hillary wollten sie nicht und eine Fortsetzung von Obama-Politik erst recht nicht..!\nOTHER\nOTHER\n4\n140\nNA\nNA\n\n\n(spdde?) kein verl√§√ülicher Verhandlungspartner. Nachkarteln nach den Sondierzngsgespr√§chen - schickt diese St√ºmper #SPD in die Versenkung.\nOFFENSE\nINSULT\n5\n136\nNA\nNA\n\n\n(Dirki_M?) Ja, aber wo widersprechen die Zahlen denn denen, die im von uns verlinkten Artikel stehen? In unserem Tweet geht es rein um subs. Gesch√ºtzte. 2017 ist der gesamte Familiennachzug im Vergleich zu 2016 - die Zahlen, die Hr. Brandner bem√ºht - √ºbrigens leicht r√ºckl√§ufig gewesen.\nOTHER\nOTHER\n6\n284\n0.004\n-0.2042\n\n\n\n\n\n\n\n\n\n\n\n\nHinweis\n\n\n\nDie Sentimentanalyse hier vernachl√§ssigt Flexionen der W√∂rter. Der Autor f√ºhlt den Drang zu schreiben: ‚ÄúLeft as an exercise for the reader‚Äù :-)\n\n\n\n\n7.3.3 Schimpfw√∂rter\nZ√§hlen wir die Schimpfw√∂rter pro Text. Dazu nutzen wir die Daten von LDNOOBW, lizensiert nach CC-BY-4.0-Int.\n\nschimpf1 <- data_read(\"https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/de\", format = \",\", header = FALSE)\n\nReading data...\n\n\nPreparing data... Almost there!\n\n\nL√§nger aber noch ist die Liste aus dem InsultWiki, lizensiert CC0.\n\nschimpf2 <- \n  data_read(\"data/insult-de.txt\", header = FALSE) %>% \n  mutate_all(str_to_lower)\n\nDie Daten finden sich auch im Paket pradadata.\nBinden wir die Listen zusammen:\n\nschimpf <-\n  schimpf1 %>% \n  bind_rows(schimpf2) %>% \n  distinct() %>% \n  rename(word = \"V1\")\n\nnrow(schimpf)\n\n[1] 6208\n\n\nUm die Lesis vor (unn√∂tiger?) Kopfverschmutzung zu bewahren, sind diese Schimpfw√∂rter hier nicht abgedruckt.\nJetzt z√§hlen wir, ob unsere Tweets/Texte solcherlei W√∂rter enthalten.\n\nd_schimpf <- \nd2_long %>% \n  select(id, token) %>% \n  mutate(schimpf = token %in% schimpf$word)\n\nWie viele Schimpfw√∂rter haben wir gefunden?\n\nd_schimpf %>% \n  count(schimpf)\n\n\n\n\n\nschimpf\nn\n\n\n\n\nFALSE\n99081\n\n\nTRUE\n1136\n\n\n\n\n\n\nEtwa ein Prozent der W√∂rter sind Schimpfw√∂rter in unserem Corpus.\n\nd_schimpf2 <-\n  d_schimpf %>% \n  group_by(id) %>% \n  summarise(schimpf_n = sum(schimpf))\n\nhead(d_schimpf2)\n\n\n\n\n\nid\nschimpf_n\n\n\n\n\n1\n0\n\n\n10\n0\n\n\n100\n0\n\n\n1000\n0\n\n\n1001\n0\n\n\n1002\n0\n\n\n\n\n\n\n\nd_main <-\n  d3 %>% \n  full_join(d_schimpf2)\n\nJoining, by = \"id\"\n\n\n\n\n\n\n\n\nWichtig\n\n\n\nNamen wie final, main oder result sind gef√§hrlich, da es unter Garantie ein ‚Äúfinal-final geben wird, oder der‚ÄùHaupt-Datensat‚Äù pl√∂tzlich nicht mehr so wichtig erscheint und so weiter.\n\n\n\n\n7.3.4 Emojis\n\nemj <- emoji(list_emoji(), pad = FALSE)\n\nhead(emj)\n\n[1] \"üòÑ\" \"üòÉ\" \"üòÄ\" \"üòä\" \"‚ò∫Ô∏è\"  \"üòâ\"\n\n\nDiese Liste umfasst knapp 900 Emojis, das sind allerdings noch nicht alle, die es gibt. Diese Liste umfasst mit gut 1800 Emojis gut das Doppelte.\nSelbstkuratierte Liste an ‚Äúwilden‚Äù Emoji; diese Liste ist inspiriert von emojicombos.com.\n\nwild_emojis <- \n  c(\n    emoji(find_emoji(\"gun\")),\n    emoji(find_emoji(\"bomb\")),\n    emoji(find_emoji(\"fist\")),\n    emoji(find_emoji(\"knife\"))[1],\n    emoji(find_emoji(\"ambulance\")),\n    emoji(find_emoji(\"fist\")),\n    emoji(find_emoji(\"skull\")),\n    \"‚ò†Ô∏è\",     \"üóë\",       \"üò†\",    \"üëπ\",    \"üí©\" ,\n    \"üñï\",    \"üëéÔ∏è\",\n    emoji(find_emoji(\"middle finger\")),    \"üò°\",    \"ü§¢\",    \"ü§Æ\",  \n    \"üòñ\",    \"üò£\",    \"üò©\",    \"üò®\",    \"üòù\",    \"üò≥\",    \"üò¨\",    \"üò±\",    \"üòµ\",\n       \"üò§\",    \"ü§¶‚Äç‚ôÄÔ∏è\",    \"ü§¶‚Äç\"\n  )\n\n\nwild_emojis_df <-\n  tibble(emoji = wild_emojis)\n\nsave(wild_emojis_df, file = \"data/wild_emojis.RData\")\n\nAuf dieser Basis k√∂nnen wir einen Pr√§diktor erstellen, der z√§hlt, ob ein Tweet einen oder mehrere der ‚Äúwilden‚Äù Emojis enth√§lt."
  },
  {
    "objectID": "klassifikation.html#workflow-1-rezept-1-naive-bayes",
    "href": "klassifikation.html#workflow-1-rezept-1-naive-bayes",
    "title": "7¬† Klassifikation von Hatespeech",
    "section": "7.4 Workflow 1: Rezept 1 + Naive-Bayes",
    "text": "7.4 Workflow 1: Rezept 1 + Naive-Bayes\n\n7.4.1 Dummy-Rezept\nHier ist ein einfaches Beispiel, um die Textvorbereitung mit {textrecipes} zu verdeutlichen.\nWir erstellen uns einen Dummy-Text:\n\ndummy <- \n  tibble(text = c(\"Ich gehe heim und der die das nicht in ein and the\"))\n\nDann tokenisieren wir den Text:\n\nrec_dummy <-\n  recipe(text ~ 1, data = dummy) %>% \n  step_tokenize(text)\n  \nrec_dummy\n\nRecipe\n\nInputs:\n\n    role #variables\n outcome          1\n\nOperations:\n\nTokenization for text\n\n\nDie Tokens kann man sich so zeigen lassen:\n\nshow_tokens(rec_dummy, text)\n\n[[1]]\n [1] \"ich\"   \"gehe\"  \"heim\"  \"und\"   \"der\"   \"die\"   \"das\"   \"nicht\" \"in\"   \n[10] \"ein\"   \"and\"   \"the\"  \n\n\nJetzt entfernen wir die Stopw√∂rter deutscher Sprache; daf√ºr nutzen wir die Stopwort-Quelle snowball:\n\nrec_dummy <-\n  recipe(text ~ 1, data = dummy) %>% \n  step_tokenize(text) %>% \n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\")\n\nrec_dummy\n\nRecipe\n\nInputs:\n\n    role #variables\n outcome          1\n\nOperations:\n\nTokenization for text\nStop word removal for text\n\n\nPr√ºfen wir die Tokens; sind die Stopw√∂rter wirklich entfernt?\n\nshow_tokens(rec_dummy, text)\n\n[[1]]\n[1] \"gehe\" \"heim\" \"and\"  \"the\" \n\n\nJa, die deutschen Stopw√∂rter sind entfernt. Die englischen nicht; das macht Sinn!\n\n\n7.4.2 Datenaufteilung\n\nd_split <- initial_split(d_main, strata = c1)\n\nd_train <- training(d_split)\nd_test <- testing(d_split)\n\n\n\n7.4.3 Rezept 1\nRezept definieren:\n\nrec1 <- \n  recipe(c1 ~ ., data = select(d_train, text, c1, id)) %>% \n  update_role(id, new_role = \"id\") %>% \n  step_tokenize(text) %>% \n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %>% \n  step_stem(text) %>% \n  step_tokenfilter(text, max_tokens = 1e2) %>% \n  step_tfidf(text) %>% \n  step_normalize(all_numeric_predictors())\n\nrec1\n\nRecipe\n\nInputs:\n\n      role #variables\n        id          1\n   outcome          1\n predictor          1\n\nOperations:\n\nTokenization for text\nStop word removal for text\nStemming for text\nText filtering for text\nTerm frequency-inverse document frequency with text\nCentering and scaling for all_numeric_predictors()\n\n\nPreppen:\n\nrec1_prepped <- prep(rec1)\n\nUnd backen:\n\nd_rec1 <- bake(rec1_prepped, new_data = NULL)\n\nhead(d_rec1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nc1\ntfidf_text__macmik\ntfidf_text_2\ntfidf_text_ab\ntfidf_text_afd\ntfidf_text_amp\ntfidf_text_anna_iina\ntfidf_text_athinamala\ntfidf_text_beim\ntfidf_text_besser\ntfidf_text_bild\ntfidf_text_cdu\ntfidf_text_charlie_silv\ntfidf_text_d\ntfidf_text_daf√ºr\ntfidf_text_dank\ntfidf_text_dass\ntfidf_text_deutsch\ntfidf_text_deutschen\ntfidf_text_deutschland\ntfidf_text_dumm\ntfidf_text_eigentlich\ntfidf_text_einfach\ntfidf_text_ellibisathid\ntfidf_text_endlich\ntfidf_text_ennof_\ntfidf_text_erst\ntfidf_text_eu\ntfidf_text_europa\ntfidf_text_feldenfrizz\ntfidf_text_focusonlin\ntfidf_text_frage\ntfidf_text_frau\ntfidf_text_ganz\ntfidf_text_geht\ntfidf_text_genau\ntfidf_text_gerad\ntfidf_text_gibt\ntfidf_text_gr√ºnen\ntfidf_text_gt\ntfidf_text_gut\ntfidf_text_heut\ntfidf_text_immer\ntfidf_text_info2099\ntfidf_text_islam\ntfidf_text_israel\ntfidf_text_ja\ntfidf_text_jahr\ntfidf_text_klar\ntfidf_text_kommt\ntfidf_text_krippmari\ntfidf_text_land\ntfidf_text_lassen\ntfidf_text_lbr\ntfidf_text_leben\ntfidf_text_leider\ntfidf_text_lifetrend\ntfidf_text_link\ntfidf_text_macht\ntfidf_text_machtjanix23\ntfidf_text_mal\ntfidf_text_md_franz\ntfidf_text_mehr\ntfidf_text_menschen\ntfidf_text_merkel\ntfidf_text_miriamozen\ntfidf_text_moslem\ntfidf_text_m√ºssen\ntfidf_text_nancypeggymandi\ntfidf_text_nasanas\ntfidf_text_noherrman\ntfidf_text_norbinator2403\ntfidf_text_petpanther0\ntfidf_text_politik\ntfidf_text_recht\ntfidf_text_richtig\ntfidf_text_schmiddiemaik\ntfidf_text_schon\ntfidf_text_schulz\ntfidf_text_seit\ntfidf_text_sicher\ntfidf_text_sollten\ntfidf_text_spd\ntfidf_text_tagesschau\ntfidf_text_thomasgbau\ntfidf_text_troll_putin\ntfidf_text_trump\ntfidf_text_tun\ntfidf_text_u\ntfidf_text_unser\ntfidf_text_viel\ntfidf_text_volk\ntfidf_text_w√§re\ntfidf_text_warum\ntfidf_text_welt\ntfidf_text_wer\ntfidf_text_willjrosenblatt\ntfidf_text_wissen\ntfidf_text_wohl\ntfidf_text_wurd\ntfidf_text_zeit\n\n\n\n\n7\nOFFENSE\n-0.1478606\n-0.1017932\n-0.1030146\n-0.157801\n-0.1237411\n-0.1069335\n-0.1421622\n-0.0952091\n-0.0970237\n-0.0999172\n-0.1136532\n-0.1446274\n-0.1331066\n-0.0923332\n-0.1187328\n-0.1907487\n-0.1557646\n-0.1525502\n-0.2023399\n-0.1038507\n-0.0936705\n-0.117269\n-0.1107154\n-0.0994985\n-0.113482\n-0.114578\n-0.115141\n-0.1053888\n-0.1478606\n-0.1057552\n-0.0935953\n-0.0936319\n-0.1104678\n3.9532756\n-0.0973827\n-0.1010069\n-0.1637995\n-0.1014353\n-0.0912218\n-0.1176659\n2.8693459\n-0.1583496\n-0.080984\n-0.1109831\n-0.0896051\n-0.170903\n-0.0974452\n-0.1010331\n-0.1001024\n-0.1374319\n-0.1395844\n-0.1036183\n-0.4078297\n-0.0885768\n-0.0958063\n-0.1433179\n-0.0988073\n-0.1261982\n-0.1072451\n-0.1802804\n-0.1309638\n-0.1873138\n-0.1223844\n-0.2228536\n-0.1091668\n-0.0953965\n-0.134878\n-0.1110202\n-0.1191285\n-0.093113\n-0.0604024\n-0.0923776\n-0.1481462\n-0.1196747\n-0.1059602\n-0.1414784\n-0.1940836\n-0.098983\n-0.1269244\n-0.0922149\n-0.098992\n-0.1349804\n-0.1009173\n-0.1416992\n-0.1174202\n-0.1083908\n-0.1038686\n-0.1535957\n-0.1537918\n-0.1004169\n-0.1119121\n-0.1188893\n-0.1068939\n-0.1496574\n-0.1548429\n-0.1414784\n-0.0910888\n-0.106692\n-0.1127138\n5.9093440\n\n\n9\nOFFENSE\n-0.1478606\n-0.1017932\n-0.1030146\n5.670118\n-0.1237411\n-0.1069335\n-0.1421622\n-0.0952091\n-0.0970237\n-0.0999172\n-0.1136532\n-0.1446274\n-0.1331066\n-0.0923332\n-0.1187328\n-0.1907487\n-0.1557646\n-0.1525502\n-0.2023399\n-0.1038507\n-0.0936705\n-0.117269\n-0.1107154\n-0.0994985\n-0.113482\n-0.114578\n-0.115141\n-0.1053888\n-0.1478606\n-0.1057552\n-0.0935953\n-0.0936319\n-0.1104678\n-0.1465598\n-0.0973827\n-0.1010069\n-0.1637995\n-0.1014353\n-0.0912218\n-0.1176659\n-0.1701938\n-0.1583496\n-0.080984\n-0.1109831\n-0.0896051\n-0.170903\n-0.0974452\n-0.1010331\n-0.1001024\n-0.1374319\n-0.1395844\n-0.1036183\n-0.4078297\n-0.0885768\n-0.0958063\n-0.1433179\n-0.0988073\n-0.1261982\n-0.1072451\n-0.1802804\n-0.1309638\n-0.1873138\n-0.1223844\n-0.2228536\n-0.1091668\n-0.0953965\n-0.134878\n-0.1110202\n-0.1191285\n-0.093113\n-0.0604024\n-0.0923776\n-0.1481462\n-0.1196747\n-0.1059602\n-0.1414784\n-0.1940836\n-0.098983\n-0.1269244\n-0.0922149\n-0.098992\n-0.1349804\n9.1018767\n-0.1416992\n-0.1174202\n-0.1083908\n-0.1038686\n-0.1535957\n-0.1537918\n-0.1004169\n-0.1119121\n-0.1188893\n-0.1068939\n-0.1496574\n-0.1548429\n-0.1414784\n-0.0910888\n-0.106692\n-0.1127138\n-0.1048312\n\n\n10\nOFFENSE\n-0.1478606\n-0.1017932\n-0.1030146\n-0.157801\n-0.1237411\n-0.1069335\n-0.1421622\n-0.0952091\n-0.0970237\n-0.0999172\n-0.1136532\n-0.1446274\n-0.1331066\n-0.0923332\n-0.1187328\n-0.1907487\n11.0326231\n-0.1525502\n-0.2023399\n-0.1038507\n-0.0936705\n-0.117269\n-0.1107154\n-0.0994985\n-0.113482\n-0.114578\n-0.115141\n-0.1053888\n-0.1478606\n-0.1057552\n-0.0935953\n-0.0936319\n-0.1104678\n-0.1465598\n-0.0973827\n-0.1010069\n-0.1637995\n-0.1014353\n-0.0912218\n-0.1176659\n-0.1701938\n-0.1583496\n-0.080984\n-0.1109831\n-0.0896051\n-0.170903\n-0.0974452\n-0.1010331\n-0.1001024\n-0.1374319\n-0.1395844\n-0.1036183\n-0.4078297\n-0.0885768\n-0.0958063\n-0.1433179\n-0.0988073\n-0.1261982\n-0.1072451\n-0.1802804\n-0.1309638\n-0.1873138\n-0.1223844\n-0.2228536\n-0.1091668\n-0.0953965\n-0.134878\n-0.1110202\n-0.1191285\n-0.093113\n-0.0604024\n-0.0923776\n-0.1481462\n-0.1196747\n-0.1059602\n-0.1414784\n-0.1940836\n-0.098983\n-0.1269244\n-0.0922149\n-0.098992\n-0.1349804\n-0.1009173\n-0.1416992\n-0.1174202\n-0.1083908\n-0.1038686\n-0.1535957\n-0.1537918\n-0.1004169\n-0.1119121\n-0.1188893\n-0.1068939\n-0.1496574\n-0.1548429\n-0.1414784\n-0.0910888\n-0.106692\n-0.1127138\n-0.1048312\n\n\n12\nOFFENSE\n-0.1478606\n-0.1017932\n-0.1030146\n-0.157801\n-0.1237411\n-0.1069335\n-0.1421622\n-0.0952091\n-0.0970237\n-0.0999172\n-0.1136532\n-0.1446274\n-0.1331066\n-0.0923332\n-0.1187328\n-0.1907487\n-0.1557646\n-0.1525502\n-0.2023399\n-0.1038507\n-0.0936705\n-0.117269\n-0.1107154\n-0.0994985\n-0.113482\n-0.114578\n-0.115141\n-0.1053888\n-0.1478606\n-0.1057552\n-0.0935953\n-0.0936319\n-0.1104678\n-0.1465598\n-0.0973827\n-0.1010069\n-0.1637995\n-0.1014353\n-0.0912218\n-0.1176659\n-0.1701938\n3.7091315\n-0.080984\n-0.1109831\n-0.0896051\n3.511705\n-0.0974452\n-0.1010331\n-0.1001024\n-0.1374319\n-0.1395844\n-0.1036183\n1.2316433\n-0.0885768\n-0.0958063\n-0.1433179\n-0.0988073\n-0.1261982\n-0.1072451\n-0.1802804\n-0.1309638\n-0.1873138\n-0.1223844\n-0.2228536\n-0.1091668\n-0.0953965\n-0.134878\n-0.1110202\n-0.1191285\n-0.093113\n-0.0604024\n-0.0923776\n-0.1481462\n-0.1196747\n-0.1059602\n-0.1414784\n-0.1940836\n-0.098983\n-0.1269244\n-0.0922149\n-0.098992\n-0.1349804\n-0.1009173\n-0.1416992\n-0.1174202\n-0.1083908\n-0.1038686\n-0.1535957\n-0.1537918\n-0.1004169\n-0.1119121\n-0.1188893\n-0.1068939\n-0.1496574\n-0.1548429\n-0.1414784\n-0.0910888\n-0.106692\n-0.1127138\n-0.1048312\n\n\n17\nOFFENSE\n-0.1478606\n-0.1017932\n-0.1030146\n-0.157801\n-0.1237411\n-0.1069335\n-0.1421622\n-0.0952091\n-0.0970237\n-0.0999172\n-0.1136532\n-0.1446274\n-0.1331066\n-0.0923332\n-0.1187328\n-0.1907487\n-0.1557646\n3.8058985\n-0.2023399\n-0.1038507\n-0.0936705\n-0.117269\n-0.1107154\n-0.0994985\n-0.113482\n-0.114578\n-0.115141\n-0.1053888\n-0.1478606\n-0.1057552\n-0.0935953\n-0.0936319\n-0.1104678\n-0.1465598\n-0.0973827\n-0.1010069\n-0.1637995\n-0.1014353\n-0.0912218\n-0.1176659\n-0.1701938\n-0.1583496\n-0.080984\n-0.1109831\n-0.0896051\n-0.170903\n-0.0974452\n-0.1010331\n-0.1001024\n-0.1374319\n-0.1395844\n-0.1036183\n-0.4078297\n-0.0885768\n-0.0958063\n-0.1433179\n-0.0988073\n-0.1261982\n-0.1072451\n-0.1802804\n-0.1309638\n-0.1873138\n-0.1223844\n-0.2228536\n-0.1091668\n-0.0953965\n-0.134878\n-0.1110202\n-0.1191285\n-0.093113\n-0.0604024\n-0.0923776\n4.3487918\n-0.1196747\n-0.1059602\n-0.1414784\n-0.1940836\n-0.098983\n4.6618704\n-0.0922149\n-0.098992\n-0.1349804\n-0.1009173\n-0.1416992\n-0.1174202\n-0.1083908\n-0.1038686\n-0.1535957\n-0.1537918\n-0.1004169\n-0.1119121\n-0.1188893\n-0.1068939\n-0.1496574\n-0.1548429\n-0.1414784\n-0.0910888\n-0.106692\n-0.1127138\n-0.1048312\n\n\n33\nOFFENSE\n-0.1478606\n-0.1017932\n-0.1030146\n-0.157801\n-0.1237411\n-0.1069335\n-0.1421622\n-0.0952091\n-0.0970237\n-0.0999172\n-0.1136532\n-0.1446274\n-0.1331066\n-0.0923332\n-0.1187328\n-0.1907487\n3.5736979\n-0.1525502\n-0.2023399\n-0.1038507\n-0.0936705\n-0.117269\n-0.1107154\n-0.0994985\n-0.113482\n-0.114578\n-0.115141\n-0.1053888\n-0.1478606\n-0.1057552\n-0.0935953\n-0.0936319\n-0.1104678\n-0.1465598\n-0.0973827\n-0.1010069\n-0.1637995\n-0.1014353\n-0.0912218\n-0.1176659\n-0.1701938\n-0.1583496\n-0.080984\n-0.1109831\n-0.0896051\n-0.170903\n-0.0974452\n-0.1010331\n-0.1001024\n-0.1374319\n-0.1395844\n-0.1036183\n-0.4078297\n-0.0885768\n-0.0958063\n-0.1433179\n-0.0988073\n-0.1261982\n-0.1072451\n-0.1802804\n-0.1309638\n2.9994252\n-0.1223844\n-0.2228536\n-0.1091668\n6.1140871\n-0.134878\n-0.1110202\n-0.1191285\n-0.093113\n-0.0604024\n-0.0923776\n-0.1481462\n-0.1196747\n-0.1059602\n-0.1414784\n-0.1940836\n-0.098983\n-0.1269244\n-0.0922149\n-0.098992\n-0.1349804\n-0.1009173\n-0.1416992\n-0.1174202\n-0.1083908\n-0.1038686\n-0.1535957\n-0.1537918\n-0.1004169\n-0.1119121\n-0.1188893\n-0.1068939\n-0.1496574\n-0.1548429\n-0.1414784\n-0.0910888\n-0.106692\n-0.1127138\n-0.1048312\n\n\n\n\n\n\n\n\n7.4.4 Modellspezifikation 1\nWir definiere einen Naive-Bayes-Algorithmus:\n\nnb_spec <- naive_Bayes() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"naivebayes\")\n\nnb_spec\n\nNaive Bayes Model Specification (classification)\n\nComputational engine: naivebayes \n\n\nUnd setzen auf die klassische zehnfache Kreuzvalidierung.\n\nset.seed(42)\nfolds1 <- vfold_cv(d_train)\n\n\n\n7.4.5 Workflow 1\n\nwf1 <-\n  workflow() %>% \n  add_recipe(rec1) %>% \n  add_model(nb_spec)\n\nwf1\n\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: naive_Bayes()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n6 Recipe Steps\n\n‚Ä¢ step_tokenize()\n‚Ä¢ step_stopwords()\n‚Ä¢ step_stem()\n‚Ä¢ step_tokenfilter()\n‚Ä¢ step_tfidf()\n‚Ä¢ step_normalize()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nNaive Bayes Model Specification (classification)\n\nComputational engine: naivebayes \n\n\n\n\n7.4.6 Fitting 1\n\nfit1 <-\n  fit_resamples(\n    wf1,\n    folds1,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nDie Vorhersagen speichern wir ab, um die Performanz in den Faltungen des Hold-out-Samples zu berechnen.\nM√∂chte man sich die Zeit sparen, die Syntax wieder durchlaufen zu lassen, kann man das Objekt speichern. Aber Vorsicht: Dabei kann es passieren, dass man mit veralteten Objekten arbeitet.\n\n#write_rds(fit1, \"objects/chap_classific_fit1.rds\")\n\n\n\n\n\n\n7.4.7 Performanz 1\n\nwf1_performance <-\n  collect_metrics(fit1)\n\nwf1_performance\n\n\nwf_preds <-\n  collect_predictions(fit1)\n\nwf_preds %>% \n  group_by(id) %>% \n  roc_curve(truth = c1, .pred_OFFENSE) %>% \n  autoplot()\n\n\n\n\nconf_mat_resampled(fit1, tidy = FALSE) %>% autoplot(type = ‚Äúheatmap‚Äù)\n\nconf_mat_resampled(fit1, tidy = FALSE) %>% \n  autoplot(type = \"heatmap\")"
  },
  {
    "objectID": "klassifikation.html#nullmodell",
    "href": "klassifikation.html#nullmodell",
    "title": "7¬† Klassifikation von Hatespeech",
    "section": "7.5 Nullmodell",
    "text": "7.5 Nullmodell\n\nnull_classification <- \n  parsnip::null_model() %>%\n  set_engine(\"parsnip\") %>%\n  set_mode(\"classification\")\n\nnull_rs <- workflow() %>%\n  add_recipe(rec1) %>%\n  add_model(null_classification) %>%\n  fit_resamples(\n    folds1\n  )\n\n\n\n\n\n\n\nHier ist die Performanz des Nullmodells.\n\nnull_rs %>%\n  collect_metrics()\n\n\nshow_best(null_rs)\n\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npenalty\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\n0.0017433\nroc_auc\nbinary\n0.8116297\n10\n0.0087106\nPreprocessor1_Model22\n\n\n0.0007880\nroc_auc\nbinary\n0.8111630\n10\n0.0089414\nPreprocessor1_Model21\n\n\n0.0003562\nroc_auc\nbinary\n0.8103737\n10\n0.0092381\nPreprocessor1_Model20\n\n\n0.0001610\nroc_auc\nbinary\n0.8099159\n10\n0.0094448\nPreprocessor1_Model19\n\n\n0.0000000\nroc_auc\nbinary\n0.8095907\n10\n0.0096029\nPreprocessor1_Model01"
  },
  {
    "objectID": "klassifikation.html#workflow-2-rezept-1-lasso",
    "href": "klassifikation.html#workflow-2-rezept-1-lasso",
    "title": "7¬† Klassifikation von Hatespeech",
    "section": "7.6 Workflow 2: Rezept 1 + Lasso",
    "text": "7.6 Workflow 2: Rezept 1 + Lasso\n\nlasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"glmnet\")\n\nlasso_spec\n\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\nWir definieren die Auspr√§gungen von penalty, die wir ausprobieren wollen:\n\nlambda_grid <- grid_regular(penalty(), levels = 3)  # hier nur 3 Werte, um Rechenzeit zu sparen\n\n\nwf2 <-\n  workflow() %>% \n  add_recipe(rec1) %>% \n  add_model(lasso_spec)\n\nwf2\n\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: logistic_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n6 Recipe Steps\n\n‚Ä¢ step_tokenize()\n‚Ä¢ step_stopwords()\n‚Ä¢ step_stem()\n‚Ä¢ step_tokenfilter()\n‚Ä¢ step_tfidf()\n‚Ä¢ step_normalize()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\nTunen und Fitten:\n\nset.seed(42)\n\nfit2 <-\n  tune_grid(\n    wf2,\n    folds1,\n    grid = lambda_grid,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nfit2\n\nVorsicht beim Abspeichern.\n\n#write_rds(fit2, \"objects/chap_classific_fit2.rds\")\n\n\n\n\nHier ist die Performanz:\n\ncollect_metrics(fit2) %>% \n  filter(.metric == \"roc_auc\") %>% \n  slice_max(mean, n = 3)\n\n\nautoplot(fit2)\n\n\n\n\n\nfit2 %>% \n  show_best(\"roc_auc\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npenalty\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\n0\nroc_auc\nbinary\n0.5997476\n10\n0.0090575\nPreprocessor1_Model01\n\n\n0\nroc_auc\nbinary\n0.5997476\n10\n0.0090575\nPreprocessor1_Model02\n\n\n0\nroc_auc\nbinary\n0.5997476\n10\n0.0090575\nPreprocessor1_Model03\n\n\n0\nroc_auc\nbinary\n0.5997476\n10\n0.0090575\nPreprocessor1_Model04\n\n\n0\nroc_auc\nbinary\n0.5997476\n10\n0.0090575\nPreprocessor1_Model05\n\n\n\n\n\n\n\nchosen_auc <- \n  fit2 %>%\n  select_by_one_std_err(metric = \"roc_auc\", -penalty)\n\nFinalisieren:\n\nwf2_final <-\n  finalize_workflow(wf2, chosen_auc)\n\nwf2_final\n\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: logistic_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n6 Recipe Steps\n\n‚Ä¢ step_tokenize()\n‚Ä¢ step_stopwords()\n‚Ä¢ step_stem()\n‚Ä¢ step_tokenfilter()\n‚Ä¢ step_tfidf()\n‚Ä¢ step_normalize()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = 0.00853167852417281\n  mixture = 1\n\nComputational engine: glmnet \n\n\n\nfit2_final_train <-\n  fit(wf2_final, d_train)\n\n\nfit2_final_train %>% \n  extract_fit_parsnip() %>% \n  tidy() %>% \n  arrange(-abs(estimate)) %>% \n  head()\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-6\n\n\n\n\n\n\nterm\nestimate\npenalty\n\n\n\n\n(Intercept)\n0.7002844\n0.0085317\n\n\ntfidf_text_merkel\n-0.2356021\n0.0085317\n\n\ntfidf_text_dumm\n-0.1908203\n0.0085317\n\n\ntfidf_text_lbr\n-0.1468066\n0.0085317\n\n\ntfidf_text_moslem\n-0.1442941\n0.0085317\n\n\ntfidf_text_dank\n0.1295210\n0.0085317\n\n\n\n\n\n\n\nfit2_final_test <-\n  last_fit(wf2_final, d_split)\n\ncollect_metrics(fit2_final_test)\n\n\n\n\n\n.metric\n.estimator\n.estimate\n.config\n\n\n\n\naccuracy\nbinary\n0.6855547\nPreprocessor1_Model1\n\n\nroc_auc\nbinary\n0.6448264\nPreprocessor1_Model1\n\n\n\n\n\n\n\n7.6.1 Vorhersage\n\n\n7.6.2 Vohersagedaten\nPfad zu den Daten:\n\ntweet_data_path <- \"/Users/sebastiansaueruser/github-repos/hate-speech/data-raw/tweets/\"\n\n\ntweet_data_files_names <- list.files(path = tweet_data_path,\n                                     pattern  = \"tweets-to-.*\\\\.rds$\")\nhead(tweet_data_files_names)\n\ncharacter(0)\n\n\nWie viele Dateien sind es?\n\nlength(tweet_data_files_names)\n\n[1] 0\n\n\nWir geben den Elementen des Vektors g√§ngige Namen, das hilft uns gleich bei map:\n\nnames(tweet_data_files_names) <- str_remove(tweet_data_files_names, \"\\\\.rds\")\n\nOK, weiter: So k√∂nnen wir eine der Datendateien einlesen:\n\nd_raw <-\n  read_rds(file = paste0(tweet_data_path, tweet_data_files_names[1])) \n\nd <- \n  d_raw %>% \n  select(id, author_id, created_at, public_metrics) %>% \n  unnest_wider(public_metrics)\n\nhead(d)\n\nUnd so lesen wir alle ein:\nZun√§chst erstellen wir uns eine Helper-Funktion:\n\nread_and_select <- function(file_name, path_to_tweet_data = tweet_data_path) {\n  \n  out <- \n    read_rds(file = paste0(path_to_tweet_data, file_name)) %>% \n    select(id, author_id, created_at, text, public_metrics) %>% \n    unnest_wider(public_metrics)\n  \n  cat(\"Data file was read.\\n\")\n  \n  return(out)\n}\n\nTesten:\n\nd1 <- read_and_select(tweet_data_files_names[1])\n\nhead(d1)\n\nDie Funktion read_and_select mappen wir auf alle Datendateien:\n\ntic()\nds <-\n  tweet_data_files_names %>% \n  map_dfr(read_and_select, .id = \"dataset\")\ntoc()\n\n214.531 sec elapsed\nDa wir den Elementen von tweet_data_files_names Namen gegeben haben, finden wir diese Namen praktischerweise wieder in ds.\n\n\n\n\n\n\nVielleicht ist es zum Entwickeln besser, mit einem kleineren Datensatz einstweilen zu arbeiten:\n\nds_short <- read_rds(file = \"/Users/sebastiansaueruser/datasets/Twitter/hate-classific/ds_short.rds\")\n\nds <- ds_short\n\n\n\n\n\n\n7.6.3 Vokabular erstellen\n\nds_long <-\n  ds %>% \n  select(text) %>% \n  unnest_tweets(input = text, output = word)\n\nPuh, das hat gedauert!\nSpeichern wir uns diese Daten daher auf die Festplatte:\n\n#write_rds(ds_long, file = paste0(tweet_data_path, \"ds_long.rds\"))\n\nEntfernen wir daraus die Duplikate, um uns ein Vokabular zu erstellen:\n\nds_voc <-\n  ds_long %>% \n  distinct(word)\n\nUnd das resultierende Objekt speichern wir wieder ab:\n\n#write_rds(ds_voc, file = paste0(\"objects/\", \"ds_voc.rds\"))"
  },
  {
    "objectID": "klassifikation.html#worteinbettungen-erstellen",
    "href": "klassifikation.html#worteinbettungen-erstellen",
    "title": "7¬† Klassifikation von Hatespeech",
    "section": "7.7 Worteinbettungen erstellen",
    "text": "7.7 Worteinbettungen erstellen\n\n7.7.1 FastText-Modell\nDefiniere die Konstanten f√ºr das fastText-Modell:\n\ntexts <- ds %>% pull(text)\ntexts <- tolower(texts)\n\n\nout_file_txt <- \"/Users/sebastiansaueruser/datasets/Twitter/twitter-polit-model.vec\"\nout_file_model <- \"/Users/sebastiansaueruser/datasets/Twitter/twitter-polit-model.bin\"\n\nfile.exists(out_file_txt)\n\n[1] TRUE\n\nfile.exists(out_file_model)\n\n[1] TRUE\n\n\n\n#writeLines(text = texts, con = out_file_txt)\n#execute(commands = c(\"skipgram\", \"-input\", tmp_file_txt, \"-output\", out_file_model, \"-verbose\", 1))\n\nRead 22M words\nNumber of words:  130328\nNumber of labels: 0\nProgress: 100.0% words/sec/thread:   49218 lr:  0.000000 avg.loss:  1.720812 ETA:   0h 0m 0s\nJetzt laden wir das Modell von der Festplatte:\n\ntwitter_fasttext_model <- load_model(out_file_model)\ndict <- get_dictionary(twitter_fasttext_model)\n\nSchauen wir uns einige Begriffe aus dem Vokabular an:\n\nprint(head(dict, 10))\n\n [1] \"</s>\"            \"die\"             \"und\"             \"der\"            \n [5] \"sie\"             \"das\"             \"nicht\"           \"in\"             \n [9] \"ist\"             \"@_friedrichmerz\"\n\n\nHier sind die ersten paar Elemente des Vektors f√ºr menschen:\n\nget_word_vectors(twitter_fasttext_model, c(\"menschen\")) %>% `[`(1:10)\n\n [1]  0.14156282  0.44875699  0.23911817 -0.02580349  0.29811972  0.03870077\n [7]  0.06518744  0.22527063  0.28198120  0.39931887\nErstellen wir uns einen Tibble, der als erste Spalte das Vokabular und in den √ºbrigen 100 Spalten die Dimensionen enth√§lt:\n\nword_embedding_twitter <-\n  tibble(\n    word = dict\n  )\n\n\nwords_vecs_twitter <-\n  get_word_vectors(twitter_fasttext_model)\n\n\nword_embedding_twitter <-\n  word_embedding_twitter %>% \n  bind_cols(words_vecs_twitter)\n\nnames(word_embedding_twitter) <- c(\"word\", paste0(\"v\", sprintf(\"%03d\", 1:100)))  # Namen versch√∂nern\n\nUnd als Worteinbettungs-Datei abspeichern:\n\n#write_rds(word_embedding_twitter, file = paste0(tweet_data_path, \"word_embedding_twitter.rds\"))\n\n\n\n\n\n\n7.7.2 Aufbereiten\nAm besten nur die Spalten behalten, die wir zum Modellieren nutzen:\n\nds_short2 <-\n  ds_short %>% \n  select(text, id)\n\nDann backen wir die Daten mit dem vorhandenen Rezept:\n\nds_baked <- bake(rec1_prepped, new_data = ds_short2)\n\nIst das nicht komfortabel? Das Textrezept √ºbernimmt die Arbeit f√ºr uns, mit den richtigen Features zu arbeiten, die tf-idfs f√ºr die richtigen Tokens zu berechnen.\nWer dem Frieden nicht traut, dem sei geraten, nachzupr√ºfen :-)"
  },
  {
    "objectID": "klassifikation.html#workflow-3-rezept-2-lasso",
    "href": "klassifikation.html#workflow-3-rezept-2-lasso",
    "title": "7¬† Klassifikation von Hatespeech",
    "section": "7.8 Workflow 3: Rezept 2 + Lasso",
    "text": "7.8 Workflow 3: Rezept 2 + Lasso\n\n7.8.1 Daten aufteilen\n\nd_split <- initial_split(d2, strata = c1)\n\nd_train <- training(d_split)\nd_test <- testing(d_split)\n\n\n\n7.8.2 Hilfsfunktionen\n\nsource(\"funs/helper-funs-recipes.R\")\n\nTesten wir die Funktionen:\n\ndummy <- c(\"hallo\", \"baby\", \"fatal\")\n\ncount_profane(dummy) \n\n[1] 1\n\ncount_emo_words(dummy)\n\n[1] 1\n\ndummy <- c(\"baby\", \"und\", \"üÜó\", \"üñï\")\n\ncount_emojis(dummy)\n\n[1] 1\n\ncount_wild_emojis(dummy) \n\n[1] 1\n\n\n\n\n7.8.3 Rezept mit Worteinbettungen\n\nrec2 <- \n  recipe(c1 ~ ., data = select(d_train, text, c1, id)) %>% \n  update_role(id, new_role = \"id\") %>% \n  step_text_normalization(text) %>% \n  step_mutate(text_copy = text,\n              profane_n = map_int(text, count_profane),\n              emo_words_n = map_int(text, count_emo_words),\n              emojis_n = map_int(text, count_emojis),\n              wild_emojis_n = map_int(text, count_wild_emojis)\n  ) %>% \n  step_textfeature(text_copy) %>% \n  step_tokenize(text, token = \"tweets\") %>% \n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %>% \n  step_word_embeddings(text, embeddings = word_embedding_twitter)\n \nrec2\n\nRecipe\n\nInputs:\n\n      role #variables\n        id          1\n   outcome          1\n predictor          1\n\nOperations:\n\nText Normalization for text\nVariable mutation for text, map_int(text, count_profane), map_in...\nText feature extraction for text_copy\nTokenization for text\nStop word removal for text\nWord embeddings aggregated from text\n\n\nJetzt preppen:\n\nrec2_prepped <- prep(rec2)\n\nVielleicht macht es Sinn, sich das Objekt zur sp√§teren Verwendung abzuspeichern.1 Feather verarbeitet nur Dataframes, daher nutzen wir hier RDS.\n\n#write_rds(rec2_prepped, file = \"~/datasets/Twitter/klassifik-rec2-prepped.rds\")\n\nDas Element rec2_prepped ist recht gro√ü:\n\nformat(object.size(rec2_prepped), units  = \"Mb\")\n\n[1] \"113.8 Mb\"\n\n\nJetzt k√∂nnen wir das pr√§parierte (‚Äúgepreppte‚Äù) Rezept ‚Äúbacken‚Äù:\n\nrec2_baked <- bake(rec2_prepped, new_data = NULL)\n\n\nrec2_baked %>% \n  select(1:15) %>% \n  glimpse()\n\nRows: 3,756\nColumns: 15\n$ id                                  <fct> 5, 7, 10, 12, 17, 27, 33, 42, 44, ‚Ä¶\n$ c1                                  <fct> OFFENSE, OFFENSE, OFFENSE, OFFENSE‚Ä¶\n$ profane_n                           <int> 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ emo_words_n                         <int> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1‚Ä¶\n$ emojis_n                            <int> 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0‚Ä¶\n$ wild_emojis_n                       <int> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ textfeature_text_copy_n_words       <int> 16, 32, 15, 26, 19, 15, 33, 30, 31‚Ä¶\n$ textfeature_text_copy_n_uq_words    <int> 16, 28, 15, 25, 17, 15, 32, 29, 29‚Ä¶\n$ textfeature_text_copy_n_charS       <int> 121, 145, 119, 134, 112, 101, 196,‚Ä¶\n$ textfeature_text_copy_n_uq_charS    <int> 31, 29, 30, 39, 36, 31, 35, 42, 35‚Ä¶\n$ textfeature_text_copy_n_digits      <int> 0, 4, 0, 2, 4, 0, 0, 0, 1, 0, 2, 0‚Ä¶\n$ textfeature_text_copy_n_hashtags    <int> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ textfeature_text_copy_n_uq_hashtags <int> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ textfeature_text_copy_n_mentions    <int> 1, 1, 0, 1, 0, 1, 1, 5, 1, 0, 1, 1‚Ä¶\n$ textfeature_text_copy_n_uq_mentions <int> 1, 1, 0, 1, 0, 1, 1, 5, 1, 0, 1, 1‚Ä¶\n\n\n\n\n7.8.4 Fitting 3\n\nwf3 <-\n  workflow() %>% \n  add_recipe(rec2) %>% \n  add_model(lasso_spec)\n\nwf3\n\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: logistic_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n6 Recipe Steps\n\n‚Ä¢ step_text_normalization()\n‚Ä¢ step_mutate()\n‚Ä¢ step_textfeature()\n‚Ä¢ step_tokenize()\n‚Ä¢ step_stopwords()\n‚Ä¢ step_word_embeddings()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\nTunen und Fitten:\n\nset.seed(42)\n\ntic()\nfit3 <-\n  tune_grid(\n    wf3,\n    folds1,\n    grid = lambda_grid,\n    control = control_resamples(save_pred = TRUE)\n  )\n(toc)\nfit3\n\n\n#write_rds(fit3, \"objects/chap_classific_fit3.rds\")\n\n\n\n\nHier ist die Performanz:\n\ncollect_metrics(fit3)\n\n\nautoplot(fit3)\n\n\n\n\n\nfit3 %>% \n  show_best(\"roc_auc\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npenalty\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\n0.0017433\nroc_auc\nbinary\n0.8116297\n10\n0.0087106\nPreprocessor1_Model22\n\n\n0.0007880\nroc_auc\nbinary\n0.8111630\n10\n0.0089414\nPreprocessor1_Model21\n\n\n0.0003562\nroc_auc\nbinary\n0.8103737\n10\n0.0092381\nPreprocessor1_Model20\n\n\n0.0001610\nroc_auc\nbinary\n0.8099159\n10\n0.0094448\nPreprocessor1_Model19\n\n\n0.0000000\nroc_auc\nbinary\n0.8095907\n10\n0.0096029\nPreprocessor1_Model01\n\n\n\n\n\n\n\nchosen_auc_fit3 <- \n  fit3 %>%\n  select_by_one_std_err(metric = \"roc_auc\", -penalty)\n\nFinalisieren:\n\nwf3_final <-\n  finalize_workflow(wf3, chosen_auc_fit3)\n\nwf3_final\n\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: logistic_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n6 Recipe Steps\n\n‚Ä¢ step_text_normalization()\n‚Ä¢ step_mutate()\n‚Ä¢ step_textfeature()\n‚Ä¢ step_tokenize()\n‚Ä¢ step_stopwords()\n‚Ä¢ step_word_embeddings()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = 0.00853167852417281\n  mixture = 1\n\nComputational engine: glmnet \n\n\n\nfit3_final_train <-\n  fit(wf3_final, d_train)\n\n\n\n\n\nfit3_final_train %>% \n  extract_fit_parsnip() %>% \n  tidy() %>% \n  arrange(-abs(estimate)) %>% \n  head()\n\n\n\n\n\nterm\nestimate\npenalty\n\n\n\n\n(Intercept)\n1.2444586\n0.0085317\n\n\nprofane_n\n-0.6241517\n0.0085317\n\n\nwordembed_text_v055\n0.1928172\n0.0085317\n\n\ntextfeature_text_copy_n_exclaims\n-0.1836999\n0.0085317\n\n\nwordembed_text_v054\n0.1697083\n0.0085317\n\n\nwordembed_text_v070\n-0.1415024\n0.0085317\n\n\n\n\n\n\n\nfit3_final_test <-\n  last_fit(wf3_final, d_split)\n\n\n\n\nUnd endlich: Wie gut ist die Performanz?\n\ncollect_metrics(fit3_final_test)\n\n\n\n\n\n.metric\n.estimator\n.estimate\n.config\n\n\n\n\naccuracy\nbinary\n0.7525938\nPreprocessor1_Model1\n\n\nroc_auc\nbinary\n0.8120519\nPreprocessor1_Model1\n\n\n\n\n\n\nAm Ende so eines Arbeitsganges, bei dem man wieder (und wieder) die gleichen Funktionen kopiert, und nur aufpassen muss, aus fit2 an der richtigen Stelle fit3 zu machen: Da blickt man jedem Umbau dieses Codes zu einer Funktion freudig ins Gesicht.\nEin anderes Problem, f√ºr das hier keine elegante L√∂sung vorliegt, sind die langen Berechnungszeiten, die, wenn man Pech hat, auch noch mehrfach wiederholt werden m√ºssen.\nDie Gefahr mit dem Abspeichern via write_rds ist klar: Man riskiert, sp√§ter ein veraltetes Objekt zu laden.\nZu diesen Punkten sp√§ter mehr.\n\n\n\n\nRemus, Robert, Uwe Quasthoff, und Gerhard Heyer. 2010. ‚ÄûSentiWS - a Publicly Available German-language Resource for Sentiment Analysis‚Äú. Proceedings of the 7th International Language Ressources and Evaluation (LREC‚Äô10), 1168‚Äì71.\n\n\nWiegand, Michael. 2019a. ‚ÄûGermEval-2018 Corpus (DE)‚Äú. heiDATA. https://doi.org/10.11588/DATA/0B5VML.\n\n\n‚Äî‚Äî‚Äî. 2019b. ‚ÄûGermEval-2018-Data-master‚Äú. In GermEval-2018 corpus (DE). heiDATA. https://doi.org/10.11588/data/0B5VML/XIUWJ7."
  },
  {
    "objectID": "hatespeech2.html",
    "href": "hatespeech2.html",
    "title": "8¬† Fallstudie Hatespeech",
    "section": "",
    "text": "Wir sagen vorher, welche Tweets an f√ºhrende deutsche Politikis Hassrede bzw. hasserf√ºllte Rede enthalten."
  },
  {
    "objectID": "hatespeech2.html#vorab",
    "href": "hatespeech2.html#vorab",
    "title": "8¬† Fallstudie Hatespeech",
    "section": "8.1 Vorab",
    "text": "8.1 Vorab\n\n8.1.1 Lernziele\n\nSie k√∂nnen grundlegende Verfahren zur Klassifikation von Hatespeech einsetzen und erkl√§ren\nSie k√∂nnen mit echten Daten umgehen im Sinne eines Projektmanagement von Data Science\n\n\n\n8.1.2 Ben√∂tigte R-Pakete\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(easystats)\nlibrary(tidytext)\nlibrary(textrecipes)\nlibrary(tictoc)  # Zeitmessung\nlibrary(beepr)  # piebt, wenn fertig\nlibrary(remoji)  # Emojis\nlibrary(feather)  # Daten speichern\nlibrary(pradadata)  # Hilfsdaten wie Schimpfwoerter\nlibrary(lubridate)  # Datum und Zeit\nlibrary(tokenizers)\nlibrary(feather)  # feather data\nlibrary(pradadata)  # helper data\nlibrary(remoji)  # processing emojis"
  },
  {
    "objectID": "hatespeech2.html#daten",
    "href": "hatespeech2.html#daten",
    "title": "8¬† Fallstudie Hatespeech",
    "section": "8.2 Daten",
    "text": "8.2 Daten\n\n8.2.1 Train- und Testdaten\n\nd1 <- read_rds(\"objects/d1.rds\")  # Traindaten einlesen\n\nIn Train- und Test-Datensatz aufsplitten:\n\nd_split <- initial_split(d1, strata = c1)\n\nd_train <- training(d_split)\nd_test <- testing(d_split)\n\n\n\n8.2.2 Vorhersagedaten\nWir importieren die Tweets f√ºhrender deutscher Politikis.\nF√ºr diese Daten haben wir keine Werte der Zielvariablen. Wir k√∂nnen nur vorhersagen, aber nicht unsere Modellg√ºte berechnen. Diese Daten bezeichnen wir als Vorhersagedaten.\nPfad zu den Daten:\n\ntweet_data_path <- \"/Users/sebastiansaueruser/github-repos/hate-speech/data-raw/tweets-small\"\n\nfile.exists(tweet_data_path)\n\n[1] TRUE\n\n\nDie Nutzungsrechte von Twitter erlauben nicht, diese Daten √∂ffentlich zu teilen.\n\ntweet_data_files_names <-\n  list.files(\n    path = tweet_data_path,\n    full.names = TRUE,\n    pattern = \".rds\")\n\n\nnames(tweet_data_files_names) <-  \n  list.files(\n    path = tweet_data_path,\n    full.names = FALSE,\n    pattern = \".rds\") %>% \n  str_remove(\".rds$\") %>% \n  str_remove(\"^tweets-to-\")\n\ntweet_data_files_names\n\n                                                                                                   BMWK_2021 \n          \"/Users/sebastiansaueruser/github-repos/hate-speech/data-raw/tweets-small/tweets-to-BMWK_2021.rds\" \n                                                                                         Janine_Wissler_2021 \n\"/Users/sebastiansaueruser/github-repos/hate-speech/data-raw/tweets-small/tweets-to-Janine_Wissler_2021.rds\" \n                                                                                         Janine_Wissler_2022 \n\"/Users/sebastiansaueruser/github-repos/hate-speech/data-raw/tweets-small/tweets-to-Janine_Wissler_2022.rds\" \n\n\nSo lesen wir alle Dateien aus diesem Ordner ein. Zun√§chst erstellen wir uns eine Helper-Funktion:\n\nsource(\"funs/read-and-select.R\")\n\nDie Funktion read_and_select mappen wir auf alle Datendateien:\n\ntic()\nds <-\n  tweet_data_files_names %>% \n  map_dfr(read_and_select, .id = \"dataset\")\n\nData file was read.\nData file was read.\nData file was read.\n\ntoc()\n\n1.73 sec elapsed\n\n\nEin Blick zur Probe:\n\nds %>% \n  glimpse()\n\nRows: 10,310\nColumns: 9\n$ dataset       <chr> \"BMWK_2021\", \"BMWK_2021\", \"BMWK_2021\", \"BMWK_2021\", \"BMW‚Ä¶\n$ id            <chr> \"1476982045268185091\", \"1476948509706407942\", \"147694476‚Ä¶\n$ author_id     <chr> \"749510675811139585\", \"146337393\", \"841768687245918208\",‚Ä¶\n$ created_at    <chr> \"2021-12-31T18:22:15.000Z\", \"2021-12-31T16:08:59.000Z\", ‚Ä¶\n$ text          <chr> \"@BMWi_Bund @twittlik @Pendolino70 @nextmove_de Richtig.‚Ä¶\n$ retweet_count <int> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ reply_count   <int> 0, 2, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,‚Ä¶\n$ like_count    <int> 0, 0, 1, 0, 0, 1, 1, 3, 3, 0, 3, 0, 0, 1, 0, 0, 1, 2, 1,‚Ä¶\n$ quote_count   <int> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n\n\nDa wir den Elementen von tweet_data_files_names Namen gegeben haben, finden wir diese Namen praktischerweise wieder in ds.\n\n\n\nEine Alternative zum Format RDS besteht im Format Feather:\n\nFeather: fast, interoperable data frame storage Feather provides binary columnar serialization for data frames. It is designed to make reading and writing data frames efficient, and to make sharing data across data analysis languages easy.\n\n\n\n8.2.3 Worteinbettungen\nWie in Kapitel¬†7.7.1 dargestellt, importieren wir unser FastText-Modell.\n\nword_embedding_twitter <- read_rds(file = \"/Users/sebastiansaueruser/datasets/Twitter/word_embedding_twitter.rds\")\n\nWie viel Speicher ben√∂tigt das Worteinbettungsobjekt?\n\nformat(object.size(word_embedding_twitter), units = \"Mb\")\n\n[1] \"108.3 Mb\"\n\n\n\n\n8.2.4 Hilfsdaten\n\ndata(\"schimpwoerter\")\ndata(\"sentiws\")\ndata(\"wild_emojis\")"
  },
  {
    "objectID": "hatespeech2.html#aufbereiten-der-vorhersagedaten",
    "href": "hatespeech2.html#aufbereiten-der-vorhersagedaten",
    "title": "8¬† Fallstudie Hatespeech",
    "section": "8.3 Aufbereiten der Vorhersagedaten",
    "text": "8.3 Aufbereiten der Vorhersagedaten\n\n8.3.1 Hilfsfunktionen\n\nsource(\"funs/helper-funs-recipes.R\")"
  },
  {
    "objectID": "hatespeech2.html#rezept",
    "href": "hatespeech2.html#rezept",
    "title": "8¬† Fallstudie Hatespeech",
    "section": "8.4 Rezept",
    "text": "8.4 Rezept\nDa wir schon ein Rezept ‚Äútrainiert‚Äù haben, k√∂nnen wir die Test-Daten einfach mit dem Rezept ‚Äúbacken‚Äù.\nStreng genommen m√ºssten wir nicht mal das tun, denn tidymodels w√ºrde das beim Vorhersagen f√ºr uns √ºbernehmen. Aber es ist n√ºtzlich, die Daten in aufbereiteter Form zu sehen, bzw. sie direkt zug√§nglich zu haben.\n\nrec2 <- \n  recipe(c1 ~ ., data = select(d_train, text, c1, id)) %>% \n  update_role(id, new_role = \"id\") %>% \n  step_text_normalization(text) %>% \n  step_mutate(text_copy = text,\n              profane_n = map_int(text_copy, count_profane, profane_list = schimpfwoerter$word),\n              emo_words_n = map_int(text_copy, count_emo_words, emo_list = sentiws$word),\n              emojis_n = map_int(text_copy, count_emojis, emoji_list = emoji(list_emoji(), pad = FALSE)),\n              wild_emojis_n = map_int(text_copy, count_wild_emojis, wild_emoji_list = wild_emojis$emojis)\n  ) %>% \n  step_textfeature(text_copy) %>% \n  step_tokenize(text, token = \"tweets\") %>% \n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %>% \n  step_word_embeddings(text, embeddings = word_embedding_twitter)\n \nrec2\n\n\n8.4.1 Preppen und Backen\nPreppen:\n\ntic()\nrec2_prepped <- prep(rec2)\ntoc()\n\n29.377 sec elapsed\nBraucht ganz sch√∂n Zeit ‚Ä¶\nZur Sicherheit speichern wir auch dieses Objekt ab.\n\n# write_rds(rec2_prepped, \"objects/rec2_prepped.rds\")\nrec2_prepped <- read_rds(\"/Users/sebastiansaueruser/datasets/Twitter/hate-classific/rec2_prepped.rds\")\n\nAls n√§chstes kommt das Backen der Vorhersagedaten. Das ist die Stelle, an der zum ersten Mal die neuen Daten (die Vorhersagedaten) ins Spiel kommen.\n\ntic()\nd_predict_baken <-\n  bake(rec2_prepped, new_data = ds)\n\nd_predict_baken$id <- ds$id\ntoc()\nbeepr::beep()\n\nPuh, das Backen dauert - bei gro√üen Datens√§tzen - gef√ºhlt ewig! Daher ist das beepen praktisch: Es klingelt, wenn die Berechnung fertig ist.\n\n\n\nZur Erinnerung: d_predict_baken ist der ‚Äúgebackene‚Äù Testdatensatz. Der Testdatensatz also, auf dem die ganzen Operationen der Vorverarbeitung angewandt wurden.\n\n\n8.4.2 Git Large File System\nWenn Sie Ihre Arbeit mit einem Versionierungssystem sch√ºtzen - und Sie sollten es tun - dann verwenden Sie vermutlich Git. Git ist f√ºr Textdateien ausgelegt - was bei Quellcode ja auch Sinn macht, und f√ºr Quellcode ist Git gemacht. Allerdings will man manchmal auch bin√§re Dateien sichern, etwa Daten im RDS-Format. Solche bin√§ren Formante funktionieren nicht wirklich aus der Sicht von Git, sie lassen sich nicht zeilenweise nachverfolgen. Kurz gesagt sollte man sie aus diesem Grund nicht in Git nachverfolgen. Eine bequeme L√∂sung ist dasLarge File System von Github (git lfs), das diese gro√üen Dateien au√üerhalb des Git-Index verwaltet. Trotzdem sieht es f√ºr Nutzis aus wie immer, ist also sehr komfortabel. Dazu ist es n√∂tig, git lfs zu installieren.\n\n\n8.4.3 Metadaten\nMetadaten wieder hinzuf√ºgen:\n\nd_predict2 <-\n  d_predict_baken %>% \n  left_join(ds, by = \"id\") %>% \n  relocate(dataset, id, author_id, created_at, text, retweet_count, reply_count, quote_count, .after = id) %>% \n  mutate(id = as.integer(id))\n\nWarning in mask$eval_all_mutate(quo): NAs introduced by coercion to integer\nrange\n\n\nLeider m√ºssen wir id in Integer umwandeln, das wir dies im Rezept auch so gemacht hatten. Dabei geht die Spalte kaputt, bzw. die Daten werden NA, da die resultierende Integerzahl zu gro√ü f√ºr R ist. Aber nicht so schlimm: Wir f√ºgen sie sp√§ter wieder hinzu.\nSpaltennamen mal anschauen:\n\nnames(d_predict2)[1:33]\n\n [1] \"dataset\"                             \"id\"                                 \n [3] \"author_id\"                           \"created_at\"                         \n [5] \"text\"                                \"retweet_count\"                      \n [7] \"reply_count\"                         \"quote_count\"                        \n [9] \"profane_n\"                           \"emo_words_n\"                        \n[11] \"emojis_n\"                            \"wild_emojis_n\"                      \n[13] \"textfeature_text_copy_n_words\"       \"textfeature_text_copy_n_uq_words\"   \n[15] \"textfeature_text_copy_n_charS\"       \"textfeature_text_copy_n_uq_charS\"   \n[17] \"textfeature_text_copy_n_digits\"      \"textfeature_text_copy_n_hashtags\"   \n[19] \"textfeature_text_copy_n_uq_hashtags\" \"textfeature_text_copy_n_mentions\"   \n[21] \"textfeature_text_copy_n_uq_mentions\" \"textfeature_text_copy_n_commas\"     \n[23] \"textfeature_text_copy_n_periods\"     \"textfeature_text_copy_n_exclaims\"   \n[25] \"textfeature_text_copy_n_extraspaces\" \"textfeature_text_copy_n_caps\"       \n[27] \"textfeature_text_copy_n_lowers\"      \"textfeature_text_copy_n_urls\"       \n[29] \"textfeature_text_copy_n_uq_urls\"     \"textfeature_text_copy_n_nonasciis\"  \n[31] \"textfeature_text_copy_n_puncts\"      \"textfeature_text_copy_politeness\"   \n[33] \"textfeature_text_copy_first_person\""
  },
  {
    "objectID": "hatespeech2.html#vorhersagen",
    "href": "hatespeech2.html#vorhersagen",
    "title": "8¬† Fallstudie Hatespeech",
    "section": "8.5 Vorhersagen",
    "text": "8.5 Vorhersagen\nWir beziehen uns auf das Modell von Kapitel¬†7.8.4.\n\nfit3 <- read_rds(\"/Users/sebastiansaueruser/github-repos/datascience-text/objects/chap_classific_fit3.rds\")\n\nfit3_final_train <- read_rds(\"/Users/sebastiansaueruser/datasets/Twitter/hate-classific/fit3_final_train.rds\")\n\nUnd nutzen dann die predict-Methode von {tidymodels}:\n\ntic()\nd_predicted_values <- predict(fit3_final_train, d_predict2)\ntoc()\nbeep()\n\nPuh, hier ist mein Rechner abgest√ºrzt, als ich es mit ca. 2 Millionen Tweets versucht habe!\nBesser, wir probieren erstmal mit einem winzigen Teil der Daten, ob unsere Funktion ‚Äúim Prinzip‚Äù oder ‚Äúgrunds√§tzlich‚Äù funktioniert:\n\nd_predicted_values_tiny <- predict(fit3_final_train, head(d_predict2))\n\nError:\n! Can't convert `data$id` <integer> to match type of `id` <character>.\n\nd_predicted_values_tiny\n\nError in eval(expr, envir, enclos): object 'd_predicted_values_tiny' not found\n\n\nFunktioniert! Gut! Also weiter.\nPasst!"
  },
  {
    "objectID": "hatespeech2.html#ergebnisse",
    "href": "hatespeech2.html#ergebnisse",
    "title": "8¬† Fallstudie Hatespeech",
    "section": "8.6 Ergebnisse",
    "text": "8.6 Ergebnisse\n\n8.6.1 Hass-Proxis pro Politiki insgesamt\n\nres_summary1 <- \nd_predict2 %>% \n  group_by(dataset) %>% \n  summarise(emo_words_n_mean = mean(emo_words_n),\n            profane_words_count_mean = mean(profane_n),\n            wild_emojis_n_mean = mean(wild_emojis_n),\n            exclaims_n_mean = mean(textfeature_text_copy_n_exclaims))\n\n\nres_summary1_long <-\n  res_summary1 %>% \n    pivot_longer(-dataset, names_to = \"hate_proxy\", values_to = \"prop\")\n\n\nres_summary1_long %>% \n  ggplot(aes(x = prop, y = hate_proxy)) +\n  geom_col() +\n  facet_wrap(~ dataset)\n\n\n\n\n\n\n8.6.2 Hass-Proxis pro Politiki im Zeitverlauf\n\nres_summary2 <- \nd_predict2 %>%\n  select(created_at, profane_n, dataset, emo_words_n, wild_emojis_n, textfeature_text_copy_n_exclaims) %>% \n  mutate(month = ymd_hms(created_at) %>% round_date(unit = \"month\")) %>% \n  group_by(month, dataset) %>% \n  summarise(emo_words_n_mean = mean(emo_words_n),\n            profane_words_count_mean = mean(profane_n),\n            wild_emojis_n_mean = mean(wild_emojis_n),\n            exclaims_n_mean = mean(textfeature_text_copy_n_exclaims)) %>% \n  rowwise() %>% \n  mutate(hate_proxy = mean(c_across(emo_words_n_mean:exclaims_n_mean))) %>% \n  ungroup()\n\n`summarise()` has grouped output by 'month'. You can override using the\n`.groups` argument.\n\nres_summary2 %>% \n  head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonth\ndataset\nemo_words_n_mean\nprofane_words_count_mean\nwild_emojis_n_mean\nexclaims_n_mean\nhate_proxy\n\n\n\n\n2021-04-01\nJanine_Wissler_2021\n0\n0\n0\n0\n0.00\n\n\n2021-05-01\nJanine_Wissler_2021\n1\n0\n0\n0\n0.25\n\n\n2021-06-01\nJanine_Wissler_2021\n1\n0\n0\n0\n0.25\n\n\n2021-09-01\nJanine_Wissler_2021\n0\n0\n0\n0\n0.00\n\n\n2021-10-01\nJanine_Wissler_2021\n0\n0\n0\n0\n0.00\n\n\n2021-11-01\nJanine_Wissler_2021\n0\n0\n0\n0\n0.00\n\n\n\n\n\n\nLangifizieren f√ºrs Plotten:\n\nres_summary2_long <- \n  res_summary2 %>% \n  pivot_longer(emo_words_n_mean:hate_proxy)\n\nres_summary2_long %>% \n  head()\n\n\n\n\n\nmonth\ndataset\nname\nvalue\n\n\n\n\n2021-04-01\nJanine_Wissler_2021\nemo_words_n_mean\n0\n\n\n2021-04-01\nJanine_Wissler_2021\nprofane_words_count_mean\n0\n\n\n2021-04-01\nJanine_Wissler_2021\nwild_emojis_n_mean\n0\n\n\n2021-04-01\nJanine_Wissler_2021\nexclaims_n_mean\n0\n\n\n2021-04-01\nJanine_Wissler_2021\nhate_proxy\n0\n\n\n2021-05-01\nJanine_Wissler_2021\nemo_words_n_mean\n1\n\n\n\n\n\n\n\nres_summary2_long %>% \n  count(month)\n\n\n\n\n\nmonth\nn\n\n\n\n\n2021-04-01\n5\n\n\n2021-05-01\n5\n\n\n2021-06-01\n5\n\n\n2021-09-01\n5\n\n\n2021-10-01\n5\n\n\n2021-11-01\n5\n\n\n2021-12-01\n5\n\n\n2022-06-01\n5\n\n\n2022-09-01\n5\n\n\n2022-11-01\n5\n\n\nNA\n5\n\n\n\n\n\n\n\nres_summary2_long %>% \n  ggplot() +\n  aes(x = month, y = value) +\n  facet_grid(dataset  ~ name) +\n  geom_point() +\n  geom_line(group=1, alpha = .7)\n\nWarning: Removed 5 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 5 rows containing missing values (`geom_line()`)."
  },
  {
    "objectID": "projektmgt.html",
    "href": "projektmgt.html",
    "title": "9¬† Projektmanagement",
    "section": "",
    "text": "Sie haben Gro√ües vor! Naja, zumindest planen Sie ein neues Data-Science-Projekt.\nUnd, schlau wie Sie sind, st√ºrzen Sie nicht sofort an die Tastatur, um sich einige Modelle berechnen zu lassen. Nein! Sie denken erst einmal nach. Zum Beispiel, wie die einzelnen Analyseschritte aussehen, worin sie bestehen, und in welcher Abfolge sie zu berechnen sind, s. ?fig-projekt1.\nSo k√∂nnte Ihr Projektplan am Anfang aussehen, man spricht auch von einer Pipeline"
  },
  {
    "objectID": "projektmgt.html#sie-tr√§umen-von-einem-werkzeug",
    "href": "projektmgt.html#sie-tr√§umen-von-einem-werkzeug",
    "title": "9¬† Projektmanagement",
    "section": "9.2 Sie tr√§umen von einem Werkzeug",
    "text": "9.2 Sie tr√§umen von einem Werkzeug\nNach einiger Zeit √ºberlegen Sie sich, dass Sie ein System br√§uchten, das Ihre Skizze umsetzt in tats√§chliche Berechnungen. Und zwar suchen Sie ein Projektmanagement-System das folgendes Desiderata erf√ºllt:\n\nEs f√ºhrt die einzelnen Schritte Ihres Projekt, die ‚ÄúPipeline‚Äù in der richtigen Reihenfolge\nEs aktualisiert veraltete Objekte, aber es berechnet nicht Modelle neu, die unver√§ndert sind\nEs ist gut zu debuggen\n\nJa, von so einem Werkzeug tr√§umen Sie.\nUnd tats√§chlich, Ihr Traum geht in Erf√ºllung. Dieses System existiert. Genau genommen gibt es viele Systeme, die sich anschicken, Ihre W√ºnsche zu erf√ºllen. Wir schauen uns eines n√§her an, das speziell f√ºr R gemacht ist. Das R-Paket targets."
  },
  {
    "objectID": "projektmgt.html#targets",
    "href": "projektmgt.html#targets",
    "title": "9¬† Projektmanagement",
    "section": "9.3 Targets",
    "text": "9.3 Targets\nEs lohnt sich, an dieser Stelle den ‚ÄúWalkthrough‚Äù aus dem Benutzerhandbuch von Targets durchzuarbeiten.\nF√ºr ein Projekt √§hnlich zu den, die wir in diesem Buch bearbeiten, ist folgende _targets.R-Datei ein guter Start.\n\nlibrary(targets)\n\n\n# Funktionen einlesen:\n#purrr::walk(list.files(path = \"funs\", pattern = \".R\", full.names = TRUE), source)\nsource(\"funs/def-recipe.R\")\nsource(\"funs/read-train-data.R\")\nsource(\"funs/read-test-data.R\")\n\n# Optionen, z.B. allgemein verf√ºgbare Pakete in den Targets:tar_option_set(packages = c(\"readr\", \n                            \"dplyr\", \n                            \"ggplot2\", \n                            \"purrr\", \n                            \"easystats\", \n                            \"tidymodels\", \n                            \"textrecipes\"))\n\n# Definition der Pipeline:\nlist(\n  tar_target(data_train, read_train_data()),\n  tar_target(data_test, read_test_data()),\n  tar_target(recipe1, def_recipe(data_train)\n  ),\n  tar_target(model1,\n             logistic_reg(penalty = tune(), mixture = 1) %>%\n               set_mode(\"classification\") %>%\n               set_engine(\"glmnet\")\n             ),\n  tar_target(workflow1,\n             workflow() %>% add_recipe(recipe1) %>% add_model(model1)\n             ),\n  tar_target(grid1,\n             grid_regular(penalty(), levels = 3)\n             ),\n  tar_target(grid_fitted,\n             tune_grid(workflow1, \n                       resamples = vfold_cv(data_train, v = 2),\n                       grid = grid1)\n  ),\n  tar_target(best_hyperparams,\n             select_by_one_std_err(grid_fitted, metric = \"roc_auc\", penalty)\n             ),\n  tar_target(fit1,\n             workflow1 %>% finalize_workflow(best_hyperparams) %>% fit(data_train)),\n  tar_target(preds,\n             fit1 %>% \n               predict(data_test) %>% \n               bind_cols(data_test) %>% \n               mutate(c1 = factor(c1))),\n  tar_target(metrics1,\n             preds %>% metrics(truth = c1, .pred_class))\n)\n\nDann kann man auf den Play-Button dr√ºcken und die ganze Pipeline wird berechnet:\n\ntar_make()\n\nWenn die Pipeline aktuell ist, und nichts berechnet werden muss (und daher auch schon fehlerfrei durchgelaufen ist), sieht die Ausgabe so aus:\n‚úî skip target grid1\n‚úî skip target model1\n‚úî skip target data_train\n‚úî skip target data_test\n‚úî skip target recipe1\n‚úî skip target workflow1\n‚úî skip target grid_fitted\n‚úî skip target best_hyperparams\n‚úî skip target fit1\n‚úî skip target preds\n‚úî skip target metrics1\n‚úî skip pipeline [0.121 seconds]\nDie Pipeline kann man sich als DAG bzw. als Abh√§ngigkeitsgraph visualisieren lassen:\n\ntar_visnetwork()\n\n\n\n\nAbh√§ngigkeitsgraph der Pipeline\n\n\nEinzelne Objekte kann man sich komfortabel anschauen mit tar_load(objekt), z.B. tar_load(fit1) usw."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Hvitfeldt, Emil, and Julia Silge. 2022. Supervised Machine Learning\nfor Text Analysis in r. 1st ed. Boca Raton: Chapman;\nHall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nK√∂nig, Tim, Wolf J. Sch√ºnemann, Alexander Brand, Julian Freyberg, and\nMichael Gertz. 2022. ‚ÄúThe EPINetz Twitter Politicians\nDataset 2021. A¬†New Resource for the Study of the German Twittersphere\nand Its Application for the 2021 Federal Elections.‚Äù\nPolitische Vierteljahresschrift 63 (3): 529‚Äì47. https://doi.org/10.1007/s11615-022-00405-7.\n\n\nKurz, A. Solomon. 2021. Statistical Rethinking with Brms, Ggplot2,\nand the Tidyverse: Second Edition. https://bookdown.org/content/4857/.\n\n\nLex, Alexander, Nils Gehlenborg, Hendrik Strobelt, Romain Vuillemot, and\nHanspeter Pfister. 2014. ‚ÄúUpSet: Visualization of\nIntersecting Sets.‚Äù IEEE Transactions on\nVisualization and Computer Graphics 20 (12): 1983‚Äì92. https://doi.org/10.1109/TVCG.2014.2346248.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course\nwith Examples in r and Stan. 2nd ed. CRC Texts in\nStatistical Science. Boca Raton: Taylor; Francis, CRC\nPress.\n\n\nM√ºller, Andreas. 2015. ‚ÄúAnalyse von\nWort-Vektoren deutscher Textkorpora.‚Äù Bachelor's Thesis,\nTechnische Universit√§t Berlin. https://devmount.github.io/GermanWordEmbeddings.\n\n\nPennington, Jeffrey, Richard Socher, and Christopher Manning. 2014.\n‚ÄúGloVe: Global Vectors for Word\nRepresentation.‚Äù In Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), 1532‚Äì43. Doha, Qatar: Association for\nComputational Linguistics. https://doi.org/10.3115/v1/D14-1162.\n\n\nRemus, Robert, Uwe Quasthoff, and Gerhard Heyer. 2010.\n‚ÄúSentiWS - a Publicly Available German-Language\nResource for Sentiment Analysis.‚Äù Proceedings of the 7th\nInternational Language Ressources and Evaluation\n(LREC‚Äô10), 1168‚Äì71.\n\n\nShannon, C. E. 1948. ‚ÄúA Mathematical Theory of\nCommunication.‚Äù Bell System Technical Journal 27 (3):\n379‚Äì423. https://doi.org/10.1002/j.1538-7305.1948.tb01338.x.\n\n\nWickham, Hadley, and Garrett Grolemund. 2018. R F√ºr Data Science:\nDaten Importieren, Bereinigen, Umformen, Modellieren Und\nVisualisieren. Translated by Frank Langenau. 1. Auflage.\nHeidelberg: O‚ÄôReilly. https://r4ds.had.co.nz/index.html.\n\n\nWiegand, Michael. 2019b. ‚ÄúGermEval-2018 Corpus\n(DE).‚Äù heiDATA. https://doi.org/10.11588/DATA/0B5VML.\n\n\n‚Äî‚Äî‚Äî. 2019a. ‚ÄúGermEval-2018 Corpus\n(DE).‚Äù heiDATA. https://doi.org/10.11588/DATA/0B5VML.\n\n\n‚Äî‚Äî‚Äî. 2019c. ‚ÄúGermEval-2018-Data-Master.‚Äù In\nGermEval-2018 Corpus (DE).\nheiDATA. https://doi.org/10.11588/data/0B5VML/XIUWJ7."
  }
]