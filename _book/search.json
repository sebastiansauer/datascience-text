[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science 2: Textdaten als Grundlage pr√§diktiver Modelle",
    "section": "",
    "text": "falls Sie die Pakete schon installiert haben, k√∂nnten Sie mal in RStudio auf ‚Äúupdate.packages‚Äù klicken‚Ü©Ô∏é"
  },
  {
    "objectID": "pruefung.html",
    "href": "pruefung.html",
    "title": "1¬† Pr√ºfung",
    "section": "",
    "text": "Text als Datenbasis pr√§diktiver Modelle\nBild von mcmurryjulie auf Pixabay"
  },
  {
    "objectID": "pruefung.html#pr√ºfungsform-datenanalyse-als-quarto-blog-post",
    "href": "pruefung.html#pr√ºfungsform-datenanalyse-als-quarto-blog-post",
    "title": "1¬† Pr√ºfung",
    "section": "1.1 Pr√ºfungsform: Datenanalyse als Quarto-Blog-Post",
    "text": "1.1 Pr√ºfungsform: Datenanalyse als Quarto-Blog-Post\nAls Pr√ºfungsleistung ist ein Corpus an Twitter-Daten, die an deutsche, aktuelle Politiker gerichtet sind, auf Hate Speech hin zu untersuchen.\n\nDer Dozent wei√üt jedis Studenti einen deutschen Politiker (bzw. dessen Twitter-Account) zu.\nDer Bericht der Analyse ist als Quarto Blog-Posts zu formatieren.\nEinzureichen ist die URL des Posts.\nDer Post muss w√§hrend des gesamten Pr√ºfungszeitraums online sein, gehostet von einem beliebigen Provider (z.B. Netlify oder Github).\nNach Einreichen des Posts d√ºrfen keine √Ñnderungen mehr vorgenommen werden.\nZu Dokumentationszwecken soll ein PDF-Print des Posts in die Abgabe mit hochgeladen werden. Das PDF-Print des Posts muss identisch (exakt gleich) sein zum Post, der √ºber die URL verf√ºgbar ist.\nDer Quelltext des Posts soll bei Github vorliegen.\nDie Methoden des Textminings aus dem Unterricht sollen angewendet werden\nZus√§tzlich d√ºrfen sonstige Techniken des Textminings (die nicht im Unterricht behandelt wurden), angewendet werden\nDar√ºber hinaus sollen pr√§diktive Modelle zur Klassifikation von Hate-Speech (ja/nein) berechnet werden.\nEin Trainingsdatensatz wird gemeinsam erstellt.\nMethoden der Inferenzstatistik (wie Bayes) sind nicht n√∂tig.\nEs soll eine mittlere vierstellige Zahl an Tweets verarbeitet werden oder wenigstens so viele Tweets wie verf√ºgbar."
  },
  {
    "objectID": "pruefung.html#politiker-accounts",
    "href": "pruefung.html#politiker-accounts",
    "title": "1¬† Pr√ºfung",
    "section": "1.2 Politiker-Accounts",
    "text": "1.2 Politiker-Accounts\nFolgende Politiker-Accounts k√∂nnten als Pr√ºfungsgegenstand verwendet werden (nach Hinweisen des Dozenten):\n\nOlaf Scholz\nAnnalena Baerbock\nChristian Lindner\nRobert Habeck (bzw. der Account seines Ministeriums)\nCem √ñzdemir\nVolker Wissing\nNancy Faeser\nFriedrich Merz\nBj√∂rn H√∂cke\nSarah Wagenknecht\n\n\n\nAlle folgenden Hinweise gelten nur insoweit Ihre Lehrkraft Ihnen keine anders lautenden Hinweise gegeben hat (schriftlich).\n\n1.3 Allgemeines\n\nGegenstand dieser Pr√ºfungsform ist die Analyse eines Datensatzes nach einer Forschungsfrage und die Dokumentation dieser Analyse.\nSchreiben Sie Ihre Datenanalyse in Form eines Berichts, der sich an den Gliederungspunkten wie unten dargestellt orientiert.\nWenden Sie die passenden, im Unterricht eingef√ºhrten, statistischen Verfahren an. Es steht Ihnen frei, andere (nicht im Unterricht behandelte) Verfahren zur Analyse der Daten anzuwenden, nach Ma√ügabe der fachlichen Angemessenheit.\nWerten Sie die Daten mit R aus.\nDie R-Syntax soll im Hauptteil des Berichts dokumentiert werden. R-Output darf ggf. gek√ºrzt wiedergegeben werden.\nF√ºgen Sie keine Erkl√§rungen oder Definitionen von statistischen Verfahren an.\nBeschreiben und interpretieren Sie jede Analyse bzw. jeden R-Code bzw. jedes Ergebnis (jede R-Ausgabe).\nVon hoher Bedeutung ist die Korrektheit der Beschreibung und Interpretation der statistischen Modellierung (z.B. mit der Regressionsanalyse).\nEs hat keinen Einfluss auf Ihre Note, ob sich ein (erwarteter) Effekt zeigt und wie stark dieser Effekt ggf. ist.\n\n\n\n1.4 Formatierung des Berichts\n\nDer Bericht ist nur elektronisch, nicht ausgedruckt einzureichen.\nDer Bericht kann als paginiertes Dokument oder als nicht-paginiertes Dokument (d.h. als HTML-Dokument) eingereicht werden.\nPaginierte Formate sind als PDF-Datei einzureichen. HTML-Formate sind entweder in HTML-Form oder (nur bei Bedarf) als PDF-Druck eines HTML-Dokuments einzureichen.\nDie Wahl eines bestimmten Stylesheets ist nicht von Bedeutung. Lesbarkeit und √úbersichtlichkeit in der Formatierung sind unabh√§ngig davon anzustreben.\nIm Kopfbereich (oder auf einem Deckblatt) sind die relevanten Metadatan anzugeben wie Name (Nachname, Vorname) der Autorin/des Autors, Abgabedatum, Titel der Arbeit, Modul.\n\n\n\n1.5 Formalia\n\nRichtlinien einer Wortzahl gibt es nicht. Entscheidend ist, dass relevante Analysen durchgef√ºhrt und beschrieben wurden.\nDer Anspruch richtet sich nach dem Inhalt und Niveau der auf dieses Modul vorbereitenden Module. Oft sind das Module in quantitativer Datenanalyse (und wissenschaftliches Arbeiten). Deren Inhalte sollen im Rahmen dieser Pr√ºfungsleistung als selbst√§ndig und fl√ºssig verf√ºgbare Kompetenz von den Studentis demonstriert werden.\nDie Gliederung der Arbeit kann sich an den PPDAC-Zyklus und am Data Science Model von Wickham und Grolemund orientieren.\n\n\n\n1.6 Beurteilungskriterien\nDie Arbeit wird im Hinblick auf drei Kriterien bewertet:\n\nFormalia (z. B. Vollst√§ndigkeit der Abarbeitung, Angemessenheit der √§u√üeren Gestaltung, Fokus auf Wesentliches)\nMethodik (z. B. Richtige Auswahl und Anwendung der Verfahren)\nInhalt (z. B. Verst√§ndlichkeit, Breite und Tiefe der Probleml√∂sung, Korrektheit der Interpretation)\n\nSie erhalten f√ºr jedes der drei Kriterien eine Teilnote sowie eine Gesamtnote. Au√üerdem erhalten Sie ggf. f√ºr die Kriterien noch ausformulierte Hinweise.\nDie Gesamtnote muss sich nicht als Mittelwert der Teilnoten ergeben.\nInsbesondere kann eine F√ºnf in einem der Kriterien zum Durchfallen f√ºhren, auch wenn die anderen beiden Kriterien gut oder sehr gut beurteilt wurden.\n\n\n1.7 Beispiele f√ºr Aspekte der Beurteilungskriterien\n\nWurden deskriptive Statistiken (an angemessenen Ort) berichtet?\nWurden Diagramme und Tabellen angemessen eingesetzt?\nWurde Inferenzstatistik (angemessen) eingesetzt?\nWurden Effektst√§rkema√üe (idealerweise mit Konfidenzintervallen dazu) berichtet?\nWurden alle relevanten Informationen f√ºr ein statistisches Verfahren angegeben (z.B. zum gew√§hlten Prior)?\nWurde die Aussagekraft von Modellergebnissen richtig eingesch√§tzt?\nWaren die Schlussfolgerungen, die aus den statistischen Ergebnissen gezogen wurden, angemessen (z. B. wurde erkannt, dass ein Nicht-Verwerfeen einer Hypothese nicht automatisch ein Best√§tigen derselben bedeutet)?\nWurde angemessen gerundet (inkl. konsistente Anzahl von Nachkommastellen)?\nPassen die statistischen Verfahren zu den Hypothesen?\nWurden die Voraussetzungen der statistischen Verfahren gepr√ºft?\nSind die Ergebnisse reproduzierbar (Daten und Syntax eingereicht)?\n\n\n\n1.8 Beispiele f√ºr Fehler\nSchwere Fehler, die zum Durchfallen oder deutlichem Abwerten der Note f√ºhren k√∂nnen, sind z.B.:\n\nfehlende Inferenzstatistik (oder ad√§quatem Ersatz)\nfalsche Interpretation von Posteriori-Verteilungen oder p-Werten\nkeine Angabe von Konfidenzintervallen\nfalsche Interpretation von Konfidenzintervallen\nWahl des falschen Intervalls (Vorhersageintervall vs.¬†Perzentilintervall vs.¬†HDI)\nfalsche Entscheidung zum Hypothesentest auf Basis entsprechender Kennwerte (wie ROPE-Wahrscheinlichkeit oder p-Wert)\nfalsche Wahl des statistischen Verfahrens\nfehlende Deskriptivstatistik\n\nH√§ufige kleinere M√§ngel sind z. B.\n\npixelige Abbildungen\nR-Ausgaben oder R-Syntax als Screenshot\nfehlende Seitenzahlen (nur bei paginierten Formaten, nicht bei HTML)\nun√ºbersichtliche Diagramme\nkein (verlinktes) Inhaltsverzeichnis Ô∏é\nfehlende oder unverst√§ndliche Achsenbeschriftung bei Diagrammen\nfehlende oder falsche Beschreibung der/des Skalenniveau(s) der untersuchten Variablen"
  },
  {
    "objectID": "pruefung.html#allgemeines",
    "href": "pruefung.html#allgemeines",
    "title": "1¬† Pr√ºfung",
    "section": "1.3 Allgemeines",
    "text": "1.3 Allgemeines\n\nGegenstand dieser Pr√ºfungsform ist die Analyse eines Datensatzes nach einer Forschungsfrage und die Dokumentation dieser Analyse.\nSchreiben Sie Ihre Datenanalyse in Form eines Berichts, der sich an den Gliederungspunkten wie unten dargestellt orientiert.\nWenden Sie die passenden, im Unterricht eingef√ºhrten, statistischen Verfahren an. Es steht Ihnen frei, andere (nicht im Unterricht behandelte) Verfahren zur Analyse der Daten anzuwenden, nach Ma√ügabe der fachlichen Angemessenheit.\nWerten Sie die Daten mit R aus.\nDie R-Syntax soll im Hauptteil des Berichts dokumentiert werden. R-Output darf ggf. gek√ºrzt wiedergegeben werden.\nF√ºgen Sie keine Erkl√§rungen oder Definitionen von statistischen Verfahren an.\nBeschreiben und interpretieren Sie jede Analyse bzw. jeden R-Code bzw. jedes Ergebnis (jede R-Ausgabe).\nVon hoher Bedeutung ist die Korrektheit der Beschreibung und Interpretation der statistischen Modellierung (z.B. mit der Regressionsanalyse).\nEs hat keinen Einfluss auf Ihre Note, ob sich ein (erwarteter) Effekt zeigt und wie stark dieser Effekt ggf. ist."
  },
  {
    "objectID": "pruefung.html#formatierung-des-berichts",
    "href": "pruefung.html#formatierung-des-berichts",
    "title": "1¬† Pr√ºfung",
    "section": "1.4 Formatierung des Berichts",
    "text": "1.4 Formatierung des Berichts\n\nDer Bericht ist nur elektronisch, nicht ausgedruckt einzureichen.\nDer Bericht kann als paginiertes Dokument oder als nicht-paginiertes Dokument (d.h. als HTML-Dokument) eingereicht werden.\nPaginierte Formate sind als PDF-Datei einzureichen. HTML-Formate sind entweder in HTML-Form oder (nur bei Bedarf) als PDF-Druck eines HTML-Dokuments einzureichen.\nDie Wahl eines bestimmten Stylesheets ist nicht von Bedeutung. Lesbarkeit und √úbersichtlichkeit in der Formatierung sind unabh√§ngig davon anzustreben.\nIm Kopfbereich (oder auf einem Deckblatt) sind die relevanten Metadatan anzugeben wie Name (Nachname, Vorname) der Autorin/des Autors, Abgabedatum, Titel der Arbeit, Modul."
  },
  {
    "objectID": "pruefung.html#formalia",
    "href": "pruefung.html#formalia",
    "title": "1¬† Pr√ºfung",
    "section": "1.5 Formalia",
    "text": "1.5 Formalia\n\nRichtlinien einer Wortzahl gibt es nicht. Entscheidend ist, dass relevante Analysen durchgef√ºhrt und beschrieben wurden.\nDer Anspruch richtet sich nach dem Inhalt und Niveau der auf dieses Modul vorbereitenden Module. Oft sind das Module in quantitativer Datenanalyse (und wissenschaftliches Arbeiten). Deren Inhalte sollen im Rahmen dieser Pr√ºfungsleistung als selbst√§ndig und fl√ºssig verf√ºgbare Kompetenz von den Studentis demonstriert werden.\nDie Gliederung der Arbeit kann sich an den PPDAC-Zyklus und am Data Science Model von Wickham und Grolemund orientieren."
  },
  {
    "objectID": "pruefung.html#beurteilungskriterien",
    "href": "pruefung.html#beurteilungskriterien",
    "title": "1¬† Pr√ºfung",
    "section": "1.6 Beurteilungskriterien",
    "text": "1.6 Beurteilungskriterien\nDie Arbeit wird im Hinblick auf drei Kriterien bewertet:\n\nFormalia (z. B. Vollst√§ndigkeit der Abarbeitung, Angemessenheit der √§u√üeren Gestaltung, Fokus auf Wesentliches)\nMethodik (z. B. Richtige Auswahl und Anwendung der Verfahren)\nInhalt (z. B. Verst√§ndlichkeit, Breite und Tiefe der Probleml√∂sung, Korrektheit der Interpretation)\n\nSie erhalten f√ºr jedes der drei Kriterien eine Teilnote sowie eine Gesamtnote. Au√üerdem erhalten Sie ggf. f√ºr die Kriterien noch ausformulierte Hinweise.\nDie Gesamtnote muss sich nicht als Mittelwert der Teilnoten ergeben.\nInsbesondere kann eine F√ºnf in einem der Kriterien zum Durchfallen f√ºhren, auch wenn die anderen beiden Kriterien gut oder sehr gut beurteilt wurden."
  },
  {
    "objectID": "pruefung.html#beispiele-f√ºr-aspekte-der-beurteilungskriterien",
    "href": "pruefung.html#beispiele-f√ºr-aspekte-der-beurteilungskriterien",
    "title": "1¬† Pr√ºfung",
    "section": "1.7 Beispiele f√ºr Aspekte der Beurteilungskriterien",
    "text": "1.7 Beispiele f√ºr Aspekte der Beurteilungskriterien\n\nWurden deskriptive Statistiken (an angemessenen Ort) berichtet?\nWurden Diagramme und Tabellen angemessen eingesetzt?\nWurde Inferenzstatistik (angemessen) eingesetzt?\nWurden Effektst√§rkema√üe (idealerweise mit Konfidenzintervallen dazu) berichtet?\nWurden alle relevanten Informationen f√ºr ein statistisches Verfahren angegeben (z.B. zum gew√§hlten Prior)?\nWurde die Aussagekraft von Modellergebnissen richtig eingesch√§tzt?\nWaren die Schlussfolgerungen, die aus den statistischen Ergebnissen gezogen wurden, angemessen (z. B. wurde erkannt, dass ein Nicht-Verwerfeen einer Hypothese nicht automatisch ein Best√§tigen derselben bedeutet)?\nWurde angemessen gerundet (inkl. konsistente Anzahl von Nachkommastellen)?\nPassen die statistischen Verfahren zu den Hypothesen?\nWurden die Voraussetzungen der statistischen Verfahren gepr√ºft?\nSind die Ergebnisse reproduzierbar (Daten und Syntax eingereicht)?"
  },
  {
    "objectID": "pruefung.html#beispiele-f√ºr-fehler",
    "href": "pruefung.html#beispiele-f√ºr-fehler",
    "title": "1¬† Pr√ºfung",
    "section": "1.8 Beispiele f√ºr Fehler",
    "text": "1.8 Beispiele f√ºr Fehler\nSchwere Fehler, die zum Durchfallen oder deutlichem Abwerten der Note f√ºhren k√∂nnen, sind z.B.:\n\nfehlende Inferenzstatistik (oder ad√§quatem Ersatz)\nfalsche Interpretation von Posteriori-Verteilungen oder p-Werten\nkeine Angabe von Konfidenzintervallen\nfalsche Interpretation von Konfidenzintervallen\nWahl des falschen Intervalls (Vorhersageintervall vs.¬†Perzentilintervall vs.¬†HDI)\nfalsche Entscheidung zum Hypothesentest auf Basis entsprechender Kennwerte (wie ROPE-Wahrscheinlichkeit oder p-Wert)\nfalsche Wahl des statistischen Verfahrens\nfehlende Deskriptivstatistik\n\nH√§ufige kleinere M√§ngel sind z. B.\n\npixelige Abbildungen\nR-Ausgaben oder R-Syntax als Screenshot\nfehlende Seitenzahlen (nur bei paginierten Formaten, nicht bei HTML)\nun√ºbersichtliche Diagramme\nkein (verlinktes) Inhaltsverzeichnis Ô∏é\nfehlende oder unverst√§ndliche Achsenbeschriftung bei Diagrammen\nfehlende oder falsche Beschreibung der/des Skalenniveau(s) der untersuchten Variablen"
  },
  {
    "objectID": "twittermining.html",
    "href": "twittermining.html",
    "title": "2¬† Twitter Mining",
    "section": "",
    "text": "Text als Datenbasis pr√§diktiver Modelle\nBild von mcmurryjulie auf Pixabay"
  },
  {
    "objectID": "twittermining.html#vorab",
    "href": "twittermining.html#vorab",
    "title": "2¬† Twitter Mining",
    "section": "2.1 Vorab",
    "text": "2.1 Vorab\n\n2.1.1 Lernziele\n\nTwitterdaten via API von Twitter auslesen\n\n\n\n2.1.2 Vorbereitung\n\nLesen Sie in Hvitfeldt und Silge (2022) Kap. 1.\nLegen Sie sich ein Konto bei Github an.\nLegen Sie sich ein Konto bei Twitter an.\nLesen Sie diesen Artikel zur Anmeldung bei der Twitter API1\n\n\n\n2.1.3 Ben√∂tigte R-Pakete\n\nlibrary(tidyverse)\nlibrary(rtweet)\nlibrary(tweetbotornot)\n\n\n\n\nR-Paket {rtweet}\n\n\nEinen √úberblick √ºber die Funktionen des Pakets (function reference) findet sich hier."
  },
  {
    "objectID": "twittermining.html#anmelden-bei-twitter",
    "href": "twittermining.html#anmelden-bei-twitter",
    "title": "2¬† Twitter Mining",
    "section": "2.2 Anmelden bei Twitter",
    "text": "2.2 Anmelden bei Twitter\n\n2.2.1 Welche Accounts interessieren uns?\nHier ist eine (subjektive) Auswahl von deutschen Politikern2, die einen Startpunkt gibt zur Analyse von Art und Ausma√ü von Hate Speech gerichtet an deutsche Politiker:innen.\n\nd_path <- \"data/twitter-german-politicians.csv\"\n\nd <- read_csv(d_path)\nd\n\n\n\n\n\n\n\n\n\n\n\nname\nparty\nscreenname\ncomment\n\n\n\n\nKarl Lauterbach\nSPD\nKarl_Lauterbach\nNA\n\n\nOlaf Scholz\nSPD\nOlafScholz\nNA\n\n\nAnnalena Baerback\nGruene\nABaerbock\nNA\n\n\nBundesministerium f√ºr Wirtschaft und Klimaschutz\nGruene\nBMWK\nRobert Habeck ist der Minister im BMWK\n\n\nFriedrich Merz\nCDU\n_FriedrichMerz\nCDU-Chef\n\n\nMarkus S√∂der\nCSU\nMarkus_Soeder\nCSU-Chef\n\n\nCem √ñzdemir\nGruene\ncem_oezdemir\nBMEL\n\n\nJanine Wissler\nLinke\nJanine_Wissler\nLinke-Chefin\n\n\nMartin Schirdewan\nLinke\nschirdewan\nLinke-Chef\n\n\nChristian Lindner\nFDP\nc_lindner\nFDP-Chef\n\n\nMarie-Agnes Strack-Zimmermann\nFDP\nMAStrackZi\nVorsitzende Verteidigungsausschuss\n\n\nTino Chrupalla\nAFD\nTino_Chrupalla\nAFD-Bundessprecher\n\n\nAlice Weidel\nAFD\nAlice_Weidel\nAFD-Bundessprecherin\n\n\n\n\n\n\n\n\n2.2.2 Twitter App erstellen\nTutorial\n\n\n2.2.3 Intro\nDie Seite von rtweet gibt eine gute Starthilfe in die Benutzung des Pakets.\n\n\n2.2.4 Zugangsdaten\nZugangsdaten sollte man gesch√ºtzt speichern, also z.B. nicht in einem geteilten Ordner.\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\n\nAnmelden:\n\nauth <- rtweet_bot(api_key = api_key,\n                   api_secret = api_secret,\n                   access_token = access_token,\n                   access_secret = access_secret)\n\nAlternativ kann man sich auch als App anmelden, damit kann man z.B. nicht posten, aber daf√ºr mehr herunterladen.\n\nauth <- rtweet_app(bearer_token = Bearer_Token)"
  },
  {
    "objectID": "twittermining.html#tweets-einlesen",
    "href": "twittermining.html#tweets-einlesen",
    "title": "2¬† Twitter Mining",
    "section": "2.3 Tweets einlesen",
    "text": "2.3 Tweets einlesen\nZu beachten ist, dass es Limits gibt, wie viele Informationen (pro Zeiteinheit) man √ºber die Twitter-API auslesen darf. Informationen dazu findet man z.B. hier oder auch mit rate_limit().\nEin g√§ngiges Limit der Twitter-API sind 900 Anfragen (z.B. Tweets auslesen) pro 15 Minuten.\n\n2.3.1 Timeline einlesen einzelner Accounts\nMal ein paar Tweets zur Probe:\n\nsesa_test <- get_timeline(user = \"sauer_sebastian\", n = 3) %>% \n  select(full_text)\n\n\n\nRT @pia_lamberty: Ein Ansatz, der sich beim Debunking wissenschaftlich als erfolgreich herausgestellt hat, ist das sog. Faktensandwich: htt‚Ä¶\nRT @ianbremmer: sure, it‚Äôs the hottest summer europe has ever had in history \n\nbut look at the upside\n\nit‚Äôs one of the coolest summers euro‚Ä¶\nRT @twisteddoodles: Balanced news reporting https://t.co/O1iiItEQrs\n\n\n\ntweets <- get_timeline(user = d$screenname)\nsaveRDS(tweets, file = \"tweets/tweets01.rds\")\n\nMichael Kearney r√§t uns:\n\nPRO TIP #4: (for developer accounts only) Use bearer_token() to increase rate limit to 45,000 per fifteen minutes.\n\n\n\n2.3.2 Retweets einlesen\n\ntweets01_retweets <- \n  tweets$id_str %>% \n  head(3) %>% \n  map_dfr( ~ get_retweets(status_id = .x, retryonratelimit = TRUE))\n\nDa die meisten Retweets aber nix sagen, sondern nur auf das einen Tweet wiederholen, ist das Sammeln der Retweets ziemlich langweilig.\nM√∂chte man retry on rate limit im Standard auf TRUE setzen, so kann man das √ºber die Optionen von R tun.\n\noptions(rtweet.retryonratelimit = TRUE)\n\n\n\n2.3.3 EPINetz Twitter Politicians 2021\nK√∂nig u.¬†a. (2022) Volltext hier haben einen Datensatz mit knapp 2500 Twitter Accounts deutscher Politiker zusammengestellt, zum Stand 2021.\nDer Datensatz kann √ºber Gesis bezogen werden.\nAuf der gleichen Seite findet sich auch eine Dokumentation des Vorgehens.\nNachdem wir den Datensatz heruntergeladen haben, k√∂nnen wir ihn einlesen:\n\npoliticians_path <- \"data/EPINetz_TwitterPoliticians_2021.RDs\"\npoliticians_twitter <- read_rds(politicians_path)\n\nhead(politicians_twitter)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nofficial_name\nparty\nregion\ninstitution\noffice\nuser_id\ntwitter_name\ntwitter_handle\nfrom\nuntil\nyear_of_birth\nabgeordnetenwatch_id\ngender\nwikidata_id\n\n\n\n\n535\nManja Sch√ºle\nSPD\nBrandenburg\nState Parliament\nParliamentarian\n827090742162100224\nManja Sch√ºle\nManjaSchuele\n2019-09-25\nNA\n1976\n146790\nfemale\nQ40974942\n\n\n962\nPetra Pau\nDIE LINKE\nFederal\nFederal Parliament\nParliamentarian\n1683845126\nTeam PetraPau\nTeamPetraPau\n2017-10-24\nNA\n1963\n79091\nfemale\nQ77195\n\n\n864\nDagmar Schmidt\nSPD\nFederal\nFederal Parliament\nParliamentarian\n1377117206\nTeam #dieschmidt\nTeamDieSchmidt\n2017-10-24\nNA\n1973\n79036\nfemale\nQ15433815\n\n\n2517\nBernd Buchholz\nFDP\nSchleswig-Holstein\nState Parliament\nParliamentarian\n1073605033\nBernd Buchholz\nBerndBuchholz\n2017-06-06\nNA\n1961\n121092\nmale\nQ823715\n\n\n1378\nIngrid Remmers\nDIE LINKE\nFederal\nFederal Parliament\nParliamentarian\n551802475\nIngrid Remmers MdB\ningrid_remmers\n2017-10-24\nNA\n1965\n120775\nfemale\nQ1652660\n\n\n1116\nReinhard Brandl\nCSU\nFederal\nFederal Parliament\nParliamentarian\n262730721\nReinhard Brandl\nreinhardbrandl\n2017-10-24\nNA\n1977\n79427\nmale\nQ111160\n\n\n\n\n\n\nDann lesen wir die Timelines (die Tweets) dieser Konten aus; in diesem Beispiel nur 10 Tweets pro Account:\n\nepi_tweets <- get_timeline(user = head(politicians_twitter$twitter_name), n = 10)\nhead(epi_tweets)\n\nNat√ºrlich k√∂nnte man auch mehr als 10 Tweets pro Konto einsammeln, braucht nur seine Zeit.\n\n\n2.3.4 Followers suchen\n\nfollowers01 <-\n  d$screenname %>% \n map_dfr( ~ get_followers(user = .x, retryonratelimit = TRUE))\n\nDa es dauern kann, Daten auszulesen (wir d√ºrfen pro 15 Min. nur eine begrenzte Zahl an Information abrufen), kann es Sinn machen, die Daten lokal zu speichern.\n\nsaveRDS(followers01, file = \"tweets/followers01.rds\")\n\nUnd ggf. wieder importieren:\n\nfollowers01 <- read_rds(file = \"tweets/followers01.rds\")\n\nWie viele unique Followers haben wir identifiziert?\n\nfollowers02 <- \n  followers01 %>% \n  distinct(from_id)\n\nDie Screennames w√§ren noch n√ºtzlich:\n\nlookup_users(users = \"1690868335\")\n\nDie Anzahl der Users, die man nachschauen kann, ist begrenzt auf 180 pro 15 Minuten.\n\nfollowers03 <-\n  followers02 %>% \n  mutate(screenname = \n           list(lookup_users(users = from_id, retryonratelimit = TRUE,verbose = TRUE)))\n\nEntsprechend kann man wieder einlesen:\nDamit haben wir eine Liste an Followers, deren Tweets wir einlesen und analysieren k√∂nnen, z.B. nach Hate Speech.\nIm Gegensatz zu Followers hei√üen bei Twitter die Accounts, denen ei Nutzi folgt ‚ÄúFriends‚Äù.\nLesen wir mal die Followers von karl_lauterbach ein:\n\nkarl_followers <- get_followers(user = \"karl_lauterbach\", verbose = TRUE)\n\nUm nicht jedes Mal aufs Neue die Daten herunterzuladen, bietet es sich an, die Daten lokal zu speichern:\n\nwrite_rds(karl_followers, file = \"tweets/karl_followers.rds\",\n          compress = \"gz\")\n\nEntsprechend kann man die Daten dann auch wieder einlesen:\n\nkarl_followers <- read_rds(file = \"tweets/karl_followers.rds\")\n\n\n\n2.3.5 Follower Tweets einlesen\n\nfollowers_tweets <- get_timeline(user = head(followers01$from_id), n = 10)"
  },
  {
    "objectID": "twittermining.html#tweets-verarbeiten",
    "href": "twittermining.html#tweets-verarbeiten",
    "title": "2¬† Twitter Mining",
    "section": "2.4 Tweets verarbeiten",
    "text": "2.4 Tweets verarbeiten\n\n2.4.1 Grundlegende Verarbeitung\nSind die Tweets eingelesen, kann man z.B. eine Sentimentanalyse, s. Kapitel¬†3.2.10, durchf√ºhren, oder schlicht vergleichen, welche Personen welche W√∂rter h√§ufig verwenden, s. Kapitel¬†3.2.3.\n\n\n2.4.2 Bot or not?\nEine interessante Methode, Tweets zu verarbeiten, bietet das R-Paket tweetbotornot von M. Kearney.\nAus der Readme:\n\nDue to Twitter‚Äôs REST API rate limits, users are limited to only 180 estimates per every 15 minutes. To maximize the number of estimates per 15 minutes (at the cost of being less accurate), use the fast = TRUE argument. This method uses only users-level data, which increases the maximum number of estimates per 15 minutes to 90,000! Due to losses in accuracy, this method should be used with caution!\n\n\nusers <- c(\"sauer_sebastian\")\nbot01 <-\n  tweetbotornot(users)\n\n\n\n\n\n\n\nWichtig\n\n\n\nIch habe ein Fehlermeldung bekommen bei tweetbotornot. Da k√∂nnte ein technisches Problem in der Funktion vorliegen."
  },
  {
    "objectID": "twittermining.html#cron-jobs",
    "href": "twittermining.html#cron-jobs",
    "title": "2¬† Twitter Mining",
    "section": "2.5 Cron Jobs",
    "text": "2.5 Cron Jobs\n\n2.5.1 Was ist ein Cron Job?\nCron ist ein Programm auf Unix-artigen Betriebssystemen, das Skripte zu eingestellten Zeiten (wiederholt) ausf√ºhrt, das sind dann ‚ÄúCron Jobs‚Äù. Auf Windows gibt es aber analoge Funktionen. Cron Jobs sind praktisch, da man nicht jedes Mal selber z.B. Tweets, die heute zu einem Thema getweetet wurden, herunterladen muss. Das wird dann vom Cron Job √ºbernommen.\nIn R gibt es eine API zum Programm Cron mit dem Paket {cronR}, s. Anleitung hier.\nDas analoge R-Paket f√ºr Windows hei√üt {taskscheduleR}.\n\n\n2.5.2 Beispiel f√ºr einen Cron Job\n\nlibrary(cronR)\n\nscrape_script <- cron_rscript(\"scrape_tweets.R\")\n\n# Cron Job hinzuf√ºgen:\ncron_add(command = scrape_script, \n         frequency = 'daily', \n         at = \"10AM\",\n         id = 'Hate Speech')  # Name des Cron Jobs\n\ncron_clear(ask = FALSE)  # Alle Cron Jobs l√∂schen\ncron_ls()  # Liste aller Cron Jobs\n\nIm obigen Beispiel wird das R-Skript scrape_tweets.R t√§glich um 10h ausgef√ºhrt.\nDer Inhalt von scrape_tweets.R k√∂nnte dann, in Grundz√ºgen, so aussehen:\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(rtweet)\nfollowers_lauterbach <-\n  followers01 %>% \n  filter(to_id == \"Karl_Lauterbach\")\n\nfollowers_lauterbach_tweets <- \n  get_timeline(user = followers_lauterbach$from_id[1:10], n = 10, retryonratelimit = TRUE, verbose = FALSE)\n\n\npath_output <- \"/Users/sebastiansaueruser/Google Drive/RRRing/Scrape-Tweets/tweets/\"\n\nwrite_csv(x = followers_lauterbach_tweets,\n          file = paste0(path_output, \"followers_lauterbach_tweets.csv\"),\n          append = TRUE)\n\nWir schreiben nicht jedes Mal (jeden Tag) eine neue CSV-Datei, sondern wir h√§ngen hier die neu ausgelesenen Daten an die Datei an.\nLeider ist es mit rtweet nicht m√∂glich, ein Datum anzugeben, ab dem man Tweets auslesen m√∂chte3"
  },
  {
    "objectID": "twittermining.html#datenbank-an-tweets-aufbauen",
    "href": "twittermining.html#datenbank-an-tweets-aufbauen",
    "title": "2¬† Twitter Mining",
    "section": "2.6 Datenbank an Tweets aufbauen",
    "text": "2.6 Datenbank an Tweets aufbauen\n\n2.6.1 Stamm an bisherigen Tweets\nIn diesem Abschnitt k√ºmmern wir uns in gr√∂√üerem Detail um das Aufbauen einer Tweets-Datenbank.\nDiese Pakete ben√∂tigen wir:\n\nlibrary(rtweet)\nlibrary(tidyverse)\nlibrary(rio)  # R Data import/export\n\nDann melden wir uns an:\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\nauth <- rtweet_app(bearer_token = Bearer_Token)\n\nDann brauchen wir eine Liste an Twitterkonten, die uns interessieren. Im Kontext von Hate Speech soll uns hier interessieren, welche Tweets an deutsche Spitzenpolitikis4 gesendet werden. Wir suchen also nach Tweets mit dem Text @karl_lauterbach, um ein Beispiel f√ºr einen Spitzenpolitiker zu nennen, der vermutlich von Hate Speech in h√∂herem Ma√üe betroffen ist.\n\npoliticians_twitter_path <- \"/Users/sebastiansaueruser/github-repos/datascience-text/data/twitter-german-politicians.csv\"\n\npoliticians_twitter <- rio::import(file = politicians_twitter_path)\n\nIn der Liste befinden sich 13 Politiker. Es macht die Sache vielleicht einfacher, wenn wir die Rate nicht √ºberziehen. Bleiben wir daher bei 1000 Tweets pro Politiki:\n\nn_tweets_per_politician <- 1e3\n\nDie R-Syntax, die die Arbeit leistet, ist in Funktionen ausgelagert, der √úbersichtlichkeit halber.\n\nsource(\"funs/filter_recent_tweets.R\")\nsource(\"funs/download_recent_tweets.R\")\nsource(\"funs/add_tweets_to_tweet_db.R\")\n\n\n\n\nJetzt laden wir einfach die aktuellsten 1000 Tweets pro Konto herunter, daher brauchen wir keine Tweet-ID angeben, die ein Mindest- oder Maximum-Datum (bzw. ID) f√ºr einen Tweet angibt:\n\ntweets_older <-\n  download_recent_tweets(screenname = politicians_twitter$screenname,\n                         max_or_since_id_str = NULL,\n                         n = n_tweets_per_politician,\n                         strip_columns = TRUE,\n                         reverse = TRUE)\n\nWie weit in die Vergangenheit reicht unsere Tweet-Sammlung?\n\noldest_tweets <- filter_recent_tweets(tweets_older, max_or_min_id_str = is_min_id_str)\noldest_tweets\n\n\n\n\n\n\n\n\n\n\n\n\nid_str\nscreenname\ncreated_at\nis_min_id_str\nis_max_id_str\n\n\n\n\n1590754620649115648\nKarl_Lauterbach\n2022-11-10 18:13:51\nTRUE\nFALSE\n\n\n1589837879693352960\nOlafScholz\n2022-11-08 05:31:02\nTRUE\nFALSE\n\n\n1590485330742083585\nABaerbock\n2022-11-10 00:23:47\nTRUE\nFALSE\n\n\n1589983737621942272\nBMWK\n2022-11-08 15:10:38\nTRUE\nFALSE\n\n\n1590646264433373184\n_FriedrichMerz\n2022-11-10 11:03:16\nTRUE\nFALSE\n\n\n1588595577360875520\nMarkus_Soeder\n2022-11-04 19:14:34\nTRUE\nFALSE\n\n\n1590097264613425152\ncem_oezdemir\n2022-11-08 22:41:45\nTRUE\nFALSE\n\n\n1588082031656898560\nJanine_Wissler\n2022-11-03 09:13:56\nTRUE\nFALSE\n\n\n1588082031656898560\nschirdewan\n2022-11-03 09:13:56\nTRUE\nFALSE\n\n\n1590628128791007233\nc_lindner\n2022-11-10 09:51:13\nTRUE\nFALSE\n\n\n1589277825152208898\nMAStrackZi\n2022-11-06 16:25:35\nTRUE\nFALSE\n\n\n1587964349993422848\nTino_Chrupalla\n2022-11-03 01:26:18\nTRUE\nFALSE\n\n\n1589696708447186945\nAlice_Weidel\n2022-11-07 20:10:05\nTRUE\nFALSE\n\n\n\n\n\n\nWas sind die neuesten Tweets, die wir habven?\n\nmost_recent_tweets <- filter_recent_tweets(oldest_tweets)\nmost_recent_tweets\n\n\n\n\n\n\n\n\n\n\n\n\nid_str\nscreenname\ncreated_at\nis_min_id_str\nis_max_id_str\n\n\n\n\n1590754620649115648\nKarl_Lauterbach\n2022-11-10 18:13:51\nTRUE\nTRUE\n\n\n1589837879693352960\nOlafScholz\n2022-11-08 05:31:02\nTRUE\nTRUE\n\n\n1590485330742083585\nABaerbock\n2022-11-10 00:23:47\nTRUE\nTRUE\n\n\n1589983737621942272\nBMWK\n2022-11-08 15:10:38\nTRUE\nTRUE\n\n\n1590646264433373184\n_FriedrichMerz\n2022-11-10 11:03:16\nTRUE\nTRUE\n\n\n1588595577360875520\nMarkus_Soeder\n2022-11-04 19:14:34\nTRUE\nTRUE\n\n\n1590097264613425152\ncem_oezdemir\n2022-11-08 22:41:45\nTRUE\nTRUE\n\n\n1588082031656898560\nJanine_Wissler\n2022-11-03 09:13:56\nTRUE\nTRUE\n\n\n1588082031656898560\nschirdewan\n2022-11-03 09:13:56\nTRUE\nTRUE\n\n\n1590628128791007233\nc_lindner\n2022-11-10 09:51:13\nTRUE\nTRUE\n\n\n1589277825152208898\nMAStrackZi\n2022-11-06 16:25:35\nTRUE\nTRUE\n\n\n1587964349993422848\nTino_Chrupalla\n2022-11-03 01:26:18\nTRUE\nTRUE\n\n\n1589696708447186945\nAlice_Weidel\n2022-11-07 20:10:05\nTRUE\nTRUE\n\n\n\n\n\n\nJetzt laden wir die neueren Tweets herunter, also mit einer ID gr√∂√üer als die gr√∂√üte in unserer Sammlung:\n\n\n\n\ntweets_new <- \n  download_recent_tweets(screenname = most_recent_tweets$screenname,\n                         max_or_since_id_str = most_recent_tweets$id_str)\n\ntweets_new %>% \n  select(screenname, created_at, id_str) %>% \n  head()\n\nJetzt - und jedes Mal, wenn wir Tweets herunterladen - f√ºgen wir diese einer Datenbank (oder zumindest einer ‚ÄúGesamt-Tabelle‚Äù) hinzu:\n\ntweets_db <- add_tweets_to_tweets_db(tweets_new, tweets_older)\n\nnrow(tweets_db)\n\n[1] 10969\n\n\nSchlie√ülich sollten wir nicht vergessen diese in einer Datei zu speichern:\n\nwrite_rds(tweets_db, file = \"~/datasets/Twitter/tweets-db-2022-11-11.rds\")\n\n‚Ä¶ ‚Ä¶ So, einige Zeit ist vergangen. Laden wir noch √§ltere Tweets herunter und f√ºgen Sie unserer Datenbank hinzu:\n\ntweets_older2 <-\n  download_recent_tweets(screenname = politicians_twitter$screenname,\n                         max_or_since_id_str = oldest_tweets$id_str,\n                         n = 1e3,\n                         strip_columns = TRUE,\n                         reverse = TRUE)\n\n\n\n\n\ntweets_db <- add_tweets_to_tweets_db(tweets_new, tweets_older2)\n\nnrow(tweets_db)\n\n[1] 10011\n\n\nUnd wieder speichern wir die vergr√∂√üerte Datenbasis auf der Festplatte:\n\nwrite_rds(tweets_db, file = \"~/datasets/Twitter/hate-speech-twitter.rds\")\n\nLeider ist die Datenbasis nicht mehr deutlich gewachsen. Eine plausible Ursache ist, dass Twitter den Zugriff auf alte Tweets einschr√§nkt.\n\n\n2.6.2 Neue Tweets per Cron Job\nWie oben schon ausprobiert, legen wir uns einen Cron Job an.\n\nlibrary(cronR)\n\nscrape_script <- cron_rscript(\"/Users/sebastiansaueruser/github-repos/datascience-text/funs/get_tweets_politicians.R\")\n\n# Cron Job hinzuf√ºgen:\ncron_add(command = scrape_script, \n         frequency = 'daily', \n         at = \"10AM\",\n         id = 'Hate Speech')  # Name des Cron Jobs\n\nDas Skript get_tweets_politicians.R birgt die Schritte, die wir in diesem Abschnitt ausprobiert haben. Kurz gesagt sucht es nach neuen Tweets, die also noch nicht in Ihrer ‚ÄúDatenbank‚Äù vorhanden sind, und l√§dt diese herunter. Dabei werden maximal 1000 Tweets pro Konto (derer sind es 13) heruntergeladen.\nM√∂chte man den Cron Job wieder l√∂schen, so kann man das so tun:\n\ncron_clear(ask = FALSE)  # Alle Cron Jobs l√∂schen\ncron_ls()  # Liste aller Cron Jobs"
  },
  {
    "objectID": "twittermining.html#aufgaben",
    "href": "twittermining.html#aufgaben",
    "title": "2¬† Twitter Mining",
    "section": "2.7 Aufgaben",
    "text": "2.7 Aufgaben\n\n√úberlegen Sie, wie Sie das Ausma√ü an Hate Speech, dem deutsche Politikerinnen und Politiker konfrontiert sind, messen k√∂nnen.\nArgumentieren Sie die Vorteile und Nachteile Ihres Ansatzes. Au√üerdem, auf welches Ergebnis dieser Analyse sie gespannt sind bzw. w√§ren.\n√úberlegen Sie Korrelate, oder besser noch: (m√∂gliche) Ursachen, des Hasses in den Tweets, gerichtet auf Polikter:innen. Sie k√∂nnen auch Gruppen von Ursachen bilden, etwas personengebundene Variablen der Politiker:innen (z.B. Alter? Geschlecht? Migrationshintergrund?).\nErstellen Sie sich eine Liste an Personen, deren Tweets sich lohnen (k√∂nnten), auf Hate Speech hin analysiert zu werden. Laden Sie deren Tweets (ggf. in Ausz√ºgen) herunter.\nDas Skript zu scrape_tweets.R k√∂nnte man noch verbessern, in dem man jeden Tag nur die neuesten Tweets herunterl√§dt. Dazu kann man bei get_timeline() mit dem Argument since_id eine Untergrenze der ID festlegen, so dass nur neuere Tweets (d.h. mit gr√∂√üerem Wert bei ID) ausgelesen werden. √Ñndern Sie das Skript entsprechend, so dass nur neuerer Tweets gelesen werden.\nErarbeiten Sie die Folien zu diesem rtweet-Workshop. Eine Menge guter Tipps!\n\n\n\n\n\nHvitfeldt, Emil, und Julia Silge. 2022. Supervised Machine Learning for Text Analysis in R. 1. Aufl. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nK√∂nig, Tim, Wolf J. Sch√ºnemann, Alexander Brand, Julian Freyberg, und Michael Gertz. 2022. ‚ÄûThe EPINetz Twitter Politicians Dataset 2021. A¬†New Resource for the Study of the German Twittersphere and Its Application for the 2021 Federal Elections‚Äú. Politische Vierteljahresschrift 63 (3): 529‚Äì47. https://doi.org/10.1007/s11615-022-00405-7."
  },
  {
    "objectID": "textmining1.html",
    "href": "textmining1.html",
    "title": "3¬† Textmining1",
    "section": "",
    "text": "Bild von mcmurryjulie auf Pixabay"
  },
  {
    "objectID": "textmining1.html#vorab",
    "href": "textmining1.html#vorab",
    "title": "3¬† Textmining1",
    "section": "3.1 Vorab",
    "text": "3.1 Vorab\n\n3.1.1 Lernziele\n\nDie vorgestellten Techniken des Textminings mit R anwenden k√∂nnen\n\n\n\n3.1.2 Vorbereitung\n\nLesen Sie in Hvitfeldt und Silge (2022) Kap. 2.\n\n\n\n3.1.3 Ben√∂tigte R-Pakete\n\nlibrary(tidyverse)\nlibrary(tokenizers)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(hcandersenr)\nlibrary(SnowballC)  # Stemming\nlibrary(lsa)  # Stopw√∂rter\nlibrary(easystats)  # Komfort f√ºr deskriptive Statistiken, wie `describe_distribution`\nlibrary(textclean)  # Emojis ersetzen"
  },
  {
    "objectID": "textmining1.html#einfache-methoden-des-textminings",
    "href": "textmining1.html#einfache-methoden-des-textminings",
    "title": "3¬† Textmining1",
    "section": "3.2 Einfache Methoden des Textminings",
    "text": "3.2 Einfache Methoden des Textminings\nArbeiten Sie die folgenden grundlegenden Methoden des Textminigs durch.\n\n3.2.1 Tokenisierung\nErarbeiten Sie dieses Kapitel: Hvitfeldt und Silge (2022), Kap. 2\nWie viele Zeilen hat das M√§rchen ‚ÄúThe Fir tree‚Äù (in der englischen Fassung?)\n\nhcandersen_en %>% \n  filter(book == \"The fir tree\") %>% \n  nrow()\n\n[1] 253\n\n\n\n\n3.2.2 Stopw√∂rter entfernen\nErarbeiten Sie dieses Kapitel: s. Hvitfeldt und Silge (2022), Kap. 3\nEine alternative Quelle von Stopw√∂rtern - in verschiedenen Sprachen - biwetet das Paket quanteda:\n\nstop2 <-\n  tibble(word = quanteda::stopwords(\"german\"))\n\nhead(stop2)\n\n\n\n\n\nword\n\n\n\n\naber\n\n\nalle\n\n\nallem\n\n\nallen\n\n\naller\n\n\nalles\n\n\n\n\n\n\nEs bestehst (in der deutschen Version) aus 231 W√∂rtern.\n\n\n3.2.3 W√∂rter z√§hlen\nIst der Text tokenisiert, kann man einfach mit ‚ÄúBordmitteln‚Äù die W√∂rter z√§hlen.\n\nhcandersen_de %>% \n  filter(book == \"Das Feuerzeug\") %>% \n  unnest_tokens(output = word, input = text) %>% \n  anti_join(stop2) %>% \n  count(word, sort = TRUE) %>% \n  head()\n\nJoining, by = \"word\"\n\n\n\n\n\n\nword\nn\n\n\n\n\nsoldat\n35\n\n\nsagte\n28\n\n\nhund\n23\n\n\nprinzessin\n17\n\n\nhexe\n16\n\n\nfeuerzeug\n14\n\n\n\n\n\n\n\n\n3.2.4 Stemming (Wortstamm finden)\nErarbeiten Sie dieses Kapitel: Hvitfeldt und Silge (2022), Kap. 4\nVertiefende Hinweise zum UpSet plot finden Sie hier, Lex u.¬†a. (2014).\nF√ºr welche Sprachen gibt es Stemming im Paket SnowballC?\n\nlibrary(SnowballC)\ngetStemLanguages()\n\n [1] \"arabic\"     \"basque\"     \"catalan\"    \"danish\"     \"dutch\"     \n [6] \"english\"    \"finnish\"    \"french\"     \"german\"     \"greek\"     \n[11] \"hindi\"      \"hungarian\"  \"indonesian\" \"irish\"      \"italian\"   \n[16] \"lithuanian\" \"nepali\"     \"norwegian\"  \"porter\"     \"portuguese\"\n[21] \"romanian\"   \"russian\"    \"spanish\"    \"swedish\"    \"tamil\"     \n[26] \"turkish\"   \n\n\nEinfacher Test: Suchen wir den Wordstamm f√ºr das Wort ‚Äúwissensdurstigen‚Äù, wie in ‚Äúdie wissensdurstigen Studentis l√∂cherten dis armi Professi‚Äù1.\n\nwordStem(\"wissensdurstigen\", language = \"german\")\n\n[1] \"wissensdurst\"\n\n\nWerfen Sie mal einen Blick in das Handbuch von SnowballC.\n\n\n3.2.5 Fallstudie AfD-Parteiprogramm\nDaten einlesen:\n\nd_link <- \"https://raw.githubusercontent.com/sebastiansauer/pradadata/master/data-raw/afd_2022.csv\"\nafd <- read_csv(d_link, show_col_types = FALSE)\n\nWie viele Seiten hat das Dokument?\n\nnrow(afd)\n\n[1] 190\n\n\nUnd wie viele W√∂rter?\n\nstr_count(afd$text, pattern = \"\\\\w\") %>% sum(na.rm = TRUE)\n\n[1] 179375\n\n\nAus breit mach lang, oder: wir tokenisieren (nach W√∂rtern):\n\nafd %>% \n  unnest_tokens(output = token, input = text) %>% \n  filter(str_detect(token, \"[a-z]\")) -> afd_long\n\nStopw√∂rter entfernen:\n\ndata(stopwords_de, package = \"lsa\")\n\nstopwords_de <- tibble(word = stopwords_de)\n\n# F√ºr das Joinen werden gleiche Spaltennamen ben√∂tigt:\nstopwords_de <- stopwords_de %>% \n  rename(token = word)  \n\nafd_long %>% \n  anti_join(stopwords_de) -> afd_no_stop\n\nJoining, by = \"token\"\n\n\nW√∂rter z√§hlen:\n\nafd_no_stop %>% \n  count(token, sort = TRUE) -> afd_count\n\nhead(afd_count)\n\n\n\n\n\ntoken\nn\n\n\n\n\nafd\n174\n\n\ndeutschland\n113\n\n\nwollen\n66\n\n\neuro\n60\n\n\nb√ºrger\n57\n\n\neu\n54\n\n\n\n\n\n\nW√∂rter trunkieren:\n\nafd_no_stop %>% \n  mutate(token_stem = wordStem(token, language = \"de\")) %>% \n  count(token_stem, sort = TRUE) -> afd_count_stemmed\n\nhead(afd_no_stop)\n\n\n\n\n\npage\ntoken\n\n\n\n\n1\nprogramm\n\n\n1\ndeutschland\n\n\n1\ngrundsatzprogramm\n\n\n1\nalternative\n\n\n1\ndeutschland\n\n\n2\ninhaltsverzeichnis\n\n\n\n\n\n\n\n\n3.2.6 Stringverarbeitung\nErarbeiten Sie dieses Kapitel: Wickham und Grolemund (2018), Kap. 14\n\n3.2.6.1 Regul√§rausdr√ºcke\nDas \"[a-z]\" in der Syntax oben steht f√ºr ‚Äúalle Buchstaben von a-z‚Äù. D iese flexible Art von ‚ÄúString-Verarbeitung mit Jokern‚Äù nennt man Regul√§rausdr√ºcke (regular expressions; regex). Es gibt eine ganze Reihe von diesen Regul√§rausdr√ºcken, die die Verarbeitung von Texten erleichert. Mit dem Paket stringr geht das - mit etwas √úbung - gut von der Hand. Nehmen wir als Beispiel den Text eines Tweets:\n\nstring <- \"Correlation of unemployment and #AfD votes at #btw17: ***r = 0.18***\\n\\nhttps://t.co/YHyqTguVWx\"  \n\nM√∂chte man Ziffern identifizieren, so hilft der Reul√§rausdruck [:digit:]:\n‚ÄúGibt es mindestens eine Ziffer in dem String?‚Äù\n\nstr_detect(string, \"[:digit:]\")\n\n[1] TRUE\n\n\n‚ÄúFinde die Position der ersten Ziffer! Welche Ziffer ist es?‚Äù\n\nstr_locate(string, \"[:digit:]\")\n\n     start end\n[1,]    51  51\n\nstr_extract(string, \"[:digit:]\")\n\n[1] \"1\"\n\n\n‚ÄúFinde alle Ziffern!‚Äù\n\nstr_extract_all(string, \"[:digit:]\")\n\n[[1]]\n[1] \"1\" \"7\" \"0\" \"1\" \"8\"\n\n\n‚ÄúFinde alle Stellen an denen genau 2 Ziffern hintereinander folgen!‚Äù\n\nstr_extract_all(string, \"[:digit:]{2}\")\n\n[[1]]\n[1] \"17\" \"18\"\n\n\nDer Quantit√§tsoperator {n} findet alle Stellen, in der der der gesuchte Ausdruck genau \\(n\\) mal auftaucht.\n‚ÄúZeig die Hashtags!‚Äù\n\nstr_extract_all(string, \"#[:alnum:]+\")\n\n[[1]]\n[1] \"#AfD\"   \"#btw17\"\n\n\nDer Operator [:alnum:] steht f√ºr ‚Äúalphanumerischer Charakter‚Äù - also eine Ziffer oder ein Buchstabe; synonym h√§tte man auch \\\\w schreiben k√∂nnen (w wie word). Warum werden zwei Backslashes gebraucht? Mit \\\\w wird signalisiert, dass nicht der Buchstabe w, sondern etwas Besonderes, eben der Regex-Operator \\w gesucht wird.\n‚ÄúZeig die URLs!‚Äù\n\nstr_extract_all(string, \"https?://[:graph:]+\")\n\n[[1]]\n[1] \"https://t.co/YHyqTguVWx\"\n\n\nDas Fragezeichen ? ist eine Quantit√§tsoperator, der einen Treffer liefert, wenn das vorherige Zeichen (hier s) null oder einmal gefunden wird. [:graph:] ist die Summe von [:alpha:] (Buchstaben, gro√ü und klein), [:digit:] (Ziffern) und [:punct:] (Satzzeichen u.√§.).\n‚ÄúZ√§hle die W√∂rter im String!‚Äù\n\nstr_count(string, boundary(\"word\"))\n\n[1] 13\n\n\n‚ÄúLiefere nur Buchstabenfolgen zur√ºck, l√∂sche alles √ºbrige‚Äù\n\nstr_extract_all(string, \"[:alpha:]+\")\n\n[[1]]\n [1] \"Correlation\"  \"of\"           \"unemployment\" \"and\"          \"AfD\"         \n [6] \"votes\"        \"at\"           \"btw\"          \"r\"            \"https\"       \n[11] \"t\"            \"co\"           \"YHyqTguVWx\"  \n\n\nDer Quantit√§tsoperator + liefert alle Stellen zur√ºck, in denen der gesuchte Ausdruck einmal oder h√§ufiger vorkommt. Die Ergebnisse werden als Vektor von W√∂rtern zur√ºckgegeben. Ein anderer Quantit√§tsoperator ist *, der f√ºr 0 oder mehr Treffer steht. M√∂chte man einen Vektor, der aus Stringen-Elementen besteht zu einem Strring zusammenf√ºngen, hilft paste(string) oder str_c(string, collapse = \" \").\n\nstr_replace_all(string, \"[^[:alpha:]+]\", \"\")\n\n[1] \"CorrelationofunemploymentandAfDvotesatbtwrhttpstcoYHyqTguVWx\"\n\n\nMit dem Negationsoperator [^x] wird der Regul√§rausrck x negiert; die Syntax oben hei√üt also ‚Äúersetze in string alles au√üer Buchstaben durch Nichts‚Äù. Mit ‚ÄúNichts‚Äù sind hier Strings der L√§nge Null gemeint; ersetzt man einen belieibgen String durch einen String der L√§nge Null, so hat man den String gel√∂scht.\nDas Cheatsheet zur Strings bzw zu stringr von RStudio gibt einen guten √úberblick √ºber Regex; im Internet finden sich viele Beispiele.\n\n\n3.2.6.2 Regex im Texteditor\nEinige Texteditoren unterst√ºtzen Regex, so auch RStudio.\nDas ist eine praktische Sache. Ein Beispiel: Sie haben eine Liste mit Namen der Art:\n\nNachname1, Vorname1\nNachname2, Vorname2\nNachname3, Vorname3\n\nUnd Sie m√∂chten jetzt aber die Liste mit Stil Vorname Nachname sortiert haben.\nRStudio mit Regex macht‚Äôs m√∂glich, s. ?fig-regex-rstudio.\n\n\n \nAbbildung¬†3.1: ?(caption)\n\n\n\n\n\n3.2.7 Emoji-Analyse\nEine einfache Art, Emojis in einer Textmining-Analyse zu verarbeiten, bietet das Paket textclean:\n\nfls <- system.file(\"docs/emoji_sample.txt\", package = \"textclean\")\nx <- readLines(fls)[1]\nx\n\n[1] \"Proin üòç ut maecenas üòè condimentum üòî purus eget. Erat, üòÇvitae nunc elit. Condimentum üò¢ semper iaculis bibendum sed tellus. Ut suscipit interdumüòë in. Faucibüòû us nunc quis a vitae posuere. üòõ Eget amet sit condimentum non. Nascetur vitae ‚òπ et. Auctor ornare ‚ò∫ vestibulum primis justo congue üòÄurna ac magna. Quam üò• pharetra üòü eros üòífacilisis ac lectus nibh est üòôvehicula üòê ornare! Vitae, malesuada üòé erat sociosqu urna, üòè nec sed ad aliquet üòÆ .\"\n\n\n\nreplace_emoji(x)\n\n[1] \"Proin smiling face with heart-eyes ut maecenas smirking face condimentum pensive face purus eget. Erat, face with tears of joy vitae nunc elit. Condimentum crying face semper iaculis bibendum sed tellus. Ut suscipit interdum expressionless face in. Faucib disappointed face us nunc quis a vitae posuere. face with tongue Eget amet sit condimentum non. Nascetur vitae frowning face et. Auctor ornare smiling face vestibulum primis justo congue grinning face urna ac magna. Quam sad but relieved face pharetra worried face eros unamused face facilisis ac lectus nibh est kissing face with smiling eyes vehicula neutral face ornare! Vitae, malesuada smiling face with sunglasses erat sociosqu urna, smirking face nec sed ad aliquet face with open mouth .\"\n\nreplace_emoji_identifier(x)\n\n[1] \"Proin lexiconwiutsdotskrupggpgmhm ut maecenas lexiconwizbukzesopzflfinotj condimentum lexiconwlnxqescoesytfatoevi purus eget. Erat, lexiconwcaiviebiytolowkanmb vitae nunc elit. Condimentum lexiconwpujksvgujncexktvyrn semper iaculis bibendum sed tellus. Ut suscipit interdum lexiconwknnasgueiicggptyzbx in. Faucib lexiconwoxfeslcareuqfkbyjgy us nunc quis a vitae posuere. lexiconwobmhqdrrzgygdexhnkk Eget amet sit condimentum non. Nascetur vitae lexiconbfalxvockmnmtmycmwyq et. Auctor ornare lexiconbgmujofaalvxqrklfqgd vestibulum primis justo congue lexiconvygwtlyrpywfarytvfis urna ac magna. Quam lexiconwurhpvewhizayynmfxqo pharetra lexiconwpmuduwgbxxrxeltrueb eros lexiconwkrvakxddtqckcjxeksl facilisis ac lectus nibh est lexiconwmsjgfnelqfeyhgudmfj vehicula lexiconwjfhkpcsgcjtotwlapxa ornare! Vitae, malesuada lexiconwivnupleicqgksianinp erat sociosqu urna, lexiconwizbukzesopzflfinotj nec sed ad aliquet lexiconxbwhfeflxbuupjezgdwl .\"\n\n\n\n\n3.2.8 Text aufr√§umen\nEine Reihe generischer Tests bietet das Paket textclean von Tyler Rinker:\nHier ist ein ‚Äúunaufger√§umeter‚Äù Text:\n\nx <- c(\"i like\", \"<p>i want. </p>. thet them ther .\", \"I am ! that|\", \"\", NA, \n    \"&quot;they&quot; they,were there\", \".\", \"   \", \"?\", \"3;\", \"I like goud eggs!\", \n    \"bi\\xdfchen Z\\xfcrcher\", \"i 4like...\", \"\\\\tgreat\",  \"She said \\\"yes\\\"\")\n\nLassen wir uns dazu ein paar Diagnostiken ausgeben.\n\nEncoding(x) <- \"latin1\"\nx <- as.factor(x)\ncheck_text(x)\n\n\n=============\nNON CHARACTER\n=============\n\nThe text variable is not a character column (likely `factor`):\n\n\n*Suggestion: Consider using `as.character` or `stringsAsFactors = FALSE` when reading in\n             Also, consider rerunning `check_text` after fixing\n\n\n=====\nDIGIT\n=====\n\nThe following observations contain digits/numbers:\n\n10, 13\n\nThis issue affected the following text:\n\n10: 3;\n13: i 4like...\n\n*Suggestion: Consider using `replace_number`\n\n\n========\nEMOTICON\n========\n\nThe following observations contain emoticons:\n\n6\n\nThis issue affected the following text:\n\n6: &quot;they&quot; they,were there\n\n*Suggestion: Consider using `replace_emoticons`\n\n\n=====\nEMPTY\n=====\n\nThe following observations contain empty text cells (all white space):\n\n1\n\nThis issue affected the following text:\n\n1: i like\n\n*Suggestion: Consider running `drop_empty_row`\n\n\n=======\nESCAPED\n=======\n\nThe following observations contain escaped back spaced characters:\n\n14\n\nThis issue affected the following text:\n\n14: \\tgreat\n\n*Suggestion: Consider using `replace_white`\n\n\n====\nHTML\n====\n\nThe following observations contain HTML markup:\n\n2, 6\n\nThis issue affected the following text:\n\n2: <p>i want. </p>. thet them ther .\n6: &quot;they&quot; they,were there\n\n*Suggestion: Consider running `replace_html`\n\n\n==========\nINCOMPLETE\n==========\n\nThe following observations contain incomplete sentences (e.g., uses ending punctuation like '...'):\n\n13\n\nThis issue affected the following text:\n\n13: i 4like...\n\n*Suggestion: Consider using `replace_incomplete`\n\n\n=============\nMISSING VALUE\n=============\n\nThe following observations contain missing values:\n\n5\n\n*Suggestion: Consider running `drop_NA`\n\n\n========\nNO ALPHA\n========\n\nThe following observations contain elements with no alphabetic (a-z) letters:\n\n4, 7, 8, 9, 10\n\nThis issue affected the following text:\n\n4: \n7: .\n8:    \n9: ?\n10: 3;\n\n*Suggestion: Consider cleaning the raw text or running `filter_row`\n\n\n==========\nNO ENDMARK\n==========\n\nThe following observations contain elements with missing ending punctuation:\n\n1, 3, 4, 6, 8, 10, 12, 14, 15\n\nThis issue affected the following text:\n\n1: i like\n3: I am ! that|\n4: \n6: &quot;they&quot; they,were there\n8:    \n10: 3;\n12: bi√üchen Z√ºrcher\n14: \\tgreat\n15: She said \"yes\"\n\n*Suggestion: Consider cleaning the raw text or running `add_missing_endmark`\n\n\n====================\nNO SPACE AFTER COMMA\n====================\n\nThe following observations contain commas with no space afterwards:\n\n6\n\nThis issue affected the following text:\n\n6: &quot;they&quot; they,were there\n\n*Suggestion: Consider running `add_comma_space`\n\n\n=========\nNON ASCII\n=========\n\nThe following observations contain non-ASCII text:\n\n12\n\nThis issue affected the following text:\n\n12: bi√üchen Z√ºrcher\n\n*Suggestion: Consider running `replace_non_ascii`\n\n\n==================\nNON SPLIT SENTENCE\n==================\n\nThe following observations contain unsplit sentences (more than one sentence per element):\n\n2, 3\n\nThis issue affected the following text:\n\n2: <p>i want. </p>. thet them ther .\n3: I am ! that|\n\n*Suggestion: Consider running `textshape::split_sentence`\n\n\n\n\n3.2.9 Diverse Wortlisten\nTyler Rinker stellt mit dem Paket lexicon eine Zusammenstellung von Wortlisten zu diversen Zwecken zur Verf√ºgung.\n\n\n3.2.10 Sentimentanalyse\n\n3.2.10.1 Einf√ºhrung\nEine weitere interessante Analyse ist, die ‚ÄúStimmung‚Äù oder ‚ÄúEmotionen‚Äù (Sentiments) eines Textes auszulesen. Die Anf√ºhrungszeichen deuten an, dass hier ein Ma√ü an Verst√§ndnis suggeriert wird, welches nicht (unbedingt) von der Analyse eingehalten wird. Jedenfalls ist das Prinzip der Sentiment-Analyse im einfachsten Fall so:\n\nSchau dir jeden Token aus dem Text an.\n\nPr√ºfe, ob sich das Wort im Lexikon der Sentiments wiederfindet.\n\nWenn ja, dann addiere den Sentimentswert dieses Tokens zum bestehenden Sentiments-Wert.\n\nWenn nein, dann gehe weiter zum n√§chsten Wort.\n\nLiefere zum Schluss die Summenwerte pro Sentiment zur√ºck.\n\nEs gibt Sentiment-Lexika, die lediglich einen Punkt f√ºr ‚Äúpositive Konnotation‚Äù bzw. ‚Äúnegative Konnotation‚Äù geben; andere Lexiko weisen differenzierte Gef√ºhlskonnotationen auf. Wir nutzen hier das deutsche Sentimentlexikon sentiws (Remus, Quasthoff, und Heyer 2010). Sie k√∂nnen das Lexikon als CSV hier herunterladen:\n\nsentiws <- read_csv(\"https://osf.io/x89wq/?action=download\")\n\nDen Volltext zum Paper finden Sie z.B. hier.\nAlternativ k√∂nnen Sie die Daten aus dem Paket pradadata laden. Allerdings m√ºssen Sie dieses Paket von Github installieren:\n\ninstall.packages(\"devtools\", dep = TRUE)\ndevtools::install_github(\"sebastiansauer/pradadata\")\n\n\ndata(sentiws, package = \"pradadata\")\n\nTabelle¬†3.1 zeigt einen Ausschnitt aus dem Sentiment-Lexikon SentiWS.\n\n\n\n\n\nTabelle¬†3.1: Auszug aus SentiWS\n\n\nneg_pos\nword\nvalue\ninflections\n\n\n\n\nneg\nAbbau\n-0.0580\nAbbaus,Abbaues,Abbauen,Abbaue\n\n\nneg\nAbbruch\n-0.0048\nAbbruches,Abbr√ºche,Abbruchs,Abbr√ºchen\n\n\nneg\nAbdankung\n-0.0048\nAbdankungen\n\n\nneg\nAbd√§mpfung\n-0.0048\nAbd√§mpfungen\n\n\nneg\nAbfall\n-0.0048\nAbfalles,Abf√§lle,Abfalls,Abf√§llen\n\n\nneg\nAbfuhr\n-0.3367\nAbfuhren\n\n\n\n\n\n\n\n\n\n3.2.10.2 Ungewichtete Sentiment-Analyse\nNun k√∂nnen wir jedes Token des Textes mit dem Sentiment-Lexikon abgleichen; dabei z√§hlen wir die Treffer f√ºr positive bzw. negative Terme. Zuvor m√ºssen wir aber noch die Daten (afd_long) mit dem Sentimentlexikon zusammenf√ºhren (joinen). Das geht nach bew√§hrter Manier mit inner_join; ‚Äúinner‚Äù sorgt dabei daf√ºr, dass nur Zeilen behalten werden, die in beiden Dataframes vorkommen. Tabelle Tabelle¬†3.2 zeigt Summe, Anzahl und Anteil der Emotionswerte.\nWir nutzen die Tabelle afd_long, die wir oben definiert haben.\n\nafd_long %>% \n  inner_join(sentiws, by = c(\"token\" = \"word\")) %>% \n  select(-inflections) -> afd_senti  # die Spalte brauchen wir nicht\n\nafd_senti %>% \n  group_by(neg_pos) %>% \n  summarise(polarity_sum = sum(value),\n            polarity_count = n()) %>% \n  mutate(polarity_prop = (polarity_count / sum(polarity_count)) %>% round(2)) ->\n  afd_senti_tab\n\n\n\n\n\n\nTabelle¬†3.2: Zusammenfassung von SentiWS\n\n\nneg_pos\npolarity_sum\npolarity_count\npolarity_prop\n\n\n\n\nneg\n-48.5307\n210\n0.27\n\n\npos\n30.6595\n578\n0.73\n\n\n\n\n\n\n\nDie Analyse zeigt, dass die emotionale Bauart des Textes durchaus interessant ist: Es gibt viel mehr positiv get√∂nte W√∂rter als negativ get√∂nte. Allerdings sind die negativen W√∂rter offenbar deutlich st√§rker emotional aufgeladen, denn die Summe an Emotionswert der negativen W√∂rter ist (√ºberraschenderweise?) deutlich gr√∂√üer als die der positiven.\nBetrachten wir also die intensivsten negativ und positive konnotierten W√∂rter n√§her.\n\nafd_senti %>% \n  distinct(token, .keep_all = TRUE) %>% \n  mutate(value_abs = abs(value)) %>% \n  top_n(20, value_abs) %>% \n  pull(token)\n\n [1] \"ungerecht\"    \"besonders\"    \"gef√§hrlich\"   \"√ºberfl√ºssig\"  \"behindern\"   \n [6] \"gelungen\"     \"brechen\"      \"unzureichend\" \"gemein\"       \"verletzt\"    \n[11] \"zerst√∂ren\"    \"trennen\"      \"falsch\"       \"vermeiden\"    \"zerst√∂rt\"    \n[16] \"schwach\"      \"belasten\"     \"sch√§dlich\"    \"t√∂ten\"        \"verbieten\"   \n\n\nDiese ‚ÄúHitliste‚Äù wird zumeist (19/20) von negativ polarisierten Begriffen aufgef√ºllt, wobei ‚Äúbesonders‚Äù ein Intensivierwort ist, welches das Bezugswort verst√§rt (‚Äúbesonders gef√§hrlich‚Äù). Das Argument keep_all = TRUE sorgt daf√ºr, dass alle Spalten zur√ºckgegeben werden, nicht nur die durchsuchte Spalte token. Mit pull haben wir aus dem Dataframe, der von den dplyr-Verben √ºbergeben wird, die Spalte pull ‚Äúherausgezogen‚Äù; hier nur um Platz zu sparen bzw. der √úbersichtlichkeit halber.\nNun k√∂nnte man noch den erzielten ‚ÄúNetto-Sentimentswert‚Äù des Corpus ins Verh√§ltnis setzen Sentimentswert des Lexikons: Wenn es insgesamt im Sentiment-Lexikon sehr negativ zuginge, w√§re ein negativer Sentimentwer in einem beliebigen Corpus nicht √ºberraschend. describe_distribution aus {easystats} gibt uns einen √úberblick der √ºblichen deskriptiven Statistiken.\n\nsentiws %>% \n  select(value, neg_pos) %>% \n  #group_by(neg_pos) %>% \n  describe_distribution()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nMean\nSD\nIQR\nRange\nSkewness\nKurtosis\nn\nn_Missing\n\n\n\n\nvalue\n-0.05\n0.20\n0.05\n(-1.00, 1.00)\n-0.68\n2.36\n3468\n0\n\n\n\n\n\nInsgesamt ist das Lexikon ziemlich ausgewogen; negative Werte sind leicht in der √úberzahl im Lexikon. Unser Corpus hat eine √§hnliche mittlere emotionale Konnotation wie das Lexikon:\n\nafd_senti %>% \n  summarise(senti_sum = mean(value) %>% round(2))\n\n\n\n\n\nsenti_sum\n\n\n\n\n-0.02\n\n\n\n\n\n\n\n\n\n3.2.11 Weitere Sentiment-Lexika\nTyler Rinker stellt das Paket sentimentr zur Verf√ºgung. Matthew Jockers stellt das Paket Syushet zur Verf√ºgung.\n\n\n3.2.12 Google Trends\nEine weitere M√∂glichkeit, ‚ÄúWorth√§ufigkeiten‚Äù zu identifizieren ist Google Trends. Dieser Post zeigt Ihnen eine Einsatzm√∂glichkeit."
  },
  {
    "objectID": "textmining1.html#aufgaben",
    "href": "textmining1.html#aufgaben",
    "title": "3¬† Textmining1",
    "section": "3.3 Aufgaben",
    "text": "3.3 Aufgaben\n\npurrr-map01\npurrr-map02\npurrr-map03\npurrr-map04\nRegex-√úbungen\nAufgaben zum Textmining von Tweets"
  },
  {
    "objectID": "textmining1.html#fallstudie-hate-speech",
    "href": "textmining1.html#fallstudie-hate-speech",
    "title": "3¬† Textmining1",
    "section": "3.4 Fallstudie Hate-Speech",
    "text": "3.4 Fallstudie Hate-Speech\n\n3.4.1 Daten\nEs finden sich mehrere Datens√§tze zum Thema Hate-Speech im √∂ffentlichen Internet, eine Quelle ist Hate Speech Data, ein Repositorium, das mehrere Datens√§tze beinhaltet.\n\nKaggle Hate Speech and Offensive Language Dataset\nBretschneider and Peters Prejudice on Facebook Dataset\nDaten zum Fachartikel‚ÄùLarge Scale Crowdsourcing and Characterization of Twitter Abusive Behavior‚Äù\n\nF√ºr Textmining kann eine Liste mit anst√∂√üigen (obsz√∂nen) W√∂rten n√ºtzlich sein, auch wenn man solche Dinge ungern anf√§sst, verst√§ndlicherweise. Jenyay bietet solche Listen in verschiedenen Sprachen an. Die Liste von KDNOOBW sieht sehr √§hnlich aus (zumindest die deutsche Version). Eine lange Sammlung deutscher Schimpfw√∂rter findet sich im insult.wiki; √§hnlich bei Hyperhero.\nTwitterdaten d√ºrfen nur in ‚Äúdehydrierter‚Äù Form weitergegeben werden, so dass kein R√ºckschluss von ID zum Inhalt des Tweets m√∂glich ist. Daher werden √∂ffentlich nur die IDs der Tweets, als einzige Information zum Tweet, also ohne den eigentlichen Inhalt des Tweets, bereitgestellt.\n√úber die Twitter-API kann man sich, wie oben dargestellt, dann die Tweets wieder ‚Äúrehydrieren‚Äù, also wieder mit dem zugeh√∂rigen Tweet-Text (und sonstigen Infos des Tweets) zu versehen.\n\n\n3.4.2 Grundlegendes Text Mining\nWenden Sie die oben aufgef√ºhrten Techniken des grundlegenden Textminings auf einen der oben dargestellten Hate-Speech-Datens√§tze an. Erstellen Sie ein (HTML-Dokument) mit Ihren Ergebnissen. Stellen Sie die Ergebnisse auf dem Github-Repo dieses Kurses ein. Vergleichen Sie Ihre L√∂sung mit den L√∂sungen der anderen Kursmitglieder.\nWir nutzen noch nicht eigene Daten, die wir von Twitter ausgelesen haben, das heben wir uns f√ºr sp√§ter auf.\n\n\n\n\nHvitfeldt, Emil, und Julia Silge. 2022. Supervised Machine Learning for Text Analysis in R. 1. Aufl. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nLex, Alexander, Nils Gehlenborg, Hendrik Strobelt, Romain Vuillemot, und Hanspeter Pfister. 2014. ‚ÄûUpSet: Visualization of Intersecting Sets‚Äú. IEEE Transactions on Visualization and Computer Graphics 20 (12): 1983‚Äì92. https://doi.org/10.1109/TVCG.2014.2346248.\n\n\nRemus, Robert, Uwe Quasthoff, und Gerhard Heyer. 2010. ‚ÄûSentiWS - a Publicly Available German-language Resource for Sentiment Analysis‚Äú. Proceedings of the 7th International Language Ressources and Evaluation (LREC‚Äô10), 1168‚Äì71.\n\n\nWickham, Hadley, und Garrett Grolemund. 2018. R f√ºr Data Science: Daten importieren, bereinigen, umformen, modellieren und visualisieren. √úbersetzt von Frank Langenau. 1. Auflage. Heidelberg: O‚ÄôReilly. https://r4ds.had.co.nz/index.html."
  },
  {
    "objectID": "populismus.html",
    "href": "populismus.html",
    "title": "4¬† Fallstudie Populismus",
    "section": "",
    "text": "Bild von mcmurryjulie auf Pixabay"
  },
  {
    "objectID": "populismus.html#vorab",
    "href": "populismus.html#vorab",
    "title": "4¬† Fallstudie Populismus",
    "section": "4.1 Vorab",
    "text": "4.1 Vorab\n\n4.1.1 Lernziele\n\nDie Fallstudie erkl√§ren k√∂nnen\n\n\n\n4.1.2 Vorbereitung\n\nClonen Sie das Projekt-Repositorium oder laden Sie es herunter1.\nArbeiten Sie die Syntax zu dem Projekt durch.\n\n\n\n4.1.3 Ben√∂tigte R-Pakete\nIn dem vorgestellten Projekt werden die folgenden R-Pakete verwendet.\n\nlibrary(tidyverse)\nlibrary(twitteR)\nlibrary(tidytext)\nlibrary(lubridate)\nlibrary(magrittr)\nlibrary(tidytext)\nlibrary(stringr)\nlibrary(viridis)\nlibrary(wordcloud)\nlibrary(SnowballC)\nlibrary(knitr)\nlibrary(testthat)"
  },
  {
    "objectID": "populismus.html#wie-populistisch-tweeten-unsere-politikerinnen",
    "href": "populismus.html#wie-populistisch-tweeten-unsere-politikerinnen",
    "title": "4¬† Fallstudie Populismus",
    "section": "4.2 Wie populistisch tweeten unsere Politiker:innen?",
    "text": "4.2 Wie populistisch tweeten unsere Politiker:innen?\nVerschaffen Sie sich einen √úberblick √ºber dieses Projekt! Im Rahmen dieses Projekts vergleicht der Autor den Populismus von deutschen Politiker:innen, so wie er sich in den Tweets dieser Personen niederschl√§gt. Auf dieser Basis wird ein Populismuswert, bestehend aus mehreren Teilwerten, berechnet und auf Parteiebenen (als Mittel der zugeh√∂rigen Politiker:innen) berechnet. Nat√ºrlich fragt man sich, wie Populismus definiert ist und wie diese Definition in den Berechnungen umgesetzt wurde. Finden Sie es selber heraus: Im Github-Repo sind alle Details dokumentiert.\nZum Einstieg hilft ein √úberblick √ºber die Ergebnisse der Analyse, die in diesem Vortrag zusammengefasst sind.\nDieser Post stellt die Ergebnisse mit etwas Kontext dar."
  },
  {
    "objectID": "word-embedding.html",
    "href": "word-embedding.html",
    "title": "5¬† Word Embedding",
    "section": "",
    "text": "Bild von mcmurryjulie auf Pixabay"
  },
  {
    "objectID": "word-embedding.html#vorab",
    "href": "word-embedding.html#vorab",
    "title": "5¬† Word Embedding",
    "section": "5.1 Vorab",
    "text": "5.1 Vorab\n\n5.1.1 Lernziele\n\nDie grundlegenden Konzepte der Informationstheorie erkl√§ren k√∂nnen\nDie vorgestellten Techniken des Textminings mit R anwenden k√∂nnen\n\n\n\n5.1.2 Vorbereitung\n\nLesen Sie diesen Text als Vorbereitung\nArbeiten Sie Hvitfeldt und Silge (2022), Kap. 5 durch.\n\n\n\n5.1.3 Ben√∂tigte R-Pakete\n\nlibrary(tidyverse)\nlibrary(easystats)  # Komfort f√ºr deskriptive Statistiken, wie `describe_distribution`\nlibrary(tidytext)\nlibrary(hcandersenr)   # Textdaten\nlibrary(slider)  # slide\nlibrary(widyr)  # pairwise_pmi\nlibrary(furrr)  # mehrere Kerne gleichzeitig nutzen\nlibrary(textdata)  # Worteinbettungen, vorgekocht\nlibrary(entropy)  # Entropie berechnen\nlibrary(widyr)  # √Ñhnlichkeit berechnen mit widyr_svd\nlibrary(furrr)  # Mehrere Kerne gleichzeitig"
  },
  {
    "objectID": "word-embedding.html#informationstheorie",
    "href": "word-embedding.html#informationstheorie",
    "title": "5¬† Word Embedding",
    "section": "5.2 Informationstheorie",
    "text": "5.2 Informationstheorie\n\n5.2.1 Einf√ºhrung\nDie Informationstheorie ist eine der Sternstunden der Wissenschaft. Manche sagen dass Claude Shannon, der Autor der Theorie, auf einer Stufe mit Darwin und Einstein stehen sollte:\n\nIn this single paper, Shannon introduced this new fundamental theory. He raised the right questions, which no one else even thought of asking. This would have been enough to make this contribution earthshaking. But amazingly enough, Shannon also provided most of the right answers with class and elegance. In comparison, it took decades for a dozen of top physicists to define the basics of quantum theory. Meanwhile, Shannon constructed something equivalent, all by himself, in a single paper. Shannon‚Äôs theory has since transformed the world like no other ever had, from information technologies to telecommunications, from theoretical physics to economical globalization, from everyday life to philosophy. (‚Ä¶) I don‚Äôt think Shannon has had the credits he deserves. He should be right up there, near Darwin and Einstein, among the few greatest scientists mankind has ever had the chance to have.\n\nF√ºr die Statistik ist die Informationstheorie von hoher Bedeutung. Im Folgenden schauen wir uns einige Grundlagen an.\n\n\n5.2.2 Wozu ist das gut?\nBevor man sich mit einem Thema wie der Informationtheorie (Informationsentropie mit verwandten Konstrukten oder kurz Entropie) besch√§ftigt, sollte die Frage gekl√§rt sein, wozu das Thema gut ist. Hier sind drei Antworten dazu:\n\nIm Maschinenlernen wird die Informationtheorie verwendet, um die G√ºte von Klassifikationsmodellen zu berechnen.\nSpeziell im Textmining wird die Entropie verwendet, um den Zusammenhang von W√∂rtern zu quantifizieren.\nWenige Theorien haben so viel neue Forschung initiert, wie Shannon (1948) ber√ºhmtes Paper. Es ist also auf jeden Fall eine Perle der Geistesgeschichte.\n\n\n\n5.2.3 Shannon-Information\nMit der Shannon-Information (Information, Selbstinformation) quantifizieren wir, wie viel ‚Äú√úberraschung‚Äù sich in einem Ereignis verbirgt (Shannon 1948).\nEin Ereignis mit ‚Ä¶\n\ngeringer Wahrscheinlichkeit: Viel √úberraschung (Information)\nhoher Wahrscheinlichkeit: Wenig √úberraschung (Information)\n\nWenn wir also erfahren, dass ein unwahrscheinliches Ereignis eingetreten ist (Schnee im Sommer), sind wir √ºberraschter als wenn wir h√∂hen, dass ein wahrscheinliches Ereignis eingetreten ist (Schnee im Winter).\nDie Information eines Ereignis ist also eng verbunden mit seiner Wahrscheinlichkeit oder auch mit den Odds eines Ereignisses.\nDie Shannon-Information ist die einzige Gr√∂√üe, die einige w√ºnschenswerte Anforderungen1 erf√ºllt:\n\nStetig\nJe mehr Ereignisse in einem Zufallsexperiment m√∂glich sind, desto h√∂her die Information, wenn ein bestimmtes Ereignis eintritt\nAdditiv: Die Summe der Information zweier Teilereignisse ist gleich der Information des Gesamtereignis\n\n\nDefinition 5.1 (Shannon-Information) Die Information ist so definiert:\n\\[I(x) = - \\log_2 \\left( Pr(x) \\right)\\]\n\nAndere Logaritmusbasen sind m√∂glich. Bei einem bin√§ren Logarithmus nennt man die Einheit Bit2.\nEin M√ºnwzurf3 hat 1 Bit Information:\n\n-log(1/2, base = 2)\n\n[1] 1\n\n\nDamit gilt: \\(I = \\frac{1}{Pr(x)}\\)\nDie Information ist eng verwandt mit den Odds bzw. Log-Odds (Logits):\n\\(\\text{log-odds}(x)=\\log \\left({\\frac {p(x)}{p(\\lnot x)}}\\right)\\)\nLogits k√∂nnen als Differenz zweier Shannon-Infos ausgedr√ºckt werden:\n\\(\\text{log-odds}(x)=I(\\lnot x)-I(x)\\)\nDie Information zweier unabh√§ngiger Ereignisse ist additiv.\nDie gemeinsame Wahrscheinlichkeit zweier unabh√§ngiger Ereignisse ist das Produkt der einzelnen Wahrscheinlichkeiten:\n\\(Pr(x,y) = Pr(x) \\cdot Pr(y)\\)\nDie gemeinsame Information ist dann\n\\[\n{\\displaystyle {\\begin{aligned}\\operatorname {I}(x,y)&=-\\log _{2}\\left[p(x,y)\\right]=-\\log _{2}\\left[p(x)p(y)\\right]\\\\[5pt]&=-\\log _{2}\\left[p{(x)}\\right]-\\log _{2}\\left[p{(y)}\\right]\\\\[5pt]&=\\operatorname {I} (x)+\\operatorname {I} (y)\\end{aligned}}}\n\\]\n\nBeispiel 5.1 (Information eines wahrscheinlichen Ereignisses) Die Information eines fast sicheren Ereignisses ist gering.\n\n-log(99/100, base = 2)\n\n[1] 0.01449957\n\n\n\n\nBeispiel 5.2 (Information eines unwahrscheinlichen Ereignisses) Die Information eines unwahrscheinlichen Ereignisses ist hoch.\n\n-log(01/100, base = 2)\n\n[1] 6.643856\n\n\n\n\nBeispiel 5.3 (Information eines W√ºrfelwurfs) Die Wahrscheinlichkeitsfunktion eines W√ºrfel ist\n\\({\\displaystyle Pr(k)={\\begin{cases}{\\frac {1}{6}},&k\\in \\{1,2,3,4,5,6\\}\\\\0,&{\\text{ansonsten}}\\end{cases}}}\\)\nDie Wahrscheinlichkeit, eine 6 zu w√ºrfeln, ist \\(Pr(X=6) = \\frac{1}{6}\\).\nDie Information von \\(X=6\\) betr√§gt also\n\\(I(X=6) = -\\log_2 \\left( Pr(X=6) \\right) = -\\log_2(1/6) \\approx 2.585 \\, \\text{bits}\\).\n\n-log(1/6, base = 2)\n\n[1] 2.584963\n\n\n\n\nBeispiel 5.4 (Information zweier W√ºrfelwurfe) Die Wahrscheinlichkeit, mit zwei W√ºrfeln, \\(X\\) und \\(Y\\), jeweils 6 zu w√ºrfeln, betr√§gt \\(Pr(X=6, Y=6) = \\frac{1}{36}\\)\nDie Information betr√§gt also\n\\(I(X=6, Y=6) = -\\log_2 \\left( Pr(6,6) \\right)\\)\n\n-log(1/36, base = 2)\n\n[1] 5.169925\n\n\nAufgrund der Additivit√§t der Information gilt\n\\(I(6,6) = I(6) + I(6)\\)\n\n-log(1/6, base = 2) + -log(1/6, base = 2)\n\n[1] 5.169925\n\n\n\n\n\n5.2.4 Entropie\n(Informations)entropie ist die Summe der Informationen der Ereignisse einer Zufallsvariablen, \\(X\\).\n\nDefinition 5.2 (Informationsentropie) Informationsentropie ist so definiert:\n\\[H(p) = - \\text{E log} (p_i) = - \\sum_{i = 1}^n p_i \\text{log} (p_i) = E\\left[I(X) \\right]\\]\n\nDie Informationsentropie ist also die ‚Äúmittlere‚Äù oder ‚Äúerwartete Information einer Zufallsvariablen.\nDie Entropie eines M√ºnzwurf-Versuchs (Bernoulli-Experiment) ist dann maximal (1 bit), wenn die Trefferwahrscheinlichkeit 50% betr√§gt: \\(Pr(X=x) = 1/2\\), s. Abb. Abbildung¬†5.1.\n\n\n\nAbbildung¬†5.1: Der Zusammenhang von Wahrscheinlichkeit und Entropie bei einem Bernoulli-Versuch, Quelle: Wikipedia, CC-BY-SA, Brona, Rubber Duck\n\n\n\n\n5.2.5 Gemeinsame Information\nDie gemeinsame Information (mutual information, MI) zweier Zufallsvariablen \\(X\\) und \\(Y\\), \\(I(X,Y)\\), quantifiziert die Informationsmenge, die man √ºber \\(Y\\) erh√§lt, wenn man \\(X\\) beobachtet. Mit anderen Worten: Die MI ist ein Ma√ü des Zusammenhangs zweier (nominaler) Variablen. Im Gegensatz zur Korrelation ist die MI nicht auf lineare Abh√§ngigkeiten beschr√§nkt.\nDie MI quantifiziert den Unterschied zwischen der gemeinsamen Verteilung \\(Pr(X,Y)\\) und dem Produkt einer einzelnen4 Wahrscheinlichkeitsverteilungen, d.h. \\(Pr(X)\\) und \\(Pr(Y)\\).\nWenn die beiden Variablen (stochastisch) unabh√§ngig5 sind, ist ihre gemeinsame Information Null:\n\\(I(X,Y) = 0 \\quad \\text{gdw} \\quad \\bot(X,Y)\\).\nDann gilt n√§mlich:\n\\(\\log \\left( \\frac{Pr(X,Y)} {Pr(X) \\cdot Pr(Y)} \\right) =\\log(1) = 0\\).\nDas macht intuitiv Sinn: Sind zwei Variablen unabh√§ngig, so erf√§hrt man nichts √ºber die zweite, wenn man die erste kennt. So ist Kenntnis der Sternzeichens einer Person und ihrer K√∂rpergr√∂√üe unabh√§ngig.\nDas Gegenteil ist auch wahr: Sind zwei Variablen voneinander komplett abh√§ngig, so wei√ü man alles √ºber die zweite, wenn man die erste kennt.\nDie gemeinsame Information kann man sich als Summe der einzelnen gemeinsamen Informationen von \\(XY\\) sehen (s. Tabelle¬†5.1):\n\nd <- tibble::tribble(\n     ~x1,    ~x2,    ~x3,\n  \"x1y2\", \"x2y1\", \"x3y1\",\n  \"x2y1\", \"x2y2\", \"x3y2\",\n  \"x1y3\", \"x2y3\", \"x3y3\"\n  )\nd\n\n\n\n\nTabelle¬†5.1: Summe der punktweisen gemeinsamen Informationen\n\n\nx1\nx2\nx3\n\n\n\n\nx1y2\nx2y1\nx3y1\n\n\nx2y1\nx2y2\nx3y2\n\n\nx1y3\nx2y3\nx3y3\n\n\n\n\n\n\n\n\\(I(X,Y) = \\Sigma_Y \\Sigma_y Pr(x,y) \\underbrace{\\log \\left( \\frac{Pr(X,Y)}{Pr(X) Pr(Y)} \\right)}_\\text{punktweise MI}\\)\nDie Summanden der gemeinsamen Information bezeichnet man auch als punktweise gemeinsame Information (pointwise mutual information, PMI), entsprechend, s. Gleichung¬†5.1. MI ist also der Erwartungswert der PMI.\n\\[{\\displaystyle \\operatorname {PMI} (x,y)\\equiv \\log_{2}{\\frac {p(x,y)}{p(x)p(y)}}=\\log _{2}{\\frac {p(x|y)}{p(x)}}=\\log _{2}{\\frac {p(y|x)}{p(y)}}}\n\\tag{5.1}\\]\nAndere Basen als log2 sind gebr√§uchlich, vor allem der nat√ºrliche Logarithmus.\n\nAnmerkung. Die zwei rechten Umformungen in Gleichung¬†5.1 basieren auf der Umformung der gemeinsamen Wahrscheinlichkeit.\nZur Erinnerung: \\(p(x,y) = p(y)p(x|y) = p(x)p(y|x)\\)\n\n\nBeispiel 5.5 (Interpretation der PMI) Sei \\(p(x) = p(y) = 1/10\\) und \\(p(x,y) = 1/10\\). W√§ren \\(x\\) und \\(y\\) unabh√§ngig, dann w√§re \\(p^{\\prime}(x,y) = p(x)p(y) = 1/100\\). Das Verh√§ltnis der Produkte der einzelnen Wahrscheinlichkeit zur gemeinsamen Wahrscheinlichkeit w√§re dann 1 und der Logarithmus von 1 ist 0. Das Verh√§ltnis von 1 entspricht also der Unabh√§ngigkeit. Ist das Verh√§ltnis z.B. 5, so zeigt das eine gewisse Abh√§ngigkeit an. Im obigen Beispiel gilt: \\(\\frac{1/20}{1/100}=5\\).\n\nDie MI wird auch √ºber die sog. Kullback-Leibler-Divergenz definiert, die die Differenz zwischen gemeinsamer Wahrscheinlichkeitsfunktion bzw. -dichte und dem Produkt der jeweiligen Randwahrscheinlichkeiten bzw. -dichte angibt.\n\n\n5.2.6 Maximumentropie\n\nDefinition 5.3 (Maximumentropie) Die Verteilungsform, f√ºr die es die meisten M√∂glichkeiten (Pfade im Baumdiagramm) gibt, hat die h√∂chste Informationsentropie.\n\nAbbildung¬†5.2 zeigt ein Baumdiagramm f√ºr einen 3-fachen M√ºnzwurf. In den ‚ÄúBl√§ttern‚Äù (Endknoten) sind die Ergebnisse des Experiments dargestellt sowie die Zufallsvariable \\(X\\), die die Anzahl der ‚ÄúTreffer‚Äù (Kopf) fasst. Wie man sieht, vereinen einige Werte der Zufallsvariable mehr Pfade auf sich als andere: Der Wert \\(X=1\\) vereinigt 3 Pfade (von 8) auf sich; der Wert \\(X=3\\) nur 1 Pfad.\n\n\n\nAbbildung¬†5.2: Pfade im Baumdiagramm: 3-facher M√ºnzwurf\n\n\n\n\n5.2.7 Ilustration\nSagen wir, Sie stehen vor 5 Eimern und haben 10 Kieselsteine bei sich, die mit den Nummern 1 bis 10 beschriftet sind (McElreath 2020). Weil Sie nichts besseres zu tun haben, werfen Sie die Kiesel in die Eimer und zwar so, dass die Wahrscheinlichkeit f√ºr einen Kiesel in einen bestimmten Eimer zu landen f√ºr alle Eimer gleich ist. Sie werfen also Ihre 10 Kiesel und betrachten das Ergebnis; die Kiesel sind jetzt in einem bestimmten (zuf√§lligen) Arrangement auf die Eimer verteilt. Jede Aufteilung (der 10 Kiesel in den 5 Eimern) ist gleich wahrscheinlich6 ‚Äì die Wahrscheinlichkeit, dass alle 10 Kiesel in Eimer 1 landen ist also gleich hoch wie die Wahrscheinlichkeit, dass jeder Eimer einen Kiesel abkriegt. Jetzt kommt‚Äôs: Manche Arrangements k√∂nnen auf mehrere Arten erzielt werden als andere. So gibt es nur eine Aufteilung f√ºr alle 10 Kiesel in einem Eimer (Teildiagramm a, in Abbildung¬†5.3). Aber es gibt 90 M√∂glichkeiten, die Kiesel so aufzuteilen, dass 2 in Eimer 2 landen, 8 in Eimer 4 und 2 in Eimer 4, s. Teildiagramm b in Abbildung¬†5.3. Teildiagramme c bis e zeigen, dass die Anzahl der Aufteilungen schnell astronomisch hoch wird, wenn sich die Kiesel ‚Äúgleichm√§√üiger‚Äù auf die Eimer verteilen. Die gleichm√§√üigste Aufteilung (Diagramm e) hat die gr√∂√üte Zahl an m√∂glichen Anordnungen. Eine Aufteilung der Kiesel auch als Pfad durch ein Baumdiagramm beschrieben werden.\nHier sind ein paar verschiedene Arrangements, deren Anzahl von Aufteilungen wir hier anschauen, s. Tabelle¬†5.2:\n\nd <-\n  tibble(a = c(0, 0, 10, 0, 0),\n         b = c(0, 1, 8, 1, 0),\n         c = c(0, 2, 6, 2, 0),\n         d = c(1, 2, 4, 2, 1),\n         e = 2) \nd\n\n\n\n\nTabelle¬†5.2: Ein paar verschiedene Arrangements (a-e) der Kiesel in den f√ºnf Eimern\n\n\na\nb\nc\nd\ne\n\n\n\n\n0\n0\n0\n1\n2\n\n\n0\n1\n2\n2\n2\n\n\n10\n8\n6\n4\n2\n\n\n0\n1\n2\n2\n2\n\n\n0\n0\n0\n1\n2\n\n\n\n\n\n\n\n\n\n\n\n\nAbbildung¬†5.3: Anzahl von Aufteilungen der 10 Kiesel in 5 Eimer\n\n\n\n\nHier sind die Wahrscheinlichkeitsverteilungen der 5 Arrangements7:\n\nd %>% \n  mutate_all(~. / sum(.))\n\n\n\n\n\na\nb\nc\nd\ne\n\n\n\n\n0\n0.0\n0.0\n0.1\n0.2\n\n\n0\n0.1\n0.2\n0.2\n0.2\n\n\n1\n0.8\n0.6\n0.4\n0.2\n\n\n0\n0.1\n0.2\n0.2\n0.2\n\n\n0\n0.0\n0.0\n0.1\n0.2\n\n\n\n\n\n\nDann kann man die Wahrscheinlichkeit einfach in Entropie umrechnen8:\n\nd %>% \n  mutate_all(~ . / sum(.)) %>% \n  gather() %>% \n  group_by(key) %>% \n  summarise(h = -sum(ifelse(value == 0, 0, value * log(value))))\n\n\n\n\n\nkey\nh\n\n\n\n\na\n0.0000000\n\n\nb\n0.6390319\n\n\nc\n0.9502705\n\n\nd\n1.4708085\n\n\ne\n1.6094379\n\n\n\n\n\n\nDas ifelse dient nur dazu, eine Wahrscheinlichkeit von 0 eine Entropie von 0 zu verpassen9, denn sonst w√ºrden wir ein Problem rennen, wenn wir \\(log(0)\\) ausrechnen.\n\nlog(0)\n\n[1] -Inf"
  },
  {
    "objectID": "word-embedding.html#zufallstext-erkennen",
    "href": "word-embedding.html#zufallstext-erkennen",
    "title": "5¬† Word Embedding",
    "section": "5.3 Zufallstext erkennen",
    "text": "5.3 Zufallstext erkennen\n\n5.3.1 Entropie von Zufallstext\nKann man wohl Zufallstext maschinell erkennen? Sicher gibt es viele Ans√§tze, um das Problem anzugehen. Lassen Sie uns einen Ansatz erforschen. Erforschen hei√üt, wir erforschen f√ºr uns, es handelt sich um eine didaktische √úbung, das Ziel ist nicht, Neuland f√ºr die Menschheit zu betreten.\nAber zuerst m√ºssen wir √ºberlegen, was ‚ÄúZufallstext‚Äù bedeuten soll.\nNehmen wir uns dazu zuerst einen richtigen Text, ein M√§rchen von H.C. Andersen zum Beispiel. Nehmen wir das Erste aus der Liste in dem Tibble hcandersen_de, ‚Äúdas Feuerzeug‚Äù.\n\ndas_feuerzeug <-\n  hcandersen_de  %>% \n  filter(book == \"Das Feuerzeug\") %>% \n  unnest_tokens(input = text, output = word) %>% \n  pull(word) \n\nhead(das_feuerzeug)\n\n[1] \"es\"         \"kam\"        \"ein\"        \"soldat\"     \"die\"       \n[6] \"landstra√üe\"\n\n\nDas M√§rchen ist 2688 W√∂rter lang.\n\nwortliste <- \nhcandersen_de  %>% \n  filter(book == \"Das Feuerzeug\") %>% \n  unnest_tokens(output = word, input = text) %>% \n  pull(word) %>% \n  unique()\n\nhead(wortliste)\n\n[1] \"es\"         \"kam\"        \"ein\"        \"soldat\"     \"die\"       \n[6] \"landstra√üe\"\n\n\nJetzt ziehen wir Stichproben (mit Zur√ºcklegen) aus dieser Liste und erstellen damit eine Art Zufallstext.\n\nzufallstext <- \n  sample(x = wortliste, size = length(das_feuerzeug)*10^2, replace = TRUE)\nhead(zufallstext)\n\n[1] \"hat\"    \"kreide\" \"ins\"    \"komme\"  \"seinen\" \"dort\"  \n\n\nZ√§hlen wir, wie h√§ufig jedes Wort vorkommt:\n\nzufallstext_count <-\ntibble(zufallstext = zufallstext) %>% \n  count(zufallstext)\n\nhead(zufallstext_count)\n\n\n\n\n\nzufallstext\nn\n\n\n\n\nab\n356\n\n\nabend\n386\n\n\naber\n347\n\n\nabflog\n350\n\n\nabschlagen\n388\n\n\nacht\n379\n\n\n\n\n\n\nDer H√§ufigkeitsvektor von wortliste besteht nur aus Einsen, so haben wir ja gerade die Wortliste definiert:\n\nwortliste_count <-\ntibble(wortliste = wortliste) %>% \n  count(wortliste)\n\nhead(wortliste_count)\n\n\n\n\n\nwortliste\nn\n\n\n\n\nab\n1\n\n\nabend\n1\n\n\naber\n1\n\n\nabflog\n1\n\n\nabschlagen\n1\n\n\nacht\n1\n\n\n\n\n\n\nDaher ist ihre Informationsentropy maximal.\n\nentropy(wortliste_count$n, unit = \"log2\")\n\n[1] 9.47978\n\n\nDie H√§ufigkeiten der W√∂rter in zufallstext hat eine hohe Entropie.\n\nentropy(zufallstext_count$n, unit = \"log2\")\n\n[1] 9.47792\n\n\nZ√§hlen wir die H√§ufigkeiten in der Geschichte ‚ÄúDas Feuerzeug‚Äù.\n\ndas_feuerzeug_count <-\n  tibble(text = das_feuerzeug) %>% \n  count(text)\n\nhead(das_feuerzeug_count)\n\n\n\n\n\ntext\nn\n\n\n\n\nab\n2\n\n\nabend\n3\n\n\naber\n21\n\n\nabflog\n1\n\n\nabschlagen\n1\n\n\nacht\n1\n\n\n\n\n\n\nUnd berechnen dann die Entropie:\n\nentropy(das_feuerzeug_count$n, unit = \"log2\")\n\n[1] 8.075194\n\n\nDer Zufallstext hat also eine h√∂here Entropie als der echte M√§rchentext. Der Zufallstext ist also gleichverteilter in den Worth√§ufigkeiten.\nPro Bit weniger Entropie halbiert sich die Anzahl der M√∂glichkeiten einer H√§ufigkeitsverteilung.\n\n\n5.3.2 MI von Zufallstext\nLeft as an exercises for the reader10 ü•≥."
  },
  {
    "objectID": "word-embedding.html#daten",
    "href": "word-embedding.html#daten",
    "title": "5¬† Word Embedding",
    "section": "5.4 Daten",
    "text": "5.4 Daten\n\n5.4.1 Complaints-Datensatz\nDer Datensatz complaints stammt aus dieser Quelle.\nDen Datensatz complaints kann man hier herunterladen. Im Buch ist die Webseite nicht (direkt?) angegeben. Die Datei ist mit gz gepackt; read_csv sollte das automatisch entpacken. Achtung: Die Datei ist recht gro√ü.\n\nd_path <- \"https://github.com/EmilHvitfeldt/smltar/raw/master/data/complaints.csv.gz\"\ncomplaints <- read_csv(d_path)\n\nGeschickter als jedes Mal die Datei herunterzuladen, ist es, sie einmal herunterzuladen, und dann lokal zu speichern, etwa im Unterordner data des RStudio-Projektordners.\nNach dem Importieren wird der Datensatz in eine Tidy-Form gebracht (mit unnest_tokens) und dann verschachtelt, mit nest.\n\n\n5.4.2 Complaints verk√ºrzt und geschachtelt\nUm ein Herumprobieren zu erleichtern, ist hier der Datensatz complaints in zwei verk√ºrzten Formen bereitgestellt:\n\nnested_words2_path <- \"https://github.com/sebastiansauer/datascience-text/raw/main/data/nested_words2.rds\"\nnested_words3_path <- \"https://github.com/sebastiansauer/datascience-text/raw/main/data/nested_words3.rds\"\n\nnested_words2 enth√§lt die ersten 10% des Datensatz nested_wordsund ist gut 4 MB gro√ü (mit gz gezippt); er besteht aus ca. 11 Tausend Beschwerden. nested_words3 enth√§lt nur die ersten 10 Beschwerden, was ihn gut zum Handhaben macht.\nBeide sind verschachtelt und aus tidy_complaints (s. Kap. 5.1) hervorgegangen.\n\nnested_words3 <- read_rds(nested_words3_path)\n\nDas sieht dann so aus:\n\nnested_words3 %>% \n  head(3)\n\n\n\n\n\n\n\n\n\ncomplaint_id\nwords\n\n\n\n\n3384392\nsystems , inc , is , trying , to , collect , a , debt , that , is , not , mine , not , owed , and , is , inaccurate\n\n\n3417821\ni , would , like , to , request , the , of , the , following , items , from , my , credit , report , which , are , the , result , of , my , victim , to , identity , theft , this , information , does , not , to , transactions , that , i , have , made , accounts , that , i , have , opened , as , the , attached , supporting , documentation, can , as , such , it , should , be , blocked , from , on , my , credit , report , pursuant , to , section , of , the , fair , credit , reporting , act\n\n\n3433198\nover , the , past , 2 , weeks , i , have , been , receiving , amounts , of , telephone , calls , from , the , company , listed , in , this , complaint , the , calls , between , xxxx , xxxx , and , xxxx , xxxx , to , my , cell , and , at , my , job , the , company , does , not , have , the , right , to , me , at , work , and , i , want , this , to , stop , it , is , extremely , to , be , told , 5 , times , a , day , that , i , have , a , call , from , this , collection, agency , while , at , work\n\n\n\n\n\n\nWerfen wir einen Blick in den TExt der ersten Beschwerde des Datensatzes mit der ID nested_words3_path$complaint_id[1].\n\nbeschwerde1_text <- nested_words3$words[[1]]\n\nDas ist ein Tibble mit einer Spalte und 17 W√∂rtern; da wir schon auf Unigramme aufgeteilt haben, ist jedes Wort ein Element des Vektors word:\n\nbeschwerde1_text %>% \n  head()\n\n\n\n\n\nword\n\n\n\n\nsystems\n\n\ninc\n\n\nis\n\n\ntrying\n\n\nto\n\n\ncollect\n\n\n\n\n\n\n\nbeschwerde1_text$word\n\n [1] \"systems\"    \"inc\"        \"is\"         \"trying\"     \"to\"        \n [6] \"collect\"    \"a\"          \"debt\"       \"that\"       \"is\"        \n[11] \"not\"        \"mine\"       \"not\"        \"owed\"       \"and\"       \n[16] \"is\"         \"inaccurate\""
  },
  {
    "objectID": "word-embedding.html#kommentare-und-hilfestellungen",
    "href": "word-embedding.html#kommentare-und-hilfestellungen",
    "title": "5¬† Word Embedding",
    "section": "5.5 Kommentare und Hilfestellungen",
    "text": "5.5 Kommentare und Hilfestellungen\n\n5.5.1 PMI berechnen\nRufen Sie sich die Definition der PMI ins Ged√§chtnis, s. Gleichung¬†5.1.\nMit R kann man die PMI z.B. so berechnen, s. ? pairwise_pmi aus dem Paket {widyr}.\nZum Paket widyr von Robinson und Silge:\n\nThis package wraps the pattern of un-tidying data into a wide matrix, performing some processing, then turning it back into a tidy form. This is useful for several mathematical operations such as co-occurrence counts, correlations, or clustering that are best done on a wide matrix.\n\nQuelle\nErzeugen wir uns Dummy-Daten:\n\ndat <- tibble(feature = rep(1:5, each = 2),\n              item = c(\"a\", \"b\",\n                       \"a\", \"c\",\n                       \"a\", \"c\",\n                       \"b\", \"e\",\n                       \"b\", \"f\"))\n\ndat\n\n\n\n\n\nfeature\nitem\n\n\n\n\n1\na\n\n\n1\nb\n\n\n2\na\n\n\n2\nc\n\n\n3\na\n\n\n3\nc\n\n\n4\nb\n\n\n4\ne\n\n\n5\nb\n\n\n5\nf\n\n\n\n\n\n\nAus der Hilfe der Funktion:\n\nFind pointwise mutual information of pairs of items in a column, based on a ‚Äúfeature‚Äù column that links them together. This is an example of the spread-operate-retidy pattern.\n\nDie Argumente der Funktion sind:\nitem\nItem to compare; will end up in item1 and item2 columns\nfeature\nColumn describing the feature that links one item to others\nManche Berechnungen (Operationen) lassen sich vielleicht leichter nicht in der Tidy-Form (Langform), sondern in der ‚Äúbreiten‚Äù oder Matrixform ausf√ºhren. Wandeln wir mal dat von der Langform in die Breitform um:\n\ntable(dat$item, dat$feature)\n\n   \n    1 2 3 4 5\n  a 1 1 1 0 0\n  b 1 0 0 1 1\n  c 0 1 1 0 0\n  e 0 0 0 1 0\n  f 0 0 0 0 1\n\n\nSilge und Robinson verdeutlichen das Prinzip von widyr so, s. Abbildung¬†5.4.\n\n\n\nAbbildung¬†5.4: Die Funktionsweise von widyr, Quelle: Silge und Robinson\n\n\n(Vgl. auch die Erkl√§rung hier.)\nBauen wir das mal von Hand nach.\nRandwahrscheinlichkeiten von a und c sowie deren Produkt, p_a_p_c:\n\np_a <- 3/5\np_c <- 2/5\n\np_a_p_c <- p_a * p_c\np_a_p_c\n\n[1] 0.24\n\n\nGemeinsame Wahrscheinlichkeit von a und c:\n\np_ac <- 2/5\n\nPMI von Hand berechnet:\n\nlog(p_ac/p_a_p_c)\n\n[1] 0.5108256\n\n\nMan beachte, dass hier als Basis \\(e\\), der nat√ºrliche Logarithmus, verwendet wurde (nicht 2).\nJetzt berechnen wir die PMI mit pairwise_pmi.\n\npairwise_pmi(dat, item = item, feature = feature)\n\n\n\n\n\nitem1\nitem2\npmi\n\n\n\n\nb\na\n-0.5877867\n\n\nc\na\n0.5108256\n\n\na\nb\n-0.5877867\n\n\ne\nb\n0.5108256\n\n\nf\nb\n0.5108256\n\n\na\nc\n0.5108256\n\n\nb\ne\n0.5108256\n\n\nb\nf\n0.5108256\n\n\n\n\n\n\nWie man sieht, entspricht unserer Berechnung von Hand der Berechnung mit pairwise_pmi.\n\n\n5.5.2 Sliding\nSliding ist ein interessantes Konzept, aber man braucht vielleicht etwas Zeit, um sein Hirn um das Konzept zu wickeln‚Ä¶\nHier eine Illustration:\n\ntxt_vec <- \"Das ist ein Test, von dem nicht viel zu erwarten ist\"\n\nslider::slide(txt_vec, ~ .x, .before = 2)\n\n[[1]]\n[1] \"Das ist ein Test, von dem nicht viel zu erwarten ist\"\n\n\nOh, da passiert nichts?! Kaputt? Nein, wir m√ºssen jedes Wort als ein Element des Vektors auffassen.\n\ntxt_df <-\n  tibble(txt = txt_vec) %>% \n  unnest_tokens(input = txt, output = word)\n\nhead(txt_df)\n\n\n\n\n\nword\n\n\n\n\ndas\n\n\nist\n\n\nein\n\n\ntest\n\n\nvon\n\n\ndem\n\n\n\n\n\n\n\nslider::slide(txt_df$word, ~ .x, .before = 2)\n\n[[1]]\n[1] \"das\"\n\n[[2]]\n[1] \"das\" \"ist\"\n\n[[3]]\n[1] \"das\" \"ist\" \"ein\"\n\n[[4]]\n[1] \"ist\"  \"ein\"  \"test\"\n\n[[5]]\n[1] \"ein\"  \"test\" \"von\" \n\n[[6]]\n[1] \"test\" \"von\"  \"dem\" \n\n[[7]]\n[1] \"von\"   \"dem\"   \"nicht\"\n\n[[8]]\n[1] \"dem\"   \"nicht\" \"viel\" \n\n[[9]]\n[1] \"nicht\" \"viel\"  \"zu\"   \n\n[[10]]\n[1] \"viel\"     \"zu\"       \"erwarten\"\n\n[[11]]\n[1] \"zu\"       \"erwarten\" \"ist\"     \n\n\nAh!\nDas Aufteilen in einzelne W√∂rter pro Element des Vektors k√∂nnte man auch so erreichen:\n\ntxt_vec2 <- str_split(txt_vec, pattern = boundary(\"word\")) %>% \n  simplify() \n\nslide(txt_vec2, .f = ~.x, .before = 2)\n\nShifting non-numeric variables is not possible.\n  Try using 'to_numeric()' and specify the 'lowest' argument.\n\n\n [1] \"Das\"      \"ist\"      \"ein\"      \"Test\"     \"von\"      \"dem\"     \n [7] \"nicht\"    \"viel\"     \"zu\"       \"erwarten\" \"ist\"     \n\n\nIn unserem Beispiel mit den Beschwerden:\n\nslide(beschwerde1_text$word,  ~.x, .before = 2)\n\nShifting non-numeric variables is not possible.\n  Try using 'to_numeric()' and specify the 'lowest' argument.\n\n\n [1] \"systems\"    \"inc\"        \"is\"         \"trying\"     \"to\"        \n [6] \"collect\"    \"a\"          \"debt\"       \"that\"       \"is\"        \n[11] \"not\"        \"mine\"       \"not\"        \"owed\"       \"and\"       \n[16] \"is\"         \"inaccurate\"\n\n\n\n\n5.5.3 Funktion slide_windows\nDie Funktion slide_windows im Kapitel 5.2 ist recht kompliziert. In solchen F√§llen ist es hilfreich, sich jeden Schritt einzeln ausf√ºhren zu lassen. Das machen wir jetzt mal.\nHier ist die Syntax der Funktion slide_windows:\n\nslide_windows <- function(tbl, window_size) {\n  skipgrams <- slider::slide(\n    tbl, \n    ~.x,  # Syntax √§hnlich zu purrr::map()\n    .after = window_size - 1, \n    .step = 1, \n    .complete = TRUE\n  )\n  \n  safe_mutate <- safely(mutate)\n  \n  out <- map2(skipgrams,\n              1:length(skipgrams),\n              ~ safe_mutate(.x, window_id = .y))\n  \n  out %>%\n    transpose() %>%\n    pluck(\"result\") %>%\n    compact() %>%\n    bind_rows()\n}\n\nErschwerend kommt eine gro√üe Datenmenge und eine lange Berechnungszeit dazu, was das Debuggen (Nachvollziehen und Durchdenken) der Schritte zus√§tzlich erschwert. In solchen F√§llen hilft die goldene Regel: Mach es dir so einfach wie m√∂glich (aber nicht einfacher). Wir nutzen also den stark verkleinerten Datensatz nested_words3, den wir oben importiert haben.\nZuerst erlauben wir mal, dasss unsere R-Session mehrere Kerne benutzen darf.\n\nplan(multisession)  ## for parallel processing\n\nDie Funktion slide_windows ist recht kompliziert. Es hilft oft, sich mit debug(fun) eine Funktion Schritt f√ºr Schritt anzuschauen.\nGehen wir Schritt f√ºr Schritt durch die Syntax von slide_windows.\nWerfen wir einen Blick in words, erstes Element (ein Tibble mit einer Spalte). Denn die einzelnen Elemente vonwordswerden an die Funktionslide_windows` als ‚ÄúFutter‚Äù √ºbergeben.\n\nfutter1 <- nested_words3[[\"words\"]][[1]]\nfutter1\n\n\n\n\n\nword\n\n\n\n\nsystems\n\n\ninc\n\n\nis\n\n\ntrying\n\n\nto\n\n\ncollect\n\n\na\n\n\ndebt\n\n\nthat\n\n\nis\n\n\nnot\n\n\nmine\n\n\nnot\n\n\nowed\n\n\nand\n\n\nis\n\n\ninaccurate\n\n\n\n\n\n\nDas ist der Text der ersten Beschwerde.\nOkay, also dann geht‚Äôs los durch die einzelnen Schritte der Funktion slide_windows.\nZun√§chst holen wir uns die ‚ÄúFenster‚Äù oder ‚ÄúSkipgrams‚Äù:\n\nskipgrams1 <- slider::slide(\n   futter1, \n    ~.x, \n    .after = 3, \n    .step = 1, \n    .complete = TRUE\n  )\n\nBei slide(tbl, ~.x) geben wir die Funktion an, die auf tbl angewendet werden soll. Daher auch die Tilde, die uns von purrr::map() her bekannt ist. In unserem Fall wollen wir nur die Elemente auslesen; Elemente auslesen erreicht man, in dem man sie mit Namen anspricht, in diesem Fall mit dem Platzhalter .x.\nJedes Element von skipgrams1 ist ein 4*1-Tibble und ist ein Skripgram.\n\nskipgrams1 %>% str()\n\nList of 17\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"systems\" \"inc\" \"is\" \"trying\"\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"inc\" \"is\" \"trying\" \"to\"\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"is\" \"trying\" \"to\" \"collect\"\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"trying\" \"to\" \"collect\" \"a\"\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"to\" \"collect\" \"a\" \"debt\"\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"collect\" \"a\" \"debt\" \"that\"\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"a\" \"debt\" \"that\" \"is\"\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"debt\" \"that\" \"is\" \"not\"\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"that\" \"is\" \"not\" \"mine\"\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"is\" \"not\" \"mine\" \"not\"\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"not\" \"mine\" \"not\" \"owed\"\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"mine\" \"not\" \"owed\" \"and\"\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"not\" \"owed\" \"and\" \"is\"\n $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"owed\" \"and\" \"is\" \"inaccurate\"\n $ : NULL\n $ : NULL\n $ : NULL\n\n\nDas zweite Skipgram von skipgrams1 enth√§lt, naja, das zweite Skipgram.\n\nskipgrams1[[2]] %>% str()\n\ntibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n $ word: chr [1:4] \"inc\" \"is\" \"trying\" \"to\"\n\n\nUnd so weiter.\nOkay, weiter im Programm. Jetzt mappen wir das Erstellen der Skipgrams\n\nsafe_mutate <- safely(mutate)\n  \nout1 <- map2(skipgrams1,\n             1:length(skipgrams1),\n             ~ safe_mutate(.x, window_id = .y))\n  \nout1 %>% \n  head(2) %>% \n  str()\n\nList of 2\n $ :List of 2\n  ..$ result: tibble [4 √ó 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ word     : chr [1:4] \"systems\" \"inc\" \"is\" \"trying\"\n  .. ..$ window_id: int [1:4] 1 1 1 1\n  ..$ error : NULL\n $ :List of 2\n  ..$ result: tibble [4 √ó 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ word     : chr [1:4] \"inc\" \"is\" \"trying\" \"to\"\n  .. ..$ window_id: int [1:4] 2 2 2 2\n  ..$ error : NULL\n\n\nout1 ist eine Liste mit 17 Elementen; jedes Element mit jeweils zwei Elementen: Den Ergebnissen und ob es einen Fehler gab bei safe_mutate. Die 10 Elemente entsprechen den 10 Skipgrams. Wir k√∂nnen aber out1 auch ‚Äúdrehen‚Äù, transponieren genauer gesagt. so dass wir eine Liste mit zwei Elementen bekommen: das erste Element hat die (zehn) Ergebnisse (n√§mlich die Skipgrams) und das zweite Elemente, ob es Fehler gab.\nDas Prinzip des Transponierens ist in Abbildung¬†5.5 dargestellt.\n\n\n\nAbbildung¬†5.5: Transponieren einer Matrix (‚ÄúTabelle‚Äù)\n\n\n\nout2 <-\nout1 %>%\n  transpose() \n\nPuh, das ist schon anstrengendes Datenyoga‚Ä¶\nAber jetzt ist es einfach. Wir ziehen das erste der beiden Elemente, die Ergebnisse heraus (pluck), entfernen leere Elemente (compact) und machen einen Tibble daraus (bind_rows):\n\nout2 %>% \n  pluck(\"result\") %>%\n  compact() %>%\n  bind_rows() %>% \n  head()\n\n\n\n\n\nword\nwindow_id\n\n\n\n\nsystems\n1\n\n\ninc\n1\n\n\nis\n1\n\n\ntrying\n1\n\n\ninc\n2\n\n\nis\n2\n\n\n\n\n\n\nGeschafft!\n\n\n5.5.4 √Ñhnlichkeit berechnen\nNachdem wir jetzt slide_windows kennen, schauen wir uns die n√§chsten Schritte an:\n\ntidy_pmi1 <- nested_words3 %>%  # <--- Kleiner Datensatz!\n  mutate(words = future_map(words, slide_windows, 4L))\n\nWir werden slide_windows auf die Liste words an, die die Beschwerden enth√§lt. F√ºr jede Beschwerde erstellen wir die Skipgrams; diese Schleife wird realisiert √ºber map bzw. future_map, die uns erlaubt, mehrere Kerne des Rechners gleichzeitig zu nutzen, damit es schneller geht.\nHier sehen wir z.B. die Skipgram-IDs der ersten Beschwerde.\n\ntidy_pmi1[[\"words\"]][[1]] %>% \n  head()\n\n\n\n\n\nword\nwindow_id\n\n\n\n\nsystems\n1\n\n\ninc\n1\n\n\nis\n1\n\n\ntrying\n1\n\n\ninc\n2\n\n\nis\n2\n\n\n\n\n\n\nGenestet siehst es so aus:\n\ntidy_pmi1 %>% \n  head(1)\n\n\n\n\n\n\n\n\n\ncomplaint_id\nwords\n\n\n\n\n3384392\nsystems , inc , is , trying , inc , is , trying , to , is , trying , to , collect , trying , to , collect , a , to , collect , a , debt , collect , a , debt , that , a , debt , that , is , debt , that , is , not , that , is , not , mine , is , not , mine , not , not , mine , not , owed , mine , not , owed , and , not , owed , and , is , owed , and , is , inaccurate, 1 , 1 , 1 , 1 , 2 , 2 , 2 , 2 , 3 , 3 , 3 , 3 , 4 , 4 , 4 , 4 , 5 , 5 , 5 , 5 , 6 , 6 , 6 , 6 , 7 , 7 , 7 , 7 , 8 , 8 , 8 , 8 , 9 , 9 , 9 , 9 , 10 , 10 , 10 , 10 , 11 , 11 , 11 , 11 , 12 , 12 , 12 , 12 , 13 , 13 , 13 , 13 , 14 , 14 , 14 , 14\n\n\n\n\n\n\nDie Listenspalte entschachteln wir mal:\n\ntidy_pmi2 <- tidy_pmi1 %>% \n  unnest(words)  # entschachtele\n\ntidy_pmi2 %>% \n  head()\n\n\n\n\n\ncomplaint_id\nword\nwindow_id\n\n\n\n\n3384392\nsystems\n1\n\n\n3384392\ninc\n1\n\n\n3384392\nis\n1\n\n\n3384392\ntrying\n1\n\n\n3384392\ninc\n2\n\n\n3384392\nis\n2\n\n\n\n\n\n\nZum Berechnen der √Ñhnlichkeit brauchen wir eineindeutige IDs, nach dem Prinzip ‚Äú1. Skipgram der 1. Beschwerde‚Äù etc:\n\ntidy_pmi3 <- tidy_pmi2 %>% \n  unite(window_id, complaint_id, window_id)  # f√ºhre Spalten zusammen\n\ntidy_pmi3 %>% \n  head()\n\n\n\n\n\nwindow_id\nword\n\n\n\n\n3384392_1\nsystems\n\n\n3384392_1\ninc\n\n\n3384392_1\nis\n\n\n3384392_1\ntrying\n\n\n3384392_2\ninc\n\n\n3384392_2\nis\n\n\n\n\n\n\nSchlie√ülich berechnen wir die √Ñhnlichkeit mit pairwise_pmi, das hatten wir uns oben schon mal n√§her angeschaut:\n\ntidy_pmi4 <- tidy_pmi3 %>% \n  pairwise_pmi(word, window_id)  # berechne √Ñhnlichkeit\n\ntidy_pmi <- tidy_pmi4  # mit dem Objekt arbeiten wir dann weiter\n\ntidy_pmi %>% \n  head()\n\n\n\n\n\nitem1\nitem2\npmi\n\n\n\n\ninc\nsystems\n5.728498\n\n\nis\nsystems\n2.838126\n\n\ntrying\nsystems\n5.035351\n\n\nsystems\ninc\n5.728498\n\n\nis\ninc\n2.838126\n\n\ntrying\ninc\n5.035351\n\n\n\n\n\n\n\n\n5.5.5 SVD\nDie Singul√§rwertzerlegung (singular value decomposition, SVD) ist ein Verfahren zur Verringerung der Spaltenzahl (Dimension), vergleichbar zur Faktorenanalyse. Zur Anschaulichkeit - und ohne substanziellen Hintergrund - sei folgendes Beispiel genannt: Die Verben ‚Äúgehen‚Äù, ‚Äúrennen‚Äù, ‚Äúlaufen‚Äù, ‚Äúschwimmen‚Äù, ‚Äúfahren‚Äù, ‚Äúrutschen‚Äù k√∂nnten zu einer gemeinsamen Dimension, etwa ‚Äúfortbewegen‚Äù reduziert werden. Jedes einzelne der eingehenden Verben erh√§lt eine Zahl von 0 bis 1, das die konzeptionelle N√§he des Verbs zur ‚Äúdahinterliegenden‚Äù Dimension (fortbewegen) quantifiziert; die Zahl nennt man auch die ‚ÄúLadung‚Äù des Items (Worts) auf die Dimension. Sagen wir, wir identifizieren 10 Dimensionen. Man erh√§lt dann f√ºr jedes unique Wort im Corpus einen Vektor mit den Ladungen auf die Dimensionen. Im genannten Beispiel w√§re es ein 10-stelliger Vektor. So wie ein 3-stelliger Vektor die Position einer Biene im Raum beschreibt11, beschreibt hier unser 10-stelliger Vektor die ‚ÄúPosition‚Äù eines Worts in unserem Einbettungsvektor.\nDie Syntax dazu ist dieses Mal einfach:\n\ntidy_word_vectors <- \n  tidy_pmi %>%\n  widely_svd(\n    item1, item2, pmi,\n    nv = 100\n  )\n\ntidy_word_vectors %>% \n  (head)\n\n\n\n\n\nitem1\ndimension\nvalue\n\n\n\n\ninc\n1\n-0.0378963\n\n\nis\n1\n-0.1132069\n\n\ntrying\n1\n-0.0512764\n\n\nsystems\n1\n-0.0333332\n\n\nto\n1\n-0.1203434\n\n\ncollect\n1\n-0.0554211\n\n\n\n\n\n\nMit nv = 100 haben wir die Anzahl (n) der Dimensionen (Variablen, v) auf 100 bestimmt.\n\n\n5.5.6 Wort√§hnlichkeit\nJetzt, da wir mit der SVD jedes Wort in einem Koordinatensystem verortet haben, k√∂nnen wir die Abst√§nde der W√∂rter im Koordinatensystem bestimmen. Das geht mit Hilfe des alten Pythagoras, s. Abbildung¬†5.6. Der Abstand, den man mit Hilfe des Satz des Pythagoras berechnet, nennt man auch euklidische Distanz.\n\n\n\nAbbildung¬†5.6: Euklidische Distanz in 2D, Quelle: Wikipedia, CC BY4.0, Kmhkmh\n\n\nOkay, wir sind in einem Raum mit vielen Dimensionen, was mein Hirn nicht mitmacht, aber der Algebra ist das egal. Pythagoras‚Äô Satz l√§sst sich genauso anwenden, wenn es mehr als Dimensionen sind.\nDie Autoren basteln sich selber eine Funktion in Kap. 5.3, aber der Einfachheit halber nehme ich (erstmal) die entsprechende Funktion aus widyr:\n\nword_neighbors <- \ntidy_word_vectors %>% \n  pairwise_dist(item1, dimension, value)\n\nword_neighbors %>% \n  head()\n\n\n\n\n\nitem1\nitem2\ndistance\n\n\n\n\nis\ninc\n1.0220141\n\n\ntrying\ninc\n0.9332851\n\n\nsystems\ninc\n0.4161215\n\n\nto\ninc\n1.0913872\n\n\ncollect\ninc\n0.5221759\n\n\na\ninc\n1.0309566\n\n\n\n\n\n\nSchauen wir uns ein Beispiel an. Was sind die Nachbarn von ‚Äúinaccurate‚Äù?\n\nword_neighbors %>% \n  filter(item1 == \"inaccurate\") %>% \n  arrange(distance) %>% \n  head()\n\n\n\n\n\nitem1\nitem2\ndistance\n\n\n\n\ninaccurate\nmine\n0.5248868\n\n\ninaccurate\nscore\n0.5310116\n\n\ninaccurate\noh\n0.5400913\n\n\ninaccurate\nny\n0.5400913\n\n\ninaccurate\ndob\n0.5801281\n\n\ninaccurate\ncell\n0.6093670\n\n\n\n\n\n\nHier ist die Datenmenge zu klein, um vern√ºnftige Schl√ºsse zu ziehen. Aber ‚Äúincorrectly‚Äù, ‚Äúcorrect‚Äù, ‚Äúbalance‚Äù sind wohl plausible Nachbarn von ‚Äúinaccurate‚Äù.\n\n\n5.5.7 Cosinus-√Ñhnlichkeit\nDie N√§he zweier Vektoren l√§sst sich, neben der euklidischen Distanz, auch z.B. √ºber die Cosinus-√Ñhnlichkeit (Cosine similarity) berechnen, vgl. auch Abbildung¬†5.7:\n\n\n\nAbbildung¬†5.7: Die Cosinus-√Ñhnlichkeit zweier Vektoren\n\n\nQuelle: Mazin07, Lizenz: PD\n\\[{\\displaystyle {\\text{Cosinus-√Ñhnlichkeit}}=S_{C}(A,B):=\\cos(\\theta )={\\mathbf {A} \\cdot \\mathbf {B}  \\over \\|\\mathbf {A} \\|\\|\\mathbf {B} \\|}={\\frac {\\sum \\limits _{i=1}^{n}{A_{i}B_{i}}}{{\\sqrt {\\sum \\limits _{i=1}^{n}{A_{i}^{2}}}}{\\sqrt {\\sum \\limits _{i=1}^{n}{B_{i}^{2}}}}}},}\\]\nwobei \\(A\\) und \\(B\\) zwei Vektoren sind und \\(\\|\\mathbf {A} \\|\\) das Skalarprodukt von A (und B genauso). Das Skalarprodukt von \\(\\color {red} {a = {\\displaystyle [a_{1},a_{2},\\cdots ,a_{n}]}}\\) und \\(\\color {blue} {b = {\\displaystyle [b_{1},b_{2},\\cdots ,b_{n}]}}\\) ist so definiert:\n\\[{\\displaystyle \\mathbf {\\color {red}a} \\cdot \\mathbf {\\color {blue}b} =\\sum _{i=1}^{n}{\\color {red}a}_{i}{\\color {blue}b}_{i}={\\color {red}a}_{1}{\\color {blue}b}_{1}+{\\color {red}a}_{2}{\\color {blue}b}_{2}+\\cdots +{\\color {red}a}_{n}{\\color {blue}b}_{n}}\\]\nEntsprechend ist die Funktion nearest_neighbors zu verstehen aus Kap. 5.3:\n\nnearest_neighbors <- function(df, token) {\n  df %>%\n    widely(\n      ~ {\n        y <- .[rep(token, nrow(.)), ]\n        res <- rowSums(. * y) / \n          (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))\n        \n        matrix(res, ncol = 1, dimnames = list(x = names(res)))\n      },\n      sort = TRUE\n    )(item1, dimension, value) %>%\n    select(-item2)\n}\n\nWobei mit widely zuerst noch von der Langform in die Breitform umformatiert wird, da die Breitform von der Formel verlangt wird bzw. das Rechnen einfacher macht.\nDer eine Vektor ist das Embedding des Tokens, der andere Vektor ist das mittlere Embedding √ºber alle Tokens des Corpus. Wenn die Anzahl der Elemente konstant bleibt, kann man sich das Teilen durch \\(n\\) schenken, wenn man einen Mittelwert berechnen; so h√§lt es auch die Syntax von nearest_neighbors.\nEin n√ºtzlicher Post zur Cosinus-√Ñhnlichkeit findet sich hier. Dieses Bild zeigt das Konzept der Cosinus-√Ñhnlichkeit anschaulich.\nZur Erinnerung: Der Cosinus eines Winkels ist definiert als Verh√§ltnis der L√§nge der Ankathete (das ist jene Kathete, die einen Schenkel des Winkels bildet) zur L√§nge der Hypotenuse12 in einem rechtwinkligen, vgl. Abbildung¬†5.8.\n\n\n\nAbbildung¬†5.8: Ein rechtwinkliges Dreieck als Grundlage der trigonometrischen Funktionen\n\n\nAlso: \\({\\displaystyle \\cos \\alpha ={\\frac {b}{c}}}\\)\nQuelle: PaterSigmund, Wikipedia,dre CC-BY-SA 2.5\nHilfreich ist auch die Visualisierung von Sinus und Cosinus am Einheitskreis; gerne animiert betrachten.\n\n\n5.5.8 Word-Embeddings vorgekocht: Glove6B\nIn Kap. 5.4 schreiben die Autoren:\n\nIf your data set is too small, you typically cannot train reliable word embeddings.\n\nEin paar Millionen W√∂rter sollte der Corpus schon enthalten, so die Autoren. Da solche ‚ÄúWorteinbettungen‚Äù (word embedings) aufw√§ndig zu erstellen sind, kann man fertige, ‚Äúvorgekochte‚Äù Produkte nutzen.\nGlove6B wurde anhand von Wikipedia und anderen Datenquellen erstellt (Pennington, Socher, und Manning 2014).\n\n\n\n\n\n\nHinweis\n\n\n\nDie zugeh√∂rigen Daten sind recht gro√ü; f√ºr glove6b (Pennington, Socher, und Manning 2014) ist fast ein Gigabyte f√§llig. Sie sollten sich die Daten in einem ruhigen Moment (mit stabiler Internetverbindung) herunterladen und in einem Verzeichnis meiner Wahl abgespeichert (datasets). Da bei mir Download abbrach, als ich embedding_glove6b(dimensions = 100) aufrief, habe ich die Daten manuell heruntergeladen, s.u.\n\n\n\nglove6b <- \n  embedding_glove6b(dir = \"~/datasets\", dimensions = 50, manual_download = TRUE)\n\nglove6b %>% \n  select(1:5) %>% \n  head()\n\n\n\n\n\ntoken\nd1\nd2\nd3\nd4\n\n\n\n\nthe\n0.418000\n0.249680\n-0.41242\n0.121700\n\n\n,\n0.013441\n0.236820\n-0.16899\n0.409510\n\n\n.\n0.151640\n0.301770\n-0.16763\n0.176840\n\n\nof\n0.708530\n0.570880\n-0.47160\n0.180480\n\n\nto\n0.680470\n-0.039263\n0.30186\n-0.177920\n\n\nand\n0.268180\n0.143460\n-0.27877\n0.016257\n\n\n\n\n\n\nIn eine Tidyform bringen:\n\ntidy_glove <- \n  glove6b %>%\n  pivot_longer(contains(\"d\"),\n               names_to = \"dimension\") %>%\n  rename(item1 = token)\n\nhead(tidy_glove)\n\n\n\n\n\nitem1\ndimension\nvalue\n\n\n\n\nthe\nd1\n-0.038194\n\n\nthe\nd2\n-0.244870\n\n\nthe\nd3\n0.728120\n\n\nthe\nd4\n-0.399610\n\n\nthe\nd5\n0.083172\n\n\nthe\nd6\n0.043953\n\n\n\n\n\n\nGanz sch√∂n gro√ü:\n\nobject.size(tidy_glove)\n\n983837536 bytes\n\n\nIn Megabyte13\n\nobject.size(tidy_glove) / 2^20\n\n938.3 bytes\n\n\nEinfacher und genauer geht es so:\n\npryr::object_size(tidy_glove)\n\n983.83 MB\n\n\n\npryr::mem_used()\n\n1.5 GB\n\n\nUm Speicher zu sparen, k√∂nnte man glove6b wieder direkt l√∂schen, wenn man nur mit der Tidyform weiterarbeitet.\n\nrm(glove6b)\n\nJetzt k√∂nnen wir wieder nach Nachbarn fragen, im euklidischen Sinne, wie oben. Probieren wir aus, welche W√∂rter nah zu ‚Äúinaccurate‚Äù stehen.\n\n\n\n\n\n\nHinweis\n\n\n\nWie wir oben gesehen haben, ist der Datensatz riesig14, was die Berechnungen (zeitaufw√§ndig) und damit nervig machen k√∂nnen. Dar√ºber hinaus kann es n√∂tig sein, dass Sie mehr Speicher auf Ihrem Computer zur Verf√ºgung stellen m√ºssen15. Wir m√ºssen noch maximum_size = NULL, um das Jonglieren mit riesigen Matrixen zu erlauben. M√∂ge der Gott der RAMs und Arbeitsspeicher uns gn√§dig sein!\n\n\nMit pairwise_dist dauert die Berechnung sehr lange und braucht wohl sehr viel Speicher. Mitunter kam folgender Fehler auf: ‚ÄúR error: vector memory exhausted (limit reached?)‚Äù.\n\nword_neighbors_glove6b <- \ntidy_glove %>% \n  slice_head(prop = .1) %>% \n  pairwise_dist(item1, dimension, value, maximum_size = NULL)\n\nhead(word_neighbors_glove6b)\n\ntidy_glove %>% \n  filter(item1 == \"inaccurate\") %>% \n  arrange(-value) %>% \n  slice_head(n = 5)\n\nDeswegen probieren wir doch die Funktion nearest_neighbors, so wie es im Buch vorgeschlagen wird, s. Kap 5.3.\n\nnearest_neighbors <- function(df, token) {\n  df %>%\n    widely(\n      ~ {\n        y <- .[rep(token, nrow(.)), ]\n        res <- rowSums(. * y) / \n          (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))\n        matrix(res, ncol = 1, dimnames = list(x = names(res)))\n      },\n      sort = TRUE,\n      maximum_size = NULL\n    )(item1, dimension, value) %>%\n    select(-item2)\n}\n\n\ntidy_glove %>%\n  # slice_head(prob = .1) %>% \n  nearest_neighbors(\"error\") %>% \n  head()\n\n\n\n\n\nitem1\nvalue\n\n\n\n\nerror\n1.0000000\n\n\nerrors\n0.7916719\n\n\nmistake\n0.6641135\n\n\ncorrect\n0.6205814\n\n\nincorrect\n0.6132556\n\n\nfault\n0.6068035\n\n\n\n\n\n\nEntschachteln wir unsere Daten zu complaints:\n\ntidy_complaints3 <-\n  nested_words3 %>% \n  unnest(words)\n\nDann erstellen wir uns eine Tabelle, in der nur die Schnittmenge der W√∂rter aus den Beschwerden und Glove vorkommen. Dazu nutzen winr einen inneren Join\n\n\n\nInner Join, Quelle: Garrick Adenbuie\n\n\nQuelle\n\ncomplaints_glove <- \ntidy_complaints3 %>% \n  inner_join(by = \"word\", \n  tidy_glove %>% \n  distinct(item1) %>% \n  rename(word = item1)) \n\nhead(complaints_glove)\n\n\n\n\n\ncomplaint_id\nword\n\n\n\n\n3384392\nsystems\n\n\n3384392\ninc\n\n\n3384392\nis\n\n\n3384392\ntrying\n\n\n3384392\nto\n\n\n3384392\ncollect\n\n\n\n\n\n\nWie viele unique (distinkte) W√∂rter gibt es in unserem Corpus?\n\ntidy_complaints3_distinct_words_n <- \ntidy_complaints3 %>% \n  distinct(word) %>% \n  nrow()\n\ntidy_complaints3_distinct_words_n\n\n[1] 222\n\n\nIn tidy_complaints gibt es √ºbrigens 222 verschiedene W√∂rter.\n\nword_matrix <- tidy_complaints3 %>%\n  inner_join(by = \"word\",\n             tidy_glove %>%\n               distinct(item1) %>%\n               rename(word = item1)) %>%\n  count(complaint_id, word) %>%\n  cast_sparse(complaint_id, word, n)\n\n#word_matrix\n\nword_matrix z√§hlt f√ºr jede der 10 Beschwerden, welche W√∂rter (und wie h√§ufig) vorkommen.\n\ndim(word_matrix)\n\n[1]  10 222\n\n\nBeschwerden (Dokumente) und 222 unique W√∂rter.\n\nglove_matrix <- tidy_glove %>%\n  inner_join(by = \"item1\",\n             tidy_complaints3 %>%\n               distinct(word) %>%\n               rename(item1 = word)) %>%\n  cast_sparse(item1, dimension, value)\n\n#glove_matrix\n\nglove_matrix gibt f√ºr jedes unique Wort den Einbettungsvektor an.\n\ndim(glove_matrix)\n\n[1] 222 100\n\n\nDas sind 222 unique W√∂rter und 100 Dimensionen des Einbettungsvektors.\nJetzt k√∂nnen wir noch pro Dokument (10 in diesem Beispiel) die mittlere ‚ÄúPosition‚Äù jedes Dokuments im Einbettungsvektor ausrechnen. Bildlich gesprochen: Was ist der mittlere Raumpunkt (Zentroid) des Bienenschwarms, wobei eine Biene ein Dokument darstellt und die Raumachsen die Dimensionen des Einbettungsvektors.\nDazu gewichten wir jedes Wort eines Dokuments mit den Ladungen des Einbettungsvektor und summieren diese Terme. Es resultiert eine Matrix mit einem Einbettungsvektor pro Dokument. Diese Matrix k√∂nnen wir jetzt als Pr√§diktorenmatrix hernehmen.\n\ndoc_matrix <- word_matrix %*% glove_matrix\n#doc_matrix %>% head()\n\n\ndim(doc_matrix)\n\n[1]  10 100\n\n\nDie Anzahl der Dokumente ist 10 und die Anzahl der Dimensionen (des Einbettungsvektors) ist 100."
  },
  {
    "objectID": "word-embedding.html#fazit",
    "href": "word-embedding.html#fazit",
    "title": "5¬† Word Embedding",
    "section": "5.6 Fazit",
    "text": "5.6 Fazit\nWorteinbettungen sind eine aufw√§ndige Angelegenheit. Positiv gesprochen kann ein Analysti die Muskeln spielen lassen und zeigen, was sie oder er so alles drauf hat. Ist ja schon cooles Zeugs, die Word Embeddings. Es besteht die Chance, dass man mit dieser Methode bessere Vorhersagen erreicht, als mit anderen, einfachen Ans√§tzen wir Worth√§ufigkeiten oder tf-idf. Auf der anderen Seite ist es oft sinnvoll, mit einfachen Ans√§tzen zu starten, und zu sehen, wie weit man kommt. Vielleicht ja weit genug."
  },
  {
    "objectID": "word-embedding.html#literatur",
    "href": "word-embedding.html#literatur",
    "title": "5¬† Word Embedding",
    "section": "5.7 Literatur",
    "text": "5.7 Literatur\n\n5.7.1 Wikipedia\nEs gibt eine Reihe n√ºtzlicher (und recht informationsdichter) Wikipedia-Eintr√§ge zum Thema Informationstheorie.\n\nWikipedia: Pointwise Mutual Information\nWikipedia: Mutual information\nWikipedia: Information theory\n\n\n\n\n\nHvitfeldt, Emil, und Julia Silge. 2022. Supervised Machine Learning for Text Analysis in R. 1. Aufl. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nKurz, A. Solomon. 2021. Statistical rethinking with brms, ggplot2, and the tidyverse: Second edition. https://bookdown.org/content/4857/.\n\n\nMcElreath, Richard. 2020. Statistical rethinking: a Bayesian course with examples in R and Stan. 2. Aufl. CRC texts in statistical science. Boca Raton: Taylor; Francis, CRC Press.\n\n\nPennington, Jeffrey, Richard Socher, und Christopher Manning. 2014. ‚ÄûGloVe: Global Vectors for Word Representation‚Äú. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1532‚Äì43. Doha, Qatar: Association for Computational Linguistics. https://doi.org/10.3115/v1/D14-1162.\n\n\nShannon, C. E. 1948. ‚ÄûA Mathematical Theory of Communication‚Äú. Bell System Technical Journal 27 (3): 379‚Äì423. https://doi.org/10.1002/j.1538-7305.1948.tb01338.x."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Hvitfeldt, Emil, and Julia Silge. 2022. Supervised Machine Learning\nfor Text Analysis in r. 1st ed. Boca Raton: Chapman;\nHall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nK√∂nig, Tim, Wolf J. Sch√ºnemann, Alexander Brand, Julian Freyberg, and\nMichael Gertz. 2022. ‚ÄúThe EPINetz Twitter Politicians\nDataset 2021. A¬†New Resource for the Study of the German Twittersphere\nand Its Application for the 2021 Federal Elections.‚Äù\nPolitische Vierteljahresschrift 63 (3): 529‚Äì47. https://doi.org/10.1007/s11615-022-00405-7.\n\n\nKurz, A. Solomon. 2021. Statistical Rethinking with Brms, Ggplot2,\nand the Tidyverse: Second Edition. https://bookdown.org/content/4857/.\n\n\nLex, Alexander, Nils Gehlenborg, Hendrik Strobelt, Romain Vuillemot, and\nHanspeter Pfister. 2014. ‚ÄúUpSet: Visualization of\nIntersecting Sets.‚Äù IEEE Transactions on\nVisualization and Computer Graphics 20 (12): 1983‚Äì92. https://doi.org/10.1109/TVCG.2014.2346248.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course\nwith Examples in r and Stan. 2nd ed. CRC Texts in\nStatistical Science. Boca Raton: Taylor; Francis, CRC\nPress.\n\n\nPennington, Jeffrey, Richard Socher, and Christopher Manning. 2014.\n‚ÄúGloVe: Global Vectors for Word\nRepresentation.‚Äù In Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), 1532‚Äì43. Doha, Qatar: Association for\nComputational Linguistics. https://doi.org/10.3115/v1/D14-1162.\n\n\nRemus, Robert, Uwe Quasthoff, and Gerhard Heyer. 2010.\n‚ÄúSentiWS - a Publicly Available German-Language\nResource for Sentiment Analysis.‚Äù Proceedings of the 7th\nInternational Language Ressources and Evaluation\n(LREC‚Äô10), 1168‚Äì71.\n\n\nShannon, C. E. 1948. ‚ÄúA Mathematical Theory of\nCommunication.‚Äù Bell System Technical Journal 27 (3):\n379‚Äì423. https://doi.org/10.1002/j.1538-7305.1948.tb01338.x.\n\n\nWickham, Hadley, and Garrett Grolemund. 2018. R F√ºr Data Science:\nDaten Importieren, Bereinigen, Umformen, Modellieren Und\nVisualisieren. Translated by Frank Langenau. 1. Auflage.\nHeidelberg: O‚ÄôReilly. https://r4ds.had.co.nz/index.html."
  }
]