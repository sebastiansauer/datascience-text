[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science 2: Textdaten als Grundlage prädiktiver Modelle",
    "section": "",
    "text": "falls Sie die Pakete schon installiert haben, könnten Sie mal in RStudio auf “update.packages” klicken↩︎"
  },
  {
    "objectID": "pruefung.html",
    "href": "pruefung.html",
    "title": "1  Prüfung",
    "section": "",
    "text": "Text als Datenbasis prädiktiver Modelle\nBild von mcmurryjulie auf Pixabay"
  },
  {
    "objectID": "pruefung.html#prüfungsform-datenanalyse-als-quarto-blog-post",
    "href": "pruefung.html#prüfungsform-datenanalyse-als-quarto-blog-post",
    "title": "1  Prüfung",
    "section": "1.1 Prüfungsform: Datenanalyse als Quarto-Blog-Post",
    "text": "1.1 Prüfungsform: Datenanalyse als Quarto-Blog-Post\nAls Prüfungsleistung ist ein Corpus an Twitter-Daten, die an deutsche, aktuelle Politiker gerichtet sind, auf Hate Speech hin zu untersuchen.\n\nDer Dozent weißt jedis Studenti einen deutschen Politiker (bzw. dessen Twitter-Account) zu.\nDer Bericht der Analyse ist als Quarto Blog-Posts zu formatieren.\nEinzureichen ist die URL des Posts.\nDer Post muss während des gesamten Prüfungszeitraums online sein, gehostet von einem beliebigen Provider (z.B. Netlify oder Github).\nNach Einreichen des Posts dürfen keine Änderungen mehr vorgenommen werden.\nZu Dokumentationszwecken soll ein PDF-Print des Posts in die Abgabe mit hochgeladen werden. Das PDF-Print des Posts muss identisch (exakt gleich) sein zum Post, der über die URL verfügbar ist.\nDer Quelltext des Posts soll bei Github vorliegen.\nDie Methoden des Textminings aus dem Unterricht sollen angewendet werden\nZusätzlich dürfen sonstige Techniken des Textminings (die nicht im Unterricht behandelt wurden), angewendet werden\nDarüber hinaus sollen prädiktive Modelle zur Klassifikation von Hate-Speech (ja/nein) berechnet werden.\nEin Trainingsdatensatz wird gemeinsam erstellt.\nMethoden der Inferenzstatistik (wie Bayes) sind nicht nötig.\nEs soll eine mittlere vierstellige Zahl an Tweets verarbeitet werden oder wenigstens so viele Tweets wie verfügbar."
  },
  {
    "objectID": "pruefung.html#politiker-accounts",
    "href": "pruefung.html#politiker-accounts",
    "title": "1  Prüfung",
    "section": "1.2 Politiker-Accounts",
    "text": "1.2 Politiker-Accounts\nFolgende Politiker-Accounts könnten als Prüfungsgegenstand verwendet werden (nach Hinweisen des Dozenten):\n\nOlaf Scholz\nAnnalena Baerbock\nChristian Lindner\nRobert Habeck (bzw. der Account seines Ministeriums)\nCem Özdemir\nVolker Wissing\nNancy Faeser\nFriedrich Merz\nBjörn Höcke\nSarah Wagenknecht\n\n\n\nAlle folgenden Hinweise gelten nur insoweit Ihre Lehrkraft Ihnen keine anders lautenden Hinweise gegeben hat (schriftlich).\n\n1.3 Allgemeines\n\nGegenstand dieser Prüfungsform ist die Analyse eines Datensatzes nach einer Forschungsfrage und die Dokumentation dieser Analyse.\nSchreiben Sie Ihre Datenanalyse in Form eines Berichts, der sich an den Gliederungspunkten wie unten dargestellt orientiert.\nWenden Sie die passenden, im Unterricht eingeführten, statistischen Verfahren an. Es steht Ihnen frei, andere (nicht im Unterricht behandelte) Verfahren zur Analyse der Daten anzuwenden, nach Maßgabe der fachlichen Angemessenheit.\nWerten Sie die Daten mit R aus.\nDie R-Syntax soll im Hauptteil des Berichts dokumentiert werden. R-Output darf ggf. gekürzt wiedergegeben werden.\nFügen Sie keine Erklärungen oder Definitionen von statistischen Verfahren an.\nBeschreiben und interpretieren Sie jede Analyse bzw. jeden R-Code bzw. jedes Ergebnis (jede R-Ausgabe).\nVon hoher Bedeutung ist die Korrektheit der Beschreibung und Interpretation der statistischen Modellierung (z.B. mit der Regressionsanalyse).\nEs hat keinen Einfluss auf Ihre Note, ob sich ein (erwarteter) Effekt zeigt und wie stark dieser Effekt ggf. ist.\n\n\n\n1.4 Formatierung des Berichts\n\nDer Bericht ist nur elektronisch, nicht ausgedruckt einzureichen.\nDer Bericht kann als paginiertes Dokument oder als nicht-paginiertes Dokument (d.h. als HTML-Dokument) eingereicht werden.\nPaginierte Formate sind als PDF-Datei einzureichen. HTML-Formate sind entweder in HTML-Form oder (nur bei Bedarf) als PDF-Druck eines HTML-Dokuments einzureichen.\nDie Wahl eines bestimmten Stylesheets ist nicht von Bedeutung. Lesbarkeit und Übersichtlichkeit in der Formatierung sind unabhängig davon anzustreben.\nIm Kopfbereich (oder auf einem Deckblatt) sind die relevanten Metadatan anzugeben wie Name (Nachname, Vorname) der Autorin/des Autors, Abgabedatum, Titel der Arbeit, Modul.\n\n\n\n1.5 Formalia\n\nRichtlinien einer Wortzahl gibt es nicht. Entscheidend ist, dass relevante Analysen durchgeführt und beschrieben wurden.\nDer Anspruch richtet sich nach dem Inhalt und Niveau der auf dieses Modul vorbereitenden Module. Oft sind das Module in quantitativer Datenanalyse (und wissenschaftliches Arbeiten). Deren Inhalte sollen im Rahmen dieser Prüfungsleistung als selbständig und flüssig verfügbare Kompetenz von den Studentis demonstriert werden.\nDie Gliederung der Arbeit kann sich an den PPDAC-Zyklus und am Data Science Model von Wickham und Grolemund orientieren.\n\n\n\n1.6 Beurteilungskriterien\nDie Arbeit wird im Hinblick auf drei Kriterien bewertet:\n\nFormalia (z. B. Vollständigkeit der Abarbeitung, Angemessenheit der äußeren Gestaltung, Fokus auf Wesentliches)\nMethodik (z. B. Richtige Auswahl und Anwendung der Verfahren)\nInhalt (z. B. Verständlichkeit, Breite und Tiefe der Problemlösung, Korrektheit der Interpretation)\n\nSie erhalten für jedes der drei Kriterien eine Teilnote sowie eine Gesamtnote. Außerdem erhalten Sie ggf. für die Kriterien noch ausformulierte Hinweise.\nDie Gesamtnote muss sich nicht als Mittelwert der Teilnoten ergeben.\nInsbesondere kann eine Fünf in einem der Kriterien zum Durchfallen führen, auch wenn die anderen beiden Kriterien gut oder sehr gut beurteilt wurden.\n\n\n1.7 Beispiele für Aspekte der Beurteilungskriterien\n\nWurden deskriptive Statistiken (an angemessenen Ort) berichtet?\nWurden Diagramme und Tabellen angemessen eingesetzt?\nWurde Inferenzstatistik (angemessen) eingesetzt?\nWurden Effektstärkemaße (idealerweise mit Konfidenzintervallen dazu) berichtet?\nWurden alle relevanten Informationen für ein statistisches Verfahren angegeben (z.B. zum gewählten Prior)?\nWurde die Aussagekraft von Modellergebnissen richtig eingeschätzt?\nWaren die Schlussfolgerungen, die aus den statistischen Ergebnissen gezogen wurden, angemessen (z. B. wurde erkannt, dass ein Nicht-Verwerfeen einer Hypothese nicht automatisch ein Bestätigen derselben bedeutet)?\nWurde angemessen gerundet (inkl. konsistente Anzahl von Nachkommastellen)?\nPassen die statistischen Verfahren zu den Hypothesen?\nWurden die Voraussetzungen der statistischen Verfahren geprüft?\nSind die Ergebnisse reproduzierbar (Daten und Syntax eingereicht)?\n\n\n\n1.8 Beispiele für Fehler\nSchwere Fehler, die zum Durchfallen oder deutlichem Abwerten der Note führen können, sind z.B.:\n\nfehlende Inferenzstatistik (oder adäquatem Ersatz)\nfalsche Interpretation von Posteriori-Verteilungen oder p-Werten\nkeine Angabe von Konfidenzintervallen\nfalsche Interpretation von Konfidenzintervallen\nWahl des falschen Intervalls (Vorhersageintervall vs. Perzentilintervall vs. HDI)\nfalsche Entscheidung zum Hypothesentest auf Basis entsprechender Kennwerte (wie ROPE-Wahrscheinlichkeit oder p-Wert)\nfalsche Wahl des statistischen Verfahrens\nfehlende Deskriptivstatistik\n\nHäufige kleinere Mängel sind z. B.\n\npixelige Abbildungen\nR-Ausgaben oder R-Syntax als Screenshot\nfehlende Seitenzahlen (nur bei paginierten Formaten, nicht bei HTML)\nunübersichtliche Diagramme\nkein (verlinktes) Inhaltsverzeichnis ︎\nfehlende oder unverständliche Achsenbeschriftung bei Diagrammen\nfehlende oder falsche Beschreibung der/des Skalenniveau(s) der untersuchten Variablen"
  },
  {
    "objectID": "pruefung.html#allgemeines",
    "href": "pruefung.html#allgemeines",
    "title": "1  Prüfung",
    "section": "1.3 Allgemeines",
    "text": "1.3 Allgemeines\n\nGegenstand dieser Prüfungsform ist die Analyse eines Datensatzes nach einer Forschungsfrage und die Dokumentation dieser Analyse.\nSchreiben Sie Ihre Datenanalyse in Form eines Berichts, der sich an den Gliederungspunkten wie unten dargestellt orientiert.\nWenden Sie die passenden, im Unterricht eingeführten, statistischen Verfahren an. Es steht Ihnen frei, andere (nicht im Unterricht behandelte) Verfahren zur Analyse der Daten anzuwenden, nach Maßgabe der fachlichen Angemessenheit.\nWerten Sie die Daten mit R aus.\nDie R-Syntax soll im Hauptteil des Berichts dokumentiert werden. R-Output darf ggf. gekürzt wiedergegeben werden.\nFügen Sie keine Erklärungen oder Definitionen von statistischen Verfahren an.\nBeschreiben und interpretieren Sie jede Analyse bzw. jeden R-Code bzw. jedes Ergebnis (jede R-Ausgabe).\nVon hoher Bedeutung ist die Korrektheit der Beschreibung und Interpretation der statistischen Modellierung (z.B. mit der Regressionsanalyse).\nEs hat keinen Einfluss auf Ihre Note, ob sich ein (erwarteter) Effekt zeigt und wie stark dieser Effekt ggf. ist."
  },
  {
    "objectID": "pruefung.html#formatierung-des-berichts",
    "href": "pruefung.html#formatierung-des-berichts",
    "title": "1  Prüfung",
    "section": "1.4 Formatierung des Berichts",
    "text": "1.4 Formatierung des Berichts\n\nDer Bericht ist nur elektronisch, nicht ausgedruckt einzureichen.\nDer Bericht kann als paginiertes Dokument oder als nicht-paginiertes Dokument (d.h. als HTML-Dokument) eingereicht werden.\nPaginierte Formate sind als PDF-Datei einzureichen. HTML-Formate sind entweder in HTML-Form oder (nur bei Bedarf) als PDF-Druck eines HTML-Dokuments einzureichen.\nDie Wahl eines bestimmten Stylesheets ist nicht von Bedeutung. Lesbarkeit und Übersichtlichkeit in der Formatierung sind unabhängig davon anzustreben.\nIm Kopfbereich (oder auf einem Deckblatt) sind die relevanten Metadatan anzugeben wie Name (Nachname, Vorname) der Autorin/des Autors, Abgabedatum, Titel der Arbeit, Modul."
  },
  {
    "objectID": "pruefung.html#formalia",
    "href": "pruefung.html#formalia",
    "title": "1  Prüfung",
    "section": "1.5 Formalia",
    "text": "1.5 Formalia\n\nRichtlinien einer Wortzahl gibt es nicht. Entscheidend ist, dass relevante Analysen durchgeführt und beschrieben wurden.\nDer Anspruch richtet sich nach dem Inhalt und Niveau der auf dieses Modul vorbereitenden Module. Oft sind das Module in quantitativer Datenanalyse (und wissenschaftliches Arbeiten). Deren Inhalte sollen im Rahmen dieser Prüfungsleistung als selbständig und flüssig verfügbare Kompetenz von den Studentis demonstriert werden.\nDie Gliederung der Arbeit kann sich an den PPDAC-Zyklus und am Data Science Model von Wickham und Grolemund orientieren."
  },
  {
    "objectID": "pruefung.html#beurteilungskriterien",
    "href": "pruefung.html#beurteilungskriterien",
    "title": "1  Prüfung",
    "section": "1.6 Beurteilungskriterien",
    "text": "1.6 Beurteilungskriterien\nDie Arbeit wird im Hinblick auf drei Kriterien bewertet:\n\nFormalia (z. B. Vollständigkeit der Abarbeitung, Angemessenheit der äußeren Gestaltung, Fokus auf Wesentliches)\nMethodik (z. B. Richtige Auswahl und Anwendung der Verfahren)\nInhalt (z. B. Verständlichkeit, Breite und Tiefe der Problemlösung, Korrektheit der Interpretation)\n\nSie erhalten für jedes der drei Kriterien eine Teilnote sowie eine Gesamtnote. Außerdem erhalten Sie ggf. für die Kriterien noch ausformulierte Hinweise.\nDie Gesamtnote muss sich nicht als Mittelwert der Teilnoten ergeben.\nInsbesondere kann eine Fünf in einem der Kriterien zum Durchfallen führen, auch wenn die anderen beiden Kriterien gut oder sehr gut beurteilt wurden."
  },
  {
    "objectID": "pruefung.html#beispiele-für-aspekte-der-beurteilungskriterien",
    "href": "pruefung.html#beispiele-für-aspekte-der-beurteilungskriterien",
    "title": "1  Prüfung",
    "section": "1.7 Beispiele für Aspekte der Beurteilungskriterien",
    "text": "1.7 Beispiele für Aspekte der Beurteilungskriterien\n\nWurden deskriptive Statistiken (an angemessenen Ort) berichtet?\nWurden Diagramme und Tabellen angemessen eingesetzt?\nWurde Inferenzstatistik (angemessen) eingesetzt?\nWurden Effektstärkemaße (idealerweise mit Konfidenzintervallen dazu) berichtet?\nWurden alle relevanten Informationen für ein statistisches Verfahren angegeben (z.B. zum gewählten Prior)?\nWurde die Aussagekraft von Modellergebnissen richtig eingeschätzt?\nWaren die Schlussfolgerungen, die aus den statistischen Ergebnissen gezogen wurden, angemessen (z. B. wurde erkannt, dass ein Nicht-Verwerfeen einer Hypothese nicht automatisch ein Bestätigen derselben bedeutet)?\nWurde angemessen gerundet (inkl. konsistente Anzahl von Nachkommastellen)?\nPassen die statistischen Verfahren zu den Hypothesen?\nWurden die Voraussetzungen der statistischen Verfahren geprüft?\nSind die Ergebnisse reproduzierbar (Daten und Syntax eingereicht)?"
  },
  {
    "objectID": "pruefung.html#beispiele-für-fehler",
    "href": "pruefung.html#beispiele-für-fehler",
    "title": "1  Prüfung",
    "section": "1.8 Beispiele für Fehler",
    "text": "1.8 Beispiele für Fehler\nSchwere Fehler, die zum Durchfallen oder deutlichem Abwerten der Note führen können, sind z.B.:\n\nfehlende Inferenzstatistik (oder adäquatem Ersatz)\nfalsche Interpretation von Posteriori-Verteilungen oder p-Werten\nkeine Angabe von Konfidenzintervallen\nfalsche Interpretation von Konfidenzintervallen\nWahl des falschen Intervalls (Vorhersageintervall vs. Perzentilintervall vs. HDI)\nfalsche Entscheidung zum Hypothesentest auf Basis entsprechender Kennwerte (wie ROPE-Wahrscheinlichkeit oder p-Wert)\nfalsche Wahl des statistischen Verfahrens\nfehlende Deskriptivstatistik\n\nHäufige kleinere Mängel sind z. B.\n\npixelige Abbildungen\nR-Ausgaben oder R-Syntax als Screenshot\nfehlende Seitenzahlen (nur bei paginierten Formaten, nicht bei HTML)\nunübersichtliche Diagramme\nkein (verlinktes) Inhaltsverzeichnis ︎\nfehlende oder unverständliche Achsenbeschriftung bei Diagrammen\nfehlende oder falsche Beschreibung der/des Skalenniveau(s) der untersuchten Variablen"
  },
  {
    "objectID": "twittermining.html",
    "href": "twittermining.html",
    "title": "2  Twitter Mining",
    "section": "",
    "text": "Text als Datenbasis prädiktiver Modelle\nBild von mcmurryjulie auf Pixabay"
  },
  {
    "objectID": "twittermining.html#vorab",
    "href": "twittermining.html#vorab",
    "title": "2  Twitter Mining",
    "section": "2.1 Vorab",
    "text": "2.1 Vorab\n\n2.1.1 Lernziele\n\nTwitterdaten via API von Twitter auslesen\n\n\n\n2.1.2 Vorbereitung\n\nLesen Sie in Hvitfeldt und Silge (2022) Kap. 1.\nLegen Sie sich ein Konto bei Github an.\nLegen Sie sich ein Konto bei Twitter an.\nLesen Sie diesen Artikel zur Anmeldung bei der Twitter API1\n\n\n\n2.1.3 Benötigte R-Pakete\n\nlibrary(tidyverse)\nlibrary(rtweet)\nlibrary(tweetbotornot)\n\n\n\n\nR-Paket {rtweet}\n\n\nEinen Überblick über die Funktionen des Pakets (function reference) findet sich hier."
  },
  {
    "objectID": "twittermining.html#anmelden-bei-twitter",
    "href": "twittermining.html#anmelden-bei-twitter",
    "title": "2  Twitter Mining",
    "section": "2.2 Anmelden bei Twitter",
    "text": "2.2 Anmelden bei Twitter\n\n2.2.1 Welche Accounts interessieren uns?\nHier ist eine (subjektive) Auswahl von deutschen Politikern2, die einen Startpunkt gibt zur Analyse von Art und Ausmaß von Hate Speech gerichtet an deutsche Politiker:innen.\n\nd_path <- \"data/twitter-german-politicians.csv\"\n\nd <- read_csv(d_path)\nd\n\n\n\n\n\n\n\n\n\n\n\nname\nparty\nscreenname\ncomment\n\n\n\n\nKarl Lauterbach\nSPD\nKarl_Lauterbach\nNA\n\n\nOlaf Scholz\nSPD\nOlafScholz\nNA\n\n\nAnnalena Baerback\nGruene\nABaerbock\nNA\n\n\nBundesministerium für Wirtschaft und Klimaschutz\nGruene\nBMWK\nRobert Habeck ist der Minister im BMWK\n\n\nFriedrich Merz\nCDU\n_FriedrichMerz\nCDU-Chef\n\n\nMarkus Söder\nCSU\nMarkus_Soeder\nCSU-Chef\n\n\nCem Özdemir\nGruene\ncem_oezdemir\nBMEL\n\n\nJanine Wissler\nLinke\nJanine_Wissler\nLinke-Chefin\n\n\nMartin Schirdewan\nLinke\nschirdewan\nLinke-Chef\n\n\nChristian Lindner\nFDP\nc_lindner\nFDP-Chef\n\n\nMarie-Agnes Strack-Zimmermann\nFDP\nMAStrackZi\nVorsitzende Verteidigungsausschuss\n\n\nTino Chrupalla\nAFD\nTino_Chrupalla\nAFD-Bundessprecher\n\n\nAlice Weidel\nAFD\nAlice_Weidel\nAFD-Bundessprecherin\n\n\n\n\n\n\n\n\n2.2.2 Twitter App erstellen\nTutorial\n\n\n2.2.3 Intro\nDie Seite von rtweet gibt eine gute Starthilfe in die Benutzung des Pakets.\n\n\n2.2.4 Zugangsdaten\nZugangsdaten sollte man geschützt speichern, also z.B. nicht in einem geteilten Ordner.\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\n\nAnmelden:\n\nauth <- rtweet_bot(api_key = api_key,\n                   api_secret = api_secret,\n                   access_token = access_token,\n                   access_secret = access_secret)\n\nAlternativ kann man sich auch als App anmelden, damit kann man z.B. nicht posten, aber dafür mehr herunterladen.\n\nauth <- rtweet_app(bearer_token = Bearer_Token)"
  },
  {
    "objectID": "twittermining.html#tweets-einlesen",
    "href": "twittermining.html#tweets-einlesen",
    "title": "2  Twitter Mining",
    "section": "2.3 Tweets einlesen",
    "text": "2.3 Tweets einlesen\nZu beachten ist, dass es Limits gibt, wie viele Informationen (pro Zeiteinheit) man über die Twitter-API auslesen darf. Informationen dazu findet man z.B. hier oder auch mit rate_limit().\nEin gängiges Limit der Twitter-API sind 900 Anfragen (z.B. Tweets auslesen) pro 15 Minuten.\n\n2.3.1 Timeline einlesen einzelner Accounts\nMal ein paar Tweets zur Probe:\n\nsesa_test <- get_timeline(user = \"sauer_sebastian\", n = 3) %>% \n  select(full_text)\n\n\n\nRT @pia_lamberty: Ein Ansatz, der sich beim Debunking wissenschaftlich als erfolgreich herausgestellt hat, ist das sog. Faktensandwich: htt…\nRT @ianbremmer: sure, it’s the hottest summer europe has ever had in history \n\nbut look at the upside\n\nit’s one of the coolest summers euro…\nRT @twisteddoodles: Balanced news reporting https://t.co/O1iiItEQrs\n\n\n\ntweets <- get_timeline(user = d$screenname)\nsaveRDS(tweets, file = \"tweets/tweets01.rds\")\n\nMichael Kearney rät uns:\n\nPRO TIP #4: (for developer accounts only) Use bearer_token() to increase rate limit to 45,000 per fifteen minutes.\n\n\n\n2.3.2 Retweets einlesen\n\ntweets01_retweets <- \n  tweets$id_str %>% \n  head(3) %>% \n  map_dfr( ~ get_retweets(status_id = .x, retryonratelimit = TRUE))\n\nDa die meisten Retweets aber nix sagen, sondern nur auf das einen Tweet wiederholen, ist das Sammeln der Retweets ziemlich langweilig.\nMöchte man retry on rate limit im Standard auf TRUE setzen, so kann man das über die Optionen von R tun.\n\noptions(rtweet.retryonratelimit = TRUE)\n\n\n\n2.3.3 EPINetz Twitter Politicians 2021\nKönig u. a. (2022) Volltext hier haben einen Datensatz mit knapp 2500 Twitter Accounts deutscher Politiker zusammengestellt, zum Stand 2021.\nDer Datensatz kann über Gesis bezogen werden.\nAuf der gleichen Seite findet sich auch eine Dokumentation des Vorgehens.\nNachdem wir den Datensatz heruntergeladen haben, können wir ihn einlesen:\n\npoliticians_path <- \"data/EPINetz_TwitterPoliticians_2021.RDs\"\npoliticians_twitter <- read_rds(politicians_path)\n\nhead(politicians_twitter)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nofficial_name\nparty\nregion\ninstitution\noffice\nuser_id\ntwitter_name\ntwitter_handle\nfrom\nuntil\nyear_of_birth\nabgeordnetenwatch_id\ngender\nwikidata_id\n\n\n\n\n535\nManja Schüle\nSPD\nBrandenburg\nState Parliament\nParliamentarian\n827090742162100224\nManja Schüle\nManjaSchuele\n2019-09-25\nNA\n1976\n146790\nfemale\nQ40974942\n\n\n962\nPetra Pau\nDIE LINKE\nFederal\nFederal Parliament\nParliamentarian\n1683845126\nTeam PetraPau\nTeamPetraPau\n2017-10-24\nNA\n1963\n79091\nfemale\nQ77195\n\n\n864\nDagmar Schmidt\nSPD\nFederal\nFederal Parliament\nParliamentarian\n1377117206\nTeam #dieschmidt\nTeamDieSchmidt\n2017-10-24\nNA\n1973\n79036\nfemale\nQ15433815\n\n\n2517\nBernd Buchholz\nFDP\nSchleswig-Holstein\nState Parliament\nParliamentarian\n1073605033\nBernd Buchholz\nBerndBuchholz\n2017-06-06\nNA\n1961\n121092\nmale\nQ823715\n\n\n1378\nIngrid Remmers\nDIE LINKE\nFederal\nFederal Parliament\nParliamentarian\n551802475\nIngrid Remmers MdB\ningrid_remmers\n2017-10-24\nNA\n1965\n120775\nfemale\nQ1652660\n\n\n1116\nReinhard Brandl\nCSU\nFederal\nFederal Parliament\nParliamentarian\n262730721\nReinhard Brandl\nreinhardbrandl\n2017-10-24\nNA\n1977\n79427\nmale\nQ111160\n\n\n\n\n\n\nDann lesen wir die Timelines (die Tweets) dieser Konten aus; in diesem Beispiel nur 10 Tweets pro Account:\n\nepi_tweets <- get_timeline(user = head(politicians_twitter$twitter_name), n = 10)\nhead(epi_tweets)\n\nNatürlich könnte man auch mehr als 10 Tweets pro Konto einsammeln, braucht nur seine Zeit.\n\n\n2.3.4 Followers suchen\n\nfollowers01 <-\n  d$screenname %>% \n map_dfr( ~ get_followers(user = .x, retryonratelimit = TRUE))\n\nDa es dauern kann, Daten auszulesen (wir dürfen pro 15 Min. nur eine begrenzte Zahl an Information abrufen), kann es Sinn machen, die Daten lokal zu speichern.\n\nsaveRDS(followers01, file = \"tweets/followers01.rds\")\n\nUnd ggf. wieder importieren:\n\nfollowers01 <- read_rds(file = \"tweets/followers01.rds\")\n\nWie viele unique Followers haben wir identifiziert?\n\nfollowers02 <- \n  followers01 %>% \n  distinct(from_id)\n\nDie Screennames wären noch nützlich:\n\nlookup_users(users = \"1690868335\")\n\nDie Anzahl der Users, die man nachschauen kann, ist begrenzt auf 180 pro 15 Minuten.\n\nfollowers03 <-\n  followers02 %>% \n  mutate(screenname = \n           list(lookup_users(users = from_id, retryonratelimit = TRUE,verbose = TRUE)))\n\nEntsprechend kann man wieder einlesen:\nDamit haben wir eine Liste an Followers, deren Tweets wir einlesen und analysieren können, z.B. nach Hate Speech.\nIm Gegensatz zu Followers heißen bei Twitter die Accounts, denen ei Nutzi folgt “Friends”.\nLesen wir mal die Followers von karl_lauterbach ein:\n\nkarl_followers <- get_followers(user = \"karl_lauterbach\", verbose = TRUE)\n\nUm nicht jedes Mal aufs Neue die Daten herunterzuladen, bietet es sich an, die Daten lokal zu speichern:\n\nwrite_rds(karl_followers, file = \"tweets/karl_followers.rds\",\n          compress = \"gz\")\n\nEntsprechend kann man die Daten dann auch wieder einlesen:\n\nkarl_followers <- read_rds(file = \"tweets/karl_followers.rds\")\n\n\n\n2.3.5 Follower Tweets einlesen\n\nfollowers_tweets <- get_timeline(user = head(followers01$from_id), n = 10)"
  },
  {
    "objectID": "twittermining.html#tweets-verarbeiten",
    "href": "twittermining.html#tweets-verarbeiten",
    "title": "2  Twitter Mining",
    "section": "2.4 Tweets verarbeiten",
    "text": "2.4 Tweets verarbeiten\n\n2.4.1 Grundlegende Verarbeitung\nSind die Tweets eingelesen, kann man z.B. eine Sentimentanalyse, s. Kapitel 3.2.10, durchführen, oder schlicht vergleichen, welche Personen welche Wörter häufig verwenden, s. Kapitel 3.2.3.\n\n\n2.4.2 Bot or not?\nEine interessante Methode, Tweets zu verarbeiten, bietet das R-Paket tweetbotornot von M. Kearney.\nAus der Readme:\n\nDue to Twitter’s REST API rate limits, users are limited to only 180 estimates per every 15 minutes. To maximize the number of estimates per 15 minutes (at the cost of being less accurate), use the fast = TRUE argument. This method uses only users-level data, which increases the maximum number of estimates per 15 minutes to 90,000! Due to losses in accuracy, this method should be used with caution!\n\n\nusers <- c(\"sauer_sebastian\")\nbot01 <-\n  tweetbotornot(users)\n\n\n\n\n\n\n\nWichtig\n\n\n\nIch habe ein Fehlermeldung bekommen bei tweetbotornot. Da könnte ein technisches Problem in der Funktion vorliegen."
  },
  {
    "objectID": "twittermining.html#cron-jobs",
    "href": "twittermining.html#cron-jobs",
    "title": "2  Twitter Mining",
    "section": "2.5 Cron Jobs",
    "text": "2.5 Cron Jobs\n\n2.5.1 Was ist ein Cron Job?\nCron ist ein Programm auf Unix-artigen Betriebssystemen, das Skripte zu eingestellten Zeiten (wiederholt) ausführt, das sind dann “Cron Jobs”. Auf Windows gibt es aber analoge Funktionen. Cron Jobs sind praktisch, da man nicht jedes Mal selber z.B. Tweets, die heute zu einem Thema getweetet wurden, herunterladen muss. Das wird dann vom Cron Job übernommen.\nIn R gibt es eine API zum Programm Cron mit dem Paket {cronR}, s. Anleitung hier.\nDas analoge R-Paket für Windows heißt {taskscheduleR}.\n\n\n2.5.2 Beispiel für einen Cron Job\n\nlibrary(cronR)\n\nscrape_script <- cron_rscript(\"scrape_tweets.R\")\n\n# Cron Job hinzufügen:\ncron_add(command = scrape_script, \n         frequency = 'daily', \n         at = \"10AM\",\n         id = 'Hate Speech')  # Name des Cron Jobs\n\ncron_clear(ask = FALSE)  # Alle Cron Jobs löschen\ncron_ls()  # Liste aller Cron Jobs\n\nIm obigen Beispiel wird das R-Skript scrape_tweets.R täglich um 10h ausgeführt.\nDer Inhalt von scrape_tweets.R könnte dann, in Grundzügen, so aussehen:\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(rtweet)\nfollowers_lauterbach <-\n  followers01 %>% \n  filter(to_id == \"Karl_Lauterbach\")\n\nfollowers_lauterbach_tweets <- \n  get_timeline(user = followers_lauterbach$from_id[1:10], n = 10, retryonratelimit = TRUE, verbose = FALSE)\n\n\npath_output <- \"/Users/sebastiansaueruser/Google Drive/RRRing/Scrape-Tweets/tweets/\"\n\nwrite_csv(x = followers_lauterbach_tweets,\n          file = paste0(path_output, \"followers_lauterbach_tweets.csv\"),\n          append = TRUE)\n\nWir schreiben nicht jedes Mal (jeden Tag) eine neue CSV-Datei, sondern wir hängen hier die neu ausgelesenen Daten an die Datei an.\nLeider ist es mit rtweet nicht möglich, ein Datum anzugeben, ab dem man Tweets auslesen möchte3"
  },
  {
    "objectID": "twittermining.html#datenbank-an-tweets-aufbauen",
    "href": "twittermining.html#datenbank-an-tweets-aufbauen",
    "title": "2  Twitter Mining",
    "section": "2.6 Datenbank an Tweets aufbauen",
    "text": "2.6 Datenbank an Tweets aufbauen\n\n2.6.1 Stamm an bisherigen Tweets\nIn diesem Abschnitt kümmern wir uns in größerem Detail um das Aufbauen einer Tweets-Datenbank.\nDiese Pakete benötigen wir:\n\nlibrary(rtweet)\nlibrary(tidyverse)\nlibrary(rio)  # R Data import/export\n\nDann melden wir uns an:\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\nauth <- rtweet_app(bearer_token = Bearer_Token)\n\nDann brauchen wir eine Liste an Twitterkonten, die uns interessieren. Im Kontext von Hate Speech soll uns hier interessieren, welche Tweets an deutsche Spitzenpolitikis4 gesendet werden. Wir suchen also nach Tweets mit dem Text @karl_lauterbach, um ein Beispiel für einen Spitzenpolitiker zu nennen, der vermutlich von Hate Speech in höherem Maße betroffen ist.\n\npoliticians_twitter_path <- \"/Users/sebastiansaueruser/github-repos/datascience-text/data/twitter-german-politicians.csv\"\n\npoliticians_twitter <- rio::import(file = politicians_twitter_path)\n\nIn der Liste befinden sich 13 Politiker. Es macht die Sache vielleicht einfacher, wenn wir die Rate nicht überziehen. Bleiben wir daher bei 1000 Tweets pro Politiki:\n\nn_tweets_per_politician <- 1e3\n\nDie R-Syntax, die die Arbeit leistet, ist in Funktionen ausgelagert, der Übersichtlichkeit halber.\n\nsource(\"funs/filter_recent_tweets.R\")\nsource(\"funs/download_recent_tweets.R\")\nsource(\"funs/add_tweets_to_tweet_db.R\")\n\n\n\n\nJetzt laden wir einfach die aktuellsten 1000 Tweets pro Konto herunter, daher brauchen wir keine Tweet-ID angeben, die ein Mindest- oder Maximum-Datum (bzw. ID) für einen Tweet angibt:\n\ntweets_older <-\n  download_recent_tweets(screenname = politicians_twitter$screenname,\n                         max_or_since_id_str = NULL,\n                         n = n_tweets_per_politician,\n                         strip_columns = TRUE,\n                         reverse = TRUE)\n\nWie weit in die Vergangenheit reicht unsere Tweet-Sammlung?\n\noldest_tweets <- filter_recent_tweets(tweets_older, max_or_min_id_str = is_min_id_str)\noldest_tweets\n\n\n\n\n\n\n\n\n\n\n\n\nid_str\nscreenname\ncreated_at\nis_min_id_str\nis_max_id_str\n\n\n\n\n1590754620649115648\nKarl_Lauterbach\n2022-11-10 18:13:51\nTRUE\nFALSE\n\n\n1589837879693352960\nOlafScholz\n2022-11-08 05:31:02\nTRUE\nFALSE\n\n\n1590485330742083585\nABaerbock\n2022-11-10 00:23:47\nTRUE\nFALSE\n\n\n1589983737621942272\nBMWK\n2022-11-08 15:10:38\nTRUE\nFALSE\n\n\n1590646264433373184\n_FriedrichMerz\n2022-11-10 11:03:16\nTRUE\nFALSE\n\n\n1588595577360875520\nMarkus_Soeder\n2022-11-04 19:14:34\nTRUE\nFALSE\n\n\n1590097264613425152\ncem_oezdemir\n2022-11-08 22:41:45\nTRUE\nFALSE\n\n\n1588082031656898560\nJanine_Wissler\n2022-11-03 09:13:56\nTRUE\nFALSE\n\n\n1588082031656898560\nschirdewan\n2022-11-03 09:13:56\nTRUE\nFALSE\n\n\n1590628128791007233\nc_lindner\n2022-11-10 09:51:13\nTRUE\nFALSE\n\n\n1589277825152208898\nMAStrackZi\n2022-11-06 16:25:35\nTRUE\nFALSE\n\n\n1587964349993422848\nTino_Chrupalla\n2022-11-03 01:26:18\nTRUE\nFALSE\n\n\n1589696708447186945\nAlice_Weidel\n2022-11-07 20:10:05\nTRUE\nFALSE\n\n\n\n\n\n\nWas sind die neuesten Tweets, die wir habven?\n\nmost_recent_tweets <- filter_recent_tweets(oldest_tweets)\nmost_recent_tweets\n\n\n\n\n\n\n\n\n\n\n\n\nid_str\nscreenname\ncreated_at\nis_min_id_str\nis_max_id_str\n\n\n\n\n1590754620649115648\nKarl_Lauterbach\n2022-11-10 18:13:51\nTRUE\nTRUE\n\n\n1589837879693352960\nOlafScholz\n2022-11-08 05:31:02\nTRUE\nTRUE\n\n\n1590485330742083585\nABaerbock\n2022-11-10 00:23:47\nTRUE\nTRUE\n\n\n1589983737621942272\nBMWK\n2022-11-08 15:10:38\nTRUE\nTRUE\n\n\n1590646264433373184\n_FriedrichMerz\n2022-11-10 11:03:16\nTRUE\nTRUE\n\n\n1588595577360875520\nMarkus_Soeder\n2022-11-04 19:14:34\nTRUE\nTRUE\n\n\n1590097264613425152\ncem_oezdemir\n2022-11-08 22:41:45\nTRUE\nTRUE\n\n\n1588082031656898560\nJanine_Wissler\n2022-11-03 09:13:56\nTRUE\nTRUE\n\n\n1588082031656898560\nschirdewan\n2022-11-03 09:13:56\nTRUE\nTRUE\n\n\n1590628128791007233\nc_lindner\n2022-11-10 09:51:13\nTRUE\nTRUE\n\n\n1589277825152208898\nMAStrackZi\n2022-11-06 16:25:35\nTRUE\nTRUE\n\n\n1587964349993422848\nTino_Chrupalla\n2022-11-03 01:26:18\nTRUE\nTRUE\n\n\n1589696708447186945\nAlice_Weidel\n2022-11-07 20:10:05\nTRUE\nTRUE\n\n\n\n\n\n\nJetzt laden wir die neueren Tweets herunter, also mit einer ID größer als die größte in unserer Sammlung:\n\n\n\n\ntweets_new <- \n  download_recent_tweets(screenname = most_recent_tweets$screenname,\n                         max_or_since_id_str = most_recent_tweets$id_str)\n\ntweets_new %>% \n  select(screenname, created_at, id_str) %>% \n  head()\n\nJetzt - und jedes Mal, wenn wir Tweets herunterladen - fügen wir diese einer Datenbank (oder zumindest einer “Gesamt-Tabelle”) hinzu:\n\ntweets_db <- add_tweets_to_tweets_db(tweets_new, tweets_older)\n\nnrow(tweets_db)\n\n[1] 10969\n\n\nSchließlich sollten wir nicht vergessen diese in einer Datei zu speichern:\n\nwrite_rds(tweets_db, file = \"~/datasets/Twitter/tweets-db-2022-11-11.rds\")\n\n… … So, einige Zeit ist vergangen. Laden wir noch ältere Tweets herunter und fügen Sie unserer Datenbank hinzu:\n\ntweets_older2 <-\n  download_recent_tweets(screenname = politicians_twitter$screenname,\n                         max_or_since_id_str = oldest_tweets$id_str,\n                         n = 1e3,\n                         strip_columns = TRUE,\n                         reverse = TRUE)\n\n\n\n\n\ntweets_db <- add_tweets_to_tweets_db(tweets_new, tweets_older2)\n\nnrow(tweets_db)\n\n[1] 10011\n\n\nUnd wieder speichern wir die vergrößerte Datenbasis auf der Festplatte:\n\nwrite_rds(tweets_db, file = \"~/datasets/Twitter/hate-speech-twitter.rds\")\n\nLeider ist die Datenbasis nicht mehr deutlich gewachsen. Eine plausible Ursache ist, dass Twitter den Zugriff auf alte Tweets einschränkt.\n\n\n2.6.2 Neue Tweets per Cron Job\nWie oben schon ausprobiert, legen wir uns einen Cron Job an.\n\nlibrary(cronR)\n\nscrape_script <- cron_rscript(\"/Users/sebastiansaueruser/github-repos/datascience-text/funs/get_tweets_politicians.R\")\n\n# Cron Job hinzufügen:\ncron_add(command = scrape_script, \n         frequency = 'daily', \n         at = \"10AM\",\n         id = 'Hate Speech')  # Name des Cron Jobs\n\nDas Skript get_tweets_politicians.R birgt die Schritte, die wir in diesem Abschnitt ausprobiert haben. Kurz gesagt sucht es nach neuen Tweets, die also noch nicht in Ihrer “Datenbank” vorhanden sind, und lädt diese herunter. Dabei werden maximal 1000 Tweets pro Konto (derer sind es 13) heruntergeladen.\nMöchte man den Cron Job wieder löschen, so kann man das so tun:\n\ncron_clear(ask = FALSE)  # Alle Cron Jobs löschen\ncron_ls()  # Liste aller Cron Jobs"
  },
  {
    "objectID": "twittermining.html#aufgaben",
    "href": "twittermining.html#aufgaben",
    "title": "2  Twitter Mining",
    "section": "2.7 Aufgaben",
    "text": "2.7 Aufgaben\n\nÜberlegen Sie, wie Sie das Ausmaß an Hate Speech, dem deutsche Politikerinnen und Politiker konfrontiert sind, messen können.\nArgumentieren Sie die Vorteile und Nachteile Ihres Ansatzes. Außerdem, auf welches Ergebnis dieser Analyse sie gespannt sind bzw. wären.\nÜberlegen Sie Korrelate, oder besser noch: (mögliche) Ursachen, des Hasses in den Tweets, gerichtet auf Polikter:innen. Sie können auch Gruppen von Ursachen bilden, etwas personengebundene Variablen der Politiker:innen (z.B. Alter? Geschlecht? Migrationshintergrund?).\nErstellen Sie sich eine Liste an Personen, deren Tweets sich lohnen (könnten), auf Hate Speech hin analysiert zu werden. Laden Sie deren Tweets (ggf. in Auszügen) herunter.\nDas Skript zu scrape_tweets.R könnte man noch verbessern, in dem man jeden Tag nur die neuesten Tweets herunterlädt. Dazu kann man bei get_timeline() mit dem Argument since_id eine Untergrenze der ID festlegen, so dass nur neuere Tweets (d.h. mit größerem Wert bei ID) ausgelesen werden. Ändern Sie das Skript entsprechend, so dass nur neuerer Tweets gelesen werden.\nErarbeiten Sie die Folien zu diesem rtweet-Workshop. Eine Menge guter Tipps!\n\n\n\n\n\nHvitfeldt, Emil, und Julia Silge. 2022. Supervised Machine Learning for Text Analysis in R. 1. Aufl. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nKönig, Tim, Wolf J. Schünemann, Alexander Brand, Julian Freyberg, und Michael Gertz. 2022. „The EPINetz Twitter Politicians Dataset 2021. A New Resource for the Study of the German Twittersphere and Its Application for the 2021 Federal Elections“. Politische Vierteljahresschrift 63 (3): 529–47. https://doi.org/10.1007/s11615-022-00405-7."
  },
  {
    "objectID": "textmining1.html",
    "href": "textmining1.html",
    "title": "3  Textmining1",
    "section": "",
    "text": "Bild von mcmurryjulie auf Pixabay"
  },
  {
    "objectID": "textmining1.html#vorab",
    "href": "textmining1.html#vorab",
    "title": "3  Textmining1",
    "section": "3.1 Vorab",
    "text": "3.1 Vorab\n\n3.1.1 Lernziele\n\nDie vorgestellten Techniken des Textminings mit R anwenden können\n\n\n\n3.1.2 Vorbereitung\n\nLesen Sie in Hvitfeldt und Silge (2022) Kap. 2.\n\n\n\n3.1.3 Benötigte R-Pakete\n\nlibrary(tidyverse)\nlibrary(tokenizers)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(hcandersenr)\nlibrary(SnowballC)  # Stemming\nlibrary(lsa)  # Stopwörter\nlibrary(easystats)  # Komfort für deskriptive Statistiken, wie `describe_distribution`\nlibrary(textclean)  # Emojis ersetzen"
  },
  {
    "objectID": "textmining1.html#einfache-methoden-des-textminings",
    "href": "textmining1.html#einfache-methoden-des-textminings",
    "title": "3  Textmining1",
    "section": "3.2 Einfache Methoden des Textminings",
    "text": "3.2 Einfache Methoden des Textminings\nArbeiten Sie die folgenden grundlegenden Methoden des Textminigs durch.\n\n3.2.1 Tokenisierung\nErarbeiten Sie dieses Kapitel: Hvitfeldt und Silge (2022), Kap. 2\nWie viele Zeilen hat das Märchen “The Fir tree” (in der englischen Fassung?)\n\nhcandersen_en %>% \n  filter(book == \"The fir tree\") %>% \n  nrow()\n\n[1] 253\n\n\n\n\n3.2.2 Stopwörter entfernen\nErarbeiten Sie dieses Kapitel: s. Hvitfeldt und Silge (2022), Kap. 3\nEine alternative Quelle von Stopwörtern - in verschiedenen Sprachen - biwetet das Paket quanteda:\n\nstop2 <-\n  tibble(word = quanteda::stopwords(\"german\"))\n\nhead(stop2)\n\n\n\n\n\nword\n\n\n\n\naber\n\n\nalle\n\n\nallem\n\n\nallen\n\n\naller\n\n\nalles\n\n\n\n\n\n\nEs bestehst (in der deutschen Version) aus 231 Wörtern.\n\n\n3.2.3 Wörter zählen\nIst der Text tokenisiert, kann man einfach mit “Bordmitteln” die Wörter zählen.\n\nhcandersen_de %>% \n  filter(book == \"Das Feuerzeug\") %>% \n  unnest_tokens(output = word, input = text) %>% \n  anti_join(stop2) %>% \n  count(word, sort = TRUE) %>% \n  head()\n\nJoining, by = \"word\"\n\n\n\n\n\n\nword\nn\n\n\n\n\nsoldat\n35\n\n\nsagte\n28\n\n\nhund\n23\n\n\nprinzessin\n17\n\n\nhexe\n16\n\n\nfeuerzeug\n14\n\n\n\n\n\n\n\n\n3.2.4 Stemming (Wortstamm finden)\nErarbeiten Sie dieses Kapitel: Hvitfeldt und Silge (2022), Kap. 4\nVertiefende Hinweise zum UpSet plot finden Sie hier, Lex u. a. (2014).\nFür welche Sprachen gibt es Stemming im Paket SnowballC?\n\nlibrary(SnowballC)\ngetStemLanguages()\n\n [1] \"arabic\"     \"basque\"     \"catalan\"    \"danish\"     \"dutch\"     \n [6] \"english\"    \"finnish\"    \"french\"     \"german\"     \"greek\"     \n[11] \"hindi\"      \"hungarian\"  \"indonesian\" \"irish\"      \"italian\"   \n[16] \"lithuanian\" \"nepali\"     \"norwegian\"  \"porter\"     \"portuguese\"\n[21] \"romanian\"   \"russian\"    \"spanish\"    \"swedish\"    \"tamil\"     \n[26] \"turkish\"   \n\n\nEinfacher Test: Suchen wir den Wordstamm für das Wort “wissensdurstigen”, wie in “die wissensdurstigen Studentis löcherten dis armi Professi”1.\n\nwordStem(\"wissensdurstigen\", language = \"german\")\n\n[1] \"wissensdurst\"\n\n\nWerfen Sie mal einen Blick in das Handbuch von SnowballC.\n\n\n3.2.5 Fallstudie AfD-Parteiprogramm\nDaten einlesen:\n\nd_link <- \"https://raw.githubusercontent.com/sebastiansauer/pradadata/master/data-raw/afd_2022.csv\"\nafd <- read_csv(d_link, show_col_types = FALSE)\n\nWie viele Seiten hat das Dokument?\n\nnrow(afd)\n\n[1] 190\n\n\nUnd wie viele Wörter?\n\nstr_count(afd$text, pattern = \"\\\\w\") %>% sum(na.rm = TRUE)\n\n[1] 179375\n\n\nAus breit mach lang, oder: wir tokenisieren (nach Wörtern):\n\nafd %>% \n  unnest_tokens(output = token, input = text) %>% \n  filter(str_detect(token, \"[a-z]\")) -> afd_long\n\nStopwörter entfernen:\n\ndata(stopwords_de, package = \"lsa\")\n\nstopwords_de <- tibble(word = stopwords_de)\n\n# Für das Joinen werden gleiche Spaltennamen benötigt:\nstopwords_de <- stopwords_de %>% \n  rename(token = word)  \n\nafd_long %>% \n  anti_join(stopwords_de) -> afd_no_stop\n\nJoining, by = \"token\"\n\n\nWörter zählen:\n\nafd_no_stop %>% \n  count(token, sort = TRUE) -> afd_count\n\nhead(afd_count)\n\n\n\n\n\ntoken\nn\n\n\n\n\nafd\n174\n\n\ndeutschland\n113\n\n\nwollen\n66\n\n\neuro\n60\n\n\nbürger\n57\n\n\neu\n54\n\n\n\n\n\n\nWörter trunkieren:\n\nafd_no_stop %>% \n  mutate(token_stem = wordStem(token, language = \"de\")) %>% \n  count(token_stem, sort = TRUE) -> afd_count_stemmed\n\nhead(afd_no_stop)\n\n\n\n\n\npage\ntoken\n\n\n\n\n1\nprogramm\n\n\n1\ndeutschland\n\n\n1\ngrundsatzprogramm\n\n\n1\nalternative\n\n\n1\ndeutschland\n\n\n2\ninhaltsverzeichnis\n\n\n\n\n\n\n\n\n3.2.6 Stringverarbeitung\nErarbeiten Sie dieses Kapitel: Wickham und Grolemund (2018), Kap. 14\n\n3.2.6.1 Regulärausdrücke\nDas \"[a-z]\" in der Syntax oben steht für “alle Buchstaben von a-z”. D iese flexible Art von “String-Verarbeitung mit Jokern” nennt man Regulärausdrücke (regular expressions; regex). Es gibt eine ganze Reihe von diesen Regulärausdrücken, die die Verarbeitung von Texten erleichert. Mit dem Paket stringr geht das - mit etwas Übung - gut von der Hand. Nehmen wir als Beispiel den Text eines Tweets:\n\nstring <- \"Correlation of unemployment and #AfD votes at #btw17: ***r = 0.18***\\n\\nhttps://t.co/YHyqTguVWx\"  \n\nMöchte man Ziffern identifizieren, so hilft der Reulärausdruck [:digit:]:\n“Gibt es mindestens eine Ziffer in dem String?”\n\nstr_detect(string, \"[:digit:]\")\n\n[1] TRUE\n\n\n“Finde die Position der ersten Ziffer! Welche Ziffer ist es?”\n\nstr_locate(string, \"[:digit:]\")\n\n     start end\n[1,]    51  51\n\nstr_extract(string, \"[:digit:]\")\n\n[1] \"1\"\n\n\n“Finde alle Ziffern!”\n\nstr_extract_all(string, \"[:digit:]\")\n\n[[1]]\n[1] \"1\" \"7\" \"0\" \"1\" \"8\"\n\n\n“Finde alle Stellen an denen genau 2 Ziffern hintereinander folgen!”\n\nstr_extract_all(string, \"[:digit:]{2}\")\n\n[[1]]\n[1] \"17\" \"18\"\n\n\nDer Quantitätsoperator {n} findet alle Stellen, in der der der gesuchte Ausdruck genau \\(n\\) mal auftaucht.\n“Zeig die Hashtags!”\n\nstr_extract_all(string, \"#[:alnum:]+\")\n\n[[1]]\n[1] \"#AfD\"   \"#btw17\"\n\n\nDer Operator [:alnum:] steht für “alphanumerischer Charakter” - also eine Ziffer oder ein Buchstabe; synonym hätte man auch \\\\w schreiben können (w wie word). Warum werden zwei Backslashes gebraucht? Mit \\\\w wird signalisiert, dass nicht der Buchstabe w, sondern etwas Besonderes, eben der Regex-Operator \\w gesucht wird.\n“Zeig die URLs!”\n\nstr_extract_all(string, \"https?://[:graph:]+\")\n\n[[1]]\n[1] \"https://t.co/YHyqTguVWx\"\n\n\nDas Fragezeichen ? ist eine Quantitätsoperator, der einen Treffer liefert, wenn das vorherige Zeichen (hier s) null oder einmal gefunden wird. [:graph:] ist die Summe von [:alpha:] (Buchstaben, groß und klein), [:digit:] (Ziffern) und [:punct:] (Satzzeichen u.ä.).\n“Zähle die Wörter im String!”\n\nstr_count(string, boundary(\"word\"))\n\n[1] 13\n\n\n“Liefere nur Buchstabenfolgen zurück, lösche alles übrige”\n\nstr_extract_all(string, \"[:alpha:]+\")\n\n[[1]]\n [1] \"Correlation\"  \"of\"           \"unemployment\" \"and\"          \"AfD\"         \n [6] \"votes\"        \"at\"           \"btw\"          \"r\"            \"https\"       \n[11] \"t\"            \"co\"           \"YHyqTguVWx\"  \n\n\nDer Quantitätsoperator + liefert alle Stellen zurück, in denen der gesuchte Ausdruck einmal oder häufiger vorkommt. Die Ergebnisse werden als Vektor von Wörtern zurückgegeben. Ein anderer Quantitätsoperator ist *, der für 0 oder mehr Treffer steht. Möchte man einen Vektor, der aus Stringen-Elementen besteht zu einem Strring zusammenfüngen, hilft paste(string) oder str_c(string, collapse = \" \").\n\nstr_replace_all(string, \"[^[:alpha:]+]\", \"\")\n\n[1] \"CorrelationofunemploymentandAfDvotesatbtwrhttpstcoYHyqTguVWx\"\n\n\nMit dem Negationsoperator [^x] wird der Regulärausrck x negiert; die Syntax oben heißt also “ersetze in string alles außer Buchstaben durch Nichts”. Mit “Nichts” sind hier Strings der Länge Null gemeint; ersetzt man einen belieibgen String durch einen String der Länge Null, so hat man den String gelöscht.\nDas Cheatsheet zur Strings bzw zu stringr von RStudio gibt einen guten Überblick über Regex; im Internet finden sich viele Beispiele.\n\n\n3.2.6.2 Regex im Texteditor\nEinige Texteditoren unterstützen Regex, so auch RStudio.\nDas ist eine praktische Sache. Ein Beispiel: Sie haben eine Liste mit Namen der Art:\n\nNachname1, Vorname1\nNachname2, Vorname2\nNachname3, Vorname3\n\nUnd Sie möchten jetzt aber die Liste mit Stil Vorname Nachname sortiert haben.\nRStudio mit Regex macht’s möglich, s. ?fig-regex-rstudio.\n\n\n \nAbbildung 3.1: ?(caption)\n\n\n\n\n\n3.2.7 Emoji-Analyse\nEine einfache Art, Emojis in einer Textmining-Analyse zu verarbeiten, bietet das Paket textclean:\n\nfls <- system.file(\"docs/emoji_sample.txt\", package = \"textclean\")\nx <- readLines(fls)[1]\nx\n\n[1] \"Proin 😍 ut maecenas 😏 condimentum 😔 purus eget. Erat, 😂vitae nunc elit. Condimentum 😢 semper iaculis bibendum sed tellus. Ut suscipit interdum😑 in. Faucib😞 us nunc quis a vitae posuere. 😛 Eget amet sit condimentum non. Nascetur vitae ☹ et. Auctor ornare ☺ vestibulum primis justo congue 😀urna ac magna. Quam 😥 pharetra 😟 eros 😒facilisis ac lectus nibh est 😙vehicula 😐 ornare! Vitae, malesuada 😎 erat sociosqu urna, 😏 nec sed ad aliquet 😮 .\"\n\n\n\nreplace_emoji(x)\n\n[1] \"Proin smiling face with heart-eyes ut maecenas smirking face condimentum pensive face purus eget. Erat, face with tears of joy vitae nunc elit. Condimentum crying face semper iaculis bibendum sed tellus. Ut suscipit interdum expressionless face in. Faucib disappointed face us nunc quis a vitae posuere. face with tongue Eget amet sit condimentum non. Nascetur vitae frowning face et. Auctor ornare smiling face vestibulum primis justo congue grinning face urna ac magna. Quam sad but relieved face pharetra worried face eros unamused face facilisis ac lectus nibh est kissing face with smiling eyes vehicula neutral face ornare! Vitae, malesuada smiling face with sunglasses erat sociosqu urna, smirking face nec sed ad aliquet face with open mouth .\"\n\nreplace_emoji_identifier(x)\n\n[1] \"Proin lexiconwiutsdotskrupggpgmhm ut maecenas lexiconwizbukzesopzflfinotj condimentum lexiconwlnxqescoesytfatoevi purus eget. Erat, lexiconwcaiviebiytolowkanmb vitae nunc elit. Condimentum lexiconwpujksvgujncexktvyrn semper iaculis bibendum sed tellus. Ut suscipit interdum lexiconwknnasgueiicggptyzbx in. Faucib lexiconwoxfeslcareuqfkbyjgy us nunc quis a vitae posuere. lexiconwobmhqdrrzgygdexhnkk Eget amet sit condimentum non. Nascetur vitae lexiconbfalxvockmnmtmycmwyq et. Auctor ornare lexiconbgmujofaalvxqrklfqgd vestibulum primis justo congue lexiconvygwtlyrpywfarytvfis urna ac magna. Quam lexiconwurhpvewhizayynmfxqo pharetra lexiconwpmuduwgbxxrxeltrueb eros lexiconwkrvakxddtqckcjxeksl facilisis ac lectus nibh est lexiconwmsjgfnelqfeyhgudmfj vehicula lexiconwjfhkpcsgcjtotwlapxa ornare! Vitae, malesuada lexiconwivnupleicqgksianinp erat sociosqu urna, lexiconwizbukzesopzflfinotj nec sed ad aliquet lexiconxbwhfeflxbuupjezgdwl .\"\n\n\n\n\n3.2.8 Text aufräumen\nEine Reihe generischer Tests bietet das Paket textclean von Tyler Rinker:\nHier ist ein “unaufgeräumeter” Text:\n\nx <- c(\"i like\", \"<p>i want. </p>. thet them ther .\", \"I am ! that|\", \"\", NA, \n    \"&quot;they&quot; they,were there\", \".\", \"   \", \"?\", \"3;\", \"I like goud eggs!\", \n    \"bi\\xdfchen Z\\xfcrcher\", \"i 4like...\", \"\\\\tgreat\",  \"She said \\\"yes\\\"\")\n\nLassen wir uns dazu ein paar Diagnostiken ausgeben.\n\nEncoding(x) <- \"latin1\"\nx <- as.factor(x)\ncheck_text(x)\n\n\n=============\nNON CHARACTER\n=============\n\nThe text variable is not a character column (likely `factor`):\n\n\n*Suggestion: Consider using `as.character` or `stringsAsFactors = FALSE` when reading in\n             Also, consider rerunning `check_text` after fixing\n\n\n=====\nDIGIT\n=====\n\nThe following observations contain digits/numbers:\n\n10, 13\n\nThis issue affected the following text:\n\n10: 3;\n13: i 4like...\n\n*Suggestion: Consider using `replace_number`\n\n\n========\nEMOTICON\n========\n\nThe following observations contain emoticons:\n\n6\n\nThis issue affected the following text:\n\n6: &quot;they&quot; they,were there\n\n*Suggestion: Consider using `replace_emoticons`\n\n\n=====\nEMPTY\n=====\n\nThe following observations contain empty text cells (all white space):\n\n1\n\nThis issue affected the following text:\n\n1: i like\n\n*Suggestion: Consider running `drop_empty_row`\n\n\n=======\nESCAPED\n=======\n\nThe following observations contain escaped back spaced characters:\n\n14\n\nThis issue affected the following text:\n\n14: \\tgreat\n\n*Suggestion: Consider using `replace_white`\n\n\n====\nHTML\n====\n\nThe following observations contain HTML markup:\n\n2, 6\n\nThis issue affected the following text:\n\n2: <p>i want. </p>. thet them ther .\n6: &quot;they&quot; they,were there\n\n*Suggestion: Consider running `replace_html`\n\n\n==========\nINCOMPLETE\n==========\n\nThe following observations contain incomplete sentences (e.g., uses ending punctuation like '...'):\n\n13\n\nThis issue affected the following text:\n\n13: i 4like...\n\n*Suggestion: Consider using `replace_incomplete`\n\n\n=============\nMISSING VALUE\n=============\n\nThe following observations contain missing values:\n\n5\n\n*Suggestion: Consider running `drop_NA`\n\n\n========\nNO ALPHA\n========\n\nThe following observations contain elements with no alphabetic (a-z) letters:\n\n4, 7, 8, 9, 10\n\nThis issue affected the following text:\n\n4: \n7: .\n8:    \n9: ?\n10: 3;\n\n*Suggestion: Consider cleaning the raw text or running `filter_row`\n\n\n==========\nNO ENDMARK\n==========\n\nThe following observations contain elements with missing ending punctuation:\n\n1, 3, 4, 6, 8, 10, 12, 14, 15\n\nThis issue affected the following text:\n\n1: i like\n3: I am ! that|\n4: \n6: &quot;they&quot; they,were there\n8:    \n10: 3;\n12: bißchen Zürcher\n14: \\tgreat\n15: She said \"yes\"\n\n*Suggestion: Consider cleaning the raw text or running `add_missing_endmark`\n\n\n====================\nNO SPACE AFTER COMMA\n====================\n\nThe following observations contain commas with no space afterwards:\n\n6\n\nThis issue affected the following text:\n\n6: &quot;they&quot; they,were there\n\n*Suggestion: Consider running `add_comma_space`\n\n\n=========\nNON ASCII\n=========\n\nThe following observations contain non-ASCII text:\n\n12\n\nThis issue affected the following text:\n\n12: bißchen Zürcher\n\n*Suggestion: Consider running `replace_non_ascii`\n\n\n==================\nNON SPLIT SENTENCE\n==================\n\nThe following observations contain unsplit sentences (more than one sentence per element):\n\n2, 3\n\nThis issue affected the following text:\n\n2: <p>i want. </p>. thet them ther .\n3: I am ! that|\n\n*Suggestion: Consider running `textshape::split_sentence`\n\n\n\n\n3.2.9 Diverse Wortlisten\nTyler Rinker stellt mit dem Paket lexicon eine Zusammenstellung von Wortlisten zu diversen Zwecken zur Verfügung.\n\n\n3.2.10 Sentimentanalyse\n\n3.2.10.1 Einführung\nEine weitere interessante Analyse ist, die “Stimmung” oder “Emotionen” (Sentiments) eines Textes auszulesen. Die Anführungszeichen deuten an, dass hier ein Maß an Verständnis suggeriert wird, welches nicht (unbedingt) von der Analyse eingehalten wird. Jedenfalls ist das Prinzip der Sentiment-Analyse im einfachsten Fall so:\n\nSchau dir jeden Token aus dem Text an.\n\nPrüfe, ob sich das Wort im Lexikon der Sentiments wiederfindet.\n\nWenn ja, dann addiere den Sentimentswert dieses Tokens zum bestehenden Sentiments-Wert.\n\nWenn nein, dann gehe weiter zum nächsten Wort.\n\nLiefere zum Schluss die Summenwerte pro Sentiment zurück.\n\nEs gibt Sentiment-Lexika, die lediglich einen Punkt für “positive Konnotation” bzw. “negative Konnotation” geben; andere Lexiko weisen differenzierte Gefühlskonnotationen auf. Wir nutzen hier das deutsche Sentimentlexikon sentiws (Remus, Quasthoff, und Heyer 2010). Sie können das Lexikon als CSV hier herunterladen:\n\nsentiws <- read_csv(\"https://osf.io/x89wq/?action=download\")\n\nDen Volltext zum Paper finden Sie z.B. hier.\nAlternativ können Sie die Daten aus dem Paket pradadata laden. Allerdings müssen Sie dieses Paket von Github installieren:\n\ninstall.packages(\"devtools\", dep = TRUE)\ndevtools::install_github(\"sebastiansauer/pradadata\")\n\n\ndata(sentiws, package = \"pradadata\")\n\nTabelle 3.1 zeigt einen Ausschnitt aus dem Sentiment-Lexikon SentiWS.\n\n\n\n\n\nTabelle 3.1: Auszug aus SentiWS\n\n\nneg_pos\nword\nvalue\ninflections\n\n\n\n\nneg\nAbbau\n-0.0580\nAbbaus,Abbaues,Abbauen,Abbaue\n\n\nneg\nAbbruch\n-0.0048\nAbbruches,Abbrüche,Abbruchs,Abbrüchen\n\n\nneg\nAbdankung\n-0.0048\nAbdankungen\n\n\nneg\nAbdämpfung\n-0.0048\nAbdämpfungen\n\n\nneg\nAbfall\n-0.0048\nAbfalles,Abfälle,Abfalls,Abfällen\n\n\nneg\nAbfuhr\n-0.3367\nAbfuhren\n\n\n\n\n\n\n\n\n\n3.2.10.2 Ungewichtete Sentiment-Analyse\nNun können wir jedes Token des Textes mit dem Sentiment-Lexikon abgleichen; dabei zählen wir die Treffer für positive bzw. negative Terme. Zuvor müssen wir aber noch die Daten (afd_long) mit dem Sentimentlexikon zusammenführen (joinen). Das geht nach bewährter Manier mit inner_join; “inner” sorgt dabei dafür, dass nur Zeilen behalten werden, die in beiden Dataframes vorkommen. Tabelle Tabelle 3.2 zeigt Summe, Anzahl und Anteil der Emotionswerte.\nWir nutzen die Tabelle afd_long, die wir oben definiert haben.\n\nafd_long %>% \n  inner_join(sentiws, by = c(\"token\" = \"word\")) %>% \n  select(-inflections) -> afd_senti  # die Spalte brauchen wir nicht\n\nafd_senti %>% \n  group_by(neg_pos) %>% \n  summarise(polarity_sum = sum(value),\n            polarity_count = n()) %>% \n  mutate(polarity_prop = (polarity_count / sum(polarity_count)) %>% round(2)) ->\n  afd_senti_tab\n\n\n\n\n\n\nTabelle 3.2: Zusammenfassung von SentiWS\n\n\nneg_pos\npolarity_sum\npolarity_count\npolarity_prop\n\n\n\n\nneg\n-48.5307\n210\n0.27\n\n\npos\n30.6595\n578\n0.73\n\n\n\n\n\n\n\nDie Analyse zeigt, dass die emotionale Bauart des Textes durchaus interessant ist: Es gibt viel mehr positiv getönte Wörter als negativ getönte. Allerdings sind die negativen Wörter offenbar deutlich stärker emotional aufgeladen, denn die Summe an Emotionswert der negativen Wörter ist (überraschenderweise?) deutlich größer als die der positiven.\nBetrachten wir also die intensivsten negativ und positive konnotierten Wörter näher.\n\nafd_senti %>% \n  distinct(token, .keep_all = TRUE) %>% \n  mutate(value_abs = abs(value)) %>% \n  top_n(20, value_abs) %>% \n  pull(token)\n\n [1] \"ungerecht\"    \"besonders\"    \"gefährlich\"   \"überflüssig\"  \"behindern\"   \n [6] \"gelungen\"     \"brechen\"      \"unzureichend\" \"gemein\"       \"verletzt\"    \n[11] \"zerstören\"    \"trennen\"      \"falsch\"       \"vermeiden\"    \"zerstört\"    \n[16] \"schwach\"      \"belasten\"     \"schädlich\"    \"töten\"        \"verbieten\"   \n\n\nDiese “Hitliste” wird zumeist (19/20) von negativ polarisierten Begriffen aufgefüllt, wobei “besonders” ein Intensivierwort ist, welches das Bezugswort verstärt (“besonders gefährlich”). Das Argument keep_all = TRUE sorgt dafür, dass alle Spalten zurückgegeben werden, nicht nur die durchsuchte Spalte token. Mit pull haben wir aus dem Dataframe, der von den dplyr-Verben übergeben wird, die Spalte pull “herausgezogen”; hier nur um Platz zu sparen bzw. der Übersichtlichkeit halber.\nNun könnte man noch den erzielten “Netto-Sentimentswert” des Corpus ins Verhältnis setzen Sentimentswert des Lexikons: Wenn es insgesamt im Sentiment-Lexikon sehr negativ zuginge, wäre ein negativer Sentimentwer in einem beliebigen Corpus nicht überraschend. describe_distribution aus {easystats} gibt uns einen Überblick der üblichen deskriptiven Statistiken.\n\nsentiws %>% \n  select(value, neg_pos) %>% \n  #group_by(neg_pos) %>% \n  describe_distribution()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nMean\nSD\nIQR\nRange\nSkewness\nKurtosis\nn\nn_Missing\n\n\n\n\nvalue\n-0.05\n0.20\n0.05\n(-1.00, 1.00)\n-0.68\n2.36\n3468\n0\n\n\n\n\n\nInsgesamt ist das Lexikon ziemlich ausgewogen; negative Werte sind leicht in der Überzahl im Lexikon. Unser Corpus hat eine ähnliche mittlere emotionale Konnotation wie das Lexikon:\n\nafd_senti %>% \n  summarise(senti_sum = mean(value) %>% round(2))\n\n\n\n\n\nsenti_sum\n\n\n\n\n-0.02\n\n\n\n\n\n\n\n\n\n3.2.11 Weitere Sentiment-Lexika\nTyler Rinker stellt das Paket sentimentr zur Verfügung. Matthew Jockers stellt das Paket Syushet zur Verfügung.\n\n\n3.2.12 Google Trends\nEine weitere Möglichkeit, “Worthäufigkeiten” zu identifizieren ist Google Trends. Dieser Post zeigt Ihnen eine Einsatzmöglichkeit."
  },
  {
    "objectID": "textmining1.html#aufgaben",
    "href": "textmining1.html#aufgaben",
    "title": "3  Textmining1",
    "section": "3.3 Aufgaben",
    "text": "3.3 Aufgaben\n\npurrr-map01\npurrr-map02\npurrr-map03\npurrr-map04\nRegex-Übungen\nAufgaben zum Textmining von Tweets"
  },
  {
    "objectID": "textmining1.html#fallstudie-hate-speech",
    "href": "textmining1.html#fallstudie-hate-speech",
    "title": "3  Textmining1",
    "section": "3.4 Fallstudie Hate-Speech",
    "text": "3.4 Fallstudie Hate-Speech\n\n3.4.1 Daten\nEs finden sich mehrere Datensätze zum Thema Hate-Speech im öffentlichen Internet, eine Quelle ist Hate Speech Data, ein Repositorium, das mehrere Datensätze beinhaltet.\n\nKaggle Hate Speech and Offensive Language Dataset\nBretschneider and Peters Prejudice on Facebook Dataset\nDaten zum Fachartikel”Large Scale Crowdsourcing and Characterization of Twitter Abusive Behavior”\n\nFür Textmining kann eine Liste mit anstößigen (obszönen) Wörten nützlich sein, auch wenn man solche Dinge ungern anfässt, verständlicherweise. Jenyay bietet solche Listen in verschiedenen Sprachen an. Die Liste von KDNOOBW sieht sehr ähnlich aus (zumindest die deutsche Version). Eine lange Sammlung deutscher Schimpfwörter findet sich im insult.wiki; ähnlich bei Hyperhero.\nTwitterdaten dürfen nur in “dehydrierter” Form weitergegeben werden, so dass kein Rückschluss von ID zum Inhalt des Tweets möglich ist. Daher werden öffentlich nur die IDs der Tweets, als einzige Information zum Tweet, also ohne den eigentlichen Inhalt des Tweets, bereitgestellt.\nÜber die Twitter-API kann man sich, wie oben dargestellt, dann die Tweets wieder “rehydrieren”, also wieder mit dem zugehörigen Tweet-Text (und sonstigen Infos des Tweets) zu versehen.\n\n\n3.4.2 Grundlegendes Text Mining\nWenden Sie die oben aufgeführten Techniken des grundlegenden Textminings auf einen der oben dargestellten Hate-Speech-Datensätze an. Erstellen Sie ein (HTML-Dokument) mit Ihren Ergebnissen. Stellen Sie die Ergebnisse auf dem Github-Repo dieses Kurses ein. Vergleichen Sie Ihre Lösung mit den Lösungen der anderen Kursmitglieder.\nWir nutzen noch nicht eigene Daten, die wir von Twitter ausgelesen haben, das heben wir uns für später auf.\n\n\n\n\nHvitfeldt, Emil, und Julia Silge. 2022. Supervised Machine Learning for Text Analysis in R. 1. Aufl. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nLex, Alexander, Nils Gehlenborg, Hendrik Strobelt, Romain Vuillemot, und Hanspeter Pfister. 2014. „UpSet: Visualization of Intersecting Sets“. IEEE Transactions on Visualization and Computer Graphics 20 (12): 1983–92. https://doi.org/10.1109/TVCG.2014.2346248.\n\n\nRemus, Robert, Uwe Quasthoff, und Gerhard Heyer. 2010. „SentiWS - a Publicly Available German-language Resource for Sentiment Analysis“. Proceedings of the 7th International Language Ressources and Evaluation (LREC’10), 1168–71.\n\n\nWickham, Hadley, und Garrett Grolemund. 2018. R für Data Science: Daten importieren, bereinigen, umformen, modellieren und visualisieren. Übersetzt von Frank Langenau. 1. Auflage. Heidelberg: O’Reilly. https://r4ds.had.co.nz/index.html."
  },
  {
    "objectID": "populismus.html",
    "href": "populismus.html",
    "title": "4  Fallstudie Populismus",
    "section": "",
    "text": "Bild von mcmurryjulie auf Pixabay"
  },
  {
    "objectID": "populismus.html#vorab",
    "href": "populismus.html#vorab",
    "title": "4  Fallstudie Populismus",
    "section": "4.1 Vorab",
    "text": "4.1 Vorab\n\n4.1.1 Lernziele\n\nDie Fallstudie erklären können\n\n\n\n4.1.2 Vorbereitung\n\nClonen Sie das Projekt-Repositorium oder laden Sie es herunter1.\nArbeiten Sie die Syntax zu dem Projekt durch.\n\n\n\n4.1.3 Benötigte R-Pakete\nIn dem vorgestellten Projekt werden die folgenden R-Pakete verwendet.\n\nlibrary(tidyverse)\nlibrary(twitteR)\nlibrary(tidytext)\nlibrary(lubridate)\nlibrary(magrittr)\nlibrary(tidytext)\nlibrary(stringr)\nlibrary(viridis)\nlibrary(wordcloud)\nlibrary(SnowballC)\nlibrary(knitr)\nlibrary(testthat)"
  },
  {
    "objectID": "populismus.html#wie-populistisch-tweeten-unsere-politikerinnen",
    "href": "populismus.html#wie-populistisch-tweeten-unsere-politikerinnen",
    "title": "4  Fallstudie Populismus",
    "section": "4.2 Wie populistisch tweeten unsere Politiker:innen?",
    "text": "4.2 Wie populistisch tweeten unsere Politiker:innen?\nVerschaffen Sie sich einen Überblick über dieses Projekt! Im Rahmen dieses Projekts vergleicht der Autor den Populismus von deutschen Politiker:innen, so wie er sich in den Tweets dieser Personen niederschlägt. Auf dieser Basis wird ein Populismuswert, bestehend aus mehreren Teilwerten, berechnet und auf Parteiebenen (als Mittel der zugehörigen Politiker:innen) berechnet. Natürlich fragt man sich, wie Populismus definiert ist und wie diese Definition in den Berechnungen umgesetzt wurde. Finden Sie es selber heraus: Im Github-Repo sind alle Details dokumentiert.\nZum Einstieg hilft ein Überblick über die Ergebnisse der Analyse, die in diesem Vortrag zusammengefasst sind.\nDieser Post stellt die Ergebnisse mit etwas Kontext dar."
  },
  {
    "objectID": "word-embedding.html",
    "href": "word-embedding.html",
    "title": "5  Word Embedding",
    "section": "",
    "text": "Bild von mcmurryjulie auf Pixabay"
  },
  {
    "objectID": "word-embedding.html#vorab",
    "href": "word-embedding.html#vorab",
    "title": "5  Word Embedding",
    "section": "5.1 Vorab",
    "text": "5.1 Vorab\n\n5.1.1 Lernziele\n\nDie grundlegenden Konzepte der Informationstheorie erklären können\nDie vorgestellten Techniken des Textminings mit R anwenden können\n\n\n\n5.1.2 Vorbereitung\n\nLesen Sie diesen Text als Vorbereitung\nArbeiten Sie Hvitfeldt und Silge (2022), Kap. 5 durch.\n\n\n\n5.1.3 Benötigte R-Pakete\n\nlibrary(tidyverse)\nlibrary(easystats)  # Komfort für deskriptive Statistiken, wie `describe_distribution`\nlibrary(tidytext)\nlibrary(hcandersenr)   # Textdaten\nlibrary(slider)  # slide\nlibrary(widyr)  # pairwise_pmi\nlibrary(furrr)  # mehrere Kerne gleichzeitig nutzen\nlibrary(textdata)  # Worteinbettungen, vorgekocht\nlibrary(entropy)  # Entropie berechnen\nlibrary(widyr)  # Ähnlichkeit berechnen mit widyr_svd\nlibrary(furrr)  # Mehrere Kerne gleichzeitig"
  },
  {
    "objectID": "word-embedding.html#informationstheorie",
    "href": "word-embedding.html#informationstheorie",
    "title": "5  Word Embedding",
    "section": "5.2 Informationstheorie",
    "text": "5.2 Informationstheorie\n\n5.2.1 Einführung\nDie Informationstheorie ist eine der Sternstunden der Wissenschaft. Manche sagen dass Claude Shannon, der Autor der Theorie, auf einer Stufe mit Darwin und Einstein stehen sollte:\n\nIn this single paper, Shannon introduced this new fundamental theory. He raised the right questions, which no one else even thought of asking. This would have been enough to make this contribution earthshaking. But amazingly enough, Shannon also provided most of the right answers with class and elegance. In comparison, it took decades for a dozen of top physicists to define the basics of quantum theory. Meanwhile, Shannon constructed something equivalent, all by himself, in a single paper. Shannon’s theory has since transformed the world like no other ever had, from information technologies to telecommunications, from theoretical physics to economical globalization, from everyday life to philosophy. (…) I don’t think Shannon has had the credits he deserves. He should be right up there, near Darwin and Einstein, among the few greatest scientists mankind has ever had the chance to have.\n\nFür die Statistik ist die Informationstheorie von hoher Bedeutung. Im Folgenden schauen wir uns einige Grundlagen an.\n\n\n5.2.2 Wozu ist das gut?\nBevor man sich mit einem Thema wie der Informationtheorie (Informationsentropie mit verwandten Konstrukten oder kurz Entropie) beschäftigt, sollte die Frage geklärt sein, wozu das Thema gut ist. Hier sind drei Antworten dazu:\n\nIm Maschinenlernen wird die Informationtheorie verwendet, um die Güte von Klassifikationsmodellen zu berechnen.\nSpeziell im Textmining wird die Entropie verwendet, um den Zusammenhang von Wörtern zu quantifizieren.\nWenige Theorien haben so viel neue Forschung initiert, wie Shannon (1948) berühmtes Paper. Es ist also auf jeden Fall eine Perle der Geistesgeschichte.\n\n\n\n5.2.3 Shannon-Information\nMit der Shannon-Information (Information, Selbstinformation) quantifizieren wir, wie viel “Überraschung” sich in einem Ereignis verbirgt (Shannon 1948).\nEin Ereignis mit …\n\ngeringer Wahrscheinlichkeit: Viel Überraschung (Information)\nhoher Wahrscheinlichkeit: Wenig Überraschung (Information)\n\nWenn wir also erfahren, dass ein unwahrscheinliches Ereignis eingetreten ist (Schnee im Sommer), sind wir überraschter als wenn wir höhen, dass ein wahrscheinliches Ereignis eingetreten ist (Schnee im Winter).\nDie Information eines Ereignis ist also eng verbunden mit seiner Wahrscheinlichkeit oder auch mit den Odds eines Ereignisses.\nDie Shannon-Information ist die einzige Größe, die einige wünschenswerte Anforderungen1 erfüllt:\n\nStetig\nJe mehr Ereignisse in einem Zufallsexperiment möglich sind, desto höher die Information, wenn ein bestimmtes Ereignis eintritt\nAdditiv: Die Summe der Information zweier Teilereignisse ist gleich der Information des Gesamtereignis\n\n\nDefinition 5.1 (Shannon-Information) Die Information ist so definiert:\n\\[I(x) = - \\log_2 \\left( Pr(x) \\right)\\]\n\nAndere Logaritmusbasen sind möglich. Bei einem binären Logarithmus nennt man die Einheit Bit2.\nEin Münwzurf3 hat 1 Bit Information:\n\n-log(1/2, base = 2)\n\n[1] 1\n\n\nDamit gilt: \\(I = \\frac{1}{Pr(x)}\\)\nDie Information ist eng verwandt mit den Odds bzw. Log-Odds (Logits):\n\\(\\text{log-odds}(x)=\\log \\left({\\frac {p(x)}{p(\\lnot x)}}\\right)\\)\nLogits können als Differenz zweier Shannon-Infos ausgedrückt werden:\n\\(\\text{log-odds}(x)=I(\\lnot x)-I(x)\\)\nDie Information zweier unabhängiger Ereignisse ist additiv.\nDie gemeinsame Wahrscheinlichkeit zweier unabhängiger Ereignisse ist das Produkt der einzelnen Wahrscheinlichkeiten:\n\\(Pr(x,y) = Pr(x) \\cdot Pr(y)\\)\nDie gemeinsame Information ist dann\n\\[\n{\\displaystyle {\\begin{aligned}\\operatorname {I}(x,y)&=-\\log _{2}\\left[p(x,y)\\right]=-\\log _{2}\\left[p(x)p(y)\\right]\\\\[5pt]&=-\\log _{2}\\left[p{(x)}\\right]-\\log _{2}\\left[p{(y)}\\right]\\\\[5pt]&=\\operatorname {I} (x)+\\operatorname {I} (y)\\end{aligned}}}\n\\]\n\nBeispiel 5.1 (Information eines wahrscheinlichen Ereignisses) Die Information eines fast sicheren Ereignisses ist gering.\n\n-log(99/100, base = 2)\n\n[1] 0.01449957\n\n\n\n\nBeispiel 5.2 (Information eines unwahrscheinlichen Ereignisses) Die Information eines unwahrscheinlichen Ereignisses ist hoch.\n\n-log(01/100, base = 2)\n\n[1] 6.643856\n\n\n\n\nBeispiel 5.3 (Information eines Würfelwurfs) Die Wahrscheinlichkeitsfunktion eines Würfel ist\n\\({\\displaystyle Pr(k)={\\begin{cases}{\\frac {1}{6}},&k\\in \\{1,2,3,4,5,6\\}\\\\0,&{\\text{ansonsten}}\\end{cases}}}\\)\nDie Wahrscheinlichkeit, eine 6 zu würfeln, ist \\(Pr(X=6) = \\frac{1}{6}\\).\nDie Information von \\(X=6\\) beträgt also\n\\(I(X=6) = -\\log_2 \\left( Pr(X=6) \\right) = -\\log_2(1/6) \\approx 2.585 \\, \\text{bits}\\).\n\n-log(1/6, base = 2)\n\n[1] 2.584963\n\n\n\n\nBeispiel 5.4 (Information zweier Würfelwurfe) Die Wahrscheinlichkeit, mit zwei Würfeln, \\(X\\) und \\(Y\\), jeweils 6 zu würfeln, beträgt \\(Pr(X=6, Y=6) = \\frac{1}{36}\\)\nDie Information beträgt also\n\\(I(X=6, Y=6) = -\\log_2 \\left( Pr(6,6) \\right)\\)\n\n-log(1/36, base = 2)\n\n[1] 5.169925\n\n\nAufgrund der Additivität der Information gilt\n\\(I(6,6) = I(6) + I(6)\\)\n\n-log(1/6, base = 2) + -log(1/6, base = 2)\n\n[1] 5.169925\n\n\n\n\n\n5.2.4 Entropie\n(Informations)entropie ist die Summe der Informationen der Ereignisse einer Zufallsvariablen, \\(X\\).\n\nDefinition 5.2 (Informationsentropie) Informationsentropie ist so definiert:\n\\[H(p) = - \\text{E log} (p_i) = - \\sum_{i = 1}^n p_i \\text{log} (p_i) = E\\left[I(X) \\right]\\]\n\nDie Informationsentropie ist also die “mittlere” oder “erwartete Information einer Zufallsvariablen.\nDie Entropie eines Münzwurf-Versuchs (Bernoulli-Experiment) ist dann maximal (1 bit), wenn die Trefferwahrscheinlichkeit 50% beträgt: \\(Pr(X=x) = 1/2\\), s. Abb. Abbildung 5.1.\n\n\n\nAbbildung 5.1: Der Zusammenhang von Wahrscheinlichkeit und Entropie bei einem Bernoulli-Versuch, Quelle: Wikipedia, CC-BY-SA, Brona, Rubber Duck\n\n\n\n\n5.2.5 Gemeinsame Information\nDie gemeinsame Information (mutual information, MI) zweier Zufallsvariablen \\(X\\) und \\(Y\\), \\(I(X,Y)\\), quantifiziert die Informationsmenge, die man über \\(Y\\) erhält, wenn man \\(X\\) beobachtet. Mit anderen Worten: Die MI ist ein Maß des Zusammenhangs zweier (nominaler) Variablen. Im Gegensatz zur Korrelation ist die MI nicht auf lineare Abhängigkeiten beschränkt.\nDie MI quantifiziert den Unterschied zwischen der gemeinsamen Verteilung \\(Pr(X,Y)\\) und dem Produkt einer einzelnen4 Wahrscheinlichkeitsverteilungen, d.h. \\(Pr(X)\\) und \\(Pr(Y)\\).\nWenn die beiden Variablen (stochastisch) unabhängig5 sind, ist ihre gemeinsame Information Null:\n\\(I(X,Y) = 0 \\quad \\text{gdw} \\quad \\bot(X,Y)\\).\nDann gilt nämlich:\n\\(\\log \\left( \\frac{Pr(X,Y)} {Pr(X) \\cdot Pr(Y)} \\right) =\\log(1) = 0\\).\nDas macht intuitiv Sinn: Sind zwei Variablen unabhängig, so erfährt man nichts über die zweite, wenn man die erste kennt. So ist Kenntnis der Sternzeichens einer Person und ihrer Körpergröße unabhängig.\nDas Gegenteil ist auch wahr: Sind zwei Variablen voneinander komplett abhängig, so weiß man alles über die zweite, wenn man die erste kennt.\nDie gemeinsame Information kann man sich als Summe der einzelnen gemeinsamen Informationen von \\(XY\\) sehen (s. Tabelle 5.1):\n\nd <- tibble::tribble(\n     ~x1,    ~x2,    ~x3,\n  \"x1y2\", \"x2y1\", \"x3y1\",\n  \"x2y1\", \"x2y2\", \"x3y2\",\n  \"x1y3\", \"x2y3\", \"x3y3\"\n  )\nd\n\n\n\n\nTabelle 5.1: Summe der punktweisen gemeinsamen Informationen\n\n\nx1\nx2\nx3\n\n\n\n\nx1y2\nx2y1\nx3y1\n\n\nx2y1\nx2y2\nx3y2\n\n\nx1y3\nx2y3\nx3y3\n\n\n\n\n\n\n\n\\(I(X,Y) = \\Sigma_Y \\Sigma_y Pr(x,y) \\underbrace{\\log \\left( \\frac{Pr(X,Y)}{Pr(X) Pr(Y)} \\right)}_\\text{punktweise MI}\\)\nDie Summanden der gemeinsamen Information bezeichnet man auch als punktweise gemeinsame Information (pointwise mutual information, PMI), entsprechend, s. Gleichung 5.1. MI ist also der Erwartungswert der PMI.\n\\[{\\displaystyle \\operatorname {PMI} (x,y)\\equiv \\log_{2}{\\frac {p(x,y)}{p(x)p(y)}}=\\log _{2}{\\frac {p(x|y)}{p(x)}}=\\log _{2}{\\frac {p(y|x)}{p(y)}}}\n\\tag{5.1}\\]\nAndere Basen als log2 sind gebräuchlich, vor allem der natürliche Logarithmus.\n\nAnmerkung. Die zwei rechten Umformungen in Gleichung 5.1 basieren auf der Umformung der gemeinsamen Wahrscheinlichkeit.\nZur Erinnerung: \\(p(x,y) = p(y)p(x|y) = p(x)p(y|x)\\)\n\n\nBeispiel 5.5 (Interpretation der PMI) Sei \\(p(x) = p(y) = 1/10\\) und \\(p(x,y) = 1/10\\). Wären \\(x\\) und \\(y\\) unabhängig, dann wäre \\(p^{\\prime}(x,y) = p(x)p(y) = 1/100\\). Das Verhältnis der Produkte der einzelnen Wahrscheinlichkeit zur gemeinsamen Wahrscheinlichkeit wäre dann 1 und der Logarithmus von 1 ist 0. Das Verhältnis von 1 entspricht also der Unabhängigkeit. Ist das Verhältnis z.B. 5, so zeigt das eine gewisse Abhängigkeit an. Im obigen Beispiel gilt: \\(\\frac{1/20}{1/100}=5\\).\n\nDie MI wird auch über die sog. Kullback-Leibler-Divergenz definiert, die die Differenz zwischen gemeinsamer Wahrscheinlichkeitsfunktion bzw. -dichte und dem Produkt der jeweiligen Randwahrscheinlichkeiten bzw. -dichte angibt.\n\n\n5.2.6 Maximumentropie\n\nDefinition 5.3 (Maximumentropie) Die Verteilungsform, für die es die meisten Möglichkeiten (Pfade im Baumdiagramm) gibt, hat die höchste Informationsentropie.\n\nAbbildung 5.2 zeigt ein Baumdiagramm für einen 3-fachen Münzwurf. In den “Blättern” (Endknoten) sind die Ergebnisse des Experiments dargestellt sowie die Zufallsvariable \\(X\\), die die Anzahl der “Treffer” (Kopf) fasst. Wie man sieht, vereinen einige Werte der Zufallsvariable mehr Pfade auf sich als andere: Der Wert \\(X=1\\) vereinigt 3 Pfade (von 8) auf sich; der Wert \\(X=3\\) nur 1 Pfad.\n\n\n\nAbbildung 5.2: Pfade im Baumdiagramm: 3-facher Münzwurf\n\n\n\n\n5.2.7 Ilustration\nSagen wir, Sie stehen vor 5 Eimern und haben 10 Kieselsteine bei sich, die mit den Nummern 1 bis 10 beschriftet sind (McElreath 2020). Weil Sie nichts besseres zu tun haben, werfen Sie die Kiesel in die Eimer und zwar so, dass die Wahrscheinlichkeit für einen Kiesel in einen bestimmten Eimer zu landen für alle Eimer gleich ist. Sie werfen also Ihre 10 Kiesel und betrachten das Ergebnis; die Kiesel sind jetzt in einem bestimmten (zufälligen) Arrangement auf die Eimer verteilt. Jede Aufteilung (der 10 Kiesel in den 5 Eimern) ist gleich wahrscheinlich6 – die Wahrscheinlichkeit, dass alle 10 Kiesel in Eimer 1 landen ist also gleich hoch wie die Wahrscheinlichkeit, dass jeder Eimer einen Kiesel abkriegt. Jetzt kommt’s: Manche Arrangements können auf mehrere Arten erzielt werden als andere. So gibt es nur eine Aufteilung für alle 10 Kiesel in einem Eimer (Teildiagramm a, in Abbildung 5.3). Aber es gibt 90 Möglichkeiten, die Kiesel so aufzuteilen, dass 2 in Eimer 2 landen, 8 in Eimer 4 und 2 in Eimer 4, s. Teildiagramm b in Abbildung 5.3. Teildiagramme c bis e zeigen, dass die Anzahl der Aufteilungen schnell astronomisch hoch wird, wenn sich die Kiesel “gleichmäßiger” auf die Eimer verteilen. Die gleichmäßigste Aufteilung (Diagramm e) hat die größte Zahl an möglichen Anordnungen. Eine Aufteilung der Kiesel auch als Pfad durch ein Baumdiagramm beschrieben werden.\nHier sind ein paar verschiedene Arrangements, deren Anzahl von Aufteilungen wir hier anschauen, s. Tabelle 5.2:\n\nd <-\n  tibble(a = c(0, 0, 10, 0, 0),\n         b = c(0, 1, 8, 1, 0),\n         c = c(0, 2, 6, 2, 0),\n         d = c(1, 2, 4, 2, 1),\n         e = 2) \nd\n\n\n\n\nTabelle 5.2: Ein paar verschiedene Arrangements (a-e) der Kiesel in den fünf Eimern\n\n\na\nb\nc\nd\ne\n\n\n\n\n0\n0\n0\n1\n2\n\n\n0\n1\n2\n2\n2\n\n\n10\n8\n6\n4\n2\n\n\n0\n1\n2\n2\n2\n\n\n0\n0\n0\n1\n2\n\n\n\n\n\n\n\n\n\n\n\n\nAbbildung 5.3: Anzahl von Aufteilungen der 10 Kiesel in 5 Eimer\n\n\n\n\nHier sind die Wahrscheinlichkeitsverteilungen der 5 Arrangements7:\n\nd %>% \n  mutate_all(~. / sum(.))\n\n\n\n\n\na\nb\nc\nd\ne\n\n\n\n\n0\n0.0\n0.0\n0.1\n0.2\n\n\n0\n0.1\n0.2\n0.2\n0.2\n\n\n1\n0.8\n0.6\n0.4\n0.2\n\n\n0\n0.1\n0.2\n0.2\n0.2\n\n\n0\n0.0\n0.0\n0.1\n0.2\n\n\n\n\n\n\nDann kann man die Wahrscheinlichkeit einfach in Entropie umrechnen8:\n\nd %>% \n  mutate_all(~ . / sum(.)) %>% \n  gather() %>% \n  group_by(key) %>% \n  summarise(h = -sum(ifelse(value == 0, 0, value * log(value))))\n\n\n\n\n\nkey\nh\n\n\n\n\na\n0.0000000\n\n\nb\n0.6390319\n\n\nc\n0.9502705\n\n\nd\n1.4708085\n\n\ne\n1.6094379\n\n\n\n\n\n\nDas ifelse dient nur dazu, eine Wahrscheinlichkeit von 0 eine Entropie von 0 zu verpassen9, denn sonst würden wir ein Problem rennen, wenn wir \\(log(0)\\) ausrechnen.\n\nlog(0)\n\n[1] -Inf"
  },
  {
    "objectID": "word-embedding.html#zufallstext-erkennen",
    "href": "word-embedding.html#zufallstext-erkennen",
    "title": "5  Word Embedding",
    "section": "5.3 Zufallstext erkennen",
    "text": "5.3 Zufallstext erkennen\n\n5.3.1 Entropie von Zufallstext\nKann man wohl Zufallstext maschinell erkennen? Sicher gibt es viele Ansätze, um das Problem anzugehen. Lassen Sie uns einen Ansatz erforschen. Erforschen heißt, wir erforschen für uns, es handelt sich um eine didaktische Übung, das Ziel ist nicht, Neuland für die Menschheit zu betreten.\nAber zuerst müssen wir überlegen, was “Zufallstext” bedeuten soll.\nNehmen wir uns dazu zuerst einen richtigen Text, ein Märchen von H.C. Andersen zum Beispiel. Nehmen wir das Erste aus der Liste in dem Tibble hcandersen_de, “das Feuerzeug”.\n\ndas_feuerzeug <-\n  hcandersen_de  %>% \n  filter(book == \"Das Feuerzeug\") %>% \n  unnest_tokens(input = text, output = word) %>% \n  pull(word) \n\nhead(das_feuerzeug)\n\n[1] \"es\"         \"kam\"        \"ein\"        \"soldat\"     \"die\"       \n[6] \"landstraße\"\n\n\nDas Märchen ist 2688 Wörter lang.\n\nwortliste <- \nhcandersen_de  %>% \n  filter(book == \"Das Feuerzeug\") %>% \n  unnest_tokens(output = word, input = text) %>% \n  pull(word) %>% \n  unique()\n\nhead(wortliste)\n\n[1] \"es\"         \"kam\"        \"ein\"        \"soldat\"     \"die\"       \n[6] \"landstraße\"\n\n\nJetzt ziehen wir Stichproben (mit Zurücklegen) aus dieser Liste und erstellen damit eine Art Zufallstext.\n\nzufallstext <- \n  sample(x = wortliste, size = length(das_feuerzeug)*10^2, replace = TRUE)\nhead(zufallstext)\n\n[1] \"hat\"    \"kreide\" \"ins\"    \"komme\"  \"seinen\" \"dort\"  \n\n\nZählen wir, wie häufig jedes Wort vorkommt:\n\nzufallstext_count <-\ntibble(zufallstext = zufallstext) %>% \n  count(zufallstext)\n\nhead(zufallstext_count)\n\n\n\n\n\nzufallstext\nn\n\n\n\n\nab\n356\n\n\nabend\n386\n\n\naber\n347\n\n\nabflog\n350\n\n\nabschlagen\n388\n\n\nacht\n379\n\n\n\n\n\n\nDer Häufigkeitsvektor von wortliste besteht nur aus Einsen, so haben wir ja gerade die Wortliste definiert:\n\nwortliste_count <-\ntibble(wortliste = wortliste) %>% \n  count(wortliste)\n\nhead(wortliste_count)\n\n\n\n\n\nwortliste\nn\n\n\n\n\nab\n1\n\n\nabend\n1\n\n\naber\n1\n\n\nabflog\n1\n\n\nabschlagen\n1\n\n\nacht\n1\n\n\n\n\n\n\nDaher ist ihre Informationsentropy maximal.\n\nentropy(wortliste_count$n, unit = \"log2\")\n\n[1] 9.47978\n\n\nDie Häufigkeiten der Wörter in zufallstext hat eine hohe Entropie.\n\nentropy(zufallstext_count$n, unit = \"log2\")\n\n[1] 9.47792\n\n\nZählen wir die Häufigkeiten in der Geschichte “Das Feuerzeug”.\n\ndas_feuerzeug_count <-\n  tibble(text = das_feuerzeug) %>% \n  count(text)\n\nhead(das_feuerzeug_count)\n\n\n\n\n\ntext\nn\n\n\n\n\nab\n2\n\n\nabend\n3\n\n\naber\n21\n\n\nabflog\n1\n\n\nabschlagen\n1\n\n\nacht\n1\n\n\n\n\n\n\nUnd berechnen dann die Entropie:\n\nentropy(das_feuerzeug_count$n, unit = \"log2\")\n\n[1] 8.075194\n\n\nDer Zufallstext hat also eine höhere Entropie als der echte Märchentext. Der Zufallstext ist also gleichverteilter in den Worthäufigkeiten.\nPro Bit weniger Entropie halbiert sich die Anzahl der Möglichkeiten einer Häufigkeitsverteilung.\n\n\n5.3.2 MI von Zufallstext\nLeft as an exercises for the reader10 🥳."
  },
  {
    "objectID": "word-embedding.html#daten",
    "href": "word-embedding.html#daten",
    "title": "5  Word Embedding",
    "section": "5.4 Daten",
    "text": "5.4 Daten\n\n5.4.1 Complaints-Datensatz\nDer Datensatz complaints stammt aus dieser Quelle.\nDen Datensatz complaints kann man hier herunterladen. Im Buch ist die Webseite nicht (direkt?) angegeben. Die Datei ist mit gz gepackt; read_csv sollte das automatisch entpacken. Achtung: Die Datei ist recht groß.\n\nd_path <- \"https://github.com/EmilHvitfeldt/smltar/raw/master/data/complaints.csv.gz\"\ncomplaints <- read_csv(d_path)\n\nGeschickter als jedes Mal die Datei herunterzuladen, ist es, sie einmal herunterzuladen, und dann lokal zu speichern, etwa im Unterordner data des RStudio-Projektordners.\nNach dem Importieren wird der Datensatz in eine Tidy-Form gebracht (mit unnest_tokens) und dann verschachtelt, mit nest.\n\n\n5.4.2 Complaints verkürzt und geschachtelt\nUm ein Herumprobieren zu erleichtern, ist hier der Datensatz complaints in zwei verkürzten Formen bereitgestellt:\n\nnested_words2_path <- \"https://github.com/sebastiansauer/datascience-text/raw/main/data/nested_words2.rds\"\nnested_words3_path <- \"https://github.com/sebastiansauer/datascience-text/raw/main/data/nested_words3.rds\"\n\nnested_words2 enthält die ersten 10% des Datensatz nested_wordsund ist gut 4 MB groß (mit gz gezippt); er besteht aus ca. 11 Tausend Beschwerden. nested_words3 enthält nur die ersten 10 Beschwerden, was ihn gut zum Handhaben macht.\nBeide sind verschachtelt und aus tidy_complaints (s. Kap. 5.1) hervorgegangen.\n\nnested_words3 <- read_rds(nested_words3_path)\n\nDas sieht dann so aus:\n\nnested_words3 %>% \n  head(3)\n\n\n\n\n\n\n\n\n\ncomplaint_id\nwords\n\n\n\n\n3384392\nsystems , inc , is , trying , to , collect , a , debt , that , is , not , mine , not , owed , and , is , inaccurate\n\n\n3417821\ni , would , like , to , request , the , of , the , following , items , from , my , credit , report , which , are , the , result , of , my , victim , to , identity , theft , this , information , does , not , to , transactions , that , i , have , made , accounts , that , i , have , opened , as , the , attached , supporting , documentation, can , as , such , it , should , be , blocked , from , on , my , credit , report , pursuant , to , section , of , the , fair , credit , reporting , act\n\n\n3433198\nover , the , past , 2 , weeks , i , have , been , receiving , amounts , of , telephone , calls , from , the , company , listed , in , this , complaint , the , calls , between , xxxx , xxxx , and , xxxx , xxxx , to , my , cell , and , at , my , job , the , company , does , not , have , the , right , to , me , at , work , and , i , want , this , to , stop , it , is , extremely , to , be , told , 5 , times , a , day , that , i , have , a , call , from , this , collection, agency , while , at , work\n\n\n\n\n\n\nWerfen wir einen Blick in den TExt der ersten Beschwerde des Datensatzes mit der ID nested_words3_path$complaint_id[1].\n\nbeschwerde1_text <- nested_words3$words[[1]]\n\nDas ist ein Tibble mit einer Spalte und 17 Wörtern; da wir schon auf Unigramme aufgeteilt haben, ist jedes Wort ein Element des Vektors word:\n\nbeschwerde1_text %>% \n  head()\n\n\n\n\n\nword\n\n\n\n\nsystems\n\n\ninc\n\n\nis\n\n\ntrying\n\n\nto\n\n\ncollect\n\n\n\n\n\n\n\nbeschwerde1_text$word\n\n [1] \"systems\"    \"inc\"        \"is\"         \"trying\"     \"to\"        \n [6] \"collect\"    \"a\"          \"debt\"       \"that\"       \"is\"        \n[11] \"not\"        \"mine\"       \"not\"        \"owed\"       \"and\"       \n[16] \"is\"         \"inaccurate\""
  },
  {
    "objectID": "word-embedding.html#kommentare-und-hilfestellungen",
    "href": "word-embedding.html#kommentare-und-hilfestellungen",
    "title": "5  Word Embedding",
    "section": "5.5 Kommentare und Hilfestellungen",
    "text": "5.5 Kommentare und Hilfestellungen\n\n5.5.1 PMI berechnen\nRufen Sie sich die Definition der PMI ins Gedächtnis, s. Gleichung 5.1.\nMit R kann man die PMI z.B. so berechnen, s. ? pairwise_pmi aus dem Paket {widyr}.\nZum Paket widyr von Robinson und Silge:\n\nThis package wraps the pattern of un-tidying data into a wide matrix, performing some processing, then turning it back into a tidy form. This is useful for several mathematical operations such as co-occurrence counts, correlations, or clustering that are best done on a wide matrix.\n\nQuelle\nErzeugen wir uns Dummy-Daten:\n\ndat <- tibble(feature = rep(1:5, each = 2),\n              item = c(\"a\", \"b\",\n                       \"a\", \"c\",\n                       \"a\", \"c\",\n                       \"b\", \"e\",\n                       \"b\", \"f\"))\n\ndat\n\n\n\n\n\nfeature\nitem\n\n\n\n\n1\na\n\n\n1\nb\n\n\n2\na\n\n\n2\nc\n\n\n3\na\n\n\n3\nc\n\n\n4\nb\n\n\n4\ne\n\n\n5\nb\n\n\n5\nf\n\n\n\n\n\n\nAus der Hilfe der Funktion:\n\nFind pointwise mutual information of pairs of items in a column, based on a “feature” column that links them together. This is an example of the spread-operate-retidy pattern.\n\nDie Argumente der Funktion sind:\nitem\nItem to compare; will end up in item1 and item2 columns\nfeature\nColumn describing the feature that links one item to others\nManche Berechnungen (Operationen) lassen sich vielleicht leichter nicht in der Tidy-Form (Langform), sondern in der “breiten” oder Matrixform ausführen. Wandeln wir mal dat von der Langform in die Breitform um:\n\ntable(dat$item, dat$feature)\n\n   \n    1 2 3 4 5\n  a 1 1 1 0 0\n  b 1 0 0 1 1\n  c 0 1 1 0 0\n  e 0 0 0 1 0\n  f 0 0 0 0 1\n\n\nSilge und Robinson verdeutlichen das Prinzip von widyr so, s. Abbildung 5.4.\n\n\n\nAbbildung 5.4: Die Funktionsweise von widyr, Quelle: Silge und Robinson\n\n\n(Vgl. auch die Erklärung hier.)\nBauen wir das mal von Hand nach.\nRandwahrscheinlichkeiten von a und c sowie deren Produkt, p_a_p_c:\n\np_a <- 3/5\np_c <- 2/5\n\np_a_p_c <- p_a * p_c\np_a_p_c\n\n[1] 0.24\n\n\nGemeinsame Wahrscheinlichkeit von a und c:\n\np_ac <- 2/5\n\nPMI von Hand berechnet:\n\nlog(p_ac/p_a_p_c)\n\n[1] 0.5108256\n\n\nMan beachte, dass hier als Basis \\(e\\), der natürliche Logarithmus, verwendet wurde (nicht 2).\nJetzt berechnen wir die PMI mit pairwise_pmi.\n\npairwise_pmi(dat, item = item, feature = feature)\n\n\n\n\n\nitem1\nitem2\npmi\n\n\n\n\nb\na\n-0.5877867\n\n\nc\na\n0.5108256\n\n\na\nb\n-0.5877867\n\n\ne\nb\n0.5108256\n\n\nf\nb\n0.5108256\n\n\na\nc\n0.5108256\n\n\nb\ne\n0.5108256\n\n\nb\nf\n0.5108256\n\n\n\n\n\n\nWie man sieht, entspricht unserer Berechnung von Hand der Berechnung mit pairwise_pmi.\n\n\n5.5.2 Sliding\nSliding ist ein interessantes Konzept, aber man braucht vielleicht etwas Zeit, um sein Hirn um das Konzept zu wickeln…\nHier eine Illustration:\n\ntxt_vec <- \"Das ist ein Test, von dem nicht viel zu erwarten ist\"\n\nslider::slide(txt_vec, ~ .x, .before = 2)\n\n[[1]]\n[1] \"Das ist ein Test, von dem nicht viel zu erwarten ist\"\n\n\nOh, da passiert nichts?! Kaputt? Nein, wir müssen jedes Wort als ein Element des Vektors auffassen.\n\ntxt_df <-\n  tibble(txt = txt_vec) %>% \n  unnest_tokens(input = txt, output = word)\n\nhead(txt_df)\n\n\n\n\n\nword\n\n\n\n\ndas\n\n\nist\n\n\nein\n\n\ntest\n\n\nvon\n\n\ndem\n\n\n\n\n\n\n\nslider::slide(txt_df$word, ~ .x, .before = 2)\n\n[[1]]\n[1] \"das\"\n\n[[2]]\n[1] \"das\" \"ist\"\n\n[[3]]\n[1] \"das\" \"ist\" \"ein\"\n\n[[4]]\n[1] \"ist\"  \"ein\"  \"test\"\n\n[[5]]\n[1] \"ein\"  \"test\" \"von\" \n\n[[6]]\n[1] \"test\" \"von\"  \"dem\" \n\n[[7]]\n[1] \"von\"   \"dem\"   \"nicht\"\n\n[[8]]\n[1] \"dem\"   \"nicht\" \"viel\" \n\n[[9]]\n[1] \"nicht\" \"viel\"  \"zu\"   \n\n[[10]]\n[1] \"viel\"     \"zu\"       \"erwarten\"\n\n[[11]]\n[1] \"zu\"       \"erwarten\" \"ist\"     \n\n\nAh!\nDas Aufteilen in einzelne Wörter pro Element des Vektors könnte man auch so erreichen:\n\ntxt_vec2 <- str_split(txt_vec, pattern = boundary(\"word\")) %>% \n  simplify() \n\nslide(txt_vec2, .f = ~.x, .before = 2)\n\nShifting non-numeric variables is not possible.\n  Try using 'to_numeric()' and specify the 'lowest' argument.\n\n\n [1] \"Das\"      \"ist\"      \"ein\"      \"Test\"     \"von\"      \"dem\"     \n [7] \"nicht\"    \"viel\"     \"zu\"       \"erwarten\" \"ist\"     \n\n\nIn unserem Beispiel mit den Beschwerden:\n\nslide(beschwerde1_text$word,  ~.x, .before = 2)\n\nShifting non-numeric variables is not possible.\n  Try using 'to_numeric()' and specify the 'lowest' argument.\n\n\n [1] \"systems\"    \"inc\"        \"is\"         \"trying\"     \"to\"        \n [6] \"collect\"    \"a\"          \"debt\"       \"that\"       \"is\"        \n[11] \"not\"        \"mine\"       \"not\"        \"owed\"       \"and\"       \n[16] \"is\"         \"inaccurate\"\n\n\n\n\n5.5.3 Funktion slide_windows\nDie Funktion slide_windows im Kapitel 5.2 ist recht kompliziert. In solchen Fällen ist es hilfreich, sich jeden Schritt einzeln ausführen zu lassen. Das machen wir jetzt mal.\nHier ist die Syntax der Funktion slide_windows:\n\nslide_windows <- function(tbl, window_size) {\n  skipgrams <- slider::slide(\n    tbl, \n    ~.x,  # Syntax ähnlich zu purrr::map()\n    .after = window_size - 1, \n    .step = 1, \n    .complete = TRUE\n  )\n  \n  safe_mutate <- safely(mutate)\n  \n  out <- map2(skipgrams,\n              1:length(skipgrams),\n              ~ safe_mutate(.x, window_id = .y))\n  \n  out %>%\n    transpose() %>%\n    pluck(\"result\") %>%\n    compact() %>%\n    bind_rows()\n}\n\nErschwerend kommt eine große Datenmenge und eine lange Berechnungszeit dazu, was das Debuggen (Nachvollziehen und Durchdenken) der Schritte zusätzlich erschwert. In solchen Fällen hilft die goldene Regel: Mach es dir so einfach wie möglich (aber nicht einfacher). Wir nutzen also den stark verkleinerten Datensatz nested_words3, den wir oben importiert haben.\nZuerst erlauben wir mal, dasss unsere R-Session mehrere Kerne benutzen darf.\n\nplan(multisession)  ## for parallel processing\n\nDie Funktion slide_windows ist recht kompliziert. Es hilft oft, sich mit debug(fun) eine Funktion Schritt für Schritt anzuschauen.\nGehen wir Schritt für Schritt durch die Syntax von slide_windows.\nWerfen wir einen Blick in words, erstes Element (ein Tibble mit einer Spalte). Denn die einzelnen Elemente vonwordswerden an die Funktionslide_windows` als “Futter” übergeben.\n\nfutter1 <- nested_words3[[\"words\"]][[1]]\nfutter1\n\n\n\n\n\nword\n\n\n\n\nsystems\n\n\ninc\n\n\nis\n\n\ntrying\n\n\nto\n\n\ncollect\n\n\na\n\n\ndebt\n\n\nthat\n\n\nis\n\n\nnot\n\n\nmine\n\n\nnot\n\n\nowed\n\n\nand\n\n\nis\n\n\ninaccurate\n\n\n\n\n\n\nDas ist der Text der ersten Beschwerde.\nOkay, also dann geht’s los durch die einzelnen Schritte der Funktion slide_windows.\nZunächst holen wir uns die “Fenster” oder “Skipgrams”:\n\nskipgrams1 <- slider::slide(\n   futter1, \n    ~.x, \n    .after = 3, \n    .step = 1, \n    .complete = TRUE\n  )\n\nBei slide(tbl, ~.x) geben wir die Funktion an, die auf tbl angewendet werden soll. Daher auch die Tilde, die uns von purrr::map() her bekannt ist. In unserem Fall wollen wir nur die Elemente auslesen; Elemente auslesen erreicht man, in dem man sie mit Namen anspricht, in diesem Fall mit dem Platzhalter .x.\nJedes Element von skipgrams1 ist ein 4*1-Tibble und ist ein Skripgram.\n\nskipgrams1 %>% str()\n\nList of 17\n $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"systems\" \"inc\" \"is\" \"trying\"\n $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"inc\" \"is\" \"trying\" \"to\"\n $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"is\" \"trying\" \"to\" \"collect\"\n $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"trying\" \"to\" \"collect\" \"a\"\n $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"to\" \"collect\" \"a\" \"debt\"\n $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"collect\" \"a\" \"debt\" \"that\"\n $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"a\" \"debt\" \"that\" \"is\"\n $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"debt\" \"that\" \"is\" \"not\"\n $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"that\" \"is\" \"not\" \"mine\"\n $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"is\" \"not\" \"mine\" \"not\"\n $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"not\" \"mine\" \"not\" \"owed\"\n $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"mine\" \"not\" \"owed\" \"and\"\n $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"not\" \"owed\" \"and\" \"is\"\n $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n  ..$ word: chr [1:4] \"owed\" \"and\" \"is\" \"inaccurate\"\n $ : NULL\n $ : NULL\n $ : NULL\n\n\nDas zweite Skipgram von skipgrams1 enthält, naja, das zweite Skipgram.\n\nskipgrams1[[2]] %>% str()\n\ntibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n $ word: chr [1:4] \"inc\" \"is\" \"trying\" \"to\"\n\n\nUnd so weiter.\nOkay, weiter im Programm. Jetzt mappen wir das Erstellen der Skipgrams\n\nsafe_mutate <- safely(mutate)\n  \nout1 <- map2(skipgrams1,\n             1:length(skipgrams1),\n             ~ safe_mutate(.x, window_id = .y))\n  \nout1 %>% \n  head(2) %>% \n  str()\n\nList of 2\n $ :List of 2\n  ..$ result: tibble [4 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ word     : chr [1:4] \"systems\" \"inc\" \"is\" \"trying\"\n  .. ..$ window_id: int [1:4] 1 1 1 1\n  ..$ error : NULL\n $ :List of 2\n  ..$ result: tibble [4 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ word     : chr [1:4] \"inc\" \"is\" \"trying\" \"to\"\n  .. ..$ window_id: int [1:4] 2 2 2 2\n  ..$ error : NULL\n\n\nout1 ist eine Liste mit 17 Elementen; jedes Element mit jeweils zwei Elementen: Den Ergebnissen und ob es einen Fehler gab bei safe_mutate. Die 10 Elemente entsprechen den 10 Skipgrams. Wir können aber out1 auch “drehen”, transponieren genauer gesagt. so dass wir eine Liste mit zwei Elementen bekommen: das erste Element hat die (zehn) Ergebnisse (nämlich die Skipgrams) und das zweite Elemente, ob es Fehler gab.\nDas Prinzip des Transponierens ist in Abbildung 5.5 dargestellt.\n\n\n\nAbbildung 5.5: Transponieren einer Matrix (“Tabelle”)\n\n\n\nout2 <-\nout1 %>%\n  transpose() \n\nPuh, das ist schon anstrengendes Datenyoga…\nAber jetzt ist es einfach. Wir ziehen das erste der beiden Elemente, die Ergebnisse heraus (pluck), entfernen leere Elemente (compact) und machen einen Tibble daraus (bind_rows):\n\nout2 %>% \n  pluck(\"result\") %>%\n  compact() %>%\n  bind_rows() %>% \n  head()\n\n\n\n\n\nword\nwindow_id\n\n\n\n\nsystems\n1\n\n\ninc\n1\n\n\nis\n1\n\n\ntrying\n1\n\n\ninc\n2\n\n\nis\n2\n\n\n\n\n\n\nGeschafft!\n\n\n5.5.4 Ähnlichkeit berechnen\nNachdem wir jetzt slide_windows kennen, schauen wir uns die nächsten Schritte an:\n\ntidy_pmi1 <- nested_words3 %>%  # <--- Kleiner Datensatz!\n  mutate(words = future_map(words, slide_windows, 4L))\n\nWir werden slide_windows auf die Liste words an, die die Beschwerden enthält. Für jede Beschwerde erstellen wir die Skipgrams; diese Schleife wird realisiert über map bzw. future_map, die uns erlaubt, mehrere Kerne des Rechners gleichzeitig zu nutzen, damit es schneller geht.\nHier sehen wir z.B. die Skipgram-IDs der ersten Beschwerde.\n\ntidy_pmi1[[\"words\"]][[1]] %>% \n  head()\n\n\n\n\n\nword\nwindow_id\n\n\n\n\nsystems\n1\n\n\ninc\n1\n\n\nis\n1\n\n\ntrying\n1\n\n\ninc\n2\n\n\nis\n2\n\n\n\n\n\n\nGenestet siehst es so aus:\n\ntidy_pmi1 %>% \n  head(1)\n\n\n\n\n\n\n\n\n\ncomplaint_id\nwords\n\n\n\n\n3384392\nsystems , inc , is , trying , inc , is , trying , to , is , trying , to , collect , trying , to , collect , a , to , collect , a , debt , collect , a , debt , that , a , debt , that , is , debt , that , is , not , that , is , not , mine , is , not , mine , not , not , mine , not , owed , mine , not , owed , and , not , owed , and , is , owed , and , is , inaccurate, 1 , 1 , 1 , 1 , 2 , 2 , 2 , 2 , 3 , 3 , 3 , 3 , 4 , 4 , 4 , 4 , 5 , 5 , 5 , 5 , 6 , 6 , 6 , 6 , 7 , 7 , 7 , 7 , 8 , 8 , 8 , 8 , 9 , 9 , 9 , 9 , 10 , 10 , 10 , 10 , 11 , 11 , 11 , 11 , 12 , 12 , 12 , 12 , 13 , 13 , 13 , 13 , 14 , 14 , 14 , 14\n\n\n\n\n\n\nDie Listenspalte entschachteln wir mal:\n\ntidy_pmi2 <- tidy_pmi1 %>% \n  unnest(words)  # entschachtele\n\ntidy_pmi2 %>% \n  head()\n\n\n\n\n\ncomplaint_id\nword\nwindow_id\n\n\n\n\n3384392\nsystems\n1\n\n\n3384392\ninc\n1\n\n\n3384392\nis\n1\n\n\n3384392\ntrying\n1\n\n\n3384392\ninc\n2\n\n\n3384392\nis\n2\n\n\n\n\n\n\nZum Berechnen der Ähnlichkeit brauchen wir eineindeutige IDs, nach dem Prinzip “1. Skipgram der 1. Beschwerde” etc:\n\ntidy_pmi3 <- tidy_pmi2 %>% \n  unite(window_id, complaint_id, window_id)  # führe Spalten zusammen\n\ntidy_pmi3 %>% \n  head()\n\n\n\n\n\nwindow_id\nword\n\n\n\n\n3384392_1\nsystems\n\n\n3384392_1\ninc\n\n\n3384392_1\nis\n\n\n3384392_1\ntrying\n\n\n3384392_2\ninc\n\n\n3384392_2\nis\n\n\n\n\n\n\nSchließlich berechnen wir die Ähnlichkeit mit pairwise_pmi, das hatten wir uns oben schon mal näher angeschaut:\n\ntidy_pmi4 <- tidy_pmi3 %>% \n  pairwise_pmi(word, window_id)  # berechne Ähnlichkeit\n\ntidy_pmi <- tidy_pmi4  # mit dem Objekt arbeiten wir dann weiter\n\ntidy_pmi %>% \n  head()\n\n\n\n\n\nitem1\nitem2\npmi\n\n\n\n\ninc\nsystems\n5.728498\n\n\nis\nsystems\n2.838126\n\n\ntrying\nsystems\n5.035351\n\n\nsystems\ninc\n5.728498\n\n\nis\ninc\n2.838126\n\n\ntrying\ninc\n5.035351\n\n\n\n\n\n\n\n\n5.5.5 SVD\nDie Singulärwertzerlegung (singular value decomposition, SVD) ist ein Verfahren zur Verringerung der Spaltenzahl (Dimension), vergleichbar zur Faktorenanalyse. Zur Anschaulichkeit - und ohne substanziellen Hintergrund - sei folgendes Beispiel genannt: Die Verben “gehen”, “rennen”, “laufen”, “schwimmen”, “fahren”, “rutschen” könnten zu einer gemeinsamen Dimension, etwa “fortbewegen” reduziert werden. Jedes einzelne der eingehenden Verben erhält eine Zahl von 0 bis 1, das die konzeptionelle Nähe des Verbs zur “dahinterliegenden” Dimension (fortbewegen) quantifiziert; die Zahl nennt man auch die “Ladung” des Items (Worts) auf die Dimension. Sagen wir, wir identifizieren 10 Dimensionen. Man erhält dann für jedes unique Wort im Corpus einen Vektor mit den Ladungen auf die Dimensionen. Im genannten Beispiel wäre es ein 10-stelliger Vektor. So wie ein 3-stelliger Vektor die Position einer Biene im Raum beschreibt11, beschreibt hier unser 10-stelliger Vektor die “Position” eines Worts in unserem Einbettungsvektor.\nDie Syntax dazu ist dieses Mal einfach:\n\ntidy_word_vectors <- \n  tidy_pmi %>%\n  widely_svd(\n    item1, item2, pmi,\n    nv = 100\n  )\n\ntidy_word_vectors %>% \n  (head)\n\n\n\n\n\nitem1\ndimension\nvalue\n\n\n\n\ninc\n1\n-0.0378963\n\n\nis\n1\n-0.1132069\n\n\ntrying\n1\n-0.0512764\n\n\nsystems\n1\n-0.0333332\n\n\nto\n1\n-0.1203434\n\n\ncollect\n1\n-0.0554211\n\n\n\n\n\n\nMit nv = 100 haben wir die Anzahl (n) der Dimensionen (Variablen, v) auf 100 bestimmt.\n\n\n5.5.6 Wortähnlichkeit\nJetzt, da wir mit der SVD jedes Wort in einem Koordinatensystem verortet haben, können wir die Abstände der Wörter im Koordinatensystem bestimmen. Das geht mit Hilfe des alten Pythagoras, s. Abbildung 5.6. Der Abstand, den man mit Hilfe des Satz des Pythagoras berechnet, nennt man auch euklidische Distanz.\n\n\n\nAbbildung 5.6: Euklidische Distanz in 2D, Quelle: Wikipedia, CC BY4.0, Kmhkmh\n\n\nOkay, wir sind in einem Raum mit vielen Dimensionen, was mein Hirn nicht mitmacht, aber der Algebra ist das egal. Pythagoras’ Satz lässt sich genauso anwenden, wenn es mehr als Dimensionen sind.\nDie Autoren basteln sich selber eine Funktion in Kap. 5.3, aber der Einfachheit halber nehme ich (erstmal) die entsprechende Funktion aus widyr:\n\nword_neighbors <- \ntidy_word_vectors %>% \n  pairwise_dist(item1, dimension, value)\n\nword_neighbors %>% \n  head()\n\n\n\n\n\nitem1\nitem2\ndistance\n\n\n\n\nis\ninc\n1.0220141\n\n\ntrying\ninc\n0.9332851\n\n\nsystems\ninc\n0.4161215\n\n\nto\ninc\n1.0913872\n\n\ncollect\ninc\n0.5221759\n\n\na\ninc\n1.0309566\n\n\n\n\n\n\nSchauen wir uns ein Beispiel an. Was sind die Nachbarn von “inaccurate”?\n\nword_neighbors %>% \n  filter(item1 == \"inaccurate\") %>% \n  arrange(distance) %>% \n  head()\n\n\n\n\n\nitem1\nitem2\ndistance\n\n\n\n\ninaccurate\nmine\n0.5248868\n\n\ninaccurate\nscore\n0.5310116\n\n\ninaccurate\noh\n0.5400913\n\n\ninaccurate\nny\n0.5400913\n\n\ninaccurate\ndob\n0.5801281\n\n\ninaccurate\ncell\n0.6093670\n\n\n\n\n\n\nHier ist die Datenmenge zu klein, um vernünftige Schlüsse zu ziehen. Aber “incorrectly”, “correct”, “balance” sind wohl plausible Nachbarn von “inaccurate”.\n\n\n5.5.7 Cosinus-Ähnlichkeit\nDie Nähe zweier Vektoren lässt sich, neben der euklidischen Distanz, auch z.B. über die Cosinus-Ähnlichkeit (Cosine similarity) berechnen, vgl. auch Abbildung 5.7:\n\n\n\nAbbildung 5.7: Die Cosinus-Ähnlichkeit zweier Vektoren\n\n\nQuelle: Mazin07, Lizenz: PD\n\\[{\\displaystyle {\\text{Cosinus-Ähnlichkeit}}=S_{C}(A,B):=\\cos(\\theta )={\\mathbf {A} \\cdot \\mathbf {B}  \\over \\|\\mathbf {A} \\|\\|\\mathbf {B} \\|}={\\frac {\\sum \\limits _{i=1}^{n}{A_{i}B_{i}}}{{\\sqrt {\\sum \\limits _{i=1}^{n}{A_{i}^{2}}}}{\\sqrt {\\sum \\limits _{i=1}^{n}{B_{i}^{2}}}}}},}\\]\nwobei \\(A\\) und \\(B\\) zwei Vektoren sind und \\(\\|\\mathbf {A} \\|\\) das Skalarprodukt von A (und B genauso). Das Skalarprodukt von \\(\\color {red} {a = {\\displaystyle [a_{1},a_{2},\\cdots ,a_{n}]}}\\) und \\(\\color {blue} {b = {\\displaystyle [b_{1},b_{2},\\cdots ,b_{n}]}}\\) ist so definiert:\n\\[{\\displaystyle \\mathbf {\\color {red}a} \\cdot \\mathbf {\\color {blue}b} =\\sum _{i=1}^{n}{\\color {red}a}_{i}{\\color {blue}b}_{i}={\\color {red}a}_{1}{\\color {blue}b}_{1}+{\\color {red}a}_{2}{\\color {blue}b}_{2}+\\cdots +{\\color {red}a}_{n}{\\color {blue}b}_{n}}\\]\nEntsprechend ist die Funktion nearest_neighbors zu verstehen aus Kap. 5.3:\n\nnearest_neighbors <- function(df, token) {\n  df %>%\n    widely(\n      ~ {\n        y <- .[rep(token, nrow(.)), ]\n        res <- rowSums(. * y) / \n          (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))\n        \n        matrix(res, ncol = 1, dimnames = list(x = names(res)))\n      },\n      sort = TRUE\n    )(item1, dimension, value) %>%\n    select(-item2)\n}\n\nWobei mit widely zuerst noch von der Langform in die Breitform umformatiert wird, da die Breitform von der Formel verlangt wird bzw. das Rechnen einfacher macht.\nDer eine Vektor ist das Embedding des Tokens, der andere Vektor ist das mittlere Embedding über alle Tokens des Corpus. Wenn die Anzahl der Elemente konstant bleibt, kann man sich das Teilen durch \\(n\\) schenken, wenn man einen Mittelwert berechnen; so hält es auch die Syntax von nearest_neighbors.\nEin nützlicher Post zur Cosinus-Ähnlichkeit findet sich hier. Dieses Bild zeigt das Konzept der Cosinus-Ähnlichkeit anschaulich.\nZur Erinnerung: Der Cosinus eines Winkels ist definiert als Verhältnis der Länge der Ankathete (das ist jene Kathete, die einen Schenkel des Winkels bildet) zur Länge der Hypotenuse12 in einem rechtwinkligen, vgl. Abbildung 5.8.\n\n\n\nAbbildung 5.8: Ein rechtwinkliges Dreieck als Grundlage der trigonometrischen Funktionen\n\n\nAlso: \\({\\displaystyle \\cos \\alpha ={\\frac {b}{c}}}\\)\nQuelle: PaterSigmund, Wikipedia,dre CC-BY-SA 2.5\nHilfreich ist auch die Visualisierung von Sinus und Cosinus am Einheitskreis; gerne animiert betrachten.\n\n\n5.5.8 Word-Embeddings vorgekocht: Glove6B\nIn Kap. 5.4 schreiben die Autoren:\n\nIf your data set is too small, you typically cannot train reliable word embeddings.\n\nEin paar Millionen Wörter sollte der Corpus schon enthalten, so die Autoren. Da solche “Worteinbettungen” (word embedings) aufwändig zu erstellen sind, kann man fertige, “vorgekochte” Produkte nutzen.\nGlove6B wurde anhand von Wikipedia und anderen Datenquellen erstellt (Pennington, Socher, und Manning 2014).\n\n\n\n\n\n\nHinweis\n\n\n\nDie zugehörigen Daten sind recht groß; für glove6b (Pennington, Socher, und Manning 2014) ist fast ein Gigabyte fällig. Sie sollten sich die Daten in einem ruhigen Moment (mit stabiler Internetverbindung) herunterladen und in einem Verzeichnis meiner Wahl abgespeichert (datasets). Da bei mir Download abbrach, als ich embedding_glove6b(dimensions = 100) aufrief, habe ich die Daten manuell heruntergeladen, s.u.\n\n\n\nglove6b <- \n  embedding_glove6b(dir = \"~/datasets\", dimensions = 50, manual_download = TRUE)\n\nglove6b %>% \n  select(1:5) %>% \n  head()\n\n\n\n\n\ntoken\nd1\nd2\nd3\nd4\n\n\n\n\nthe\n0.418000\n0.249680\n-0.41242\n0.121700\n\n\n,\n0.013441\n0.236820\n-0.16899\n0.409510\n\n\n.\n0.151640\n0.301770\n-0.16763\n0.176840\n\n\nof\n0.708530\n0.570880\n-0.47160\n0.180480\n\n\nto\n0.680470\n-0.039263\n0.30186\n-0.177920\n\n\nand\n0.268180\n0.143460\n-0.27877\n0.016257\n\n\n\n\n\n\nIn eine Tidyform bringen:\n\ntidy_glove <- \n  glove6b %>%\n  pivot_longer(contains(\"d\"),\n               names_to = \"dimension\") %>%\n  rename(item1 = token)\n\nhead(tidy_glove)\n\n\n\n\n\nitem1\ndimension\nvalue\n\n\n\n\nthe\nd1\n-0.038194\n\n\nthe\nd2\n-0.244870\n\n\nthe\nd3\n0.728120\n\n\nthe\nd4\n-0.399610\n\n\nthe\nd5\n0.083172\n\n\nthe\nd6\n0.043953\n\n\n\n\n\n\nGanz schön groß:\n\nobject.size(tidy_glove)\n\n983837536 bytes\n\n\nIn Megabyte13\n\nobject.size(tidy_glove) / 2^20\n\n938.3 bytes\n\n\nEinfacher und genauer geht es so:\n\npryr::object_size(tidy_glove)\n\n983.83 MB\n\n\n\npryr::mem_used()\n\n1.5 GB\n\n\nUm Speicher zu sparen, könnte man glove6b wieder direkt löschen, wenn man nur mit der Tidyform weiterarbeitet.\n\nrm(glove6b)\n\nJetzt können wir wieder nach Nachbarn fragen, im euklidischen Sinne, wie oben. Probieren wir aus, welche Wörter nah zu “inaccurate” stehen.\n\n\n\n\n\n\nHinweis\n\n\n\nWie wir oben gesehen haben, ist der Datensatz riesig14, was die Berechnungen (zeitaufwändig) und damit nervig machen können. Darüber hinaus kann es nötig sein, dass Sie mehr Speicher auf Ihrem Computer zur Verfügung stellen müssen15. Wir müssen noch maximum_size = NULL, um das Jonglieren mit riesigen Matrixen zu erlauben. Möge der Gott der RAMs und Arbeitsspeicher uns gnädig sein!\n\n\nMit pairwise_dist dauert die Berechnung sehr lange und braucht wohl sehr viel Speicher. Mitunter kam folgender Fehler auf: “R error: vector memory exhausted (limit reached?)”.\n\nword_neighbors_glove6b <- \ntidy_glove %>% \n  slice_head(prop = .1) %>% \n  pairwise_dist(item1, dimension, value, maximum_size = NULL)\n\nhead(word_neighbors_glove6b)\n\ntidy_glove %>% \n  filter(item1 == \"inaccurate\") %>% \n  arrange(-value) %>% \n  slice_head(n = 5)\n\nDeswegen probieren wir doch die Funktion nearest_neighbors, so wie es im Buch vorgeschlagen wird, s. Kap 5.3.\n\nnearest_neighbors <- function(df, token) {\n  df %>%\n    widely(\n      ~ {\n        y <- .[rep(token, nrow(.)), ]\n        res <- rowSums(. * y) / \n          (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))\n        matrix(res, ncol = 1, dimnames = list(x = names(res)))\n      },\n      sort = TRUE,\n      maximum_size = NULL\n    )(item1, dimension, value) %>%\n    select(-item2)\n}\n\n\ntidy_glove %>%\n  # slice_head(prob = .1) %>% \n  nearest_neighbors(\"error\") %>% \n  head()\n\n\n\n\n\nitem1\nvalue\n\n\n\n\nerror\n1.0000000\n\n\nerrors\n0.7916719\n\n\nmistake\n0.6641135\n\n\ncorrect\n0.6205814\n\n\nincorrect\n0.6132556\n\n\nfault\n0.6068035\n\n\n\n\n\n\nEntschachteln wir unsere Daten zu complaints:\n\ntidy_complaints3 <-\n  nested_words3 %>% \n  unnest(words)\n\nDann erstellen wir uns eine Tabelle, in der nur die Schnittmenge der Wörter aus den Beschwerden und Glove vorkommen. Dazu nutzen winr einen inneren Join\n\n\n\nInner Join, Quelle: Garrick Adenbuie\n\n\nQuelle\n\ncomplaints_glove <- \ntidy_complaints3 %>% \n  inner_join(by = \"word\", \n  tidy_glove %>% \n  distinct(item1) %>% \n  rename(word = item1)) \n\nhead(complaints_glove)\n\n\n\n\n\ncomplaint_id\nword\n\n\n\n\n3384392\nsystems\n\n\n3384392\ninc\n\n\n3384392\nis\n\n\n3384392\ntrying\n\n\n3384392\nto\n\n\n3384392\ncollect\n\n\n\n\n\n\nWie viele unique (distinkte) Wörter gibt es in unserem Corpus?\n\ntidy_complaints3_distinct_words_n <- \ntidy_complaints3 %>% \n  distinct(word) %>% \n  nrow()\n\ntidy_complaints3_distinct_words_n\n\n[1] 222\n\n\nIn tidy_complaints gibt es übrigens 222 verschiedene Wörter.\n\nword_matrix <- tidy_complaints3 %>%\n  inner_join(by = \"word\",\n             tidy_glove %>%\n               distinct(item1) %>%\n               rename(word = item1)) %>%\n  count(complaint_id, word) %>%\n  cast_sparse(complaint_id, word, n)\n\n#word_matrix\n\nword_matrix zählt für jede der 10 Beschwerden, welche Wörter (und wie häufig) vorkommen.\n\ndim(word_matrix)\n\n[1]  10 222\n\n\nBeschwerden (Dokumente) und 222 unique Wörter.\n\nglove_matrix <- tidy_glove %>%\n  inner_join(by = \"item1\",\n             tidy_complaints3 %>%\n               distinct(word) %>%\n               rename(item1 = word)) %>%\n  cast_sparse(item1, dimension, value)\n\n#glove_matrix\n\nglove_matrix gibt für jedes unique Wort den Einbettungsvektor an.\n\ndim(glove_matrix)\n\n[1] 222 100\n\n\nDas sind 222 unique Wörter und 100 Dimensionen des Einbettungsvektors.\nJetzt können wir noch pro Dokument (10 in diesem Beispiel) die mittlere “Position” jedes Dokuments im Einbettungsvektor ausrechnen. Bildlich gesprochen: Was ist der mittlere Raumpunkt (Zentroid) des Bienenschwarms, wobei eine Biene ein Dokument darstellt und die Raumachsen die Dimensionen des Einbettungsvektors.\nDazu gewichten wir jedes Wort eines Dokuments mit den Ladungen des Einbettungsvektor und summieren diese Terme. Es resultiert eine Matrix mit einem Einbettungsvektor pro Dokument. Diese Matrix können wir jetzt als Prädiktorenmatrix hernehmen.\n\ndoc_matrix <- word_matrix %*% glove_matrix\n#doc_matrix %>% head()\n\n\ndim(doc_matrix)\n\n[1]  10 100\n\n\nDie Anzahl der Dokumente ist 10 und die Anzahl der Dimensionen (des Einbettungsvektors) ist 100."
  },
  {
    "objectID": "word-embedding.html#fazit",
    "href": "word-embedding.html#fazit",
    "title": "5  Word Embedding",
    "section": "5.6 Fazit",
    "text": "5.6 Fazit\nWorteinbettungen sind eine aufwändige Angelegenheit. Positiv gesprochen kann ein Analysti die Muskeln spielen lassen und zeigen, was sie oder er so alles drauf hat. Ist ja schon cooles Zeugs, die Word Embeddings. Es besteht die Chance, dass man mit dieser Methode bessere Vorhersagen erreicht, als mit anderen, einfachen Ansätzen wir Worthäufigkeiten oder tf-idf. Auf der anderen Seite ist es oft sinnvoll, mit einfachen Ansätzen zu starten, und zu sehen, wie weit man kommt. Vielleicht ja weit genug."
  },
  {
    "objectID": "word-embedding.html#literatur",
    "href": "word-embedding.html#literatur",
    "title": "5  Word Embedding",
    "section": "5.7 Literatur",
    "text": "5.7 Literatur\n\n5.7.1 Wikipedia\nEs gibt eine Reihe nützlicher (und recht informationsdichter) Wikipedia-Einträge zum Thema Informationstheorie.\n\nWikipedia: Pointwise Mutual Information\nWikipedia: Mutual information\nWikipedia: Information theory\n\n\n\n\n\nHvitfeldt, Emil, und Julia Silge. 2022. Supervised Machine Learning for Text Analysis in R. 1. Aufl. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nKurz, A. Solomon. 2021. Statistical rethinking with brms, ggplot2, and the tidyverse: Second edition. https://bookdown.org/content/4857/.\n\n\nMcElreath, Richard. 2020. Statistical rethinking: a Bayesian course with examples in R and Stan. 2. Aufl. CRC texts in statistical science. Boca Raton: Taylor; Francis, CRC Press.\n\n\nPennington, Jeffrey, Richard Socher, und Christopher Manning. 2014. „GloVe: Global Vectors for Word Representation“. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1532–43. Doha, Qatar: Association for Computational Linguistics. https://doi.org/10.3115/v1/D14-1162.\n\n\nShannon, C. E. 1948. „A Mathematical Theory of Communication“. Bell System Technical Journal 27 (3): 379–423. https://doi.org/10.1002/j.1538-7305.1948.tb01338.x."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Hvitfeldt, Emil, and Julia Silge. 2022. Supervised Machine Learning\nfor Text Analysis in r. 1st ed. Boca Raton: Chapman;\nHall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nKönig, Tim, Wolf J. Schünemann, Alexander Brand, Julian Freyberg, and\nMichael Gertz. 2022. “The EPINetz Twitter Politicians\nDataset 2021. A New Resource for the Study of the German Twittersphere\nand Its Application for the 2021 Federal Elections.”\nPolitische Vierteljahresschrift 63 (3): 529–47. https://doi.org/10.1007/s11615-022-00405-7.\n\n\nKurz, A. Solomon. 2021. Statistical Rethinking with Brms, Ggplot2,\nand the Tidyverse: Second Edition. https://bookdown.org/content/4857/.\n\n\nLex, Alexander, Nils Gehlenborg, Hendrik Strobelt, Romain Vuillemot, and\nHanspeter Pfister. 2014. “UpSet: Visualization of\nIntersecting Sets.” IEEE Transactions on\nVisualization and Computer Graphics 20 (12): 1983–92. https://doi.org/10.1109/TVCG.2014.2346248.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course\nwith Examples in r and Stan. 2nd ed. CRC Texts in\nStatistical Science. Boca Raton: Taylor; Francis, CRC\nPress.\n\n\nPennington, Jeffrey, Richard Socher, and Christopher Manning. 2014.\n“GloVe: Global Vectors for Word\nRepresentation.” In Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), 1532–43. Doha, Qatar: Association for\nComputational Linguistics. https://doi.org/10.3115/v1/D14-1162.\n\n\nRemus, Robert, Uwe Quasthoff, and Gerhard Heyer. 2010.\n“SentiWS - a Publicly Available German-Language\nResource for Sentiment Analysis.” Proceedings of the 7th\nInternational Language Ressources and Evaluation\n(LREC’10), 1168–71.\n\n\nShannon, C. E. 1948. “A Mathematical Theory of\nCommunication.” Bell System Technical Journal 27 (3):\n379–423. https://doi.org/10.1002/j.1538-7305.1948.tb01338.x.\n\n\nWickham, Hadley, and Garrett Grolemund. 2018. R Für Data Science:\nDaten Importieren, Bereinigen, Umformen, Modellieren Und\nVisualisieren. Translated by Frank Langenau. 1. Auflage.\nHeidelberg: O’Reilly. https://r4ds.had.co.nz/index.html."
  }
]