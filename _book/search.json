[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science 2: Textdaten als Grundlage prädiktiver Modelle 📚🔮",
    "section": "",
    "text": "Zu diesem Buch\nBild von mcmurryjulie auf Pixabay"
  },
  {
    "objectID": "index.html#was-rät-meister-yoda",
    "href": "index.html#was-rät-meister-yoda",
    "title": "Data Science 2: Textdaten als Grundlage prädiktiver Modelle 📚🔮",
    "section": "Was rät Meister Yoda?",
    "text": "Was rät Meister Yoda?\nMeister Yoda rät: Lesen Sie die Hinweise (Abbildung 1).\n\n\nAbbildung 1: Lesen Sie die folgenden Hinweise im eigenen Interesse\n\n\nQuelle: made at imageflip"
  },
  {
    "objectID": "index.html#zitation",
    "href": "index.html#zitation",
    "title": "Data Science 2: Textdaten als Grundlage prädiktiver Modelle 📚🔮",
    "section": "Zitation",
    "text": "Zitation\nNutzen Sie folgende DOI, um dieses Buch zu zitieren: \nHier ist die Zitation im Bibtex-Format:"
  },
  {
    "objectID": "index.html#literatur",
    "href": "index.html#literatur",
    "title": "Data Science 2: Textdaten als Grundlage prädiktiver Modelle 📚🔮",
    "section": "Literatur",
    "text": "Literatur\nZentrale Begleitliteratur ist Hvitfeldt und Silge (2021); der Volltext ist hier verfügbar.\nPro Thema wird ggf. weitere Literatur ausgewiesen."
  },
  {
    "objectID": "index.html#quellcode",
    "href": "index.html#quellcode",
    "title": "Data Science 2: Textdaten als Grundlage prädiktiver Modelle 📚🔮",
    "section": "Quellcode",
    "text": "Quellcode\nDer Quellcode liegt öffentlich zugänglich in diesem Github-Repositorium."
  },
  {
    "objectID": "index.html#technische-details",
    "href": "index.html#technische-details",
    "title": "Data Science 2: Textdaten als Grundlage prädiktiver Modelle 📚🔮",
    "section": "Technische Details",
    "text": "Technische Details\n\nDiese Version des Buches wurde erstellt am: 2023-10-22 21:38:07\nSie haben Feedback, Fehlerhinweise oder Wünsche zur Weiterentwicklung? Am besten stellen Sie hier einen Issue ein.\nDieses Projekt steht unter der MIT-Lizenz.\nDieses Buch wurde in RStudio mit Hilfe von bookdown geschrieben.\nDiese Version des Buches wurde mit der R-Version R version 4.2.1 (2022-06-23) und den folgenden technischen Spezifikationen erstellt:\n\n\n## ─ Session info ───────────────────────────────────────────────────────────────\n##  setting  value\n##  version  R version 4.2.1 (2022-06-23)\n##  os       macOS Big Sur ... 10.16\n##  system   x86_64, darwin17.0\n##  ui       X11\n##  language (EN)\n##  collate  en_US.UTF-8\n##  ctype    en_US.UTF-8\n##  tz       Europe/Berlin\n##  date     2023-08-24\n##  pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n## \n## ─ Packages ───────────────────────────────────────────────────────────────────\n##  package     * version date (UTC) lib source\n##  cli           3.6.1   2023-03-23 [1] CRAN (R 4.2.0)\n##  colorout    * 1.2-2   2022-06-13 [1] local\n##  digest        0.6.32  2023-06-26 [1] CRAN (R 4.2.0)\n##  evaluate      0.21    2023-05-05 [1] CRAN (R 4.2.0)\n##  fastmap       1.1.1   2023-02-24 [1] CRAN (R 4.2.0)\n##  htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.2.0)\n##  htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.2.0)\n##  jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.2.0)\n##  knitr         1.43    2023-05-25 [1] CRAN (R 4.2.0)\n##  rlang         1.1.1   2023-04-28 [1] CRAN (R 4.2.0)\n##  rmarkdown     2.23    2023-07-01 [1] CRAN (R 4.2.0)\n##  rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.2.0)\n##  sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n##  xfun          0.39    2023-04-20 [1] CRAN (R 4.2.0)\n##  yaml          2.3.7   2023-01-23 [1] CRAN (R 4.2.0)\n## \n##  [1] /Users/sebastiansaueruser/Rlibs\n##  [2] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n## \n## ──────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\nHvitfeldt, Emil, und Julia Silge. 2021. Supervised Machine Learning for Text Analysis in R. 1. Aufl. Boca Raton: Chapman and Hall/CRC. https://doi.org/10.1201/9781003093459."
  },
  {
    "objectID": "010-Hinweise.html#ihr-lernerfolg",
    "href": "010-Hinweise.html#ihr-lernerfolg",
    "title": "Lernhilfen",
    "section": "\n1.1 Ihr Lernerfolg",
    "text": "1.1 Ihr Lernerfolg\n\n1.1.1 Was Sie hier lernen und wozu das gut ist\nAlle Welt spricht von Big Data, aber ohne die Analyse sind die großen Daten nur großes Rauschen. Was letztlich interessiert, sind die Erkenntnisse, die Einblicke, nicht die Daten an sich. Dabei ist es egal, ob die Daten groß oder klein sind. Natürlich erlauben die heutigen Datenmengen im Verbund mit leistungsfähigen Rechnern und neuen Analysemethoden ein Verständnis, das vor Kurzem noch nicht möglich war. Und wir stehen erst am Anfang dieser Entwicklung. Vielleicht handelt es sich bei diesem Feld um eines der dynamischsten Fachgebiete der heutigen Zeit. Sie sind dabei: Sie lernen einiges Handwerkszeugs des “Datenwissenschaftlers”. Wir konzentrieren uns auf das vielleicht bekannteste Teilgebiet: Ereignisse vorhersagen auf Basis von hoch strukturierten Daten und geeigneter Algorithmen und Verfahren. Nach diesem Kurs sollten Sie in der Lage sein, typisches Gebabbel des Fachgebiet mit Lässigkeit mitzumachen. Ach ja, und mit einigem Erfolg Vorhersagemodelle entwickeln.\n\n1.1.2 Lernziele\n\n\n\n\n\n\nWichtig\n\n\n\nKurz gesagt: Sie lernen die Grundlagen von Data Science zur Analyse von Text.\\(\\square\\)\n\n\nNach diesem Kurs sollten Sie …\n\nDaten aus Sozialen Netzwerken wie Twitter automatisiert in großer Menge auslesen können\nGängige Methoden des Textminings mit R anwenden können (z.B. Tokenizing, Stemming, Regex)\nVerfahren des Maschinenlernens auf Textdaten anwenden können\nDen Forschungsstand zum Thema Erkennung von Hatespeech in Ausschnitten kennen\n\n1.1.3 Überblick\nAbb. Abbildung 1.1 gibt einen Überblick über den Verlauf und die Inhalte des Buches. Das Diagramm hilft Ihnen zu verorten, wo welches Thema im Gesamtzusammenhang steht.\n\n\n\n\nflowchart LR\n  subgraph R[Rahmen]\n    direction LR\n    subgraph V[Grundlagen]\n      direction TB\n      E[R] --- Um[Statistisches&lt;br&gt;Lernen]\n      Um --- tm[tidymodels]\n    end\n    subgraph M[Lernalgorithmen]\n      direction TB\n      M1[Regression] --- Vis[Baeume]\n      Vis --- U[Regularisierung]\n      U --- G[...]\n    end\n    subgraph N[Anwendung]\n      direction TB\n      D[Fallstudien]\n    end\n  V --&gt; M\n  M --&gt; N\n  end\n\n\nAbbildung 1.1: Ein ‘Fahrplan’ als ‘Big Picture’ dieses Buches"
  },
  {
    "objectID": "010-Hinweise.html#selbständige-vorbereitung-vor-kursbeginn",
    "href": "010-Hinweise.html#selbständige-vorbereitung-vor-kursbeginn",
    "title": "Lernhilfen",
    "section": "\n1.2 Selbständige Vorbereitung vor Kursbeginn",
    "text": "1.2 Selbständige Vorbereitung vor Kursbeginn\nDie folgenden Inhalte werden in diesem Buch/Kurs vorausgesetzt. Falls Ihnen der Stoff nicht geläufig ist, sollten Sie sich selbständig damit vertraut machen.\n\nGrundlagen der Statistik wie im Kurs Statistik1 vermittelt\nEinführung in die Inferenzstatistik wie im Kurs Bayes:Start! vermittelt\nGrundlagen der Prognosemodellierung wie im Kurs Data Science 1 vermittelt"
  },
  {
    "objectID": "010-Hinweise.html#lernhilfen",
    "href": "010-Hinweise.html#lernhilfen",
    "title": "Lernhilfen",
    "section": "\n1.3 Lernhilfen",
    "text": "1.3 Lernhilfen\n\n1.3.1 PDF-Version\nUm eine PDF-Version eines Kapitels zu erhalten, können Sie im Browser die Druckfunktion nutzen (Strg-P). Wählen Sie dort “PDF” als Ziel.\n\n1.3.2 Videos\nAuf dem YouTube-Kanal des Autors finden sich eine Reihe von Videos mit Bezug zum Inhalt dieses Buchs. Besonders diese Playlist passt zu den Inhalten dieses Buchs.\n\n1.3.3 Software allgemein\nInstallieren Sie R und seine Freunde.\nInstallieren Sie bitte auch die folgende R-Pakete1:\n\ntidyverse\neasystats\nweitere Pakete werden im Unterricht bekannt gegeben (es schadet aber nichts, jetzt schon Pakete nach eigenem Ermessen zu installieren)\n\nR Syntax aus dem Unterricht findet sich im Github-Repo bzw. Ordner zum jeweiligen Semester.\n\n\n\n\n\n\nRStudio-Cloud-Project\n\n\n\nWenn Ihnen die Lehrkraft ein RStudio-Cloud-Projekt zur Verfügung stellt, nutzen Sie es. Dort sind alle R-Pakete, Datensätze und Syntax-Vorlagen schon bereit gestellt. Sie sparen sich also eine Menge Installationsarbeit.\\(\\square\\)\n\n\n\n\n\n\n\n\nBei Installationsproblemen\n\n\n\n\nGibt R eine Warning aus, ist das zumeist kein Problem und kann ignoriert werden.\nStarten Sie R neu, bevor Sie R-Pakete installieren.\nWenn Sie Probleme mit der Installation auf Ihrem Computer haben, können Sie (übergangsweise oder dauerhaft) die Online-Version von RStudio, RStudio Cloud verwenden (in gewissem Umfang kostenlos).\\(\\square\\)\n\n\n\n\n\n1.3.4 Software: Bayes\nWenn in diesem Modul Inferenzstatistik nötig ist, benötigen Sie Software für Bayes-Inferenz.\nFolgendes R-Paket ist für die Bayes-Inferenz nötig:\n\nrstanarm\n\n\n1.3.5 Online-Unterstützung\nDieser Kurs kann in Präsenz und Online angeboten werden. Wenn Sie die Wahl haben, empfehle ich die Teilnahme in Präsenz, da der Lernerfolg höher ist. Online ist es meist schwieriger, sich zu konzentrieren. Aber auch online ist es möglich, den Stoff gut zu lernen, s. Abbildung 1.2.\n\n\nAbbildung 1.2: We believe in you! Image Credit: Allison Horst\n\nBitte beachten Sie, dass bei einer Teilnahme in Präsenz eine aktive Mitarbeit erwartet wird. Hingegen ist bei einer Online-Teilnahme keine/kaum aktive Mitarbeit möglich.\nHier finden Sie einige Werkzeuge, die das Online-Zusammenarbeiten vereinfachen:\n\n\nFrag-Jetzt-Raum zum anonymen Fragen stellen während des Unterrichts. Der Keycode wird Ihnen bei Bedarf vom Dozenten bereitgestellt.\n\nPadlet zum einfachen (und anonymen) Hochladen von Arbeitsergebnissen der Studentis im Unterricht. Wir nutzen es als eine Art Pinwand zum Sammeln von Arbeitsbeiträgen. Die Zugangsdaten stellt Ihnen der Dozent bereit.\nNutzen Sie das vom Dozenten bereitgestelle Forum, um Fragen zu stellen und Fragen zu beantworten.\n\n1.3.6 Fundorte für Datensätze\nHier finden Sie Datensätze, die sich eignen, um die Analyse von Daten zu lernen:\n\nVincent Arel-Bundocks Datenseite\nDie Datenseite der University of California in Irvine (UCI)\n\n1.3.7 Aufgabensammlung\nDie Webseite Datenwerk beherbergt eine Sammlung an Übungsaufgaben rund um das Thema Datenanalyse. es gibt eine Suchfunktion (wenn Sie den Namen der Aufgabe wissen) und eine Tag-Liste, wenn Sie Aufgaben nach Themengebiet durchsehen wollen.\n\n1.3.8 Tipps zum Lernerfolg\n\n\n\n\n\n\nHinweis\n\n\n\nStetige Mitarbeit - auch und gerade außerhalb des Unterrichts - ist der Schlüssel zum Prüfungserfolg. Vermeiden Sie, das Lernen aufzuschieben. Bleiben Sie dran!\\(\\square\\)\n\n\n\n\nLerngruppe: Treten Sie einer Lerngruppe bei.\n\nTutorium: Besuchen Sie ein Tutorium, falls eines angeboten wird.\n\nVor- und Nachbereitung: Bereiten Sie den Unterricht vor und nach.\n\nSelbsttest: Testen Sie sich mit Flashcards (Karteikarten mit Vor- und Rückseite). Wenn Sie alle Aufgaben dieses Kurses aus dem FF beherrschen, sollte die Prüfung kein Problem sein.\n\nÜbungen: Bearbeiten Sie alle Übungsaufgaben gewissenhaft.\nPortal Datenwerk: Gehen Sie die Aufgaben auf dem Portal Datenwerk durch (soweit relevant).\n\nFallstudien: Schauen Sie sich meine Fallstudiensammlungen an: https://sebastiansauer-academic.netlify.app/courseware/casestudies/\n\nLehrkraft ansprechen: Sprechen Sie die Lehrkraft an, wenn Sie Fragen haben. Haben Sie keine Scheu! Bitte lesen Sie aber vorab die Hinweise, um Redundanz zu vermeiden.\n\nDabei bleiben: Vermeiden Sie “Bullimie-Lernen” (lange nix, dann alles auf einmal), sondern bevorzugen Sie “Lern-Snacks” (immer wieder ein bisschen)\n\n1.3.9 Selbstlernkontrolle\nFür jedes Kapitel sind (am Kapitelende) Aufgaben eingestellt, jeweils mit Lösung. Ein Teil dieser Aufgaben hat eine kurze, eindeutige Lösung (z.B. “42” oder “Antwort C”); ein (kleiner) Teil der Aufgaben verlangen komplexere Antworten (z.B. “Welche Arten von Prioris gibt es bei stan_glm()?). Nutzen Sie die Fragen mit eindeutiger, kurzer Lösung um sich selber zu prüfen. Nutzen Sie die Fragen mit komplexerer, längerer Lösung, um ein Themengebiet tiefer zu erarbeiten.\n\n\n\n\n\n\nHinweis\n\n\n\nFortwährendes Feedback zu Ihrem Lernfortschritt ist wichtig, damit Sie Ihre Lernbemühungen steuern können. Bearbeiten Sie daher die bereitgestellten Arbeiten ernsthaft.\\(\\square\\)\n\n\n\n1.3.10 Lernen lernen\nHier sind einige Quellen (Literatur), die Ihnen helfen sollen, das Lernen (noch besser) zu lernen:\n\nEssentielle Tipps für Bachelor-Studierende der Psychologie\nKonzentriert arbeiten: Regeln für eine Welt voller Ablenkungen\nWie man ein Buch liest\nErsti-Hilfe: 112 Tipps für Studienanfänger - erfolgreich studieren ab der ersten Vorlesung\nVon der Kürze des Lebens\nBlog “Studienscheiss”"
  },
  {
    "objectID": "010-Hinweise.html#literatur",
    "href": "010-Hinweise.html#literatur",
    "title": "Lernhilfen",
    "section": "\n1.4 Literatur",
    "text": "1.4 Literatur\nZentrale Kursliteratur für die theoretischen Konzepte ist Hvitfeldt und Silge (2021); das Buch ist frei online verfügbar.\nEine gute Ergänzung ist das Lehrbuch von Chollet, Kalinowski, und Allaire (2022), welches grundlegende Data-Science-Konzepte erläutert und mit tidymodels umsetzt. Es ist in einer [Online-Version beim Verlag frei zugänglich](https://livebook.manning.com/book/deep-learning-with-r-second-edition.\nJames u. a. (2021) haben ein weithin renommiertes und sehr bekanntes Buch verfasst. Es ist allerdings etwas anspruchsvoller aus Rhys (2020), daher steht es nicht im Fokus dieses Kurses, aber einige Schwenker zu Inhalten von James u. a. (2021) gibt es. Schauen Sie mal rein, das Buch ist gut!"
  },
  {
    "objectID": "010-Hinweise.html#faq",
    "href": "010-Hinweise.html#faq",
    "title": "Lernhilfen",
    "section": "\n1.5 FAQ",
    "text": "1.5 FAQ\n\n\nFolien\n\nFrage: Gibt es ein Folienskript?\nAntwort: Wo es einfache, gute Literatur gibt, gibt es kein Skript. Wo es keine gute oder keine einfach zugängliche Literatur gibt, dort gibt es ein Skript.\n\n\n\nEnglisch\n\nIst die Literatur auf Englisch?\nJa. Allerdings ist die Literatur gut zugänglich. Das Englisch ist nicht schwer. Bedenken Sie: Englisch ist die lingua franca in Wissenschaft und Wirtschaft. Ein solides Verständnis englischer (geschriebener) Sprache ist für eine gute Ausbildung unerlässlich. Zu dem sollte die Kursliteratur fachlich passende und gute Bücher umfassen; oft sind das englische Titel.\n\n\n\nAnstrengend\n\nIst der Kurs sehr anstrengend, aufwändig?\nDer Kurs hat ein mittleres Anspruchsniveau.\n\n\n\nMathe\n\nMuss man ein Mathe-Crack sein, um eine gute Note zu erreichen?\nNein. Mathe steht nicht im Vordergrund. Schauen Sie sich die Literatur an, sie werden wenig Mathe darin finden.\n\n\n\nPrüfungsliteratur\n\nWelche Literatur ist prüfungsrelevant?\nPrüfungsrelevant im engeren Sinne ist das Skript sowie alles, was im Unterricht behandelt wurde.\n\n\n\nPrüfung\n\nWie sieht die Prüfung aus?\nDie Prüfung ist angewandt, z.B. eine Datenanalyse. Es wird keine Klausur geben, in der reines Wissen abgefragt wird.\n\n\n\nNur R?\n\nWird nur R in dem Kurs gelehrt? Andere Programmiersprachen sind doch auch wichtig.\nIn der Datenanalyse gibt es zwei zentrale Programmiersprachen, R und Python. Beide sind gut und beide werden viel verwendet. In einer Grundausbildung sollte man sich auf eine Sprache begrenzen, da sonst den Sprachen zu viel Zeit eingeräumt werden muss. Wichtiger als eine zweite Programmiersprache zu lernen, mit der man nicht viel mehr kann als mit der ersten, ist es, die Inhalte des Fachs zu lernen.\n\n\n\n\n\n\n\nChollet, François, Tomasz Kalinowski, und J. J. Allaire. 2022. Deep Learning with R. Second edition. Shelter Island, NY: Manning.\n\n\nHvitfeldt, Emil, und Julia Silge. 2021. Supervised Machine Learning for Text Analysis in R. 1. Aufl. Boca Raton: Chapman and Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, und Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. Second edition. Springer Texts in Statistics. New York: Springer. https://link.springer.com/book/10.1007/978-1-0716-1418-1.\n\n\nRhys, Hefin. 2020. Machine Learning with R, the Tidyverse, and Mlr. Shelter Island, NY: Manning publications."
  },
  {
    "objectID": "010-Hinweise.html#footnotes",
    "href": "010-Hinweise.html#footnotes",
    "title": "Lernhilfen",
    "section": "",
    "text": "falls Sie die Pakete schon installiert haben, können Sie in RStudio auf “update.packages” klicken↩︎"
  },
  {
    "objectID": "020-pruefung.html#prüfungsform-datenanalyse-als-quarto-blog-post",
    "href": "020-pruefung.html#prüfungsform-datenanalyse-als-quarto-blog-post",
    "title": "2  Prüfung",
    "section": "2.1 Prüfungsform: Datenanalyse als Quarto-Blog-Post",
    "text": "2.1 Prüfungsform: Datenanalyse als Quarto-Blog-Post\nAls Prüfungsleistung ist ein Corpus an Twitter-Daten, die an deutsche, aktuelle Politiker gerichtet sind, auf Hate Speech hin zu untersuchen.\n\nDer Dozent weißt jedis Studenti einen deutschen Politiker (bzw. dessen Twitter-Account) zu.\nDer Bericht der Analyse ist als Quarto Blog-Posts zu formatieren.\nEinzureichen ist die URL des Posts.\nDer Post muss während des gesamten Prüfungszeitraums online sein, gehostet von einem beliebigen Provider (z.B. Netlify oder Github).\nNach Einreichen des Posts dürfen keine Änderungen mehr vorgenommen werden.\nZu Dokumentationszwecken soll ein PDF-Print des Posts in die Abgabe mit hochgeladen werden. Das PDF-Print des Posts muss identisch (exakt gleich) sein zum Post, der über die URL verfügbar ist.\nDer Quelltext des Posts soll bei Github vorliegen.\nDie Methoden des Textminings aus dem Unterricht sollen angewendet werden\nZusätzlich dürfen sonstige Techniken des Textminings (die nicht im Unterricht behandelt wurden), angewendet werden\nDarüber hinaus sollen prädiktive Modelle zur Klassifikation von Hate-Speech (ja/nein) berechnet werden.\nEin Trainingsdatensatz wird gemeinsam erstellt.\nMethoden der Inferenzstatistik (wie Bayes) sind nicht nötig.\nEs soll eine mittlere vierstellige Zahl an Tweets verarbeitet werden oder wenigstens so viele Tweets wie verfügbar."
  },
  {
    "objectID": "020-pruefung.html#politiker-accounts",
    "href": "020-pruefung.html#politiker-accounts",
    "title": "2  Prüfung",
    "section": "2.2 Politiker-Accounts",
    "text": "2.2 Politiker-Accounts\nU.a. folgende Politiker-Accounts können als Prüfungsgegenstand verwendet werden (nach Hinweisen des Dozenten):\n\nOlaf Scholz\nAnnalena Baerbock\nChristian Lindner\nRobert Habeck (bzw. der Account seines Ministeriums)\nCem Özdemir\nVolker Wissing\nNancy Faeser\nFriedrich Merz\nBjörn Höcke\nSarah Wagenknecht"
  },
  {
    "objectID": "020-pruefung.html#hinweise-zur-prüfungsform-datenanalyse",
    "href": "020-pruefung.html#hinweise-zur-prüfungsform-datenanalyse",
    "title": "2  Prüfung",
    "section": "2.3 Hinweise zur Prüfungsform Datenanalyse",
    "text": "2.3 Hinweise zur Prüfungsform Datenanalyse\n\nAlle folgenden Hinweise gelten nur insoweit Ihre Lehrkraft Ihnen keine anders lautenden Hinweise gegeben hat (schriftlich)."
  },
  {
    "objectID": "020-pruefung.html#allgemeines",
    "href": "020-pruefung.html#allgemeines",
    "title": "2  Prüfung",
    "section": "2.4 Allgemeines",
    "text": "2.4 Allgemeines\n\nGegenstand dieser Prüfungsform ist eine Projektarbeit in Form von Analyse eines Datensatzes nach einer Forschungsfrage und die Dokumentation dieser Analyse.\nSchreiben Sie Ihre Datenanalyse in Form eines Berichts, der sich an den Gliederungspunkten wie unten dargestellt orientiert.\nWenden Sie die passenden, im Unterricht eingeführten, statistischen Verfahren an. Es steht Ihnen frei, andere (nicht im Unterricht behandelte) Verfahren zur Analyse der Daten anzuwenden, nach Maßgabe der fachlichen Angemessenheit.\nWerten Sie die Daten mit R oder Python aus.\nDie R-Syntax soll im Hauptteil des Berichts dokumentiert werden. R-Output darf ggf. gekürzt wiedergegeben werden.\nFügen Sie keine Erklärungen oder Definitionen von statistischen Verfahren an.\nBeschreiben und interpretieren Sie jede Analyse bzw. jeden R-Code bzw. jedes Ergebnis (jede R-Ausgabe).\nVon hoher Bedeutung ist die Korrektheit der Beschreibung und Interpretation der statistischen Modellierung (z.B. mit der Regressionsanalyse).\nEs hat keinen Einfluss auf Ihre Note, ob sich ein (erwarteter) Effekt zeigt und wie stark dieser Effekt ggf. ist.\nZu Beginn der Analyse müssen folgende Metadaten gut ersichtlich platziert sein (z.B. auf einem Deckblatt):\n\n\nVorname Nachname der Autors/der Autorin\nMatrikelnummer\nModulname\nAbgabedatum\n\n\nDie Abgabefrist endet mit Verstreichen des regulären Prüfungszeitraums (soweit nicht vom Prüfer anderweitig angegeben).\nStudentis mit Nachteilsausgleich melden sich beim Prüfer und zeigen ihren Antrag auf Nachteilsausgleich an."
  },
  {
    "objectID": "020-pruefung.html#einzureichende-dateien",
    "href": "020-pruefung.html#einzureichende-dateien",
    "title": "2  Prüfung",
    "section": "2.5 Einzureichende Dateien",
    "text": "2.5 Einzureichende Dateien\n\nEinzureichen sind folgende Dateien:\n\n\nder Bericht in menschenlesbarer Form (s. Formatierungshinweise)\nalle Dateien, die Quellcode der Analyse beinhalten.\ndie Rohdaten\n\n\nDer Name der Dateien kann frei gewählt werden (bzw. folgt keinen technischen Restriktionen)."
  },
  {
    "objectID": "020-pruefung.html#formatierung-des-berichts",
    "href": "020-pruefung.html#formatierung-des-berichts",
    "title": "2  Prüfung",
    "section": "2.6 Formatierung des Berichts",
    "text": "2.6 Formatierung des Berichts\n\nDer Bericht ist nur elektronisch, nicht ausgedruckt einzureichen.\nDer Bericht kann in einem paginierten Format (z.B. Word) oder einem nicht-paginierten Format (HTML-Dokument) verfasst werden. Abzugeben ist aber eine PDF-Datei oder eine HTML-Datei, die alle Bilder und sonstige Medien enthält (“Stand-Alone-HTML”).\nDie Wahl eines bestimmten Stylesheets ist nicht von Bedeutung. Lesbarkeit und Übersichtlichkeit in der Formatierung sind unabhängig davon anzustreben.\nIm Kopfbereich (oder auf einem Deckblatt) sind die relevanten Metadatan anzugeben wie Name (Nachname, Vorname) der Autorin/des Autors, Abgabedatum, Titel der Arbeit, Modul."
  },
  {
    "objectID": "020-pruefung.html#formalia",
    "href": "020-pruefung.html#formalia",
    "title": "2  Prüfung",
    "section": "2.7 Formalia",
    "text": "2.7 Formalia\n\nRichtlinien einer Wortzahl gibt es nicht. Entscheidend ist, dass relevante Analysen durchgeführt und beschrieben wurden. Schreiben Sie so knapp wie möglich und so ausführich wie nötig.\nDer Anspruch richtet sich nach dem Inhalt und Niveau des auf diese Prüfung vorbereitenden Unterricht (auch aus Modulen vorheriger Semester). Oft sind das Module in quantitativer Datenanalyse (und wissenschaftliches Arbeiten). Deren Inhalte sollen im Rahmen dieser Prüfungsleistung als selbständig und flüssig verfügbare Kompetenz von den Studentis demonstriert werden.\nDie Gliederung der Arbeit kann sich an den PPDAC-Zyklus und am Data Science Model von Wickham und Grolemund orientieren."
  },
  {
    "objectID": "020-pruefung.html#beurteilungskriterien",
    "href": "020-pruefung.html#beurteilungskriterien",
    "title": "2  Prüfung",
    "section": "2.8 Beurteilungskriterien",
    "text": "2.8 Beurteilungskriterien\nDie Arbeit wird im Hinblick auf drei Kriterien bewertet:\n\nFormalia (z. B. Vollständigkeit der Abarbeitung, Angemessenheit der äußeren Gestaltung, Fokus auf Wesentliche, Übersichtlichkeit, Ästhetik, Reproduzierbarkeit)\nMethodik (z. B. Richtige Auswahl und Anwendung der Verfahren, methodisches Verständnis)\nInhalt (z. B. Verständlichkeit, Breite und Tiefe der Problemlösung, Korrektheit der Interpretation)\n\nSie erhalten für jedes der drei Kriterien eine Teilnote sowie eine Gesamtnote. Außerdem erhalten Sie ggf. für die Kriterien noch ausformulierte Hinweise.\nDie Gesamtnote muss sich nicht als Mittelwert der Teilnoten ergeben.\nInsbesondere kann eine Fünf in einem der Kriterien zum Durchfallen führen, auch wenn die anderen beiden Kriterien gut oder sehr gut beurteilt wurden."
  },
  {
    "objectID": "020-pruefung.html#beispiele-für-aspekte-der-beurteilungskriterien",
    "href": "020-pruefung.html#beispiele-für-aspekte-der-beurteilungskriterien",
    "title": "2  Prüfung",
    "section": "2.9 Beispiele für Aspekte der Beurteilungskriterien",
    "text": "2.9 Beispiele für Aspekte der Beurteilungskriterien\n\nWurden deskriptive Statistiken (an angemessenen Ort) berichtet?\nWurden Diagramme und Tabellen angemessen eingesetzt?\nWurde Inferenzstatistik (angemessen) eingesetzt?\nWurden Effektstärkemaße (idealerweise mit Konfidenzintervallen dazu) berichtet?\nWurden alle relevanten Informationen für ein statistisches Verfahren angegeben (z.B. zum gewählten Prior)?\nWurde die Aussagekraft von Modellergebnissen richtig eingeschätzt?\nWaren die Schlussfolgerungen, die aus den statistischen Ergebnissen gezogen wurden, angemessen (z. B. wurde erkannt, dass ein Nicht-Verwerfeen einer Hypothese nicht automatisch ein Bestätigen derselben bedeutet)?\nWurde angemessen gerundet (inkl. konsistente Anzahl von Nachkommastellen)?\nPassen die statistischen Verfahren zu den Hypothesen?\nWurden die Voraussetzungen der statistischen Verfahren geprüft?\nSind die Ergebnisse reproduzierbar (Daten und Syntax eingereicht)?"
  },
  {
    "objectID": "020-pruefung.html#beispiele-für-fehler",
    "href": "020-pruefung.html#beispiele-für-fehler",
    "title": "2  Prüfung",
    "section": "2.10 Beispiele für Fehler",
    "text": "2.10 Beispiele für Fehler\nSchwere Fehler, die zum Durchfallen oder deutlichem Abwerten der Note führen können, sind z.B.:\n\nfehlende Inferenzstatistik (oder adäquatem Ersatz)\nfalsche Interpretation von Posteriori-Verteilungen oder p-Werten\nkeine Angabe von Konfidenzintervallen\nfalsche Interpretation von Konfidenzintervallen\nWahl des falschen Intervalls (Vorhersageintervall vs. Perzentilintervall vs. HDI)\nfalsche Entscheidung zum Hypothesentest auf Basis entsprechender Kennwerte (wie ROPE-Wahrscheinlichkeit oder p-Wert)\nfalsche Wahl des statistischen Verfahrens\nfehlende Deskriptivstatistik\n\nHäufige kleinere Mängel sind z. B.\n\npixelige Abbildungen\nR-Ausgaben oder R-Syntax als Screenshot\nfehlende Seitenzahlen (nur bei paginierten Formaten, nicht bei HTML)\nunübersichtliche Diagramme\nkein (verlinktes) Inhaltsverzeichnis ︎\nfehlende oder unverständliche Achsenbeschriftung bei Diagrammen\nfehlende oder falsche Beschreibung der/des Skalenniveau(s) der untersuchten Variablen"
  },
  {
    "objectID": "020-pruefung.html#sonstiges",
    "href": "020-pruefung.html#sonstiges",
    "title": "2  Prüfung",
    "section": "2.11 Sonstiges",
    "text": "2.11 Sonstiges\nEine automatische Prüfung auf Plagiate mittels geeigneter, von der Hochschule bereitgestellter Software ist möglich."
  },
  {
    "objectID": "025-twittermining.html#vorab",
    "href": "025-twittermining.html#vorab",
    "title": "\n3  Twitter Mining\n",
    "section": "\n3.1 Vorab",
    "text": "3.1 Vorab\n\n3.1.1 Lernziele\n\nTwitterdaten via API von Twitter auslesen\n\n3.1.2 Vorbereitung\n\nLesen Sie in Hvitfeldt und Silge (2021) Kap. 1.\nLegen Sie sich ein Konto bei Github an.\nLegen Sie sich ein Konto bei Twitter an.\nLesen Sie diesen Artikel zur Anmeldung bei der Twitter API1\n\n\n3.1.3 Benötigte R-Pakete\n\nlibrary(tidyverse)\nlibrary(rtweet)\nlibrary(rio)  \nlibrary(glue)\nlibrary(tweetbotornot)  # optional\nlibrary(keyring)  # optional\nlibrary(askpass)  # optional\nlibrary(academictwitteR)\n\n\n\nR-Paket {rtweet}\n\nEinen Überblick über die Funktionen des Pakets (function reference) findet sich hier."
  },
  {
    "objectID": "025-twittermining.html#anmelden-bei-twitter",
    "href": "025-twittermining.html#anmelden-bei-twitter",
    "title": "\n3  Twitter Mining\n",
    "section": "\n3.2 Anmelden bei Twitter",
    "text": "3.2 Anmelden bei Twitter\n\n3.2.1 Welche Accounts interessieren uns?\nHier ist eine (subjektive) Auswahl von deutschen Politikern2, die einen Startpunkt gibt zur Analyse von Art und Ausmaß von Hate Speech gerichtet an deutsche Politiker:innen.\n\nd_path &lt;- \"data/twitter-german-politicians.csv\"\n\npoliticians &lt;- import(d_path)\npoliticians\n\n\n\n  \n\n\n\n\n3.2.2 Twitter App erstellen\nTutorial\nAuf der Twitter Developer Seite können Sie sich ein Konto erstellen und dann anmelden.\n\n3.2.3 Intro\nDie Seite von rtweet gibt eine gute Starthilfe in die Benutzung des Pakets.\n\n3.2.4 Zugangsdaten\nZugangsdaten sollte man geschützt speichern, also z.B. nicht in einem geteilten Ordner.\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\n\nAnmelden:\n\nauth &lt;- rtweet_bot(api_key = API_Key,\n                   api_secret = API_Key_Secret,\n                   access_token = Access_Token,\n                   access_secret = Access_Token_Secret)\n\nAlternativ kann man sich auch als App anmelden, damit kann man z.B. nicht posten, aber dafür mehr herunterladen Quelle.\n\nauth &lt;- rtweet_app(bearer_token = Bearer_Token)\n\nJetzt haben wir ein Anmeldeobjekt, das wir für die weiteren Anfragen dieser Session nutzen können. Das sagen wir jetzt der Twitter-API:\n\nauth_as(auth)\n\nInfos über Ihre aktuellen Raten kann man sich mittels rate_limit() ausgeben lassen.\n\n3.2.5 Schützen Sie Ihre Zugangsdaten\nAchtung, Sicherheitshinweis … Passwörter und andere sensitive (Anmelde-)Informationen muss man schützen, das weiß jeder. Konkret bedeutet es, dass Sie diese Daten nicht in einem öffentlichen oder geteilten Repo herumliegen lassen. Achten Sie auch darauf, dass, wenn Sie diese Information sourceen, so wie ich gerade, diese dann ungeschützt in Ihrem RStudio Environment Fenster zu sehen sind. Falls Sie also den Bildschirm teilen, oder Ihnen jemand über die Schulter schaut, sind Ihre Zugangsdaten nicht geschützt.\nEin ähnlicher Fehler wäre, die History-Dateien von R in ein öffentliches Repo einzustellen (z.B. via Git). In der Datei .gitignore sollten daher folgende Dateien aufgeführt sein:\n.Rhistory\n.Rapp.history\nEin Rat von rtweet dazu:\n\nIt’s good practice to only provide secrets interactively, because that makes it harder to accidentally share them in either your .Rhistory or an .R file.\n\nEinen alternativen, sichereren Zugang bietet z.B. das Paket keyring. Dieses Paket bietet eine Anbindung zur Schlüsselbundverwaltung Ihres Betriebssystems:\n\nPlatform independent API to access the operating systems credential store.\n\nIm MacOS wird die zentrale Schlüsselbundverwaltung genutzt, in Windows und Linux die analoge Vorrichtungen.\nWir erstellen uns einen Schlüsselbund:\n\nkeyring_create(keyring = \"hate-speech-twitter\")\n\nDann können wir einen Eintrag im Schlüsselbund erstellen. Es öffnet sich eine Maske, die nach einem Passwort fragt. Geben Sie dort die sensitiven Informationen ein, etwa die Client-ID. Ggf. werden Sie noch nach dem Passwort des Schlüsselbunds an sich gefragt3\n\nkey_set(service = \"client_id\",\n        keyring = \"hate-speech-twitter\")\n\n\nkey_set(service = \"client_secret\",\n        keyring = \"hate-speech-twitter\")\n\n\nkey_set(service = \"bearer_token\",\n        keyring = \"hate-speech-twitter\")\n\nKünftig können wir dann die Passwörter aus dem Schlüsselbund abrufen:\n\nkey_get(service = \"client_id\",\n        keyring = \"hate-speech-twitter\")\n\n\nkey_get(service = \"bearer_token\",\n        keyring = \"hate-speech-twitter\")"
  },
  {
    "objectID": "025-twittermining.html#tweets-einlesen",
    "href": "025-twittermining.html#tweets-einlesen",
    "title": "\n3  Twitter Mining\n",
    "section": "\n3.3 Tweets einlesen",
    "text": "3.3 Tweets einlesen\nZu beachten ist, dass es Limits gibt, wie viele Informationen (pro Zeiteinheit) man über die Twitter-API auslesen darf. Informationen dazu findet man z.B. hier oder auch mit rate_limit().\nEin gängiges Limit der Twitter-API sind 900 Anfragen (z.B. Tweets auslesen) pro 15 Minuten.\n\n3.3.1 Timeline einlesen einzelner Accounts\nMal ein paar Tweets zur Probe:\n\nsesa_test &lt;- get_timeline(user = \"sauer_sebastian\", n = 3) %&gt;% \n  select(full_text)\n\n\n## RT @pia_lamberty: Ein Ansatz, der sich beim Debunking wissenschaftlich als erfolgreich herausgestellt hat, ist das sog. Faktensandwich: htt…\n## RT @ianbremmer: sure, it’s the hottest summer europe has ever had in history \n## \n## but look at the upside\n## \n## it’s one of the coolest summers euro…\n## RT @twisteddoodles: Balanced news reporting https://t.co/O1iiItEQrs\n\n\ntweets &lt;- get_timeline(user = politicians$screenname)\nsaveRDS(tweets, file = \"data/tweets/tweets01.rds\")\n\nMichael Kearney rät uns:\n\nPRO TIP #4: (for developer accounts only) Use bearer_token() to increase rate limit to 45,000 per fifteen minutes.\n\n\n3.3.2 Retweets einlesen\n\ntweets01_retweets &lt;- \n  tweets$id_str %&gt;% \n  head(3) %&gt;% \n  map_dfr( ~ get_retweets(status_id = .x, retryonratelimit = TRUE))\n\nDa die meisten Retweets aber nix sagen, sondern nur auf das einen Tweet wiederholen, ist das Sammeln der Retweets ziemlich langweilig.\nMöchte man retry on rate limit im Standard auf TRUE setzen, so kann man das über die Optionen von R tun.\n\noptions(rtweet.retryonratelimit = TRUE)\n\n\n3.3.3 EPINetz Twitter Politicians 2021\nKönig u. a. (2022) Volltext hier haben einen Datensatz mit knapp 2500 Twitter Accounts deutscher Politiker zusammengestellt, zum Stand 2021.\nDer Datensatz kann über Gesis bezogen werden.\nAuf der gleichen Seite findet sich auch eine Dokumentation des Vorgehens.\nNachdem wir den Datensatz heruntergeladen haben, können wir ihn einlesen:\n\npoliticians_path &lt;- \"data/tweets/EPINetz_TwitterPoliticians_2021.RDs\"\npoliticians_twitter &lt;- read_rds(politicians_path)\n\nhead(politicians_twitter)\n\n\n\n  \n\n\n\nDann lesen wir die Timelines (die Tweets) dieser Konten aus; in diesem Beispiel nur 10 Tweets pro Account:\n\nepi_tweets &lt;- get_timeline(user = head(politicians_twitter$twitter_name), n = 10)\nhead(epi_tweets)\n\nNatürlich könnte man auch mehr als 10 Tweets pro Konto einsammeln, braucht nur seine Zeit.\n\n3.3.4 Followers suchen\n\nfollowers01 &lt;-\n  politicians$screenname %&gt;% \n map_dfr( ~ get_followers(user = .x, retryonratelimit = TRUE))\n\nDa es dauern kann, Daten auszulesen (wir dürfen pro 15 Min. nur eine begrenzte Zahl an Information abrufen), kann es Sinn machen, die Daten lokal zu speichern.\n\nsaveRDS(followers01, file = \"data/tweets/followers01.rds\")\n\nUnd ggf. wieder importieren:\n\nfollowers01 &lt;- read_rds(file = \"data/tweets/followers01.rds\")\n\nWie viele unique Followers haben wir identifiziert?\n\nfollowers02 &lt;- \n  followers01 %&gt;% \n  distinct(from_id)\n\nDie Screennames wären noch nützlich:\n\nlookup_users(users = \"1690868335\")\n\nDie Anzahl der Users, die man nachschauen kann, ist begrenzt auf 180 pro 15 Minuten.\n\nfollowers03 &lt;-\n  followers02 %&gt;% \n  mutate(screenname = \n           list(lookup_users(users = from_id, retryonratelimit = TRUE,verbose = TRUE)))\n\nEntsprechend kann man wieder einlesen:\nDamit haben wir eine Liste an Followers, deren Tweets wir einlesen und analysieren können, z.B. nach Hate Speech.\nIm Gegensatz zu Followers heißen bei Twitter die Accounts, denen ei Nutzi folgt “Friends”.\nLesen wir mal die Followers von karl_lauterbach ein:\n\nkarl_followers &lt;- get_followers(user = \"karl_lauterbach\", verbose = TRUE)\n\nUm nicht jedes Mal aufs Neue die Daten herunterzuladen, bietet es sich an, die Daten lokal zu speichern:\n\nwrite_rds(karl_followers, file = \"data/tweets/karl_followers.rds\",\n          compress = \"gz\")\n\nEntsprechend kann man die Daten dann auch wieder einlesen:\n\nkarl_followers &lt;- read_rds(file = \"data/tweets/karl_followers.rds\")\n\n\n3.3.5 Follower Tweets einlesen\n\nfollowers_tweets &lt;- get_timeline(user = head(followers01$from_id), n = 10)\n\n\n3.3.6 Tweets nach Stichwort suchen\nUm nach einem Stichwort, allgemeiner nach einem bestimmten Text, in einem Tweet zu suchen, kann man die Funktion search_tweets nutzen:\n\nmy_tweet &lt;- search_tweets(\"Sebastian Sauer\", n = 1)\nmy_tweet$full_text\n\nSchaut man sich das zurückgelieferte Objekt (einen Tibble) näher an, entdeckt man eine Fülle an Informationen. Satte 43 Spalten (teilweise Listenspalten) finden sich dort:\n\nnames(my_tweet)\n##  [1] \"created_at\"                    \"id\"                           \n##  [3] \"id_str\"                        \"full_text\"                    \n##  [5] \"truncated\"                     \"display_text_range\"           \n##  [7] \"entities\"                      \"metadata\"                     \n##  [9] \"source\"                        \"in_reply_to_status_id\"        \n## [11] \"in_reply_to_status_id_str\"     \"in_reply_to_user_id\"          \n## [13] \"in_reply_to_user_id_str\"       \"in_reply_to_screen_name\"      \n## [15] \"geo\"                           \"coordinates\"                  \n## [17] \"place\"                         \"contributors\"                 \n## [19] \"retweeted_status\"              \"is_quote_status\"              \n## [21] \"quoted_status_id\"              \"quoted_status_id_str\"         \n## [23] \"retweet_count\"                 \"favorite_count\"               \n## [25] \"favorited\"                     \"retweeted\"                    \n## [27] \"lang\"                          \"possibly_sensitive\"           \n## [29] \"quoted_status\"                 \"text\"                         \n## [31] \"favorited_by\"                  \"scopes\"                       \n## [33] \"display_text_width\"            \"quoted_status_permalink\"      \n## [35] \"quote_count\"                   \"timestamp_ms\"                 \n## [37] \"reply_count\"                   \"filter_level\"                 \n## [39] \"query\"                         \"withheld_scope\"               \n## [41] \"withheld_copyright\"            \"withheld_in_countries\"        \n## [43] \"possibly_sensitive_appealable\"\n\nDie Tweet-ID dieses Tweets bekommen Sie, wenn Sie die Variable id_str auslesen:\n\nmy_tweet$id_str\n## [1] \"1593598440675500032\"\n\n\nmy_tweet$source\n## [1] \"&lt;a href=\\\"https://mobile.twitter.com\\\" rel=\\\"nofollow\\\"&gt;Twitter Web App&lt;/a&gt;\"\n\nDabei ist source nicht etwa die Person, die tweetet, wie man vielleicht meinen könnte, sondern das Frontend, das dabei verwendet wurde, also z.B. die iphone-App oder die Twitter-Webseite.\nLeider sucht man den screenname zu einen Tweet vergeblich in my_tweet.\nGegeben eines Dataframes mit Tweets kann man sich aber wie folgt den Nutzernamen (screen_name) ausgeben lassen.\n\nusers_data(my_tweet)\n\n\n\n  \n\n\n\nAußerdem gibt es einen “Trick” laut dieser Quelle, vgl. auch diesen SO-Post: Gibt man in die URL eines Tweets einen beliebigen Nutzernamen - das kann ein Fantasiename sein - so wird man automatisch zum richtigen Nutzer geleitet.\nDie Rohform der URL sieht also so aus:\nhttps://twitter.com/irgendeinnutzer/status/&lt;id_str&gt;\nGeben Sie also z.B. Folgende URL in Ihren Browser sein:\nhttps://twitter.com/irgendeinnutzer/status/1593598440675500032\nUnd Sie werden zum Nutzer sauer_sebastian weitergeleitet bzw. zu seinem Tweet mit obiger ID.\n\n3.3.7 Der Volltext ist manchmal abgeschniten\nManchmal ist der Volltext abgeschnitten\n\nmy_tweet$full_text\n\nHier steht der Beginn des Tweet-Textes, aber dann endet der Text abrup...\"\nGlücklicherweise - sofern man bei einer so umständlichen Darstellung von Glück reden kann - findet man den kompletten Text andernorts Quelle.\nDazu schreibt in diesem SO-Post der Nutzer Jonas:\n\nYou will need to check if the tweet is a retweet. If it is, use the retweet’s full_text. If it is not, use the tweet’s full_text. – Jonas, Nov 13, 2017 at 15:00\n\n\nmy_tweet$retweeted_status[[1]][[\"full_text\"]]\n\nHier steht der Beginn des Tweet-Textes, aber dann endet der Text abrupt? \nNein,er geht weiter und irgendwann ist der dann wirklich aus.\"\n\n3.3.8 Tweets nach ID suchen\nMit lookup_tweets(id_des_tweets) können Sie sich die Informationen zu einen Tweet ausgeben lassen. Das ist natürlich primär der Volltext:\n\ntweet_example &lt;- lookup_tweets(\"1593598440675500032\")\ntweet_example$full_text\n\nAber auch die übrigen Informationen können interessant sein."
  },
  {
    "objectID": "025-twittermining.html#tweets-verarbeiten",
    "href": "025-twittermining.html#tweets-verarbeiten",
    "title": "\n3  Twitter Mining\n",
    "section": "\n3.4 Tweets verarbeiten",
    "text": "3.4 Tweets verarbeiten\n\n3.4.1 Grundlegende Verarbeitung\nSind die Tweets eingelesen, kann man z.B. eine Sentimentanalyse, s. Kapitel 4.2.10, durchführen, oder schlicht vergleichen, welche Personen welche Wörter häufig verwenden, s. Kapitel 4.2.3.\n\n3.4.2 Bot or not?\nEine interessante Methode, Tweets zu verarbeiten, bietet das R-Paket tweetbotornot von M. Kearney.\nAus der Readme:\n\nDue to Twitter’s REST API rate limits, users are limited to only 180 estimates per every 15 minutes. To maximize the number of estimates per 15 minutes (at the cost of being less accurate), use the fast = TRUE argument. This method uses only users-level data, which increases the maximum number of estimates per 15 minutes to 90,000! Due to losses in accuracy, this method should be used with caution!\n\n\nusers &lt;- c(\"sauer_sebastian\")\nbot01 &lt;-\n  tweetbotornot(users)\n\n\n\n\n\n\n\nWichtig\n\n\n\nIch habe ein Fehlermeldung bekommen bei tweetbotornot. Da könnte ein technisches Problem in der Funktion vorliegen."
  },
  {
    "objectID": "025-twittermining.html#cron-jobs",
    "href": "025-twittermining.html#cron-jobs",
    "title": "\n3  Twitter Mining\n",
    "section": "\n3.5 Cron Jobs",
    "text": "3.5 Cron Jobs\n\n3.5.1 Was ist ein Cron Job?\nCron ist ein Programm auf Unix-artigen Betriebssystemen, das Skripte zu eingestellten Zeiten (wiederholt) ausführt, das sind dann “Cron Jobs”. Auf Windows gibt es aber analoge Funktionen. Cron Jobs sind praktisch, da man nicht jedes Mal selber z.B. Tweets, die heute zu einem Thema getweetet wurden, herunterladen muss. Das wird dann vom Cron Job übernommen.\nIn R gibt es eine API zum Programm Cron mit dem Paket cronR, s. Anleitung hier.\nDas analoge R-Paket für Windows heißt {taskscheduleR}.\n\n3.5.2 Beispiel für einen Cron Job\n\nlibrary(cronR)\n\nscrape_script &lt;- cron_rscript(\"scrape_tweets.R\")\n\n# Cron Job hinzufügen:\ncron_add(command = scrape_script, \n         frequency = 'daily', \n         at = \"10AM\",\n         id = 'Hate Speech')  # Name des Cron Jobs\n\ncron_clear(ask = FALSE)  # Alle Cron Jobs löschen\ncron_ls()  # Liste aller Cron Jobs\n\nIm obigen Beispiel wird das R-Skript scrape_tweets.R täglich um 10h ausgeführt.\nDer Inhalt von scrape_tweets.R könnte dann, in Grundzügen, so aussehen:\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(rtweet)\nfollowers_lauterbach &lt;-\n  followers01 %&gt;% \n  filter(to_id == \"Karl_Lauterbach\")\n\nfollowers_lauterbach_tweets &lt;- \n  get_timeline(user = followers_lauterbach$from_id[1:10], n = 10, retryonratelimit = TRUE, verbose = FALSE)\n\n\npath_output &lt;- \"/Users/sebastiansaueruser/Google Drive/RRRing/Scrape-Tweets/tweets/\"\n\nwrite_csv(x = followers_lauterbach_tweets,\n          file = paste0(path_output, \"followers_lauterbach_tweets.csv\"),\n          append = TRUE)\n\nWir schreiben nicht jedes Mal (jeden Tag) eine neue CSV-Datei, sondern wir hängen hier die neu ausgelesenen Daten an die Datei an.\nLeider ist es mit rtweet nicht möglich, ein Datum anzugeben, ab dem man Tweets auslesen möchte4"
  },
  {
    "objectID": "025-twittermining.html#datenbank-an-tweets-aufbauen",
    "href": "025-twittermining.html#datenbank-an-tweets-aufbauen",
    "title": "\n3  Twitter Mining\n",
    "section": "\n3.6 Datenbank an Tweets aufbauen",
    "text": "3.6 Datenbank an Tweets aufbauen\n\n3.6.1 Stamm an bisherigen Tweets\nIn diesem Abschnitt kümmern wir uns in größerem Detail um das Aufbauen einer Tweets-Datenbank.\nDiese Pakete benötigen wir:\n\nlibrary(rtweet)\nlibrary(tidyverse)\nlibrary(rio)  # R Data import/export\n\nDann melden wir uns an:\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\nauth &lt;- rtweet_app(bearer_token = Bearer_Token)\n\nDann brauchen wir eine Liste an Twitterkonten, die uns interessieren. Im Kontext von Hate Speech soll uns hier interessieren, welche Tweets an deutsche Spitzenpolitikis5 gesendet werden. Wir suchen also nach Tweets mit dem Text @karl_lauterbach, um ein Beispiel für einen Spitzenpolitiker zu nennen, der vermutlich von Hate Speech in höherem Maße betroffen ist.\n\npoliticians_twitter_path &lt;- \"/Users/sebastiansaueruser/github-repos/datascience-text/data/twitter-german-politicians.csv\"\n\npoliticians_twitter &lt;- rio::import(file = politicians_twitter_path)\n\nIn der Liste befinden sich 13 Politiker. Es macht die Sache vielleicht einfacher, wenn wir die Rate nicht überziehen. Bleiben wir daher bei 1000 Tweets pro Politiki:\n\nn_tweets_per_politician &lt;- 1e3\n\nDie R-Syntax, die die Arbeit leistet, ist in Funktionen ausgelagert, der Übersichtlichkeit halber.\n\nsource(\"funs/filter_recent_tweets.R\")\nsource(\"funs/download_recent_tweets.R\")\nsource(\"funs/add_tweets_to_tweet_db.R\")\nsource(\"funs/sanitize_tweets.R\")\n\nJetzt laden wir einfach die aktuellsten 1000 Tweets pro Konto herunter, daher brauchen wir keine Tweet-ID angeben, die ein Mindest- oder Maximum-Datum (bzw. ID) für einen Tweet angibt:\n\ntweets_older &lt;-\n  download_recent_tweets(screenname = politicians_twitter$screenname,\n                         max_or_since_id_str = NULL,\n                         n = n_tweets_per_politician,\n                         strip_columns = TRUE,\n                         reverse = TRUE)\n\nWie weit in die Vergangenheit reicht unsere Tweet-Sammlung?\n\noldest_tweets &lt;- filter_recent_tweets(tweets_older, max_or_min_id_str = is_min_id_str)\noldest_tweets\n\n\n\n  \n\n\n\nWas sind die neuesten Tweets, die wir habven?\n\nmost_recent_tweets &lt;- filter_recent_tweets(oldest_tweets)\nmost_recent_tweets\n\n\n\n  \n\n\n\nJetzt laden wir die neueren Tweets herunter, also mit einer ID größer als die größte in unserer Sammlung:\n\ntweets_new &lt;- \n  download_recent_tweets(screenname = most_recent_tweets$screenname,\n                         max_or_since_id_str = most_recent_tweets$id_str)\n\ntweets_new %&gt;% \n  select(screenname, created_at, id_str) %&gt;% \n  head()\n\nJetzt - und jedes Mal, wenn wir Tweets herunterladen - fügen wir diese einer Datenbank (oder zumindest einer “Gesamt-Tabelle”) hinzu:\n\ntweets_db &lt;- add_tweets_to_tweets_db(tweets_new, tweets_older)\n\nnrow(tweets_db)\n## [1] 10969\n\nSchließlich sollten wir nicht vergessen diese in einer Datei zu speichern:\n\nwrite_rds(tweets_db, file = \"~/datasets/Twitter/tweets-db-2022-11-11.rds\")\n\n… … So, einige Zeit ist vergangen. Laden wir noch ältere Tweets herunter und fügen Sie unserer Datenbank hinzu:\n\ntweets_older2 &lt;-\n  download_recent_tweets(screenname = politicians_twitter$screenname,\n                         max_or_since_id_str = oldest_tweets$id_str,\n                         n = 1e3,\n                         strip_columns = TRUE,\n                         reverse = TRUE)\n\n\ntweets_db &lt;- add_tweets_to_tweets_db(tweets_new, tweets_older2)\n\nnrow(tweets_db)\n## [1] 10011\n\nUnd wieder speichern wir die vergrößerte Datenbasis auf der Festplatte:\n\nwrite_rds(tweets_db, file = \"~/datasets/Twitter/hate-speech-twitter.rds\")\n\nLeider ist die Datenbasis nicht mehr deutlich gewachsen. Eine plausible Ursache ist, dass Twitter den Zugriff auf alte Tweets einschränkt.\nAus der Hilfe von search_tweets:\n\nReturns Twitter statuses matching a user provided search query. ONLY RETURNS DATA FROM THE PAST 6-9 DAYS.\n\nMit Hilfe des Academic Research Access sind deutlich höhere Raten möglich.\n\n3.6.2 Neue Tweets per Cron Job\nWie oben schon ausprobiert, legen wir uns einen Cron Job an.\nDas ist übrigens auch eine komfortable Lösung.\n\nlibrary(cronR)\n\nscrape_script &lt;- cron_rscript(\"/Users/sebastiansaueruser/github-repos/datascience-text/funs/get_tweets_politicians.R\")\n\n# Cron Job hinzufügen:\ncron_add(command = scrape_script, \n         frequency = 'daily', \n         at = \"10AM\",\n         id = 'Hate Speech')  # Name des Cron Jobs\n\nDas Skript get_tweets_politicians.R birgt die Schritte, die wir in diesem Abschnitt ausprobiert haben, hier liegt es. Kurz gesagt sucht es nach neuen Tweets, die also noch nicht in Ihrer “Datenbank” vorhanden sind, und lädt diese herunter. Dabei werden maximal 1000 Tweets pro Konto (derer sind es 13) heruntergeladen.\nBei einem Cronjob sollten absolute Pfade angegeben werden, da der Cronjob nicht aus dem aktuellen Projekt-Repo startet.\nDie Ergebnisse eines Cronjob-Durchlaufs werden in einer Log-Datei abgelegt, und zwar in dem Ordner, in dem auch das Skript liegt, das im Rahmen des Cronjobs durchgeführt wird.\n\n\n\n\n\n\nHinweis\n\n\n\nSchauen Sie sich die Funktionen im Ordner /funs einmal in Ruhe an. Hier geht es zu dem Ordner im Github-Repo. Es ist alles keine Zauberei, aber im Detail gibt es immer wieder Schwierigkeiten. Am meisten lernt man, wenn man selber Hand anlegt.\n\n\nMöchte man den Cron Job wieder löschen, so kann man das so tun:\n\ncron_clear(ask = FALSE)  # Alle Cron Jobs löschen\ncron_ls()  # Liste aller Cron Jobs\n\nUm die Tweets “händisch” herunterzuladen, kann man get_tweets_politicians() aufrufen:\n\nsource(\"funs/get_tweets_politicians.R\")\nget_tweets_politicians()\n\n\n3.6.3 Tweets in Excel exportieren\nUm prädiktive Modelle zu erstellen, braucht man ein Trainingsset, Tweets also, die schon vorklassifiziert sind, z.B. im Hinblick auf Hassrede mit ja oder nein. Technisch bietet sich ein 1 vs. 0 an.\nDazu laden wir einen Datensatz mit Tweets, z.B. diesen hier:\n\ntweets_to_kl &lt;- import(\"/Users/sebastiansaueruser/datasets/Twitter/tweets_to_karl_lauterbach.rds\")\n\nDa es viele Spalten gibt, die teilweise Listenspalten sind, also komplex, begrenzen wir uns auf das Wesentliche, den Tweet-Text und die ID des Tweets.\n\ntweets_to_kl2 &lt;-\n  tweets_to_kl %&gt;% \n  select(id_str, full_text) \n\n\n\n\n\n\n\nHinweis\n\n\n\nDie Tweet-ID wird einmal als String und einmal als Integer gespeichert. Allerdings übersteigt die Anzahl der Ziffern die Speichergröße von (normalen) Integer-Formaten in R. Daher ist die Twitter-ID als Integer nicht zuverlässig; als Text hingegen schon.\n\n\nUnd schließlich können wir die Excel-Datei importieren.\n\nexport(tweets_to_kl2, file = \"~/datasets/Twitter/tweets_to_kl.xlsx\")\n\nDie Excel-Tabelle können wir dann bequem hernehmen, um Tweets manuell zu klassifizieren.\n\n3.6.4 Twitterkonten für Wissenschaftler\nTwitter stellt spezielle Konten für Wissentschaftlis bereit, die über höhere Raten und mehr Funktionen verfügen, also mehr Tweets herunterladen können, z.B. 10 Millionen Tweets pro Monat pro Projekt.\n\nauth_academic &lt;- rtweet_app(bearer_token = askpass::askpass(\"bearer token\"))\nauth_academic\n\nDas R-Paket askpass stellt eine weitere Möglichkeit bereit, um Zugangsdaten zu schützen. Es öffnet eine Maske, die interaktiv und als Punkte geschützte Buchstaben nach einem Passwort fragt, in diesem Fall nach dem Bearer-Token.\nAus der Hilfe:\n\nPrompt the user for a password to authenticate or read a protected key. By default, this function automatically uses the most appropriate method based on the user platform and front-end. Users or IDEs can override this and set a custom password entry function via the askpass option.\n\nTwitter bietet in diesem Repo einen nützlichen Kurs an, um sich mit der API vertraut zu machen.\n\ntweets_to__FriedrichMerz_2022 &lt;-\n  get_all_tweets(query = \"to:_FriedrichMerz -is:retweet\",\n                 start_tweets = \"2022-01-01T00:00:00Z\",\n                 end_tweets = \"2022-11-23T23:59:59Z\",\n                 bearer_token = askpass(\"Bearer token\"),\n                 file = \"~/datasets/Twitter/tweets-to-_FriedrichMerz_2022.rds\",\n                 n = 1e5)\n\nOder als Funktion, das ist praktischer, wenn man die Syntax mehrfach verwendet:\n\nget_all_tweets_politicians &lt;- function(screenname, bearer_token, n = 1e5) {\n  get_all_tweets(query = paste0(\"to:\", screenname, \" -is:retweet\"),\n                 start_tweets = \"2021-01-01T00:00:00Z\",\n                 end_tweets = \"2021-12-31T23:59:59Z\",\n                 bearer_token = bearer_token,\n                 file = glue::glue(\"~/datasets/Twitter/tweets_to_{screenname}_2021.rds\"),\n                 data_path = glue::glue(\"~/datasets/Twitter/{screenname}\"),\n                 n = n)\n}\n\n\n#debug(get_all_tweets_politicians)\nget_all_tweets_politicians(screenname = politicians$screenname[5],\n                           bearer_token = askpass(\"Bearer token\"),\n                           n = 1e05)\n\nDann kann man die Objekte abespeichern, etwas als RDS-Datei oder als Feather-Datei.\nDen Datensatz politicians hatten wir oben angelegt, s. Kapitel 3.2.1. Er beinhaltet die Kontonamen (screennames) einiger deutscher Politikis.\nWichtig ist, mit den Lizenzregeln in Einklang zu bleiben.\nZentral ist dabei sicherlich die Frage, ob und wie man Tweets weitergeben darf. Dazu:\n\nAcademic researchers are permitted to distribute an unlimited number of Tweet IDs and/or User IDs if they are doing so for the purposes of non-commercial research and to validate or replicate previous academic studies. You should not share the entire Tweet text directly. Instead, you can build a list of Tweet IDs and share those. The researchers who you share this set of Tweet IDs with, can then use the Twitter API to hydrate and get the full Tweet objects from the Tweet IDs.\n\nQuelle\nMehr Details finden sich den Entwicklerrichtlinien von Twitter.\nTwitter stellt eine Reihe von Lehrmaterialien für die wissenschaftliche Nutzung von Tweets bereit."
  },
  {
    "objectID": "025-twittermining.html#aufgaben",
    "href": "025-twittermining.html#aufgaben",
    "title": "\n3  Twitter Mining\n",
    "section": "\n3.7 Aufgaben",
    "text": "3.7 Aufgaben\n\nÜberlegen Sie, wie Sie das Ausmaß an Hate Speech, dem deutsche Politikerinnen und Politiker konfrontiert sind, messen können.\nArgumentieren Sie die Vorteile und Nachteile Ihres Ansatzes. Außerdem, auf welches Ergebnis dieser Analyse sie gespannt sind bzw. wären.\nÜberlegen Sie Korrelate, oder besser noch: (mögliche) Ursachen, des Hasses in den Tweets, gerichtet auf Polikter:innen. Sie können auch Gruppen von Ursachen bilden, etwas personengebundene Variablen der Politiker:innen (z.B. Alter? Geschlecht? Migrationshintergrund?).\nErstellen Sie sich eine Liste an Personen, deren Tweets sich lohnen (könnten), auf Hate Speech hin analysiert zu werden. Laden Sie deren Tweets (ggf. in Auszügen) herunter.\nDas Skript zu scrape_tweets.R könnte man noch verbessern, in dem man jeden Tag nur die neuesten Tweets herunterlädt. Dazu kann man bei get_timeline() mit dem Argument since_id eine Untergrenze der ID festlegen, so dass nur neuere Tweets (d.h. mit größerem Wert bei ID) ausgelesen werden. Ändern Sie das Skript entsprechend, so dass nur neuerer Tweets gelesen werden.\nErarbeiten Sie die Folien zu diesem rtweet-Workshop. Eine Menge guter Tipps!\n\n\n\n\n\nHvitfeldt, Emil, und Julia Silge. 2021. Supervised Machine Learning for Text Analysis in R. 1. Aufl. Boca Raton: Chapman and Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nKönig, Tim, Wolf J. Schünemann, Alexander Brand, Julian Freyberg, und Michael Gertz. 2022. „The EPINetz Twitter Politicians Dataset 2021. A New Resource for the Study of the German Twittersphere and Its Application for the 2021 Federal Elections“. Politische Vierteljahresschrift 63 (3): 529–47. https://doi.org/10.1007/s11615-022-00405-7."
  },
  {
    "objectID": "025-twittermining.html#footnotes",
    "href": "025-twittermining.html#footnotes",
    "title": "\n3  Twitter Mining\n",
    "section": "",
    "text": "Sie können hier nachlesen, was eine API ist.↩︎\nStand November 2022↩︎\nwas beruhigend ist: Man darf nicht ohne Erlaubnis in Ihrer Passwort-Sammlung herumfuhrwerken.↩︎\nMit dem R-Paket twitteR, das mittlerweile zugunsten von rtweet aufgegeben wurde, war das möglich. Allerdings zeigt ein Blick in die Dokumentation der Twitter-API, das Datumsangaben offenbar gar nicht unterstützt werden.↩︎\nzur Zeit, als diese Zeilen geschrieben wurden↩︎"
  },
  {
    "objectID": "030-textmining1.html#vorab",
    "href": "030-textmining1.html#vorab",
    "title": "\n4  Textmining1\n",
    "section": "\n4.1 Vorab",
    "text": "4.1 Vorab\n\n4.1.1 Lernziele\n\nDie vorgestellten Techniken des Textminings mit R anwenden können\n\n4.1.2 Vorbereitung\n\nLesen Sie in Hvitfeldt und Silge (2021) Kap. 2.\n\n4.1.3 Benötigte R-Pakete\n\nlibrary(tidyverse)\nlibrary(tokenizers)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(hcandersenr)\nlibrary(SnowballC)  # Stemming\nlibrary(lsa)  # Stopwörter\nlibrary(easystats)  # Komfort für deskriptive Statistiken, wie `describe_distribution`\nlibrary(textclean)  # Emojis ersetzen\nlibrary(wordcloud)"
  },
  {
    "objectID": "030-textmining1.html#einfache-methoden-des-textminings",
    "href": "030-textmining1.html#einfache-methoden-des-textminings",
    "title": "\n4  Textmining1\n",
    "section": "\n4.2 Einfache Methoden des Textminings",
    "text": "4.2 Einfache Methoden des Textminings\nArbeiten Sie die folgenden grundlegenden Methoden des Textminigs durch.\n\n4.2.1 Tokenisierung\nErarbeiten Sie dieses Kapitel: Hvitfeldt und Silge (2021), Kap. 2\nWie viele Zeilen hat das Märchen “The Fir tree” (in der englischen Fassung?)\n\nhcandersen_en %&gt;% \n  filter(book == \"The fir tree\") %&gt;% \n  nrow()\n## [1] 253\n\n\n4.2.2 Stopwörter entfernen\nErarbeiten Sie dieses Kapitel: s. Hvitfeldt und Silge (2021), Kap. 3\nEine alternative Quelle von Stopwörtern - in verschiedenen Sprachen - biwetet das Paket quanteda:\n\nstop2 &lt;-\n  tibble(word = quanteda::stopwords(\"german\"))\n\nhead(stop2)\n\n\n\n  \n\n\n\nEs bestehst (in der deutschen Version) aus 231 Wörtern.\n\n4.2.3 Wörter zählen\nIst der Text tokenisiert, kann man einfach mit “Bordmitteln” die Wörter zählen.\n\nhc_andersen_count &lt;- \n  hcandersen_de %&gt;% \n  filter(book == \"Das Feuerzeug\") %&gt;% \n  unnest_tokens(output = word, input = text) %&gt;% \n  anti_join(stop2) %&gt;% \n  count(word, sort = TRUE) \n## Joining with `by = join_by(word)`\n\nhc_andersen_count %&gt;% \n  head()\n\n\n\n  \n\n\n\nZur Visualisierung eignen sich Balkendiagramme, s. ?fig-hcandersen-count.\n\nhc_andersen_count %&gt;% \n  slice_max(order_by = n, n = 10) %&gt;% \n  mutate(word = factor(word)) %&gt;% \n  ggplot() +\n  aes(y = reorder(word, n), x = n) +\n  geom_col()\n  \n\n\n\nAbbildung 4.1: Die häufigsten Wörter in H.C. Anderssens Feuerzeug\n\n\n\nDabei macht es Sinn, aus word einen Faktor zu machen, denn Faktorstufen kann man sortieren, zumindest ist das die einfachste Lösung in ggplot2 (wenn auch nicht super komfortabel).\nEine (beliebite?) Methode, um Worthäufigkeiten in Corpora darzustellen, sind Wortwolken, s. Abbildung 4.2. Es sei hinzugefügt, dass solche Wortwolken nicht gerade optimale perzeptorische Qualitäten aufweisen.\n\nwordcloud(words = hc_andersen_count$word,\n          freq = hc_andersen_count$n,\n          max.words = 50,\n          rot.per = 0.35,\n          colors = brewer.pal(8, \"Dark2\"))\n\n\n\nAbbildung 4.2: Eine Wortwolke zu den häufigsten Wörtern in H.C. Andersens Feuerzeug\n\n\n\n\n4.2.4 Stemming (Wortstamm finden)\nErarbeiten Sie dieses Kapitel: Hvitfeldt und Silge (2021), Kap. 4\nVertiefende Hinweise zum UpSet plot finden Sie hier, Lex u. a. (2014).\nFür welche Sprachen gibt es Stemming im Paket SnowballC?\n\nlibrary(SnowballC)\ngetStemLanguages()\n##  [1] \"arabic\"     \"basque\"     \"catalan\"    \"danish\"     \"dutch\"     \n##  [6] \"english\"    \"finnish\"    \"french\"     \"german\"     \"greek\"     \n## [11] \"hindi\"      \"hungarian\"  \"indonesian\" \"irish\"      \"italian\"   \n## [16] \"lithuanian\" \"nepali\"     \"norwegian\"  \"porter\"     \"portuguese\"\n## [21] \"romanian\"   \"russian\"    \"spanish\"    \"swedish\"    \"tamil\"     \n## [26] \"turkish\"\n\nEinfacher Test: Suchen wir den Wordstamm für das Wort “wissensdurstigen”, wie in “die wissensdurstigen Studentis löcherten dis armi Professi”1.\n\nwordStem(\"wissensdurstigen\", language = \"german\")\n## [1] \"wissensdurst\"\n\nWerfen Sie mal einen Blick in das Handbuch von SnowballC.\n\n4.2.5 Fallstudie AfD-Parteiprogramm\nDaten einlesen:\n\nd_link &lt;- \"https://raw.githubusercontent.com/sebastiansauer/pradadata/master/data-raw/afd_2022.csv\"\nafd &lt;- read_csv(d_link, show_col_types = FALSE)\n\nWie viele Seiten hat das Dokument?\n\nnrow(afd)\n## [1] 190\n\nUnd wie viele Wörter?\n\nstr_count(afd$text, pattern = \"\\\\w\") %&gt;% sum(na.rm = TRUE)\n## [1] 179375\n\nAus breit mach lang, oder: wir tokenisieren (nach Wörtern):\n\nafd %&gt;% \n  unnest_tokens(output = token, input = text) %&gt;% \n  filter(str_detect(token, \"[a-z]\")) -&gt; afd_long\n\nStopwörter entfernen:\n\ndata(stopwords_de, package = \"lsa\")\n\nstopwords_de &lt;- tibble(word = stopwords_de)\n\n# Für das Joinen werden gleiche Spaltennamen benötigt:\nstopwords_de &lt;- stopwords_de %&gt;% \n  rename(token = word)  \n\nafd_long %&gt;% \n  anti_join(stopwords_de) -&gt; afd_no_stop\n## Joining with `by = join_by(token)`\n\nWörter zählen:\n\nafd_no_stop %&gt;% \n  count(token, sort = TRUE) -&gt; afd_count\n\nhead(afd_count)\n\n\n\n  \n\n\n\nWörter trunkieren:\n\nafd_no_stop %&gt;% \n  mutate(token_stem = wordStem(token, language = \"de\")) %&gt;% \n  count(token_stem, sort = TRUE) -&gt; afd_count_stemmed\n\nhead(afd_no_stop)\n\n\n\n  \n\n\n\n\n4.2.6 Stringverarbeitung\nErarbeiten Sie dieses Kapitel: Wickham und Grolemund (2016), Kap. 14\n\n4.2.6.1 Regulärausdrücke\nDas \"[a-z]\" in der Syntax oben steht für “alle Buchstaben von a-z”. D iese flexible Art von “String-Verarbeitung mit Jokern” nennt man Regulärausdrücke (regular expressions; regex). Es gibt eine ganze Reihe von diesen Regulärausdrücken, die die Verarbeitung von Texten erleichert. Mit dem Paket stringr geht das - mit etwas Übung - gut von der Hand. Nehmen wir als Beispiel den Text eines Tweets:\n\nstring &lt;- \"Correlation of unemployment and #AfD votes at #btw17: ***r = 0.18***\\n\\nhttps://t.co/YHyqTguVWx\"  \n\nMöchte man Ziffern identifizieren, so hilft der Reulärausdruck [:digit:]:\n“Gibt es mindestens eine Ziffer in dem String?”\n\nstr_detect(string, \"[:digit:]\")\n## [1] TRUE\n\n“Finde die Position der ersten Ziffer! Welche Ziffer ist es?”\n\nstr_locate(string, \"[:digit:]\")\n##      start end\n## [1,]    51  51\nstr_extract(string, \"[:digit:]\")\n## [1] \"1\"\n\n“Finde alle Ziffern!”\n\nstr_extract_all(string, \"[:digit:]\")\n## [[1]]\n## [1] \"1\" \"7\" \"0\" \"1\" \"8\"\n\n“Finde alle Stellen an denen genau 2 Ziffern hintereinander folgen!”\n\nstr_extract_all(string, \"[:digit:]{2}\")\n## [[1]]\n## [1] \"17\" \"18\"\n\nDer Quantitätsoperator {n} findet alle Stellen, in der der der gesuchte Ausdruck genau \\(n\\) mal auftaucht.\n“Zeig die Hashtags!”\n\nstr_extract_all(string, \"#[:alnum:]+\")\n## [[1]]\n## [1] \"#AfD\"   \"#btw17\"\n\nDer Operator [:alnum:] steht für “alphanumerischer Charakter” - also eine Ziffer oder ein Buchstabe; synonym hätte man auch \\\\w schreiben können (w wie word). Warum werden zwei Backslashes gebraucht? Mit \\\\w wird signalisiert, dass nicht der Buchstabe w, sondern etwas Besonderes, eben der Regex-Operator \\w gesucht wird.\n“Zeig die URLs!”\n\nstr_extract_all(string, \"https?://[:graph:]+\")\n## [[1]]\n## [1] \"https://t.co/YHyqTguVWx\"\n\nDas Fragezeichen ? ist eine Quantitätsoperator, der einen Treffer liefert, wenn das vorherige Zeichen (hier s) null oder einmal gefunden wird. [:graph:] ist die Summe von [:alpha:] (Buchstaben, groß und klein), [:digit:] (Ziffern) und [:punct:] (Satzzeichen u.ä.).\n“Zähle die Wörter im String!”\n\nstr_count(string, boundary(\"word\"))\n## [1] 13\n\n“Liefere nur Buchstabenfolgen zurück, lösche alles übrige”\n\nstr_extract_all(string, \"[:alpha:]+\")\n## [[1]]\n##  [1] \"Correlation\"  \"of\"           \"unemployment\" \"and\"          \"AfD\"         \n##  [6] \"votes\"        \"at\"           \"btw\"          \"r\"            \"https\"       \n## [11] \"t\"            \"co\"           \"YHyqTguVWx\"\n\nDer Quantitätsoperator + liefert alle Stellen zurück, in denen der gesuchte Ausdruck einmal oder häufiger vorkommt. Die Ergebnisse werden als Vektor von Wörtern zurückgegeben. Ein anderer Quantitätsoperator ist *, der für 0 oder mehr Treffer steht. Möchte man einen Vektor, der aus Stringen-Elementen besteht zu einem Strring zusammenfüngen, hilft paste(string) oder str_c(string, collapse = \" \").\n\nstr_replace_all(string, \"[^[:alpha:]+]\", \"\")\n## [1] \"CorrelationofunemploymentandAfDvotesatbtwrhttpstcoYHyqTguVWx\"\n\nMit dem Negationsoperator [^x] wird der Regulärausrck x negiert; die Syntax oben heißt also “ersetze in string alles außer Buchstaben durch Nichts”. Mit “Nichts” sind hier Strings der Länge Null gemeint; ersetzt man einen belieibgen String durch einen String der Länge Null, so hat man den String gelöscht.\nDas Cheatsheet zur Strings bzw zu stringr von RStudio gibt einen guten Überblick über Regex; im Internet finden sich viele Beispiele.\n\n4.2.6.2 Regex im Texteditor\nEinige Texteditoren unterstützen Regex, so auch RStudio.\nDas ist eine praktische Sache. Ein Beispiel: Sie haben eine Liste mit Namen der Art:\n\nNachname1, Vorname1\nNachname2, Vorname2\nNachname3, Vorname3\n\nUnd Sie möchten jetzt aber die Liste mit Stil Vorname Nachname sortiert haben.\nRStudio mit Regex macht’s möglich, s. ?fig-vorher-regex.\n\n\nAbbildung 4.3: ?(caption)\n\n\n4.2.7 Emoji-Analyse\nEine einfache Art, Emojis in einer Textmining-Analyse zu verarbeiten, bietet das Paket textclean:\n\nfls &lt;- system.file(\"docs/emoji_sample.txt\", package = \"textclean\")\nx &lt;- readLines(fls)[1]\nx\n## [1] \"Proin 😍 ut maecenas 😏 condimentum 😔 purus eget. Erat, 😂vitae nunc elit. Condimentum 😢 semper iaculis bibendum sed tellus. Ut suscipit interdum😑 in. Faucib😞 us nunc quis a vitae posuere. 😛 Eget amet sit condimentum non. Nascetur vitae ☹ et. Auctor ornare ☺ vestibulum primis justo congue 😀urna ac magna. Quam 😥 pharetra 😟 eros 😒facilisis ac lectus nibh est 😙vehicula 😐 ornare! Vitae, malesuada 😎 erat sociosqu urna, 😏 nec sed ad aliquet 😮 .\"\n\n\nreplace_emoji(x)\n## [1] \"Proin smiling face with heart-eyes ut maecenas smirking face condimentum pensive face purus eget. Erat, face with tears of joy vitae nunc elit. Condimentum crying face semper iaculis bibendum sed tellus. Ut suscipit interdum expressionless face in. Faucib disappointed face us nunc quis a vitae posuere. face with tongue Eget amet sit condimentum non. Nascetur vitae frowning face et. Auctor ornare smiling face vestibulum primis justo congue grinning face urna ac magna. Quam sad but relieved face pharetra worried face eros unamused face facilisis ac lectus nibh est kissing face with smiling eyes vehicula neutral face ornare! Vitae, malesuada smiling face with sunglasses erat sociosqu urna, smirking face nec sed ad aliquet face with open mouth .\"\nreplace_emoji_identifier(x)\n## [1] \"Proin lexiconwiutsdotskrupggpgmhm ut maecenas lexiconwizbukzesopzflfinotj condimentum lexiconwlnxqescoesytfatoevi purus eget. Erat, lexiconwcaiviebiytolowkanmb vitae nunc elit. Condimentum lexiconwpujksvgujncexktvyrn semper iaculis bibendum sed tellus. Ut suscipit interdum lexiconwknnasgueiicggptyzbx in. Faucib lexiconwoxfeslcareuqfkbyjgy us nunc quis a vitae posuere. lexiconwobmhqdrrzgygdexhnkk Eget amet sit condimentum non. Nascetur vitae lexiconbfalxvockmnmtmycmwyq et. Auctor ornare lexiconbgmujofaalvxqrklfqgd vestibulum primis justo congue lexiconvygwtlyrpywfarytvfis urna ac magna. Quam lexiconwurhpvewhizayynmfxqo pharetra lexiconwpmuduwgbxxrxeltrueb eros lexiconwkrvakxddtqckcjxeksl facilisis ac lectus nibh est lexiconwmsjgfnelqfeyhgudmfj vehicula lexiconwjfhkpcsgcjtotwlapxa ornare! Vitae, malesuada lexiconwivnupleicqgksianinp erat sociosqu urna, lexiconwizbukzesopzflfinotj nec sed ad aliquet lexiconxbwhfeflxbuupjezgdwl .\"\n\n\n4.2.8 Text aufräumen\nEine Reihe generischer Tests bietet das Paket textclean von Tyler Rinker:\nHier ist ein “unaufgeräumeter” Text:\n\nx &lt;- c(\"i like\", \"&lt;p&gt;i want. &lt;/p&gt;. thet them ther .\", \"I am ! that|\", \"\", NA, \n    \"&quot;they&quot; they,were there\", \".\", \"   \", \"?\", \"3;\", \"I like goud eggs!\", \n    \"bi\\xdfchen Z\\xfcrcher\", \"i 4like...\", \"\\\\tgreat\",  \"She said \\\"yes\\\"\")\n\nLassen wir uns dazu ein paar Diagnostiken ausgeben.\n\nEncoding(x) &lt;- \"latin1\"\nx &lt;- as.factor(x)\ncheck_text(x)\n## \n## =============\n## NON CHARACTER\n## =============\n## \n## The text variable is not a character column (likely `factor`):\n## \n## \n## *Suggestion: Consider using `as.character` or `stringsAsFactors = FALSE` when reading in\n##              Also, consider rerunning `check_text` after fixing\n## \n## \n## =====\n## DIGIT\n## =====\n## \n## The following observations contain digits/numbers:\n## \n## 10, 13\n## \n## This issue affected the following text:\n## \n## 10: 3;\n## 13: i 4like...\n## \n## *Suggestion: Consider using `replace_number`\n## \n## \n## ========\n## EMOTICON\n## ========\n## \n## The following observations contain emoticons:\n## \n## 6\n## \n## This issue affected the following text:\n## \n## 6: &quot;they&quot; they,were there\n## \n## *Suggestion: Consider using `replace_emoticons`\n## \n## \n## =====\n## EMPTY\n## =====\n## \n## The following observations contain empty text cells (all white space):\n## \n## 1\n## \n## This issue affected the following text:\n## \n## 1: i like\n## \n## *Suggestion: Consider running `drop_empty_row`\n## \n## \n## =======\n## ESCAPED\n## =======\n## \n## The following observations contain escaped back spaced characters:\n## \n## 14\n## \n## This issue affected the following text:\n## \n## 14: \\tgreat\n## \n## *Suggestion: Consider using `replace_white`\n## \n## \n## ====\n## HTML\n## ====\n## \n## The following observations contain HTML markup:\n## \n## 2, 6\n## \n## This issue affected the following text:\n## \n## 2: &lt;p&gt;i want. &lt;/p&gt;. thet them ther .\n## 6: &quot;they&quot; they,were there\n## \n## *Suggestion: Consider running `replace_html`\n## \n## \n## ==========\n## INCOMPLETE\n## ==========\n## \n## The following observations contain incomplete sentences (e.g., uses ending punctuation like '...'):\n## \n## 13\n## \n## This issue affected the following text:\n## \n## 13: i 4like...\n## \n## *Suggestion: Consider using `replace_incomplete`\n## \n## \n## =============\n## MISSING VALUE\n## =============\n## \n## The following observations contain missing values:\n## \n## 5\n## \n## *Suggestion: Consider running `drop_NA`\n## \n## \n## ========\n## NO ALPHA\n## ========\n## \n## The following observations contain elements with no alphabetic (a-z) letters:\n## \n## 4, 7, 8, 9, 10\n## \n## This issue affected the following text:\n## \n## 4: \n## 7: .\n## 8:    \n## 9: ?\n## 10: 3;\n## \n## *Suggestion: Consider cleaning the raw text or running `filter_row`\n## \n## \n## ==========\n## NO ENDMARK\n## ==========\n## \n## The following observations contain elements with missing ending punctuation:\n## \n## 1, 3, 4, 6, 8, 10, 12, 14, 15\n## \n## This issue affected the following text:\n## \n## 1: i like\n## 3: I am ! that|\n## 4: \n## 6: &quot;they&quot; they,were there\n## 8:    \n## 10: 3;\n## 12: bißchen Zürcher\n## 14: \\tgreat\n## 15: She said \"yes\"\n## \n## *Suggestion: Consider cleaning the raw text or running `add_missing_endmark`\n## \n## \n## ====================\n## NO SPACE AFTER COMMA\n## ====================\n## \n## The following observations contain commas with no space afterwards:\n## \n## 6\n## \n## This issue affected the following text:\n## \n## 6: &quot;they&quot; they,were there\n## \n## *Suggestion: Consider running `add_comma_space`\n## \n## \n## =========\n## NON ASCII\n## =========\n## \n## The following observations contain non-ASCII text:\n## \n## 12\n## \n## This issue affected the following text:\n## \n## 12: bißchen Zürcher\n## \n## *Suggestion: Consider running `replace_non_ascii`\n## \n## \n## ==================\n## NON SPLIT SENTENCE\n## ==================\n## \n## The following observations contain unsplit sentences (more than one sentence per element):\n## \n## 2, 3\n## \n## This issue affected the following text:\n## \n## 2: &lt;p&gt;i want. &lt;/p&gt;. thet them ther .\n## 3: I am ! that|\n## \n## *Suggestion: Consider running `textshape::split_sentence`\n\n\n4.2.9 Diverse Wortlisten\nTyler Rinker stellt mit dem Paket lexicon eine Zusammenstellung von Wortlisten zu diversen Zwecken zur Verfügung. Allerding nur für die englische Sprache.\n\n4.2.10 Sentimentanalyse\n\n4.2.10.1 Einführung\nEine weitere interessante Analyse ist, die “Stimmung” oder “Emotionen” (Sentiments) eines Textes auszulesen. Die Anführungszeichen deuten an, dass hier ein Maß an Verständnis suggeriert wird, welches nicht (unbedingt) von der Analyse eingehalten wird. Jedenfalls ist das Prinzip der Sentiment-Analyse im einfachsten Fall so:\n\nSchau dir jeden Token aus dem Text an.\n\nPrüfe, ob sich das Wort im Lexikon der Sentiments wiederfindet.\n\nWenn ja, dann addiere den Sentimentswert dieses Tokens zum bestehenden Sentiments-Wert.\n\nWenn nein, dann gehe weiter zum nächsten Wort.\n\nLiefere zum Schluss die Summenwerte pro Sentiment zurück.\n\nEs gibt Sentiment-Lexika, die lediglich einen Punkt für “positive Konnotation” bzw. “negative Konnotation” geben; andere Lexiko weisen differenzierte Gefühlskonnotationen auf. Wir nutzen hier das deutsche Sentimentlexikon sentiws (Remus, Quasthoff, und Heyer 2010). Sie können das Lexikon als CSV hier herunterladen:\n\nsentiws &lt;- read_csv(\"https://osf.io/x89wq/?action=download\")\n\nDen Volltext zum Paper finden Sie z.B. hier.\nAlternativ können Sie die Daten aus dem Paket pradadata laden. Allerdings müssen Sie dieses Paket von Github installieren:\n\ninstall.packages(\"devtools\", dep = TRUE)\ndevtools::install_github(\"sebastiansauer/pradadata\")\n\n\ndata(sentiws, package = \"pradadata\")\n\nTabelle 4.1 zeigt einen Ausschnitt aus dem Sentiment-Lexikon SentiWS.\n\n\n\n\n Tabelle 4.1:  Auszug aus SentiWS \n  \n\n\n\n\n\n4.2.10.2 Ungewichtete Sentiment-Analyse\nNun können wir jedes Token des Textes mit dem Sentiment-Lexikon abgleichen; dabei zählen wir die Treffer für positive bzw. negative Terme. Zuvor müssen wir aber noch die Daten (afd_long) mit dem Sentimentlexikon zusammenführen (joinen). Das geht nach bewährter Manier mit inner_join; “inner” sorgt dabei dafür, dass nur Zeilen behalten werden, die in beiden Dataframes vorkommen. Tabelle Tabelle 4.2 zeigt Summe, Anzahl und Anteil der Emotionswerte.\nWir nutzen die Tabelle afd_long, die wir oben definiert haben.\n\nafd_long %&gt;% \n  inner_join(sentiws, by = c(\"token\" = \"word\")) %&gt;% \n  select(-inflections) -&gt; afd_senti  # die Spalte brauchen wir nicht\n## Warning in inner_join(., sentiws, by = c(token = \"word\")): Detected an unexpected many-to-many relationship between `x` and `y`.\n## ℹ Row 9101 of `x` matches multiple rows in `y`.\n## ℹ Row 3190 of `y` matches multiple rows in `x`.\n## ℹ If a many-to-many relationship is expected, set `relationship =\n##   \"many-to-many\"` to silence this warning.\n\nafd_senti %&gt;% \n  group_by(neg_pos) %&gt;% \n  summarise(polarity_sum = sum(value),\n            polarity_count = n()) %&gt;% \n  mutate(polarity_prop = (polarity_count / sum(polarity_count)) %&gt;% round(2)) -&gt;\n  afd_senti_tab\n\n\n\n\n\n Tabelle 4.2:  Zusammenfassung von SentiWS \n  \n\n\n\n\nDie Analyse zeigt, dass die emotionale Bauart des Textes durchaus interessant ist: Es gibt viel mehr positiv getönte Wörter als negativ getönte. Allerdings sind die negativen Wörter offenbar deutlich stärker emotional aufgeladen, denn die Summe an Emotionswert der negativen Wörter ist (überraschenderweise?) deutlich größer als die der positiven.\nBetrachten wir also die intensivsten negativ und positive konnotierten Wörter näher.\n\nafd_senti %&gt;% \n  distinct(token, .keep_all = TRUE) %&gt;% \n  mutate(value_abs = abs(value)) %&gt;% \n  top_n(20, value_abs) %&gt;% \n  pull(token)\n##  [1] \"ungerecht\"    \"besonders\"    \"gefährlich\"   \"überflüssig\"  \"behindern\"   \n##  [6] \"gelungen\"     \"brechen\"      \"unzureichend\" \"gemein\"       \"verletzt\"    \n## [11] \"zerstören\"    \"trennen\"      \"falsch\"       \"vermeiden\"    \"zerstört\"    \n## [16] \"schwach\"      \"belasten\"     \"schädlich\"    \"töten\"        \"verbieten\"\n\nDiese “Hitliste” wird zumeist (19/20) von negativ polarisierten Begriffen aufgefüllt, wobei “besonders” ein Intensivierwort ist, welches das Bezugswort verstärt (“besonders gefährlich”). Das Argument keep_all = TRUE sorgt dafür, dass alle Spalten zurückgegeben werden, nicht nur die durchsuchte Spalte token. Mit pull haben wir aus dem Dataframe, der von den dplyr-Verben übergeben wird, die Spalte pull “herausgezogen”; hier nur um Platz zu sparen bzw. der Übersichtlichkeit halber.\nNun könnte man noch den erzielten “Netto-Sentimentswert” des Corpus ins Verhältnis setzen Sentimentswert des Lexikons: Wenn es insgesamt im Sentiment-Lexikon sehr negativ zuginge, wäre ein negativer Sentimentwer in einem beliebigen Corpus nicht überraschend. describe_distribution aus easystats gibt uns einen Überblick der üblichen deskriptiven Statistiken.\n\nsentiws %&gt;% \n  select(value, neg_pos) %&gt;% \n  #group_by(neg_pos) %&gt;% \n  describe_distribution()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nMean\nSD\nIQR\nRange\nSkewness\nKurtosis\nn\nn_Missing\n\n\nvalue\n-0.05\n0.20\n0.05\n(-1.00, 1.00)\n-0.68\n2.36\n3468\n0\n\n\n\n\nInsgesamt ist das Lexikon ziemlich ausgewogen; negative Werte sind leicht in der Überzahl im Lexikon. Unser Corpus hat eine ähnliche mittlere emotionale Konnotation wie das Lexikon:\n\nafd_senti %&gt;% \n  summarise(senti_sum = mean(value) %&gt;% round(2))\n\n\n\n  \n\n\n\n\n4.2.11 Weitere Sentiment-Lexika\nTyler Rinker stellt das Paket sentimentr zur Verfügung. Matthew Jockers stellt das Paket Syushet zur Verfügung.\n\n4.2.12 Google Trends\nEine weitere Möglichkeit, “Worthäufigkeiten” zu identifizieren ist Google Trends. Dieser Post zeigt Ihnen eine Einsatzmöglichkeit."
  },
  {
    "objectID": "030-textmining1.html#aufgaben",
    "href": "030-textmining1.html#aufgaben",
    "title": "\n4  Textmining1\n",
    "section": "\n4.3 Aufgaben",
    "text": "4.3 Aufgaben\n\npurrr-map01\npurrr-map02\npurrr-map03\npurrr-map04\nRegex-Übungen\nAufgaben zum Textmining von Tweets"
  },
  {
    "objectID": "030-textmining1.html#fallstudie-hate-speech",
    "href": "030-textmining1.html#fallstudie-hate-speech",
    "title": "\n4  Textmining1\n",
    "section": "\n4.4 Fallstudie Hate-Speech",
    "text": "4.4 Fallstudie Hate-Speech\n\n4.4.1 Daten\nEs finden sich mehrere Datensätze zum Thema Hate-Speech im öffentlichen Internet, eine Quelle ist Hate Speech Data, ein Repositorium, das mehrere Datensätze beinhaltet.\n\nKaggle Hate Speech and Offensive Language Dataset\nBretschneider and Peters Prejudice on Facebook Dataset\nDaten zum Fachartikel”Large Scale Crowdsourcing and Characterization of Twitter Abusive Behavior”\n\nFür Textmining kann eine Liste mit anstößigen (obszönen) Wörten nützlich sein, auch wenn man solche Dinge ungern anfässt, verständlicherweise. Jenyay bietet solche Listen in verschiedenen Sprachen an. Die Liste von KDNOOBW sieht sehr ähnlich aus (zumindest die deutsche Version). Eine lange Sammlung deutscher Schimpfwörter findet sich im insult.wiki; ähnlich bei Hyperhero.\nTwitterdaten dürfen nur in “dehydrierter” Form weitergegeben werden, so dass kein Rückschluss von ID zum Inhalt des Tweets möglich ist. Daher werden öffentlich nur die IDs der Tweets, als einzige Information zum Tweet, also ohne den eigentlichen Inhalt des Tweets, bereitgestellt.\nÜber die Twitter-API kann man sich, wie oben dargestellt, dann die Tweets wieder “rehydrieren”, also wieder mit dem zugehörigen Tweet-Text (und sonstigen Infos des Tweets) zu versehen.\n\n4.4.2 Grundlegendes Text Mining\nWenden Sie die oben aufgeführten Techniken des grundlegenden Textminings auf einen der oben dargestellten Hate-Speech-Datensätze an. Erstellen Sie ein (HTML-Dokument) mit Ihren Ergebnissen. Stellen Sie die Ergebnisse auf dem Github-Repo dieses Kurses ein. Vergleichen Sie Ihre Lösung mit den Lösungen der anderen Kursmitglieder.\nWir nutzen noch nicht eigene Daten, die wir von Twitter ausgelesen haben, das heben wir uns für später auf.\n\n\n\n\nHvitfeldt, Emil, und Julia Silge. 2021. Supervised Machine Learning for Text Analysis in R. 1. Aufl. Boca Raton: Chapman and Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nLex, Alexander, Nils Gehlenborg, Hendrik Strobelt, Romain Vuillemot, und Hanspeter Pfister. 2014. „UpSet: Visualization of Intersecting Sets“. IEEE Transactions on Visualization and Computer Graphics 20 (12): 1983–92. https://doi.org/10.1109/TVCG.2014.2346248.\n\n\nRemus, Robert, Uwe Quasthoff, und Gerhard Heyer. 2010. „SentiWS - a Publicly Available German-Language Resource for Sentiment Analysis“. Proceedings of the 7th International Language Ressources and Evaluation (LREC’10), 1168–71.\n\n\nWickham, Hadley, und Garrett Grolemund. 2016. R for Data Science: Visualize, Model, Transform, Tidy, and Import Data. O’Reilly Media. https://r4ds.had.co.nz/index.html."
  },
  {
    "objectID": "030-textmining1.html#footnotes",
    "href": "030-textmining1.html#footnotes",
    "title": "\n4  Textmining1\n",
    "section": "",
    "text": "Gender-i↩︎"
  },
  {
    "objectID": "040-populismus.html#vorab",
    "href": "040-populismus.html#vorab",
    "title": "\n5  Fallstudie Populismus\n",
    "section": "\n5.1 Vorab",
    "text": "5.1 Vorab\n\n5.1.1 Lernziele\n\nDie Fallstudie erklären können\n\n5.1.2 Vorbereitung\n\n\nClonen Sie das Projekt-Repositorium oder laden Sie es herunter1.\nArbeiten Sie die Syntax zu dem Projekt durch.\n\n5.1.3 Benötigte R-Pakete\nIn dem vorgestellten Projekt werden die folgenden R-Pakete verwendet.\n\nlibrary(tidyverse)\nlibrary(twitteR)\nlibrary(tidytext)\nlibrary(lubridate)\nlibrary(magrittr)\nlibrary(tidytext)\nlibrary(stringr)\nlibrary(viridis)\nlibrary(wordcloud)\nlibrary(SnowballC)\nlibrary(knitr)\nlibrary(testthat)"
  },
  {
    "objectID": "040-populismus.html#wie-populistisch-tweeten-unsere-politikerinnen",
    "href": "040-populismus.html#wie-populistisch-tweeten-unsere-politikerinnen",
    "title": "\n5  Fallstudie Populismus\n",
    "section": "\n5.2 Wie populistisch tweeten unsere Politiker:innen?",
    "text": "5.2 Wie populistisch tweeten unsere Politiker:innen?\nVerschaffen Sie sich einen Überblick über dieses Projekt! Im Rahmen dieses Projekts vergleicht der Autor den Populismus von deutschen Politiker:innen, so wie er sich in den Tweets dieser Personen niederschlägt. Auf dieser Basis wird ein Populismuswert, bestehend aus mehreren Teilwerten, berechnet und auf Parteiebenen (als Mittel der zugehörigen Politiker:innen) berechnet. Natürlich fragt man sich, wie Populismus definiert ist und wie diese Definition in den Berechnungen umgesetzt wurde. Finden Sie es selber heraus: Im Github-Repo sind alle Details dokumentiert.\nZum Einstieg hilft ein Überblick über die Ergebnisse der Analyse, die in diesem Vortrag zusammengefasst sind.\nDieser Post stellt die Ergebnisse mit etwas Kontext dar."
  },
  {
    "objectID": "040-populismus.html#footnotes",
    "href": "040-populismus.html#footnotes",
    "title": "\n5  Fallstudie Populismus\n",
    "section": "",
    "text": "Hier finden Sie Hinweise, wie man ein Github-Repo clont oder herunterlädt.↩︎"
  },
  {
    "objectID": "065-Information.html#lernsteuerung",
    "href": "065-Information.html#lernsteuerung",
    "title": "\n6  Informationstheorie\n",
    "section": "\n6.1 Lernsteuerung",
    "text": "6.1 Lernsteuerung\n\n6.1.1 Lernziele\n\nDie grundlegenden Konzepte der Informationstheorie erklären können\n\n6.1.2 Vorbereitung\n\nLesen Sie diesen Text als Vorbereitung.\n\n6.1.3 Benötigte R-Pakete\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(hcandersenr)   # Textdaten\nlibrary(easystats)  # Komfort für deskriptive Statistiken, wie `describe_distribution`\nlibrary(entropy)  # Entropie berechnen"
  },
  {
    "objectID": "065-Information.html#grundlagen",
    "href": "065-Information.html#grundlagen",
    "title": "\n6  Informationstheorie\n",
    "section": "\n6.2 Grundlagen",
    "text": "6.2 Grundlagen\nDie Informationstheorie ist eine der Sternstunden der Wissenschaft. Manche sagen dass Claude Shannon, der Autor der Theorie, auf einer Stufe mit Darwin und Einstein stehen sollte:\n\nIn this single paper, Shannon introduced this new fundamental theory. He raised the right questions, which no one else even thought of asking. This would have been enough to make this contribution earthshaking. But amazingly enough, Shannon also provided most of the right answers with class and elegance. In comparison, it took decades for a dozen of top physicists to define the basics of quantum theory. Meanwhile, Shannon constructed something equivalent, all by himself, in a single paper. Shannon’s theory has since transformed the world like no other ever had, from information technologies to telecommunications, from theoretical physics to economical globalization, from everyday life to philosophy. (…) I don’t think Shannon has had the credits he deserves. He should be right up there, near Darwin and Einstein, among the few greatest scientists mankind has ever had the chance to have.\n\nFür die Statistik ist die Informationstheorie von hoher Bedeutung. Im Folgenden schauen wir uns einige Grundlagen an.\n\n6.2.1 Shannon-Information\nMit der Shannon-Information (Information, Selbstinformation) quantifizieren wir, wie viel “Überraschung” sich in einem Ereignis verbirgt (Shannon 1948).\nEin Ereignis mit …\n\n\ngeringer Wahrscheinlichkeit: Viel Überraschung (Information)\n\nhoher Wahrscheinlichkeit: Wenig Überraschung (Information)\n\nWenn wir also erfahren, dass ein unwahrscheinliches Ereignis eingetreten ist (Schnee im Sommer), sind wir überraschter als wenn wir hören, dass ein wahrscheinliches Ereignis eingetreten ist (Schnee im Winter).\nDie Information eines Ereignis ist also eng verbunden mit seiner Wahrscheinlichkeit oder auch mit den Odds eines Ereignisses.\nDie Shannon-Information ist die einzige Größe, die einige wünschenswerte Anforderungen1 erfüllt:\n\nStetig\nJe mehr Ereignisse in einem Zufallsexperiment möglich sind, desto höher die Information, wenn ein bestimmtes Ereignis eintritt\nAdditiv: Die Summe der Information zweier Teilereignisse ist gleich der Information des Gesamtereignis\n\n\nDefinition 6.1 (Shannon-Information) Die Information, \\(I\\), ist so definiert:\n\\[I(x) = - \\log_2 \\left( Pr(x) \\right)\\qquad \\square\\]\n\nAndere Logarithmusbasen (als 2) sind möglich. Bei einem binären Logarithmus (Basis 2, logarithmus dualis) nennt man die Einheit Bit2.\nEin Münwzurf3 hat 1 Bit Information:\n\n-log(1/2, base = 2)\n## [1] 1\n\n\nDefinition 6.2 (Bit) Von 1 Bit Information spricht man, wenn ein Zufallsvorgang zwei Ausgänge hat und wir indifferent gegenüber den Ausgängen sind (also beide Ausgänge für gleich wahrscheinlich halten).\\(\\square\\)\n\nDamit gilt: \\(I = \\log_2\\left( \\frac{1}{Pr(x)} \\right)\\)\nDie Information ist eng verwandt mit den Odds bzw. Log-Odds (Logits):\n\\(\\text{log-odds}(x)=\\log \\left({\\frac {p(x)}{p(\\lnot x)}}\\right)\\)\nLogits können als Differenz zweier Shannon-Infos ausgedrückt werden:\n\\(\\text{log-odds}(x)=I(\\lnot x)-I(x)\\)\nDie Information zweier unabhängiger Ereignisse ist additiv.\nDie gemeinsame Wahrscheinlichkeit zweier unabhängiger Ereignisse ist das Produkt der einzelnen Wahrscheinlichkeiten:\n\\(Pr(x,y) = Pr(x) \\cdot Pr(y)\\)\nDie gemeinsame Information ist dann\n\\[\n{\\displaystyle {\\begin{aligned}\\operatorname {I}(x,y)&=-\\log _{2}\\left[p(x,y)\\right]=-\\log _{2}\\left[p(x)p(y)\\right]\\\\[5pt]&=-\\log _{2}\\left[p{(x)}\\right]-\\log _{2}\\left[p{(y)}\\right]\\\\[5pt]&=\\operatorname {I} (x)+\\operatorname {I} (y)\\end{aligned}}}\n\\]\n\nBeispiel 6.1 (Information eines wahrscheinlichen Ereignisses)  \n\n-log(99/100, base = 2)\n## [1] 0.01449957\n\nDie Information eines fast sicheren Ereignisses ist gering. \\(\\square\\)\n\n\nBeispiel 6.2 (Information eines unwahrscheinlichen Ereignisses)  \n\n-log(01/100, base = 2)\n## [1] 6.643856\n\nDie Information eines unwahrscheinlichen Ereignisses ist hoch. \\(\\square\\)\n\n\nBeispiel 6.3 (Information eines Würfelwurfs) Die Wahrscheinlichkeitsfunktion eines Würfel ist\n\\({\\displaystyle Pr(k)={\\begin{cases}{\\frac {1}{6}},&k\\in \\{1,2,3,4,5,6\\}\\\\0,&{\\text{ansonsten}}\\end{cases}}}\\)\nDie Wahrscheinlichkeit, eine 6 zu würfeln, ist \\(Pr(X=6) = \\frac{1}{6}\\).\nDie Information von \\(X=6\\) beträgt also\n\\(I(X=6) = -\\log_2 \\left( Pr(X=6) \\right) = -\\log_2(1/6) \\approx 2.585 \\, \\text{bits}\\).\n\n-log(1/6, base = 2)\n## [1] 2.584963\n\n\n\nBeispiel 6.4 (Information zweier Würfelwurfe) Die Wahrscheinlichkeit, mit zwei Würfeln, \\(X\\) und \\(Y\\), jeweils 6 zu würfeln, beträgt \\(Pr(X=6, Y=6) = \\frac{1}{36}\\)\nDie Information beträgt also\n\\(I(X=6, Y=6) = -\\log_2 \\left( Pr(6,6) \\right)\\)\n\n-log(1/36, base = 2)\n## [1] 5.169925\n\nAufgrund der Additivität der Information gilt\n\\(I(6,6) = I(6) + I(6)\\).\n\n-log(1/6, base = 2) + -log(1/6, base = 2)\n## [1] 5.169925\n\n\n\n6.2.2 Entropie\n(Informations)entropie ist die Summe der Informationen der Ereignisse einer Zufallsvariablen, \\(X\\).\n\nDefinition 6.3 (Informationsentropie) Informationsentropie ist so definiert:\n\\[H(p) = - \\text{E log} (p_i) = - \\sum_{i = 1}^n p_i \\text{log} (p_i) = E\\left[I(X) \\right]\\]\n\nDie Informationsentropie ist also die “mittlere” oder “erwartete Information einer Zufallsvariablen.\nDie Entropie eines Münzwurf-Versuchs (Bernoulli-Experiment) ist dann maximal (1 bit), wenn die Trefferwahrscheinlichkeit 50% beträgt: \\(Pr(X=x) = 1/2\\), s. Abb. Abbildung 6.1.\n\n\nAbbildung 6.1: Der Zusammenhang von Wahrscheinlichkeit und Entropie bei einem Bernoulli-Versuch, Quelle: Wikipedia, CC-BY-SA, Brona, Rubber Duck\n\n\n6.2.3 Gemeinsame Information\nDie gemeinsame Information (mutual information, MI) zweier Zufallsvariablen \\(X\\) und \\(Y\\), \\(I(X,Y)\\), quantifiziert die Informationsmenge, die man über \\(Y\\) erhält, wenn man \\(X\\) beobachtet. Mit anderen Worten: Die MI ist ein Maß des Zusammenhangs zweier (nominaler) Variablen. Im Gegensatz zur Korrelation ist die MI nicht auf lineare Abhängigkeiten beschränkt.\nDie MI quantifiziert den Unterschied zwischen der gemeinsamen Verteilung \\(Pr(X,Y)\\) und dem Produkt einer einzelnen4 Wahrscheinlichkeitsverteilungen, d.h. \\(Pr(X)\\) und \\(Pr(Y)\\).\nWenn die beiden Variablen (stochastisch) unabhängig5 sind, ist ihre gemeinsame Information Null:\n\\(I(X,Y) = 0 \\quad \\text{gdw} \\quad \\bot(X,Y)\\).\nDann gilt nämlich:\n\\(\\log \\left( \\frac{Pr(X,Y)} {Pr(X) \\cdot Pr(Y)} \\right) =\\log(1) = 0\\).\nDas macht intuitiv Sinn: Sind zwei Variablen unabhängig, so erfährt man nichts über die zweite, wenn man die erste kennt. So ist Kenntnis der Sternzeichens einer Person und ihrer Körpergröße unabhängig.\nDas Gegenteil ist auch wahr: Sind zwei Variablen voneinander komplett abhängig, so weiß man alles über die zweite, wenn man die erste kennt.\nDie gemeinsame Information kann man sich als Summe der einzelnen gemeinsamen Informationen von \\(XY\\) sehen (s. Tabelle 6.1):\n\n\n\n\n Tabelle 6.1:  Summe der punktweisen gemeinsamen Informationen \n  \n\n\n\n\n\\(I(X,Y) = \\Sigma_Y \\Sigma_y Pr(x,y) \\underbrace{\\log \\left( \\frac{Pr(X,Y)}{Pr(X) Pr(Y)} \\right)}_\\text{punktweise MI}\\)\nDie Summanden der gemeinsamen Information bezeichnet man auch als punktweise gemeinsame Information (pointwise mutual information, PMI), entsprechend, s. Gleichung 6.1. MI ist also der Erwartungswert der PMI.\n\\[{\\displaystyle \\operatorname {PMI} (x,y)\\equiv \\log_{2}{\\frac {p(x,y)}{p(x)p(y)}}=\\log _{2}{\\frac {p(x|y)}{p(x)}}=\\log _{2}{\\frac {p(y|x)}{p(y)}}}\n\\tag{6.1}\\]\nAndere Basen als log2 sind gebräuchlich, vor allem der natürliche Logarithmus.\n\nAnmerkung. Die zwei rechten Umformungen in Gleichung 6.1 basieren auf der Umformung der gemeinsamen Wahrscheinlichkeit.\nZur Erinnerung: \\(p(x,y) = p(y)p(x|y) = p(x)p(y|x)\\)\n\n\nBeispiel 6.5 (Interpretation der PMI) Sei \\(p(x) = p(y) = 1/10\\) und \\(p(x,y) = 1/10\\). Wären \\(x\\) und \\(y\\) unabhängig, dann wäre \\(p^{\\prime}(x,y) = p(x)p(y) = 1/100\\). Das Verhältnis der Produkte der einzelnen Wahrscheinlichkeit zur gemeinsamen Wahrscheinlichkeit wäre dann 1 und der Logarithmus von 1 ist 0. Das Verhältnis von 1 entspricht also der Unabhängigkeit. Ist das Verhältnis z.B. 5, so zeigt das eine gewisse Abhängigkeit an. Im obigen Beispiel gilt: \\(\\frac{1/20}{1/100}=5\\).\n\nDie MI wird auch über die sog. Kullback-Leibler-Divergenz definiert, die die Differenz zwischen gemeinsamer Wahrscheinlichkeitsfunktion bzw. -dichte und dem Produkt der jeweiligen Randwahrscheinlichkeiten bzw. -dichte angibt.\n\n6.2.4 Maximumentropie\n\nDefinition 6.4 (Maximumentropie) Die Verteilungsform, für die es die meisten Möglichkeiten (Pfade im Baumdiagramm) gibt, hat die höchste Informationsentropie.\n\nAbbildung 6.2 zeigt ein Baumdiagramm für einen 3-fachen Münzwurf. In den “Blättern” (Endknoten) sind die Ergebnisse des Experiments dargestellt sowie die Zufallsvariable \\(X\\), die die Anzahl der “Treffer” (Kopf) fasst. Wie man sieht, vereinen einige Werte der Zufallsvariable mehr Pfade auf sich als andere: Der Wert \\(X=1\\) vereinigt 3 Pfade (von 8) auf sich; der Wert \\(X=3\\) nur 1 Pfad.\n\n\nAbbildung 6.2: Pfade im Baumdiagramm: 3-facher Münzwurf\n\n\n6.2.4.1 Ilustration\nSagen wir, Sie stehen vor 5 Eimern und haben 10 Kieselsteine bei sich, die mit den Nummern 1 bis 10 beschriftet sind (McElreath 2020). Weil Sie nichts besseres zu tun haben, werfen Sie die Kiesel in die Eimer und zwar so, dass die Wahrscheinlichkeit für einen Kiesel in einen bestimmten Eimer zu landen für alle Eimer gleich ist. Sie werfen also Ihre 10 Kiesel und betrachten das Ergebnis; die Kiesel sind jetzt in einem bestimmten (zufälligen) Arrangement auf die Eimer verteilt. Jede Aufteilung (der 10 Kiesel in den 5 Eimern) ist gleich wahrscheinlich6 – die Wahrscheinlichkeit, dass alle 10 Kiesel in Eimer 1 landen ist also gleich hoch wie die Wahrscheinlichkeit, dass jeder Eimer einen Kiesel abkriegt. Jetzt kommt’s: Manche Arrangements können auf mehrere Arten erzielt werden als andere. So gibt es nur eine Aufteilung für alle 10 Kiesel in einem Eimer (Teildiagramm a, in Abbildung 6.3). Aber es gibt 90 Möglichkeiten, die Kiesel so aufzuteilen, dass 2 in Eimer 2 landen, 8 in Eimer 4 und 2 in Eimer 4, s. Teildiagramm b in Abbildung 6.3. Teildiagramme c bis e zeigen, dass die Anzahl der Aufteilungen schnell astronomisch hoch wird, wenn sich die Kiesel “gleichmäßiger” auf die Eimer verteilen. Die gleichmäßigste Aufteilung (Diagramm e) hat die größte Zahl an möglichen Anordnungen. Eine Aufteilung der Kiesel auch als Pfad durch ein Baumdiagramm beschrieben werden.\nHier sind ein paar verschiedene Arrangements, deren Anzahl von Aufteilungen wir hier anschauen:\n\nd &lt;-\n  tibble(a = c(0, 0, 10, 0, 0),\n         b = c(0, 1, 8, 1, 0),\n         c = c(0, 2, 6, 2, 0),\n         d = c(1, 2, 4, 2, 1),\n         e = 2) \nd\n\n\n\n  \n\n\n\n\n\n\n\nAbbildung 6.3: Anzahl von Aufteilungen der 10 Kiesel in 5 Eimer\n\n\n\nHier sind die Wahrscheinlichkeitsverteilungen der 5 Arrangements7:\n\nd %&gt;% \n  mutate_all(~. / sum(.))\n\n\n\n  \n\n\n\nDann kann man die Wahrscheinlichkeit einfach in Entropie umrechnen8:\n\nd %&gt;% \n  mutate_all(~ . / sum(.)) %&gt;% \n  gather() %&gt;% \n  group_by(key) %&gt;% \n  summarise(h = -sum(ifelse(value == 0, 0, value * log(value))))\n\n\n\n  \n\n\n\nDas ifelse dient nur dazu, eine Wahrscheinlichkeit von 0 eine Entropie von 0 zu verpassen9, denn sonst würden wir ein Problem rennen, wenn wir \\(log(0)\\) ausrechnen.\n\nlog(0)\n## [1] -Inf\n\n\n6.2.5 Kreuzentropie\n\nDefinition 6.5 Die Kreuzentropie (cross entropy) ist die Anzahl der benötigten Bits, um ein ein Ereignis aus der Verteilung \\(X\\) mit einer anderen Verteilung \\(Y\\) darzustellen, s. ?eq-cr. \\(\\square\\)\n\n\\[H(X,Y) = - \\sum_x X(x) \\cdot log(Y(x)) \\tag{6.2}\\]\nAnschaulich gesprochen gibt die Kreuzentropie die Differenz zwischen zwei Verteilugen an.\n\n6.2.6 Kullback-Leibler-Divergenz\nDie Kullback-Leibler-Divergenz, \\(D_{KL} (X\\, || \\, Y)\\), ist verwandt mit der Kreuzentropie, da\n\\[H(X,Y) = H(X) + D_{KL} (X\\, || \\, Y)\\]"
  },
  {
    "objectID": "065-Information.html#zufallstext-erkennen",
    "href": "065-Information.html#zufallstext-erkennen",
    "title": "\n6  Informationstheorie\n",
    "section": "\n6.3 Zufallstext erkennen",
    "text": "6.3 Zufallstext erkennen\n\n6.3.1 Entropie von Zufallstext\nKann man wohl Zufallstext maschinell erkennen? Sicher gibt es viele Ansätze, um das Problem anzugehen. Lassen Sie uns einen Ansatz erforschen. Erforschen heißt, wir erforschen für uns, es handelt sich um eine didaktische Übung, das Ziel ist nicht, Neuland für die Menschheit zu betreten.\nAber zuerst müssen wir überlegen, was “Zufallstext” bedeuten soll.\nNehmen wir uns dazu zuerst einen richtigen Text, ein Märchen von H.C. Andersen zum Beispiel. Nehmen wir das Erste aus der Liste in dem Tibble hcandersen_de, “das Feuerzeug”.\n\ndas_feuerzeug &lt;-\n  hcandersen_de  %&gt;% \n  filter(book == \"Das Feuerzeug\") %&gt;% \n  unnest_tokens(input = text, output = word) %&gt;% \n  pull(word) \n\nhead(das_feuerzeug)\n## [1] \"es\"         \"kam\"        \"ein\"        \"soldat\"     \"die\"       \n## [6] \"landstraße\"\n\nDas Märchen ist 2688 Wörter lang.\n\nwortliste &lt;- \nhcandersen_de  %&gt;% \n  filter(book == \"Das Feuerzeug\") %&gt;% \n  unnest_tokens(output = word, input = text) %&gt;% \n  pull(word) %&gt;% \n  unique()\n\nhead(wortliste)\n## [1] \"es\"         \"kam\"        \"ein\"        \"soldat\"     \"die\"       \n## [6] \"landstraße\"\n\nJetzt ziehen wir Stichproben (mit Zurücklegen) aus dieser Liste und erstellen damit eine Art Zufallstext.\n\nzufallstext &lt;- \n  sample(x = wortliste, size = length(das_feuerzeug)*10^2, replace = TRUE)\nhead(zufallstext)\n## [1] \"nächsten\" \"hunde\"    \"rundum\"   \"stumpf\"   \"wollte\"   \"sünder\"\n\nZählen wir, wie häufig jedes Wort vorkommt:\n\nzufallstext_count &lt;-\ntibble(zufallstext = zufallstext) %&gt;% \n  count(zufallstext)\n\nhead(zufallstext_count)\n\n\n\n  \n\n\n\nDer Häufigkeitsvektor von wortliste besteht nur aus Einsen, so haben wir ja gerade die Wortliste definiert:\n\nwortliste_count &lt;-\ntibble(wortliste = wortliste) %&gt;% \n  count(wortliste)\n\nhead(wortliste_count)\n\n\n\n  \n\n\n\nDaher ist ihre Informationsentropy maximal.\n\nentropy(wortliste_count$n, unit = \"log2\")\n## [1] 9.47978\n\nDie Häufigkeiten der Wörter in zufallstext hat eine hohe Entropie.\n\nentropy(zufallstext_count$n, unit = \"log2\")\n## [1] 9.477878\n\nZählen wir die Häufigkeiten in der Geschichte “Das Feuerzeug”.\n\ndas_feuerzeug_count &lt;-\n  tibble(text = das_feuerzeug) %&gt;% \n  count(text)\n\nhead(das_feuerzeug_count)\n\n\n\n  \n\n\n\nUnd berechnen dann die Entropie:\n\nentropy(das_feuerzeug_count$n, unit = \"log2\")\n## [1] 8.075194\n\nDer Zufallstext hat also eine höhere Entropie als der echte Märchentext. Der Zufallstext ist also gleichverteilter in den Worthäufigkeiten.\nPro Bit weniger Entropie halbiert sich die Anzahl der Möglichkeiten einer Häufigkeitsverteilung.\n\n6.3.2 MI von Zufallstext\nLeft as an exercises for the reader10 🥳."
  },
  {
    "objectID": "065-Information.html#literatur",
    "href": "065-Information.html#literatur",
    "title": "\n6  Informationstheorie\n",
    "section": "\n6.4 Literatur",
    "text": "6.4 Literatur\nStone (2019) bietet einen nützlichen Einstieg in das Thema der Informationsentropie. Shannons (1948) berühmter Artikel setzt höhere Ansprüche.\nEs gibt eine Reihe nützlicher (und recht informationsdichter) Wikipedia-Einträge zum Thema Informationstheorie.\n\nWikipedia: Pointwise Mutual Information\nWikipedia: Mutual information\nWikipedia: Information theory\n\n\n\n\n\nKurz, A. Solomon. 2021. Statistical Rethinking with Brms, Ggplot2, and the Tidyverse: Second Edition. https://bookdown.org/content/4857/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2. Aufl. CRC Texts in Statistical Science. Boca Raton: Taylor and Francis, CRC Press.\n\n\nShannon, C. E. 1948. „A Mathematical Theory of Communication“. Bell System Technical Journal 27 (3): 379–423. https://doi.org/10.1002/j.1538-7305.1948.tb01338.x.\n\n\nStone, James V. 2019. „Information Theory: A Tutorial Introduction“. 13. Juni 2019. http://arxiv.org/abs/1802.05968."
  },
  {
    "objectID": "065-Information.html#footnotes",
    "href": "065-Information.html#footnotes",
    "title": "\n6  Informationstheorie\n",
    "section": "",
    "text": "Desiderata, sagt man↩︎\noder shannon↩︎\nwie immer, als fair angenommen, wenn sonst nichts anderes angegeben ist↩︎\nauch als marginalen Wahrscheinlichkeiten oder Randwahrscheinlichkeiten bezeichnet↩︎\nFür stochastische Unabhängigkeit kann das Zeichen \\(\\bot\\) verwendet werden↩︎\nso ähnlich wie mit den Lottozahlen↩︎\nIst das nicht eine elegante Syntax?! Solomon Kurz, der Autor der Syntax, hielt sich nicht an die aktuelle Version von dplyr; mutate_all ist eigentlich überholt zugunsten von mutate mit across, aber die Prägnanz der Syntax hier ist schon beeindruckend, wie ich finde.↩︎\nSyntax aus Kurz (2021)↩︎\nRegel von L’Hopital↩︎\nVgl. hier↩︎"
  },
  {
    "objectID": "050-word-embedding.html#vorab",
    "href": "050-word-embedding.html#vorab",
    "title": "\n7  Word Embedding\n",
    "section": "\n7.1 Vorab",
    "text": "7.1 Vorab\n\n7.1.1 Lernziele\n\n\nDie Erstellung von Word-Embeddings anhand grundlegender R-Funktionen erläutern können.\n\n7.1.2 Vorbereitung\n\n\nArbeiten Sie Hvitfeldt und Silge (2021), Kap. 5 durch.\n\n7.1.3 Benötigte R-Pakete\n\nlibrary(tidyverse)\nlibrary(easystats)  # Komfort für deskriptive Statistiken, wie `describe_distribution`\nlibrary(tidytext)\nlibrary(hcandersenr)   # Textdaten\nlibrary(slider)  # slide\nlibrary(widyr)  # pairwise_pmi\nlibrary(furrr)  # mehrere Kerne gleichzeitig nutzen\nlibrary(textdata)  # Worteinbettungen, vorgekocht\nlibrary(entropy)  # Entropie berechnen\nlibrary(widyr)  # Ähnlichkeit berechnen mit widyr_svd\nlibrary(furrr)  # Mehrere Kerne gleichzeitig"
  },
  {
    "objectID": "050-word-embedding.html#daten",
    "href": "050-word-embedding.html#daten",
    "title": "\n7  Word Embedding\n",
    "section": "\n7.2 Daten",
    "text": "7.2 Daten\n\n7.2.1 Complaints-Datensatz\nDer Datensatz complaints stammt aus dieser Quelle.\nDen Datensatz complaints kann man hier herunterladen. Im Buch ist die Webseite nicht (direkt?) angegeben. Die Datei ist mit gz gepackt; read_csv sollte das automatisch entpacken. Achtung: Die Datei ist recht groß.\n\nd_path &lt;- \"https://github.com/EmilHvitfeldt/smltar/raw/master/data/complaints.csv.gz\"\ncomplaints &lt;- read_csv(d_path)\n\nGeschickter als jedes Mal die Datei herunterzuladen, ist es, sie einmal herunterzuladen, und dann lokal zu speichern, etwa im Unterordner data des RStudio-Projektordners.\nNach dem Importieren wird der Datensatz in eine Tidy-Form gebracht (mit unnest_tokens) und dann verschachtelt, mit nest.\n\n7.2.2 Complaints verkürzt und geschachtelt\nUm ein Herumprobieren zu erleichtern, ist hier der Datensatz complaints in zwei verkürzten Formen bereitgestellt:\n\nnested_words2_path &lt;- \"https://github.com/sebastiansauer/datascience-text/raw/main/data/nested_words2.rds\"\nnested_words3_path &lt;- \"https://github.com/sebastiansauer/datascience-text/raw/main/data/nested_words3.rds\"\n\nnested_words2 enthält die ersten 10% des Datensatz nested_wordsund ist gut 4 MB groß (mit gz gezippt); er besteht aus ca. 11 Tausend Beschwerden. nested_words3 enthält nur die ersten 10 Beschwerden, was ihn gut zum Handhaben macht.\nBeide sind verschachtelt und aus tidy_complaints (s. Kap. 5.1) hervorgegangen.\n\nnested_words3 &lt;- read_rds(nested_words3_path)\n\nDas sieht dann so aus:\n\nnested_words3 %&gt;% \n  head(3)\n\n\n\n  \n\n\n\nWerfen wir einen Blick in den TExt der ersten Beschwerde des Datensatzes mit der ID nested_words3_path$complaint_id[1].\n\nbeschwerde1_text &lt;- nested_words3$words[[1]]\n\nDas ist ein Tibble mit einer Spalte und 17 Wörtern; da wir schon auf Unigramme aufgeteilt haben, ist jedes Wort ein Element des Vektors word:\n\nbeschwerde1_text %&gt;% \n  head()\n\n\n\n  \n\n\n\n\nbeschwerde1_text$word\n##  [1] \"systems\"    \"inc\"        \"is\"         \"trying\"     \"to\"        \n##  [6] \"collect\"    \"a\"          \"debt\"       \"that\"       \"is\"        \n## [11] \"not\"        \"mine\"       \"not\"        \"owed\"       \"and\"       \n## [16] \"is\"         \"inaccurate\""
  },
  {
    "objectID": "050-word-embedding.html#wordembeddings-selber-erstellen",
    "href": "050-word-embedding.html#wordembeddings-selber-erstellen",
    "title": "\n7  Word Embedding\n",
    "section": "\n7.3 Wordembeddings selber erstellen",
    "text": "7.3 Wordembeddings selber erstellen\n\n7.3.1 PMI berechnen\nRufen Sie sich die Definition der PMI ins Gedächtnis, s. Gleichung 6.1.\nMit R kann man die PMI z.B. so berechnen, s. ? pairwise_pmi aus dem Paket widyr.\nZum Paket widyr von Robinson und Silge:\n\nThis package wraps the pattern of un-tidying data into a wide matrix, performing some processing, then turning it back into a tidy form. This is useful for several mathematical operations such as co-occurrence counts, correlations, or clustering that are best done on a wide matrix.\n\nQuelle\nErzeugen wir uns Dummy-Daten:\n\ndat &lt;- tibble(feature = rep(1:5, each = 2),\n              item = c(\"a\", \"b\",\n                       \"a\", \"c\",\n                       \"a\", \"c\",\n                       \"b\", \"e\",\n                       \"b\", \"f\"))\n\ndat\n\n\n\n  \n\n\n\nAus der Hilfe der Funktion:\n\nFind pointwise mutual information of pairs of items in a column, based on a “feature” column that links them together. This is an example of the spread-operate-retidy pattern.\n\nDie Argumente der Funktion sind:\nitem\nItem to compare; will end up in item1 and item2 columns\nfeature\nColumn describing the feature that links one item to others\nManche Berechnungen (Operationen) lassen sich vielleicht leichter nicht in der Tidy-Form (Langform), sondern in der “breiten” oder Matrixform ausführen. Wandeln wir mal dat von der Langform in die Breitform um:\n\ntable(dat$item, dat$feature)\n##    \n##     1 2 3 4 5\n##   a 1 1 1 0 0\n##   b 1 0 0 1 1\n##   c 0 1 1 0 0\n##   e 0 0 0 1 0\n##   f 0 0 0 0 1\n\nSilge und Robinson verdeutlichen das Prinzip von widyr so, s. Abbildung 7.1.\n\n\nAbbildung 7.1: Die Funktionsweise von widyr, Quelle: Silge und Robinson\n\n(Vgl. auch die Erklärung hier.)\nBauen wir das mal von Hand nach.\nRandwahrscheinlichkeiten von a und c sowie deren Produkt, p_a_p_c:\n\np_a &lt;- 3/5\np_c &lt;- 2/5\n\np_a_p_c &lt;- p_a * p_c\np_a_p_c\n## [1] 0.24\n\nGemeinsame Wahrscheinlichkeit von a und c:\n\np_ac &lt;- 2/5\n\nPMI von Hand berechnet:\n\nlog(p_ac/p_a_p_c)\n## [1] 0.5108256\n\nMan beachte, dass hier als Basis \\(e\\), der natürliche Logarithmus, verwendet wurde (nicht 2).\nJetzt berechnen wir die PMI mit pairwise_pmi.\n\npairwise_pmi(dat, item = item, feature = feature)\n\n\n\n  \n\n\n\nWie man sieht, entspricht unserer Berechnung von Hand der Berechnung mit pairwise_pmi.\n\n7.3.2 Sliding\nSliding ist ein interessantes Konzept, aber man braucht vielleicht etwas Zeit, um sein Hirn um das Konzept zu wickeln…\nHier eine Illustration:\n\ntxt_vec &lt;- \"Das ist ein Test, von dem nicht viel zu erwarten ist\"\n\nslider::slide(txt_vec, ~ .x, .before = 2)\n## [[1]]\n## [1] \"Das ist ein Test, von dem nicht viel zu erwarten ist\"\n\nOh, da passiert nichts?! Kaputt? Nein, wir müssen jedes Wort als ein Element des Vektors auffassen.\n\ntxt_df &lt;-\n  tibble(txt = txt_vec) %&gt;% \n  unnest_tokens(input = txt, output = word)\n\nhead(txt_df)\n\n\n\n  \n\n\n\n\nslider::slide(txt_df$word, ~ .x, .before = 2)\n## [[1]]\n## [1] \"das\"\n## \n## [[2]]\n## [1] \"das\" \"ist\"\n## \n## [[3]]\n## [1] \"das\" \"ist\" \"ein\"\n## \n## [[4]]\n## [1] \"ist\"  \"ein\"  \"test\"\n## \n## [[5]]\n## [1] \"ein\"  \"test\" \"von\" \n## \n## [[6]]\n## [1] \"test\" \"von\"  \"dem\" \n## \n## [[7]]\n## [1] \"von\"   \"dem\"   \"nicht\"\n## \n## [[8]]\n## [1] \"dem\"   \"nicht\" \"viel\" \n## \n## [[9]]\n## [1] \"nicht\" \"viel\"  \"zu\"   \n## \n## [[10]]\n## [1] \"viel\"     \"zu\"       \"erwarten\"\n## \n## [[11]]\n## [1] \"zu\"       \"erwarten\" \"ist\"\n\nAh!\nDas Aufteilen in einzelne Wörter pro Element des Vektors könnte man auch so erreichen:\n\ntxt_vec2 &lt;- str_split(txt_vec, pattern = boundary(\"word\")) %&gt;% \n  simplify() \n\nslide(txt_vec2, .f = ~.x, .before = 2)\n## [[1]]\n## [1] \"Das\"\n## \n## [[2]]\n## [1] \"Das\" \"ist\"\n## \n## [[3]]\n## [1] \"Das\" \"ist\" \"ein\"\n## \n## [[4]]\n## [1] \"ist\"  \"ein\"  \"Test\"\n## \n## [[5]]\n## [1] \"ein\"  \"Test\" \"von\" \n## \n## [[6]]\n## [1] \"Test\" \"von\"  \"dem\" \n## \n## [[7]]\n## [1] \"von\"   \"dem\"   \"nicht\"\n## \n## [[8]]\n## [1] \"dem\"   \"nicht\" \"viel\" \n## \n## [[9]]\n## [1] \"nicht\" \"viel\"  \"zu\"   \n## \n## [[10]]\n## [1] \"viel\"     \"zu\"       \"erwarten\"\n## \n## [[11]]\n## [1] \"zu\"       \"erwarten\" \"ist\"\n\nIn unserem Beispiel mit den Beschwerden:\n\nslide(beschwerde1_text$word,  ~.x, .before = 2)\n## [[1]]\n## [1] \"systems\"\n## \n## [[2]]\n## [1] \"systems\" \"inc\"    \n## \n## [[3]]\n## [1] \"systems\" \"inc\"     \"is\"     \n## \n## [[4]]\n## [1] \"inc\"    \"is\"     \"trying\"\n## \n## [[5]]\n## [1] \"is\"     \"trying\" \"to\"    \n## \n## [[6]]\n## [1] \"trying\"  \"to\"      \"collect\"\n## \n## [[7]]\n## [1] \"to\"      \"collect\" \"a\"      \n## \n## [[8]]\n## [1] \"collect\" \"a\"       \"debt\"   \n## \n## [[9]]\n## [1] \"a\"    \"debt\" \"that\"\n## \n## [[10]]\n## [1] \"debt\" \"that\" \"is\"  \n## \n## [[11]]\n## [1] \"that\" \"is\"   \"not\" \n## \n## [[12]]\n## [1] \"is\"   \"not\"  \"mine\"\n## \n## [[13]]\n## [1] \"not\"  \"mine\" \"not\" \n## \n## [[14]]\n## [1] \"mine\" \"not\"  \"owed\"\n## \n## [[15]]\n## [1] \"not\"  \"owed\" \"and\" \n## \n## [[16]]\n## [1] \"owed\" \"and\"  \"is\"  \n## \n## [[17]]\n## [1] \"and\"        \"is\"         \"inaccurate\"\n\n\n7.3.3 Funktion slide_windows\n\nDie Funktion slide_windows im Kapitel 5.2 ist recht kompliziert. In solchen Fällen ist es hilfreich, sich jeden Schritt einzeln ausführen zu lassen. Das machen wir jetzt mal.\nHier ist die Syntax der Funktion slide_windows:\n\nslide_windows &lt;- function(tbl, window_size) {\n  skipgrams &lt;- slider::slide(\n    tbl, \n    ~.x,  # Syntax ähnlich zu purrr::map()\n    .after = window_size - 1, \n    .step = 1, \n    .complete = TRUE\n  )\n  \n  safe_mutate &lt;- safely(mutate)\n  \n  out &lt;- map2(skipgrams,\n              1:length(skipgrams),\n              ~ safe_mutate(.x, window_id = .y))\n  \n  out %&gt;%\n    transpose() %&gt;%\n    pluck(\"result\") %&gt;%\n    compact() %&gt;%\n    bind_rows()\n}\n\nErschwerend kommt eine große Datenmenge und eine lange Berechnungszeit dazu, was das Debuggen (Nachvollziehen und Durchdenken) der Schritte zusätzlich erschwert. In solchen Fällen hilft die goldene Regel: Mach es dir so einfach wie möglich (aber nicht einfacher). Wir nutzen also den stark verkleinerten Datensatz nested_words3, den wir oben importiert haben.\nZuerst erlauben wir mal, dasss unsere R-Session mehrere Kerne benutzen darf.\n\nplan(multisession)  ## for parallel processing\n\nDie Funktion slide_windows ist recht kompliziert. Es hilft oft, sich mit debug(fun) eine Funktion Schritt für Schritt anzuschauen.\nGehen wir Schritt für Schritt durch die Syntax von slide_windows.\nWerfen wir einen Blick in words, erstes Element (ein Tibble mit einer Spalte). Denn die einzelnen Elemente vonwordswerden an die Funktionslide_windows` als “Futter” übergeben.\n\nfutter1 &lt;- nested_words3[[\"words\"]][[1]]\nfutter1\n\n\n\n  \n\n\n\nDas ist der Text der ersten Beschwerde.\nOkay, also dann geht’s los durch die einzelnen Schritte der Funktion slide_windows.\nZunächst holen wir uns die “Fenster” oder “Skipgrams”:\n\nskipgrams1 &lt;- slider::slide(\n   futter1, \n    ~.x, \n    .after = 3, \n    .step = 1, \n    .complete = TRUE\n  )\n\nBei slide(tbl, ~.x) geben wir die Funktion an, die auf tbl angewendet werden soll. Daher auch die Tilde, die uns von purrr::map() her bekannt ist. In unserem Fall wollen wir nur die Elemente auslesen; Elemente auslesen erreicht man, in dem man sie mit Namen anspricht, in diesem Fall mit dem Platzhalter .x.\nJedes Element von skipgrams1 ist ein 4*1-Tibble und ist ein Skripgram.\n\nskipgrams1 %&gt;% str()\n## List of 17\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"systems\" \"inc\" \"is\" \"trying\"\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"inc\" \"is\" \"trying\" \"to\"\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"is\" \"trying\" \"to\" \"collect\"\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"trying\" \"to\" \"collect\" \"a\"\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"to\" \"collect\" \"a\" \"debt\"\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"collect\" \"a\" \"debt\" \"that\"\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"a\" \"debt\" \"that\" \"is\"\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"debt\" \"that\" \"is\" \"not\"\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"that\" \"is\" \"not\" \"mine\"\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"is\" \"not\" \"mine\" \"not\"\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"not\" \"mine\" \"not\" \"owed\"\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"mine\" \"not\" \"owed\" \"and\"\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"not\" \"owed\" \"and\" \"is\"\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"owed\" \"and\" \"is\" \"inaccurate\"\n##  $ : NULL\n##  $ : NULL\n##  $ : NULL\n\nDas zweite Skipgram von skipgrams1 enthält, naja, das zweite Skipgram.\n\nskipgrams1[[2]] %&gt;% str()\n## tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##  $ word: chr [1:4] \"inc\" \"is\" \"trying\" \"to\"\n\nUnd so weiter.\nOkay, weiter im Programm. Jetzt mappen wir das Erstellen der Skipgrams\n\nsafe_mutate &lt;- safely(mutate)\n  \nout1 &lt;- map2(skipgrams1,\n             1:length(skipgrams1),\n             ~ safe_mutate(.x, window_id = .y))\n  \nout1 %&gt;% \n  head(2) %&gt;% \n  str()\n## List of 2\n##  $ :List of 2\n##   ..$ result: tibble [4 × 2] (S3: tbl_df/tbl/data.frame)\n##   .. ..$ word     : chr [1:4] \"systems\" \"inc\" \"is\" \"trying\"\n##   .. ..$ window_id: int [1:4] 1 1 1 1\n##   ..$ error : NULL\n##  $ :List of 2\n##   ..$ result: tibble [4 × 2] (S3: tbl_df/tbl/data.frame)\n##   .. ..$ word     : chr [1:4] \"inc\" \"is\" \"trying\" \"to\"\n##   .. ..$ window_id: int [1:4] 2 2 2 2\n##   ..$ error : NULL\n\nout1 ist eine Liste mit 17 Elementen; jedes Element mit jeweils zwei Elementen: Den Ergebnissen und ob es einen Fehler gab bei safe_mutate. Die 10 Elemente entsprechen den 10 Skipgrams. Wir können aber out1 auch “drehen”, transponieren genauer gesagt. so dass wir eine Liste mit zwei Elementen bekommen: das erste Element hat die (zehn) Ergebnisse (nämlich die Skipgrams) und das zweite Elemente, ob es Fehler gab.\nDas Prinzip des Transponierens ist in Abbildung 7.2 dargestellt.\n\n\nAbbildung 7.2: Transponieren einer Matrix (“Tabelle”)\n\n\nout2 &lt;-\nout1 %&gt;%\n  transpose() \n\nPuh, das ist schon anstrengendes Datenyoga…\nAber jetzt ist es einfach. Wir ziehen das erste der beiden Elemente, die Ergebnisse heraus (pluck), entfernen leere Elemente (compact) und machen einen Tibble daraus (bind_rows):\n\nout2 %&gt;% \n  pluck(\"result\") %&gt;%\n  compact() %&gt;%\n  bind_rows() %&gt;% \n  head()\n\n\n\n  \n\n\n\nGeschafft!\n\n7.3.4 Ähnlichkeit berechnen\nNachdem wir jetzt slide_windows kennen, schauen wir uns die nächsten Schritte an:\n\ntidy_pmi1 &lt;- nested_words3 %&gt;%  # &lt;--- Kleiner Datensatz!\n  mutate(words = future_map(words, slide_windows, 4L))\n\nWir werden slide_windows auf die Liste words an, die die Beschwerden enthält. Für jede Beschwerde erstellen wir die Skipgrams; diese Schleife wird realisiert über map bzw. future_map, die uns erlaubt, mehrere Kerne des Rechners gleichzeitig zu nutzen, damit es schneller geht.\nHier sehen wir z.B. die Skipgram-IDs der ersten Beschwerde.\n\ntidy_pmi1[[\"words\"]][[1]] %&gt;% \n  head()\n\n\n\n  \n\n\n\nGenestet siehst es so aus:\n\ntidy_pmi1 %&gt;% \n  head(1)\n\n\n\n  \n\n\n\nDie Listenspalte entschachteln wir mal:\n\ntidy_pmi2 &lt;- tidy_pmi1 %&gt;% \n  unnest(words)  # entschachtele\n\ntidy_pmi2 %&gt;% \n  head()\n\n\n\n  \n\n\n\nZum Berechnen der Ähnlichkeit brauchen wir eineindeutige IDs, nach dem Prinzip “1. Skipgram der 1. Beschwerde” etc:\n\ntidy_pmi3 &lt;- tidy_pmi2 %&gt;% \n  unite(window_id, complaint_id, window_id)  # führe Spalten zusammen\n\ntidy_pmi3 %&gt;% \n  head()\n\n\n\n  \n\n\n\nSchließlich berechnen wir die Ähnlichkeit mit pairwise_pmi, das hatten wir uns oben schon mal näher angeschaut:\n\ntidy_pmi4 &lt;- tidy_pmi3 %&gt;% \n  pairwise_pmi(word, window_id)  # berechne Ähnlichkeit\n\ntidy_pmi &lt;- tidy_pmi4  # mit dem Objekt arbeiten wir dann weiter\n\ntidy_pmi %&gt;% \n  head()\n\n\n\n  \n\n\n\n\n7.3.5 SVD\nDie Singulärwertzerlegung (singular value decomposition, SVD) ist ein Verfahren zur Verringerung der Spaltenzahl (Dimension), vergleichbar zur Faktorenanalyse. Zur Anschaulichkeit - und ohne substanziellen Hintergrund - sei folgendes Beispiel genannt: Die Verben “gehen”, “rennen”, “laufen”, “schwimmen”, “fahren”, “rutschen” könnten zu einer gemeinsamen Dimension, etwa “fortbewegen” reduziert werden. Jedes einzelne der eingehenden Verben erhält eine Zahl von 0 bis 1, das die konzeptionelle Nähe des Verbs zur “dahinterliegenden” Dimension (fortbewegen) quantifiziert; die Zahl nennt man auch die “Ladung” des Items (Worts) auf die Dimension. Sagen wir, wir identifizieren 10 Dimensionen. Man erhält dann für jedes unique Wort im Corpus einen Vektor mit den Ladungen auf die Dimensionen. Im genannten Beispiel wäre es ein 10-stelliger Vektor. So wie ein 3-stelliger Vektor die Position einer Biene im Raum beschreibt1, beschreibt hier unser 10-stelliger Vektor die “Position” eines Worts in unserem Einbettungsvektor.\nDie Syntax dazu ist dieses Mal einfach:\n\ntidy_word_vectors &lt;- \n  tidy_pmi %&gt;%\n  widely_svd(\n    item1, item2, pmi,\n    nv = 100\n  )\n\ntidy_word_vectors %&gt;% \n  (head)\n\n\n\n  \n\n\n\nMit nv = 100 haben wir die Anzahl (n) der Dimensionen (Variablen, v) auf 100 bestimmt.\n\n7.3.6 Wortähnlichkeit\nJetzt, da wir mit der SVD jedes Wort in einem Koordinatensystem verortet haben, können wir die Abstände der Wörter im Koordinatensystem bestimmen. Das geht mit Hilfe des alten Pythagoras, s. Abbildung 7.3. Der Abstand, den man mit Hilfe des Satz des Pythagoras berechnet, nennt man auch euklidische Distanz.\n\n\nAbbildung 7.3: Euklidische Distanz in 2D, Quelle: Wikipedia, CC BY4.0, Kmhkmh\n\nOkay, wir sind in einem Raum mit vielen Dimensionen, was mein Hirn nicht mitmacht, aber der Algebra ist das egal. Pythagoras’ Satz lässt sich genauso anwenden, wenn es mehr als Dimensionen sind.\nDie Autoren basteln sich selber eine Funktion in Kap. 5.3, aber der Einfachheit halber nehme ich (erstmal) die entsprechende Funktion aus widyr:\n\nword_neighbors &lt;- \ntidy_word_vectors %&gt;% \n  pairwise_dist(item1, dimension, value)\n\nword_neighbors %&gt;% \n  head()\n\n\n\n  \n\n\n\nSchauen wir uns ein Beispiel an. Was sind die Nachbarn von “inaccurate”?\n\nword_neighbors %&gt;% \n  filter(item1 == \"inaccurate\") %&gt;% \n  arrange(distance) %&gt;% \n  head()\n\n\n\n  \n\n\n\nHier ist die Datenmenge zu klein, um vernünftige Schlüsse zu ziehen. Aber “incorrectly”, “correct”, “balance” sind wohl plausible Nachbarn von “inaccurate”.\n\n7.3.7 Cosinus-Ähnlichkeit\nDie Nähe zweier Vektoren lässt sich, neben der euklidischen Distanz, auch z.B. über die Cosinus-Ähnlichkeit (Cosine similarity) berechnen, vgl. auch Abbildung 7.4:\n\n\nAbbildung 7.4: Die Cosinus-Ähnlichkeit zweier Vektoren\n\nQuelle: Mazin07, Lizenz: PD\n\\[{\\displaystyle {\\text{Cosinus-Ähnlichkeit}}=S_{C}(A,B):=\\cos(\\theta )={\\mathbf {A} \\cdot \\mathbf {B}  \\over \\|\\mathbf {A} \\|\\|\\mathbf {B} \\|}={\\frac {\\sum \\limits _{i=1}^{n}{A_{i}B_{i}}}{{\\sqrt {\\sum \\limits _{i=1}^{n}{A_{i}^{2}}}}{\\sqrt {\\sum \\limits _{i=1}^{n}{B_{i}^{2}}}}}},}\\]\nwobei \\(A\\) und \\(B\\) zwei Vektoren sind und \\(\\|\\mathbf {A} \\|\\) das Skalarprodukt von A (und B genauso). Das Skalarprodukt von \\(\\color {red} {a = {\\displaystyle [a_{1},a_{2},\\cdots ,a_{n}]}}\\) und \\(\\color {blue} {b = {\\displaystyle [b_{1},b_{2},\\cdots ,b_{n}]}}\\) ist so definiert:\n\\[{\\displaystyle \\mathbf {\\color {red}a} \\cdot \\mathbf {\\color {blue}b} =\\sum _{i=1}^{n}{\\color {red}a}_{i}{\\color {blue}b}_{i}={\\color {red}a}_{1}{\\color {blue}b}_{1}+{\\color {red}a}_{2}{\\color {blue}b}_{2}+\\cdots +{\\color {red}a}_{n}{\\color {blue}b}_{n}}\\]\nEntsprechend ist die Funktion nearest_neighbors zu verstehen aus Kap. 5.3:\n\nnearest_neighbors &lt;- function(df, token) {\n  df %&gt;%\n    widely(\n      ~ {\n        y &lt;- .[rep(token, nrow(.)), ]\n        res &lt;- rowSums(. * y) / \n          (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))\n        \n        matrix(res, ncol = 1, dimnames = list(x = names(res)))\n      },\n      sort = TRUE\n    )(item1, dimension, value) %&gt;%\n    select(-item2)\n}\n\nWobei mit widely zuerst noch von der Langform in die Breitform umformatiert wird, da die Breitform von der Formel verlangt wird bzw. das Rechnen einfacher macht.\nDer eine Vektor ist das Embedding des Tokens, der andere Vektor ist das mittlere Embedding über alle Tokens des Corpus. Wenn die Anzahl der Elemente konstant bleibt, kann man sich das Teilen durch \\(n\\) schenken, wenn man einen Mittelwert berechnen; so hält es auch die Syntax von nearest_neighbors.\nEin nützlicher Post zur Cosinus-Ähnlichkeit findet sich hier. Dieses Bild zeigt das Konzept der Cosinus-Ähnlichkeit anschaulich.\nZur Erinnerung: Der Cosinus eines Winkels ist definiert als Verhältnis der Länge der Ankathete (das ist jene Kathete, die einen Schenkel des Winkels bildet) zur Länge der Hypotenuse2 in einem rechtwinkligen, vgl. Abbildung 7.5.\n\n\nAbbildung 7.5: Ein rechtwinkliges Dreieck als Grundlage der trigonometrischen Funktionen\n\nAlso: \\({\\displaystyle \\cos \\alpha ={\\frac {b}{c}}}\\)\nQuelle: PaterSigmund, Wikipedia,dre CC-BY-SA 2.5\nHilfreich ist auch die Visualisierung von Sinus und Cosinus am Einheitskreis; gerne animiert betrachten."
  },
  {
    "objectID": "050-word-embedding.html#word-embeddings-vorgekocht",
    "href": "050-word-embedding.html#word-embeddings-vorgekocht",
    "title": "\n7  Word Embedding\n",
    "section": "\n7.4 Word-Embeddings vorgekocht",
    "text": "7.4 Word-Embeddings vorgekocht\n\n7.4.1 Glove6B\nIn Kap. 5.4 schreiben die Autoren:\n\nIf your data set is too small, you typically cannot train reliable word embeddings.\n\nEin paar Millionen Wörter sollte der Corpus schon enthalten, so die Autoren. Da solche “Worteinbettungen” (word embedings) aufwändig zu erstellen sind, kann man fertige, “vorgekochte” Produkte nutzen.\nGlove6B wurde anhand von Wikipedia und anderen Datenquellen erstellt (Pennington, Socher, und Manning 2014).\n\n\n\n\n\n\nHinweis\n\n\n\nDie zugehörigen Daten sind recht groß; für glove6b (Pennington, Socher, und Manning 2014) ist fast ein Gigabyte fällig. Sie sollten sich die Daten in einem ruhigen Moment (mit stabiler Internetverbindung) herunterladen und in einem Verzeichnis meiner Wahl abgespeichert (datasets). Da bei mir Download abbrach, als ich embedding_glove6b(dimensions = 100) aufrief, habe ich die Daten manuell heruntergeladen, s.u.\n\n\n\nglove6b &lt;- \n  embedding_glove6b(dir = \"~/datasets\", dimensions = 50, manual_download = TRUE)\n\n\nglove6b &lt;- read_rds(\"/Users/sebastiansaueruser/datasets/glove6b/glove_6b_50.rds\")\n\n\nglove6b %&gt;% \n  select(1:5) %&gt;% \n  head()\n\n\n\n  \n\n\n\nDie ersten paar Tokens sind:\n\nglove6b$token %&gt;% head(20)\n##  [1] \"the\"  \",\"    \".\"    \"of\"   \"to\"   \"and\"  \"in\"   \"a\"    \"\\\"\"   \"'s\"  \n## [11] \"for\"  \"-\"    \"that\" \"on\"   \"is\"   \"was\"  \"said\" \"with\" \"he\"   \"as\"\n\nIn eine Tidyform bringen:\n\ntidy_glove &lt;- \n  glove6b %&gt;%\n  pivot_longer(contains(\"d\"),\n               names_to = \"dimension\") %&gt;%\n  rename(item1 = token)\n\nhead(tidy_glove)\n\n\n\n  \n\n\n\nGanz schön groß:\n\ndim(glove6b)\n## [1] 400000     51\n\n\nobject.size(tidy_glove)\n## 503834736 bytes\n\nIn Megabyte3\n\nobject.size(tidy_glove) / 2^20\n## 480.5 bytes\n\nEinfacher und genauer geht es so:\n\npryr::object_size(tidy_glove)\n## 503.83 MB\n\n\npryr::mem_used()\n## 841 MB\n\nUm Speicher zu sparen, könnte man glove6b wieder direkt löschen, wenn man nur mit der Tidyform weiterarbeitet.\n\nrm(glove6b)\n\nJetzt können wir wieder nach Nachbarn fragen, im euklidischen Sinne, wie oben. Probieren wir aus, welche Wörter nah zu “inaccurate” stehen.\n\n\n\n\n\n\nHinweis\n\n\n\nWie wir oben gesehen haben, ist der Datensatz riesig4, was die Berechnungen (zeitaufwändig) und damit nervig machen können. Darüber hinaus kann es nötig sein, dass Sie mehr Speicher auf Ihrem Computer zur Verfügung stellen müssen5. Wir müssen noch maximum_size = NULL, um das Jonglieren mit riesigen Matrixen zu erlauben. Möge der Gott der RAMs und Arbeitsspeicher uns gnädig sein!\n\n\nMit pairwise_dist dauert die Berechnung sehr lange und braucht wohl sehr viel Speicher. Mitunter kam folgender Fehler auf: “R error: vector memory exhausted (limit reached?)”.\n\nword_neighbors_glove6b &lt;- \ntidy_glove %&gt;% \n  slice_head(prop = .1) %&gt;% \n  pairwise_dist(item1, dimension, value, maximum_size = NULL)\n\nhead(word_neighbors_glove6b)\n\ntidy_glove %&gt;% \n  filter(item1 == \"inaccurate\") %&gt;% \n  arrange(-value) %&gt;% \n  slice_head(n = 5)\n\nDeswegen probieren wir doch die Funktion nearest_neighbors, so wie es im Buch vorgeschlagen wird, s. Kap 5.3.\n\nnearest_neighbors &lt;- function(df, token) {\n  df %&gt;%\n    widely(\n      ~ {\n        y &lt;- .[rep(token, nrow(.)), ]\n        res &lt;- rowSums(. * y) / \n          (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))\n        matrix(res, ncol = 1, dimnames = list(x = names(res)))\n      },\n      sort = TRUE,\n      maximum_size = NULL\n    )(item1, dimension, value) %&gt;%\n    select(-item2)\n}\n\n\ntidy_glove %&gt;%\n  # slice_head(prob = .1) %&gt;% \n  nearest_neighbors(\"error\") %&gt;% \n  head()\n\n\n\n  \n\n\n\nEntschachteln wir unsere Daten zu complaints:\n\ntidy_complaints3 &lt;-\n  nested_words3 %&gt;% \n  unnest(words)\n\nDann erstellen wir uns eine Tabelle, in der nur die Schnittmenge der Wörter aus den Beschwerden und Glove vorkommen. Dazu nutzen winr einen inneren Join\n\n\nInner Join, Quelle: Garrick Adenbuie\n\nQuelle\n\ncomplaints_glove &lt;- \ntidy_complaints3 %&gt;% \n  inner_join(by = \"word\", \n  tidy_glove %&gt;% \n  distinct(item1) %&gt;% \n  rename(word = item1)) \n\nhead(complaints_glove)\n\n\n\n  \n\n\n\nWie viele unique (distinkte) Wörter gibt es in unserem Corpus?\n\ntidy_complaints3_distinct_words_n &lt;- \ntidy_complaints3 %&gt;% \n  distinct(word) %&gt;% \n  nrow()\n\ntidy_complaints3_distinct_words_n\n## [1] 222\n\nIn tidy_complaints gibt es übrigens 222 verschiedene Wörter.\n\nword_matrix &lt;- tidy_complaints3 %&gt;%\n  inner_join(by = \"word\",\n             tidy_glove %&gt;%\n               distinct(item1) %&gt;%\n               rename(word = item1)) %&gt;%\n  count(complaint_id, word) %&gt;%\n  cast_sparse(complaint_id, word, n)\n\n#word_matrix\n\nword_matrix zählt für jede der 10 Beschwerden, welche Wörter (und wie häufig) vorkommen.\n\ndim(word_matrix)\n## [1]  10 222\n\n10 Beschwerden (Dokumente) und 222 unique Wörter.\n\nglove_matrix &lt;- tidy_glove %&gt;%\n  inner_join(by = \"item1\",\n             tidy_complaints3 %&gt;%\n               distinct(word) %&gt;%\n               rename(item1 = word)) %&gt;%\n  cast_sparse(item1, dimension, value)\n\n#glove_matrix\n\nglove_matrix gibt für jedes unique Wort den Einbettungsvektor an.\n\ndim(glove_matrix)\n## [1] 222  50\n\nDas sind 222 unique Wörter und 50 Dimensionen des Einbettungsvektors.\nJetzt können wir noch pro Dokument (10 in diesem Beispiel) die mittlere “Position” jedes Dokuments im Einbettungsvektor ausrechnen. Bildlich gesprochen: Was ist der mittlere Raumpunkt (Zentroid) des Bienenschwarms, wobei eine Biene ein Dokument darstellt und die Raumachsen die Dimensionen des Einbettungsvektors.\nDazu gewichten wir jedes Wort eines Dokuments mit den Ladungen des Einbettungsvektor und summieren diese Terme. Es resultiert eine Matrix mit einem Einbettungsvektor pro Dokument. Diese Matrix können wir jetzt als Prädiktorenmatrix hernehmen.\n\ndoc_matrix &lt;- word_matrix %*% glove_matrix\n#doc_matrix %&gt;% head()\n\n\ndim(doc_matrix)\n## [1] 10 50\n\nDie Anzahl der Dokumente ist 10 und die Anzahl der Dimensionen (des Einbettungsvektors) ist 50.\n\n7.4.2 Wordembeddings für die deutsche Sprache\nIn diesem Github-Projekt finden sich die Materialien für ein deutsches Wordembedding (mueller2015?)."
  },
  {
    "objectID": "050-word-embedding.html#fazit",
    "href": "050-word-embedding.html#fazit",
    "title": "\n7  Word Embedding\n",
    "section": "\n7.5 Fazit",
    "text": "7.5 Fazit\nWorteinbettungen sind eine aufwändige Angelegenheit. Positiv gesprochen kann ein Analysti die Muskeln spielen lassen und zeigen, was sie oder er so alles drauf hat. Ist ja schon cooles Zeugs, die Word Embeddings. Es besteht die Chance, dass man mit dieser Methode bessere Vorhersagen erreicht, als mit anderen, einfachen Ansätzen wir Worthäufigkeiten oder tf-idf. Auf der anderen Seite ist es oft sinnvoll, mit einfachen Ansätzen zu starten, und zu sehen, wie weit man kommt. Vielleicht ja weit genug."
  },
  {
    "objectID": "050-word-embedding.html#literatur",
    "href": "050-word-embedding.html#literatur",
    "title": "\n7  Word Embedding\n",
    "section": "\n7.6 Literatur",
    "text": "7.6 Literatur\n\n7.6.1 Wikipedia\nEs gibt eine Reihe nützlicher (und recht informationsdichter) Wikipedia-Einträge zum Thema Informationstheorie.\n\nWikipedia: Pointwise Mutual Information\nWikipedia: Mutual information\nWikipedia: Information theory\n\n\n\n\n\nHvitfeldt, Emil, und Julia Silge. 2021. Supervised Machine Learning for Text Analysis in R. 1. Aufl. Boca Raton: Chapman and Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nPennington, Jeffrey, Richard Socher, und Christopher Manning. 2014. „GloVe: Global Vectors for Word Representation“. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1532–43. Doha, Qatar: Association for Computational Linguistics. https://doi.org/10.3115/v1/D14-1162."
  },
  {
    "objectID": "050-word-embedding.html#footnotes",
    "href": "050-word-embedding.html#footnotes",
    "title": "\n7  Word Embedding\n",
    "section": "",
    "text": "Man könnte ergänzen: plus eine 4. Dimension für Zeit, plus noch ein paar Weitere für die Beschleunigung in verschiedene Richtungen…↩︎\nQuelle: https://de.wikipedia.org/wiki/Sinus_und_Kosinus↩︎\n\\(1024 \\cdot 1024\\) Byte, und \\(1024 =2^{10}\\), daher \\(2^{10} \\cdot 2^{10} = 2^{20}\\)↩︎\nzugegeben, ein subjektiver Ausdruck↩︎\nKaufen…↩︎"
  },
  {
    "objectID": "060-hassrede.html#vorab",
    "href": "060-hassrede.html#vorab",
    "title": "\n8  Hassrede\n",
    "section": "\n8.1 Vorab",
    "text": "8.1 Vorab\n\n8.1.1 Lernziele\n\nFinden Sie eine operationale Definition für Hassrede (engl. hate speech) bzw. Hatespeech1!\n\n8.1.2 Vorbereitung\n\nLesen Sie die unten aufgeführte Literatur\n\n8.1.3 Benötigte R-Pakete\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "060-hassrede.html#worum-gehts",
    "href": "060-hassrede.html#worum-gehts",
    "title": "\n8  Hassrede\n",
    "section": "\n8.2 Worum geht’s?",
    "text": "8.2 Worum geht’s?\nWir möchten eine treffende und praktikable Definition, um zu erkennen, wann eis deutschis Politiki Hass entgegenschlägt.\nTreffend meint, dass Hassrede als Hassrede erkannt wird von unserer Definition, und Nicht-Hassrede als Nicht-Hassrede erkannt wird. Mit anderen Worten: Wir verlangen, dass die Sensitivität und Spezifität unserer Definition hoch ist.\nPraktikabel meint, dass wir diese Definition in der Praxis gut umsetzen können. Wir denken dabei an die Schwierigkeiten, einer (tumben) Maschine unsere Regeln beizubringen. Insbesondere muss die Definition objektiv sein in dem Sinne, dass mehrere Gutachtis zur gleichen Einschätzung kommen würden."
  },
  {
    "objectID": "060-hassrede.html#einstieg",
    "href": "060-hassrede.html#einstieg",
    "title": "\n8  Hassrede\n",
    "section": "\n8.3 Einstieg",
    "text": "8.3 Einstieg\n\n8.3.1 Einstiegsdefinition\nHier ist eine Definition als Startpunkt für Ihre Überlegungen.\nHassrede liegt vor, wenn eine oder mehrere der folgenden Inhalte in einem Text verwendet werden:\n\nSchimpfwörter (“Vollpfosten”)\nRassismus, Sexismus, Antisemitismus oder andere Formen von gruppenbezogener Menschenfeindlichkeit (“Der Schwarze schnackselt gerne”)\nAufruf oder Androhung zur Gewalt, auch in indirekter Form (“Da könnte mal jemand mit der Pistole bei dir vorbeikommen”)\nHerabsetzung (“Volksverräter”)\n\nDabei sollten wir uns mit Blick auf das Ziel, Hass gegen einzelne Personen zu erkennen, nicht auf gruppenbezogene Menschenfeindlichkeit begrenzen, sondern auch Hass auf Individuen einbeziehen. Vielleicht ist daher der Begriff Cybermobbing passender als Hatespeech.\n\n8.3.2 Einstiegsliteratur\nDer Artikel zu Hatespeech der Stanford-Enzyklopädie birgt (am Anfang) gute Hinweise; im weiteren Verlauf geht der Text mehr in die Tiefe.\nIn dieser Zotero-Gruppe finden Sie empfehlenswerte (und öffentlich zugängliche) Artikel zum Thema Hatespeech und Hate-Speech-Erkennung.\n\n8.3.3 Trainingsdaten\nDie Universität Heidelberg veröffentlicht Daten, die Tweets (oder ähnliche Kurztexte) nach Hatespeech hin untersucht (wiegand_germeval-2018_2019?). Nutzen Sie dieser Ressource.\n\n8.3.4 Los geht’s!\nLesen Sie diese und weitere Literatur, um zu einer Arbeitsdefinition von Hassrede zu kommen."
  },
  {
    "objectID": "060-hassrede.html#footnotes",
    "href": "060-hassrede.html#footnotes",
    "title": "\n8  Hassrede\n",
    "section": "",
    "text": "Zur korrekten deutschen Schreibweise vgl. Duden↩︎"
  },
  {
    "objectID": "067-miniprojekt1.html#aufgabe",
    "href": "067-miniprojekt1.html#aufgabe",
    "title": "9  Miniprojekt",
    "section": "9.1 Aufgabe",
    "text": "9.1 Aufgabe\nFühren Sie eine deskriptive Textanalyse durch. Verwenden Sie einen Text (ca. 10-30k Wörter) Ihrer Wahl, von dem Sie einen engen Fokus auf bestimmte Themen sowie eine gewisse Emotionalität erwarten. Wenden Sie dann die Methoden an, die im Teil Textmining dieses Buch vorgestellt sind. Zusätzlich es es empfehlenswert, auf typische Methoden der explorativen Datenanalyse zurückzugreifen inklusive der Datenvisualisierung.\nTypische Arbeitsschritte dabei sind:\n\nImportieren und Aufbereiten der Daten\nZählen häufiger Worte und n-Gramme\nEntfernen von Stopwörtern\nBerechnen von Sentimentstärken\nAssoziationsanalyse von Wörtern und n-Grammen\nBerechnung von Wortdistanzen\nWorteinbettungen\nDimensionsreduktion von Worteinbettungen\nClusteranalysen von Wörtern anhand ihrer Einbettungen\nThemenanalyse anhand einer Latenten-Dirichlet-Analyse"
  },
  {
    "objectID": "080-klassifikation.html#vorab",
    "href": "080-klassifikation.html#vorab",
    "title": "\n10  Klassifikation von Hatespeech\n",
    "section": "\n10.1 Vorab",
    "text": "10.1 Vorab\n\n10.1.1 Lernziele\n\nSie können grundlegende Verfahren zur Klassifikation von Hatespeech einsetzen und erklären\n\n10.1.2 Benötigte R-Pakete\n\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(tidymodels)\nlibrary(tidytext)\nlibrary(textrecipes)\nlibrary(lsa)  # stopwords\nlibrary(discrim)  # naive bayes classification\nlibrary(naivebayes)\nlibrary(tictoc)  # Zeitmessung\nlibrary(fastrtext)  # Worteinbettungen\nlibrary(remoji)  # Emojis\nlibrary(tokenizers)  # Vektoren tokenisieren"
  },
  {
    "objectID": "080-klassifikation.html#daten",
    "href": "080-klassifikation.html#daten",
    "title": "\n10  Klassifikation von Hatespeech\n",
    "section": "\n10.2 Daten",
    "text": "10.2 Daten\nFür Maschinenlernen brauchen wir Trainingsdaten, Daten also, bei denen wir pro Beobachtung der Wert der Zielvariablen kennen. Man spricht auch von “gelabelten” Daten.\nWir nutzen die Daten von (wiegand_germeval?) bzw. (wiegand-data?). Die Daten sind unter CC-By-4.0 Int. lizensiert.\n\nd_raw &lt;- \n  data_read(\"data/germeval2018.training.txt\",\n         header = FALSE)\n## Warning in data.table::fread(input = path, encoding = encoding, ...): Found and\n## resolved improper quoting out-of-sample. First healed line 111: &lt;&lt;\"Edel sei der\n## Mensch, hilfreich und gut\" - Nicht eine dieser Charaktereigenschaften kann\n## Merkel für sich beanspruchen. OTHER OTHER&gt;&gt;. If the fields are not quoted (e.g.\n## field separator does not appear within any field), try quote=\"\" to avoid this\n## warning.\n\nDie Daten finden sich auch im Paket pradadata.\nDa die Daten keine Spaltenköpfe haben, informieren wir die Funktion dazu mit header = FALSE.\nBenennen wir die die Spalten um:\n\nnames(d_raw) &lt;- c(\"text\", \"c1\", \"c2\")\n\nDabei soll c1 und c2 für die 1. bzw. 2. Klassifikation stehen.\nIn c1 finden sich diese Werte:\n\nd_raw %&gt;% \n  count(c1)\n\n\n\n  \n\n\n\nHier wurde klassifiziert, ob beleidigende Sprache (offensive language) vorlag oder nicht (isch-etal-2021-overview?):\n\nTask 1 was to decide whether a tweet includes some form of offensive language or not. The tweets had to be classiﬁed into the two classes OFFENSE and OTHER. The OFFENSE category covered abusive language, insults, as well as merely profane statements.\n\nUnd in c2 finden sich folgende Ausprägungen:\n\nd_raw %&gt;% \n  count(c2)\n\n\n\n  \n\n\n\nIn c2 ging es um eine feinere Klassifikation beleidigender Sprache (isch-etal-2021-overview?):\n\nThe second task involved four categories, a nonoffensive OTHER class and three sub-categories of what is OFFENSE in Task 1. In the case of PROFANITY, profane words are used, however, the tweet does not want to insult anyone. This typically concerns the usage of swearwords (Scheiße, Fuck etc.) and cursing (Zur Hölle! Verdammt! etc.). This can be often found in youth language. Swearwords and cursing may, but need not, co-occur with insults or abusive speech. Profane language may in fact be used in tweets with positive sentiment to express emphasis. Whenever profane words are not directed towards a speciﬁc person or group of persons and there are no separate cues of INSULT or ABUSE, then tweets are labeled as simple cases of PROFANITY.\n\nSind Texte, die als OFFENSE klassifiziert sind, auch (fast) immer als ABUSE, INSULT oder PROFANITY klassifiziert?\n\nd_raw %&gt;% \n  filter(c1 == \"OTHER\", c2 == \"OTHER\") %&gt;% \n  nrow() / nrow(d_raw)\n## [1] 0.6630066\n\nIn ca. 2/3 der Fälle wurden in beiden Klassifikation OTHER klassifiziert.\n\nd_raw %&gt;% \n  filter(c1 != \"OTHER\", c2 != \"OTHER\") %&gt;% \n  nrow() / nrow(d_raw)\n## [1] 0.3369934\n\nEntsprechend in ca. 1/3 der Fälle wurde jeweils nicht mit OTHER klassifiziert.\nWir begnügen uns hier mit der ersten, gröberen Klassifikation.\nFügen wir abschließend noch eine ID-Variable hinzu:\n\nd1 &lt;-\n  d_raw %&gt;% \n  mutate(id = as.character(1:nrow(.)))\n\nDie ID-Variable definieren als Text (nicht als Integer), da die Twitter-IDs zwar natürliche Zahlen sind, aber zu groß, um von R als Integer verarbeitet zu werden. Faktisch sind sie für uns auch nur nominal skalierte Variablen, so dass wir keinen Informationsverlust haben.\n\n#write_rds(d1, \"objects/d1.rds\")"
  },
  {
    "objectID": "080-klassifikation.html#feature-engineering",
    "href": "080-klassifikation.html#feature-engineering",
    "title": "\n10  Klassifikation von Hatespeech\n",
    "section": "\n10.3 Feature Engineering",
    "text": "10.3 Feature Engineering\nReichern wir die Daten mit weiteren Features an, in der Hoffnung, damit eine bessere Klassifikation erzielen zu können.\n\n10.3.1 Textlänge\n\nd2 &lt;-\n  d1 %&gt;% \n  mutate(text_length = str_length(text))\n\nhead(d2)\n\n\n\n  \n\n\n\n\n10.3.2 Sentimentanalyse\nWir nutzen dazu SentiWS (Remus, Quasthoff, und Heyer 2010).\n\nsentiws &lt;- read_csv(\"https://osf.io/x89wq/?action=download\")\n## Rows: 3468 Columns: 4\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (3): neg_pos, word, inflections\n## dbl (1): value\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nd2_long &lt;-\n  d2 %&gt;% \n  unnest_tokens(input = text, output = token)\n\nhead(d2_long)\n\n\n\n  \n\n\n\nJetzt filtern wir unsere Textdaten so, dass nur Wörter mit Sentimentwert übrig bleiben:\n\nd2_long_senti &lt;- \n  d2_long %&gt;%  \n  inner_join(sentiws %&gt;% select(-inflections), by = c(\"token\" = \"word\"))\n## Warning in inner_join(., sentiws %&gt;% select(-inflections), by = c(token = \"word\")): Detected an unexpected many-to-many relationship between `x` and `y`.\n## ℹ Row 1559 of `x` matches multiple rows in `y`.\n## ℹ Row 2572 of `y` matches multiple rows in `x`.\n## ℹ If a many-to-many relationship is expected, set `relationship =\n##   \"many-to-many\"` to silence this warning.\n\nhead(d2_long)\n\n\n\n  \n\n\n\nSchließlich berechnen wir die Sentimentwert pro Polarität und pro Tweet:\n\nd2_sentis &lt;-\n  d2_long_senti %&gt;% \n  group_by(id, neg_pos) %&gt;% \n  summarise(senti_avg = mean(value))\n## `summarise()` has grouped output by 'id'. You can override using the `.groups`\n## argument.\n\nhead(d2_sentis)\n\n\n\n  \n\n\n\nDiese Tabelle bringen wir wieder eine breitere Form, um sie dann wieder mit den Hauptdaten zu vereinigen.\n\nd2_sentis_wide &lt;-\n  d2_sentis %&gt;% \n  pivot_wider(names_from = \"neg_pos\", values_from = \"senti_avg\")\n\nd2_sentis_wide %&gt;% head()\n\n\n\n  \n\n\n\n\nd3 &lt;-\n  d2 %&gt;% \n  full_join(d2_sentis_wide)\n## Joining with `by = join_by(id)`\n\nhead(d3)\n\n\n\n  \n\n\n\n\n\n\n\n\n\nHinweis\n\n\n\nDie Sentimentanalyse hier vernachlässigt Flexionen der Wörter. Der Autor fühlt den Drang zu schreiben: “Left as an exercise for the reader” :-)\n\n\n\n10.3.3 Schimpfwörter\nZählen wir die Schimpfwörter pro Text. Dazu nutzen wir die Daten von LDNOOBW, lizensiert nach CC-BY-4.0-Int.\n\nschimpf1 &lt;- read_csv(\"https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/de\", col_names = FALSE)\n## Rows: 66 Columns: 1\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (1): X1\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nLänger aber noch ist die Liste aus dem InsultWiki, lizensiert CC0.\n\nschimpf2 &lt;- \n  data_read(\"data/insult-de.txt\", header = FALSE) %&gt;% \n  mutate_all(str_to_lower)\n\nDie Daten finden sich auch im Paket pradadata.\nBinden wir die Listen zusammen:\n\nschimpf &lt;-\n  schimpf1 %&gt;% \n  bind_rows(schimpf2) %&gt;% \n  distinct() %&gt;% \n  rename(word = \"V1\")\n\nnrow(schimpf)\n## [1] 6235\n\nUm die Lesis vor (unnötiger?) Kopfverschmutzung zu bewahren, sind diese Schimpfwörter hier nicht abgedruckt.\nJetzt zählen wir, ob unsere Tweets/Texte solcherlei Wörter enthalten.\n\nd_schimpf &lt;- \nd2_long %&gt;% \n  select(id, token) %&gt;% \n  mutate(schimpf = token %in% schimpf$word)\n\nWie viele Schimpfwörter haben wir gefunden?\n\nd_schimpf %&gt;% \n  count(schimpf)\n\n\n\n  \n\n\n\nEtwa ein Prozent der Wörter sind Schimpfwörter in unserem Corpus.\n\nd_schimpf2 &lt;-\n  d_schimpf %&gt;% \n  group_by(id) %&gt;% \n  summarise(schimpf_n = sum(schimpf))\n\nhead(d_schimpf2)\n\n\n\n  \n\n\n\n\nd_main &lt;-\n  d3 %&gt;% \n  full_join(d_schimpf2)\n## Joining with `by = join_by(id)`\n\n\n\n\n\n\n\nWichtig\n\n\n\nNamen wie final, main oder result sind gefährlich, da es unter Garantie ein “final-final geben wird, oder der”Haupt-Datensat” plötzlich nicht mehr so wichtig erscheint und so weiter.\n\n\n\n10.3.4 Emojis\n\nemj &lt;- emoji(list_emoji(), pad = FALSE)\n\nhead(emj)\n## [1] \"😄\" \"😃\" \"😀\" \"😊\" \"☺️\"  \"😉\"\n\nDiese Liste umfasst knapp 900 Emojis, das sind allerdings noch nicht alle, die es gibt. Diese Liste umfasst mit gut 1800 Emojis gut das Doppelte.\nSelbstkuratierte Liste an “wilden” Emoji; diese Liste ist inspiriert von emojicombos.com.\n\nwild_emojis &lt;- \n  c(\n    emoji(find_emoji(\"gun\")),\n    emoji(find_emoji(\"bomb\")),\n    emoji(find_emoji(\"fist\")),\n    emoji(find_emoji(\"knife\"))[1],\n    emoji(find_emoji(\"ambulance\")),\n    emoji(find_emoji(\"fist\")),\n    emoji(find_emoji(\"skull\")),\n    \"☠️\",     \"🗑\",       \"😠\",    \"👹\",    \"💩\" ,\n    \"🖕\",    \"👎️\",\n    emoji(find_emoji(\"middle finger\")),    \"😡\",    \"🤢\",    \"🤮\",  \n    \"😖\",    \"😣\",    \"😩\",    \"😨\",    \"😝\",    \"😳\",    \"😬\",    \"😱\",    \"😵\",\n       \"😤\",    \"🤦‍♀️\",    \"🤦‍\"\n  )\n\n\nwild_emojis_df &lt;-\n  tibble(emoji = wild_emojis)\n\nsave(wild_emojis_df, file = \"data/wild_emojis.RData\")\n\nAuf dieser Basis können wir einen Prädiktor erstellen, der zählt, ob ein Tweet einen oder mehrere der “wilden” Emojis enthält."
  },
  {
    "objectID": "080-klassifikation.html#workflow-1-rezept-1-naive-bayes",
    "href": "080-klassifikation.html#workflow-1-rezept-1-naive-bayes",
    "title": "\n10  Klassifikation von Hatespeech\n",
    "section": "\n10.4 Workflow 1: Rezept 1 + Naive-Bayes",
    "text": "10.4 Workflow 1: Rezept 1 + Naive-Bayes\n\n10.4.1 Dummy-Rezept\nHier ist ein einfaches Beispiel, um die Textvorbereitung mit textrecipes zu verdeutlichen.\nWir erstellen uns einen Dummy-Text:\n\ndummy &lt;- \n  tibble(text = c(\"Ich gehe heim und der die das nicht in ein and the\"))\n\nDann tokenisieren wir den Text:\n\nrec_dummy &lt;-\n  recipe(text ~ 1, data = dummy) %&gt;% \n  step_tokenize(text)\n  \nrec_dummy\n## \n## ── Recipe ──────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome: 1\n## \n## ── Operations\n## • Tokenization for: text\n\nDie Tokens kann man sich so zeigen lassen:\n\nshow_tokens(rec_dummy, text)\n## [[1]]\n##  [1] \"ich\"   \"gehe\"  \"heim\"  \"und\"   \"der\"   \"die\"   \"das\"   \"nicht\" \"in\"   \n## [10] \"ein\"   \"and\"   \"the\"\n\nJetzt entfernen wir die Stopwörter deutscher Sprache; dafür nutzen wir die Stopwort-Quelle snowball:\n\nrec_dummy &lt;-\n  recipe(text ~ 1, data = dummy) %&gt;% \n  step_tokenize(text) %&gt;% \n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\")\n\nrec_dummy\n## \n## ── Recipe ──────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome: 1\n## \n## ── Operations\n## • Tokenization for: text\n## • Stop word removal for: text\n\nPrüfen wir die Tokens; sind die Stopwörter wirklich entfernt?\n\nshow_tokens(rec_dummy, text)\n## [[1]]\n## [1] \"gehe\" \"heim\" \"and\"  \"the\"\n\nJa, die deutschen Stopwörter sind entfernt. Die englischen nicht; das macht Sinn!\n\n10.4.2 Datenaufteilung\n\nd_split &lt;- initial_split(d_main, strata = c1)\n\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\n\n10.4.3 Rezept 1\nRezept definieren:\n\nrec1 &lt;- \n  recipe(c1 ~ ., data = select(d_train, text, c1, id)) %&gt;% \n  update_role(id, new_role = \"id\") %&gt;% \n  step_tokenize(text) %&gt;% \n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %&gt;% \n  step_stem(text) %&gt;% \n  step_tokenfilter(text, max_tokens = 1e2) %&gt;% \n  step_tfidf(text) %&gt;% \n  step_normalize(all_numeric_predictors())\n\nrec1\n## \n## ── Recipe ──────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:   1\n## predictor: 1\n## id:        1\n## \n## ── Operations\n## • Tokenization for: text\n## • Stop word removal for: text\n## • Stemming for: text\n## • Text filtering for: text\n## • Term frequency-inverse document frequency with: text\n## • Centering and scaling for: all_numeric_predictors()\n\nPreppen:\n\nrec1_prepped &lt;- prep(rec1)\n\nUnd backen:\n\nd_rec1 &lt;- bake(rec1_prepped, new_data = NULL)\n\nhead(d_rec1)\n\n\n\n  \n\n\n\n\n10.4.4 Modellspezifikation 1\nWir definiere einen Naive-Bayes-Algorithmus:\n\nnb_spec &lt;- naive_Bayes() %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"naivebayes\")\n\nnb_spec\n## Naive Bayes Model Specification (classification)\n## \n## Computational engine: naivebayes\n\nUnd setzen auf die klassische zehnfache Kreuzvalidierung.\n\nset.seed(42)\nfolds1 &lt;- vfold_cv(d_train)\n\n\n10.4.5 Workflow 1\n\nwf1 &lt;-\n  workflow() %&gt;% \n  add_recipe(rec1) %&gt;% \n  add_model(nb_spec)\n\nwf1\n## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: naive_Bayes()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 6 Recipe Steps\n## \n## • step_tokenize()\n## • step_stopwords()\n## • step_stem()\n## • step_tokenfilter()\n## • step_tfidf()\n## • step_normalize()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Naive Bayes Model Specification (classification)\n## \n## Computational engine: naivebayes\n\n\n10.4.6 Fitting 1\n\nfit1 &lt;-\n  fit_resamples(\n    wf1,\n    folds1,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nDie Vorhersagen speichern wir ab, um die Performanz in den Faltungen des Hold-out-Samples zu berechnen.\nMöchte man sich die Zeit sparen, die Syntax wieder durchlaufen zu lassen, kann man das Objekt speichern. Aber Vorsicht: Dabei kann es passieren, dass man mit veralteten Objekten arbeitet.\n\n#write_rds(fit1, \"objects/chap_classific_fit1.rds\")\n\n\n10.4.7 Performanz 1\n\nwf1_performance &lt;-\n  collect_metrics(fit1)\n\nwf1_performance\n\n\nwf_preds &lt;-\n  collect_predictions(fit1)\n\nwf_preds %&gt;% \n  group_by(id) %&gt;% \n  roc_curve(truth = c1, .pred_OFFENSE) %&gt;% \n  autoplot()\n\n\n\n\nconf_mat_resampled(fit1, tidy = FALSE) %&gt;% autoplot(type = “heatmap”)\n\nconf_mat_resampled(fit1, tidy = FALSE) %&gt;% \n  autoplot(type = \"heatmap\")"
  },
  {
    "objectID": "080-klassifikation.html#nullmodell",
    "href": "080-klassifikation.html#nullmodell",
    "title": "\n10  Klassifikation von Hatespeech\n",
    "section": "\n10.5 Nullmodell",
    "text": "10.5 Nullmodell\n\nnull_classification &lt;- \n  parsnip::null_model() %&gt;%\n  set_engine(\"parsnip\") %&gt;%\n  set_mode(\"classification\")\n\nnull_rs &lt;- workflow() %&gt;%\n  add_recipe(rec1) %&gt;%\n  add_model(null_classification) %&gt;%\n  fit_resamples(\n    folds1\n  )\n\nHier ist die Performanz des Nullmodells.\n\nnull_rs %&gt;%\n  collect_metrics()\n\n\nshow_best(null_rs)\n## Warning: No value of `metric` was given; metric 'roc_auc' will be used."
  },
  {
    "objectID": "080-klassifikation.html#workflow-2-rezept-1-lasso",
    "href": "080-klassifikation.html#workflow-2-rezept-1-lasso",
    "title": "\n10  Klassifikation von Hatespeech\n",
    "section": "\n10.6 Workflow 2: Rezept 1 + Lasso",
    "text": "10.6 Workflow 2: Rezept 1 + Lasso\n\nlasso_spec &lt;- logistic_reg(penalty = tune(), mixture = 1) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"glmnet\")\n\nlasso_spec\n## Logistic Regression Model Specification (classification)\n## \n## Main Arguments:\n##   penalty = tune()\n##   mixture = 1\n## \n## Computational engine: glmnet\n\nWir definieren die Ausprägungen von penalty, die wir ausprobieren wollen:\n\nlambda_grid &lt;- grid_regular(penalty(), levels = 3)  # hier nur 3 Werte, um Rechenzeit zu sparen\n\n\nwf2 &lt;-\n  workflow() %&gt;% \n  add_recipe(rec1) %&gt;% \n  add_model(lasso_spec)\n\nwf2\n## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: logistic_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 6 Recipe Steps\n## \n## • step_tokenize()\n## • step_stopwords()\n## • step_stem()\n## • step_tokenfilter()\n## • step_tfidf()\n## • step_normalize()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Logistic Regression Model Specification (classification)\n## \n## Main Arguments:\n##   penalty = tune()\n##   mixture = 1\n## \n## Computational engine: glmnet\n\nTunen und Fitten:\n\nset.seed(42)\n\nfit2 &lt;-\n  tune_grid(\n    wf2,\n    folds1,\n    grid = lambda_grid,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nfit2\n\nVorsicht beim Abspeichern.\n\n#write_rds(fit2, \"objects/chap_classific_fit2.rds\")\n\nHier ist die Performanz:\n\ncollect_metrics(fit2) %&gt;% \n  filter(.metric == \"roc_auc\") %&gt;% \n  slice_max(mean, n = 3)\n\n\nautoplot(fit2)\n\n\n\n\n\nfit2 %&gt;% \n  show_best(\"roc_auc\")\n\n\n\n  \n\n\n\n\nchosen_auc &lt;- \n  fit2 %&gt;%\n  select_by_one_std_err(metric = \"roc_auc\", -penalty)\n\nFinalisieren:\n\nwf2_final &lt;-\n  finalize_workflow(wf2, chosen_auc)\n\nwf2_final\n## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: logistic_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 6 Recipe Steps\n## \n## • step_tokenize()\n## • step_stopwords()\n## • step_stem()\n## • step_tokenfilter()\n## • step_tfidf()\n## • step_normalize()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Logistic Regression Model Specification (classification)\n## \n## Main Arguments:\n##   penalty = 0.00853167852417281\n##   mixture = 1\n## \n## Computational engine: glmnet\n\n\nfit2_final_train &lt;-\n  fit(wf2_final, d_train)\n\n\nfit2_final_train %&gt;% \n  extract_fit_parsnip() %&gt;% \n  tidy() %&gt;% \n  arrange(-abs(estimate)) %&gt;% \n  head()\n## Loading required package: Matrix\n## \n## Attaching package: 'Matrix'\n## The following objects are masked from 'package:tidyr':\n## \n##     expand, pack, unpack\n## Loaded glmnet 4.1-8\n\n\n\n  \n\n\n\n\nfit2_final_test &lt;-\n  last_fit(wf2_final, d_split)\n\ncollect_metrics(fit2_final_test)\n\n\n\n  \n\n\n\n\n10.6.1 Vorhersage\n\n10.6.2 Vohersagedaten\nPfad zu den Daten:\n\ntweet_data_path &lt;- \"/Users/sebastiansaueruser/github-repos/hate-speech/data-raw/tweets/\"\n\n\ntweet_data_files_names &lt;- list.files(path = tweet_data_path,\n                                     pattern  = \"tweets-to-.*\\\\.rds$\")\nhead(tweet_data_files_names)\n## [1] \"tweets-to-_FriedrichMerz_2021.rds\" \"tweets-to-_FriedrichMerz_2022.rds\"\n## [3] \"tweets-to-ABaerbock_2021.rds\"      \"tweets-to-ABaerbock_2022.rds\"     \n## [5] \"tweets-to-Alice_Weidel_2021.rds\"   \"tweets-to-Alice_Weidel_2022.rds\"\n\nWie viele Dateien sind es?\n\nlength(tweet_data_files_names)\n## [1] 26\n\nWir geben den Elementen des Vektors gängige Namen, das hilft uns gleich bei map:\n\nnames(tweet_data_files_names) &lt;- str_remove(tweet_data_files_names, \"\\\\.rds\")\n\nOK, weiter: So können wir eine der Datendateien einlesen:\n\nd_raw &lt;-\n  read_rds(file = paste0(tweet_data_path, tweet_data_files_names[1])) \n\nd &lt;- \n  d_raw %&gt;% \n  select(id, author_id, created_at, public_metrics) %&gt;% \n  unnest_wider(public_metrics)\n\nhead(d)\n\nUnd so lesen wir alle ein:\nZunächst erstellen wir uns eine Helper-Funktion:\n\nread_and_select &lt;- function(file_name, path_to_tweet_data = tweet_data_path) {\n  \n  out &lt;- \n    read_rds(file = paste0(path_to_tweet_data, file_name)) %&gt;% \n    select(id, author_id, created_at, text, public_metrics) %&gt;% \n    unnest_wider(public_metrics)\n  \n  cat(\"Data file was read.\\n\")\n  \n  return(out)\n}\n\nTesten:\n\nd1 &lt;- read_and_select(tweet_data_files_names[1])\n\nhead(d1)\n\nDie Funktion read_and_select mappen wir auf alle Datendateien:\n\ntic()\nds &lt;-\n  tweet_data_files_names %&gt;% \n  map_dfr(read_and_select, .id = \"dataset\")\ntoc()\n\n214.531 sec elapsed\nDa wir den Elementen von tweet_data_files_names Namen gegeben haben, finden wir diese Namen praktischerweise wieder in ds.\nVielleicht ist es zum Entwickeln besser, mit einem kleineren Datensatz einstweilen zu arbeiten:\n\nds_short &lt;- read_rds(file = \"/Users/sebastiansaueruser/datasets/Twitter/hate-classific/ds_short.rds\")\n\nds &lt;- ds_short\n\n\n10.6.3 Vokabular erstellen\n\nds_long &lt;-\n  ds %&gt;% \n  select(text) %&gt;% \n  unnest_tweets(input = text, output = word)\n\nPuh, das hat gedauert!\nSpeichern wir uns diese Daten daher auf die Festplatte:\n\n#write_rds(ds_long, file = paste0(tweet_data_path, \"ds_long.rds\"))\n\nEntfernen wir daraus die Duplikate, um uns ein Vokabular zu erstellen:\n\nds_voc &lt;-\n  ds_long %&gt;% \n  distinct(word)\n\nUnd das resultierende Objekt speichern wir wieder ab:\n\n#write_rds(ds_voc, file = paste0(\"objects/\", \"ds_voc.rds\"))"
  },
  {
    "objectID": "080-klassifikation.html#worteinbettungen-erstellen",
    "href": "080-klassifikation.html#worteinbettungen-erstellen",
    "title": "\n10  Klassifikation von Hatespeech\n",
    "section": "\n10.7 Worteinbettungen erstellen",
    "text": "10.7 Worteinbettungen erstellen\n\n10.7.1 FastText-Modell\nDefiniere die Konstanten für das fastText-Modell:\n\ntexts &lt;- ds %&gt;% pull(text)\ntexts &lt;- tolower(texts)\n\n\nout_file_txt &lt;- \"/Users/sebastiansaueruser/datasets/Twitter/twitter-polit-model.vec\"\nout_file_model &lt;- \"/Users/sebastiansaueruser/datasets/Twitter/twitter-polit-model.bin\"\n\nfile.exists(out_file_txt)\n## [1] TRUE\nfile.exists(out_file_model)\n## [1] TRUE\n\n\n#writeLines(text = texts, con = out_file_txt)\n#execute(commands = c(\"skipgram\", \"-input\", tmp_file_txt, \"-output\", out_file_model, \"-verbose\", 1))\n\nRead 22M words\nNumber of words:  130328\nNumber of labels: 0\nProgress: 100.0% words/sec/thread:   49218 lr:  0.000000 avg.loss:  1.720812 ETA:   0h 0m 0s\nJetzt laden wir das Modell von der Festplatte:\n\ntwitter_fasttext_model &lt;- load_model(out_file_model)\ndict &lt;- get_dictionary(twitter_fasttext_model)\n\nSchauen wir uns einige Begriffe aus dem Vokabular an:\n\nprint(head(dict, 10))\n##  [1] \"&lt;/s&gt;\"            \"die\"             \"und\"             \"der\"            \n##  [5] \"sie\"             \"das\"             \"nicht\"           \"in\"             \n##  [9] \"ist\"             \"@_friedrichmerz\"\n\nHier sind die ersten paar Elemente des Vektors für menschen:\n\nget_word_vectors(twitter_fasttext_model, c(\"menschen\")) %&gt;% `[`(1:10)\n\n [1]  0.14156282  0.44875699  0.23911817 -0.02580349  0.29811972  0.03870077\n [7]  0.06518744  0.22527063  0.28198120  0.39931887\nErstellen wir uns einen Tibble, der als erste Spalte das Vokabular und in den übrigen 100 Spalten die Dimensionen enthält:\n\nword_embedding_twitter &lt;-\n  tibble(\n    word = dict\n  )\n\n\nwords_vecs_twitter &lt;-\n  get_word_vectors(twitter_fasttext_model)\n\n\nword_embedding_twitter &lt;-\n  word_embedding_twitter %&gt;% \n  bind_cols(words_vecs_twitter)\n\nnames(word_embedding_twitter) &lt;- c(\"word\", paste0(\"v\", sprintf(\"%03d\", 1:100)))  # Namen verschönern\n\nUnd als Worteinbettungs-Datei abspeichern:\n\n#write_rds(word_embedding_twitter, file = paste0(tweet_data_path, \"word_embedding_twitter.rds\"))\n\n\n10.7.2 Aufbereiten\nAm besten nur die Spalten behalten, die wir zum Modellieren nutzen:\n\nds_short2 &lt;-\n  ds_short %&gt;% \n  select(text, id)\n\nDann backen wir die Daten mit dem vorhandenen Rezept:\n\nds_baked &lt;- bake(rec1_prepped, new_data = ds_short2)\n\nIst das nicht komfortabel? Das Textrezept übernimmt die Arbeit für uns, mit den richtigen Features zu arbeiten, die tf-idfs für die richtigen Tokens zu berechnen.\nWer dem Frieden nicht traut, dem sei geraten, nachzuprüfen :-)"
  },
  {
    "objectID": "080-klassifikation.html#workflow-3-rezept-2-lasso",
    "href": "080-klassifikation.html#workflow-3-rezept-2-lasso",
    "title": "\n10  Klassifikation von Hatespeech\n",
    "section": "\n10.8 Workflow 3: Rezept 2 + Lasso",
    "text": "10.8 Workflow 3: Rezept 2 + Lasso\n\n10.8.1 Daten aufteilen\n\nd_split &lt;- initial_split(d2, strata = c1)\n\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\n\n10.8.2 Hilfsfunktionen\n\nsource(\"funs/helper-funs-recipes.R\")\n\nTesten wir die Funktionen:\n\ndummy &lt;- c(\"hallo\", \"baby\", \"fatal\")\n\ncount_profane(dummy) \n## [1] 1\n\ncount_emo_words(dummy)\n## [1] 1\n\ndummy &lt;- c(\"baby\", \"und\", \"🆗\", \"🖕\")\n\ncount_emojis(dummy)\n## [1] 0\n\ncount_wild_emojis(dummy) \n## [1] 0\n\n\n10.8.3 Rezept mit Worteinbettungen\n\nrec2 &lt;- \n  recipe(c1 ~ ., data = select(d_train, text, c1, id)) %&gt;% \n  update_role(id, new_role = \"id\") %&gt;% \n  step_text_normalization(text) %&gt;% \n  step_mutate(text_copy = text,\n              profane_n = map_int(text, count_profane),\n              emo_words_n = map_int(text, count_emo_words),\n              emojis_n = map_int(text, count_emojis),\n              wild_emojis_n = map_int(text, count_wild_emojis)\n  ) %&gt;% \n  step_textfeature(text_copy) %&gt;% \n  step_tokenize(text, token = \"words\") %&gt;% \n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %&gt;% \n  step_word_embeddings(text, embeddings = word_embedding_twitter)\n \nrec2\n## \n## ── Recipe ──────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:   1\n## predictor: 1\n## id:        1\n## \n## ── Operations\n## • Text Normalization for: text\n## • Variable mutation for: text, map_int(text, count_profane), ...\n## • Text feature extraction for: text_copy\n## • Tokenization for: text\n## • Stop word removal for: text\n## • Word embeddings aggregated from: text\n\nJetzt preppen:\n\nrec2_prepped &lt;- prep(rec2)\n\nVielleicht macht es Sinn, sich das Objekt zur späteren Verwendung abzuspeichern.1 Feather verarbeitet nur Dataframes, daher nutzen wir hier RDS.\n\n#write_rds(rec2_prepped, file = \"~/datasets/Twitter/klassifik-rec2-prepped.rds\")\n\nDas Element rec2_prepped ist recht groß:\n\nformat(object.size(rec2_prepped), units  = \"Mb\")\n## [1] \"113.8 Mb\"\n\nJetzt können wir das präparierte (“gepreppte”) Rezept “backen”:\n\nrec2_baked &lt;- bake(rec2_prepped, new_data = NULL)\n\n\nrec2_baked %&gt;% \n  select(1:15) %&gt;% \n  glimpse()\n## Rows: 3,756\n## Columns: 15\n## $ id                                  &lt;fct&gt; 5, 7, 10, 12, 17, 27, 33, 42, 44, …\n## $ c1                                  &lt;fct&gt; OFFENSE, OFFENSE, OFFENSE, OFFENSE…\n## $ profane_n                           &lt;int&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ emo_words_n                         &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1…\n## $ emojis_n                            &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ wild_emojis_n                       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ textfeature_text_copy_n_words       &lt;int&gt; 16, 32, 15, 26, 19, 15, 33, 30, 31…\n## $ textfeature_text_copy_n_uq_words    &lt;int&gt; 16, 28, 15, 25, 17, 15, 32, 29, 29…\n## $ textfeature_text_copy_n_charS       &lt;int&gt; 121, 145, 119, 134, 112, 101, 196,…\n## $ textfeature_text_copy_n_uq_charS    &lt;int&gt; 31, 29, 30, 39, 36, 31, 35, 42, 35…\n## $ textfeature_text_copy_n_digits      &lt;int&gt; 0, 4, 0, 2, 4, 0, 0, 0, 1, 0, 2, 0…\n## $ textfeature_text_copy_n_hashtags    &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ textfeature_text_copy_n_uq_hashtags &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ textfeature_text_copy_n_mentions    &lt;int&gt; 1, 1, 0, 1, 0, 1, 1, 5, 1, 0, 1, 1…\n## $ textfeature_text_copy_n_uq_mentions &lt;int&gt; 1, 1, 0, 1, 0, 1, 1, 5, 1, 0, 1, 1…\n\n\n10.8.4 Fitting 3\n\nwf3 &lt;-\n  workflow() %&gt;% \n  add_recipe(rec2) %&gt;% \n  add_model(lasso_spec)\n\nwf3\n## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: logistic_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 6 Recipe Steps\n## \n## • step_text_normalization()\n## • step_mutate()\n## • step_textfeature()\n## • step_tokenize()\n## • step_stopwords()\n## • step_word_embeddings()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Logistic Regression Model Specification (classification)\n## \n## Main Arguments:\n##   penalty = tune()\n##   mixture = 1\n## \n## Computational engine: glmnet\n\nTunen und Fitten:\n\nset.seed(42)\n\ntic()\nfit3 &lt;-\n  tune_grid(\n    wf3,\n    folds1,\n    grid = lambda_grid,\n    control = control_resamples(save_pred = TRUE)\n  )\n(toc)\nfit3\n\n\n#write_rds(fit3, \"objects/chap_classific_fit3.rds\")\n\nHier ist die Performanz:\n\ncollect_metrics(fit3)\n\n\nautoplot(fit3)\n\n\n\n\n\nfit3 %&gt;% \n  show_best(\"roc_auc\")\n\n\n\n  \n\n\n\n\nchosen_auc_fit3 &lt;- \n  fit3 %&gt;%\n  select_by_one_std_err(metric = \"roc_auc\", -penalty)\n\nFinalisieren:\n\nwf3_final &lt;-\n  finalize_workflow(wf3, chosen_auc_fit3)\n\nwf3_final\n## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: logistic_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 6 Recipe Steps\n## \n## • step_text_normalization()\n## • step_mutate()\n## • step_textfeature()\n## • step_tokenize()\n## • step_stopwords()\n## • step_word_embeddings()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Logistic Regression Model Specification (classification)\n## \n## Main Arguments:\n##   penalty = 0.00853167852417281\n##   mixture = 1\n## \n## Computational engine: glmnet\n\n\n#fit3_final_train &lt;-  # die Berechnung kann dauern ...\n  fit(wf3_final, d_train)\n\n\nfit3_final_train %&gt;% \n  extract_fit_parsnip() %&gt;% \n  tidy() %&gt;% \n  arrange(-abs(estimate)) %&gt;% \n  head()\n\n\n\n  \n\n\n\n\nfit3_final_test &lt;-\n  last_fit(wf3_final, d_split)  # dauert etwas\n\nUnd endlich: Wie gut ist die Performanz?\n\ncollect_metrics(fit3_final_test)\n\n\n\n  \n\n\n\nAm Ende so eines Arbeitsganges, bei dem man wieder (und wieder) die gleichen Funktionen kopiert, und nur aufpassen muss, aus fit2 an der richtigen Stelle fit3 zu machen: Da blickt man jedem Umbau dieses Codes zu einer Funktion freudig ins Gesicht.\nEin anderes Problem, für das hier keine elegante Lösung präsentiert wird, sind die langen Berechnungszeiten, die, wenn man Pech hat, auch noch mehrfach wiederholt werden müssen.\nDie Gefahr mit dem Abspeichern via write_rds ist klar: Man riskiert, später ein veraltetes Objekt zu laden.\nZu diesen Punkten später mehr.\n\n\n\n\nRemus, Robert, Uwe Quasthoff, und Gerhard Heyer. 2010. „SentiWS - a Publicly Available German-Language Resource for Sentiment Analysis“. Proceedings of the 7th International Language Ressources and Evaluation (LREC’10), 1168–71."
  },
  {
    "objectID": "080-klassifikation.html#footnotes",
    "href": "080-klassifikation.html#footnotes",
    "title": "\n10  Klassifikation von Hatespeech\n",
    "section": "",
    "text": "Aber Vorsicht beim Abspeichern, man könnte versehentlich mit einer veralteten Version weiterarbeiten.↩︎"
  },
  {
    "objectID": "070-hatespeech2.html#vorab",
    "href": "070-hatespeech2.html#vorab",
    "title": "\n11  Fallstudie Hatespeech\n",
    "section": "\n11.1 Vorab",
    "text": "11.1 Vorab\n\n11.1.1 Lernziele\n\nSie können grundlegende Verfahren zur Klassifikation von Hatespeech einsetzen und erklären\nSie können mit echten Daten umgehen im Sinne eines Projektmanagement von Data Science\n\n11.1.2 Benötigte R-Pakete\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(easystats)\nlibrary(tidytext)\nlibrary(textrecipes)\nlibrary(tictoc)  # Zeitmessung\nlibrary(beepr)  # piebt, wenn fertig\nlibrary(remoji)  # Emojis\nlibrary(feather)  # Daten speichern\nlibrary(pradadata)  # Hilfsdaten wie Schimpfwoerter\nlibrary(lubridate)  # Datum und Zeit\nlibrary(tokenizers)\nlibrary(feather)  # feather data\nlibrary(pradadata)  # helper data\nlibrary(remoji)  # processing emojis"
  },
  {
    "objectID": "070-hatespeech2.html#daten",
    "href": "070-hatespeech2.html#daten",
    "title": "\n11  Fallstudie Hatespeech\n",
    "section": "\n11.2 Daten",
    "text": "11.2 Daten\n\n11.2.1 Train- und Testdaten\n\nd1 &lt;- read_rds(\"objects/d1.rds\")  # Traindaten einlesen\n\nIn Train- und Test-Datensatz aufsplitten:\n\nd_split &lt;- initial_split(d1, strata = c1)\n\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\n\n11.2.2 Vorhersagedaten\nWir importieren die Tweets führender deutscher Politikis.\nFür diese Daten haben wir keine Werte der Zielvariablen. Wir können nur vorhersagen, aber nicht unsere Modellgüte berechnen. Diese Daten bezeichnen wir als Vorhersagedaten.\nPfad zu den Daten:\n\ntweet_data_path &lt;- \"/Users/sebastiansaueruser/github-repos/hate-speech/data-raw/tweets-small\"\n\nfile.exists(tweet_data_path)\n## [1] TRUE\n\nDie Nutzungsrechte von Twitter erlauben nicht, diese Daten öffentlich zu teilen.\n\ntweet_data_files_names &lt;-\n  list.files(\n    path = tweet_data_path,\n    full.names = TRUE,\n    pattern = \".rds\")\n\n\nnames(tweet_data_files_names) &lt;-  \n  list.files(\n    path = tweet_data_path,\n    full.names = FALSE,\n    pattern = \".rds\") %&gt;% \n  str_remove(\".rds$\") %&gt;% \n  str_remove(\"^tweets-to-\")\n\ntweet_data_files_names\n##                                                                                                    BMWK_2021 \n##           \"/Users/sebastiansaueruser/github-repos/hate-speech/data-raw/tweets-small/tweets-to-BMWK_2021.rds\" \n##                                                                                          Janine_Wissler_2021 \n## \"/Users/sebastiansaueruser/github-repos/hate-speech/data-raw/tweets-small/tweets-to-Janine_Wissler_2021.rds\" \n##                                                                                          Janine_Wissler_2022 \n## \"/Users/sebastiansaueruser/github-repos/hate-speech/data-raw/tweets-small/tweets-to-Janine_Wissler_2022.rds\"\n\nSo lesen wir alle Dateien aus diesem Ordner ein. Zunächst erstellen wir uns eine Helper-Funktion:\n\nsource(\"funs/read-and-select.R\")\n\nDie Funktion read_and_select mappen wir auf alle Datendateien:\n\ntic()\nds &lt;-\n  tweet_data_files_names %&gt;% \n  map_dfr(read_and_select, .id = \"dataset\")\n## Data file was read.\n## Data file was read.\n## Data file was read.\ntoc()\n## 2.381 sec elapsed\n\nEin Blick zur Probe:\n\nds %&gt;% \n  glimpse()\n## Rows: 10,310\n## Columns: 9\n## $ dataset       &lt;chr&gt; \"BMWK_2021\", \"BMWK_2021\", \"BMWK_2021\", \"BMWK_2021\", \"BMW…\n## $ id            &lt;chr&gt; \"1476982045268185091\", \"1476948509706407942\", \"147694476…\n## $ author_id     &lt;chr&gt; \"749510675811139585\", \"146337393\", \"841768687245918208\",…\n## $ created_at    &lt;chr&gt; \"2021-12-31T18:22:15.000Z\", \"2021-12-31T16:08:59.000Z\", …\n## $ text          &lt;chr&gt; \"@BMWi_Bund @twittlik @Pendolino70 @nextmove_de Richtig.…\n## $ retweet_count &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n## $ reply_count   &lt;int&gt; 0, 2, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,…\n## $ like_count    &lt;int&gt; 0, 0, 1, 0, 0, 1, 1, 3, 3, 0, 3, 0, 0, 1, 0, 0, 1, 2, 1,…\n## $ quote_count   &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\nDa wir den Elementen von tweet_data_files_names Namen gegeben haben, finden wir diese Namen praktischerweise wieder in ds.\nEine Alternative zum Format RDS besteht im Format Feather:\n\nFeather: fast, interoperable data frame storage Feather provides binary columnar serialization for data frames. It is designed to make reading and writing data frames efficient, and to make sharing data across data analysis languages easy.\n\n\n11.2.3 Worteinbettungen\nWie in Kapitel 10.7.1 dargestellt, importieren wir unser FastText-Modell.\n\nword_embedding_twitter &lt;- read_rds(file = \"/Users/sebastiansaueruser/datasets/Twitter/word_embedding_twitter.rds\")\n\nWie viel Speicher benötigt das Worteinbettungsobjekt?\n\nformat(object.size(word_embedding_twitter), units = \"Mb\")\n## [1] \"108.3 Mb\"\n\n\n11.2.4 Hilfsdaten\n\ndata(\"schimpwoerter\")\n## Warning in data(\"schimpwoerter\"): data set 'schimpwoerter' not found\ndata(\"sentiws\")\ndata(\"wild_emojis\")"
  },
  {
    "objectID": "070-hatespeech2.html#aufbereiten-der-vorhersagedaten",
    "href": "070-hatespeech2.html#aufbereiten-der-vorhersagedaten",
    "title": "\n11  Fallstudie Hatespeech\n",
    "section": "\n11.3 Aufbereiten der Vorhersagedaten",
    "text": "11.3 Aufbereiten der Vorhersagedaten\n\n11.3.1 Hilfsfunktionen\n\nsource(\"funs/helper-funs-recipes.R\")"
  },
  {
    "objectID": "070-hatespeech2.html#rezept",
    "href": "070-hatespeech2.html#rezept",
    "title": "\n11  Fallstudie Hatespeech\n",
    "section": "\n11.4 Rezept",
    "text": "11.4 Rezept\nDa wir schon ein Rezept “trainiert” haben, können wir die Test-Daten einfach mit dem Rezept “backen”.\nStreng genommen müssten wir nicht mal das tun, denn tidymodels würde das beim Vorhersagen für uns übernehmen. Aber es ist nützlich, die Daten in aufbereiteter Form zu sehen, bzw. sie direkt zugänglich zu haben.\n\nrec2 &lt;- \n  recipe(c1 ~ ., data = select(d_train, text, c1, id)) %&gt;% \n  update_role(id, new_role = \"id\") %&gt;% \n  step_text_normalization(text) %&gt;% \n  step_mutate(text_copy = text,\n              profane_n = map_int(text_copy, count_profane, profane_list = schimpfwoerter$word),\n              emo_words_n = map_int(text_copy, count_emo_words, emo_list = sentiws$word),\n              emojis_n = map_int(text_copy, count_emojis, emoji_list = emoji(list_emoji(), pad = FALSE)),\n              wild_emojis_n = map_int(text_copy, count_wild_emojis, wild_emoji_list = wild_emojis$emojis)\n  ) %&gt;% \n  step_textfeature(text_copy) %&gt;% \n  step_tokenize(text, token = \"tweets\") %&gt;% \n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %&gt;% \n  step_word_embeddings(text, embeddings = word_embedding_twitter)\n \nrec2\n\n\n11.4.1 Preppen und Backen\nPreppen:\n\ntic()\nrec2_prepped &lt;- prep(rec2)\ntoc()\n\n29.377 sec elapsed\nBraucht ganz schön Zeit …\nZur Sicherheit speichern wir auch dieses Objekt ab.\n\n# write_rds(rec2_prepped, \"objects/rec2_prepped.rds\")\nrec2_prepped &lt;- read_rds(\"/Users/sebastiansaueruser/datasets/Twitter/hate-classific/rec2_prepped.rds\")\n\nAls nächstes kommt das Backen der Vorhersagedaten. Das ist die Stelle, an der zum ersten Mal die neuen Daten (die Vorhersagedaten) ins Spiel kommen.\n\ntic()\nd_predict_baken &lt;-\n  bake(rec2_prepped, new_data = ds)\n\nd_predict_baken$id &lt;- ds$id\ntoc()\nbeepr::beep()\n\nPuh, das Backen dauert - bei großen Datensätzen - gefühlt ewig! Daher ist das beepen praktisch: Es klingelt, wenn die Berechnung fertig ist.\nZur Erinnerung: d_predict_baken ist der “gebackene” Testdatensatz. Der Testdatensatz also, auf dem die ganzen Operationen der Vorverarbeitung angewandt wurden.\n\n11.4.2 Git Large File System\nWenn Sie Ihre Arbeit mit einem Versionierungssystem schützen - und Sie sollten es tun - dann verwenden Sie vermutlich Git. Git ist für Textdateien ausgelegt - was bei Quellcode ja auch Sinn macht, und für Quellcode ist Git gemacht. Allerdings will man manchmal auch binäre Dateien sichern, etwa Daten im RDS-Format. Solche binären Formante funktionieren nicht wirklich aus der Sicht von Git, sie lassen sich nicht zeilenweise nachverfolgen. Kurz gesagt sollte man sie aus diesem Grund nicht in Git nachverfolgen. Eine bequeme Lösung ist dasLarge File System von Github (git lfs), das diese großen Dateien außerhalb des Git-Index verwaltet. Trotzdem sieht es für Nutzis aus wie immer, ist also sehr komfortabel. Dazu ist es nötig, git lfs zu installieren.\n\n11.4.3 Metadaten\nMetadaten wieder hinzufügen:\n\nd_predict2 &lt;-\n  d_predict_baken %&gt;% \n  left_join(ds, by = \"id\") %&gt;% \n  relocate(dataset, id, author_id, created_at, text, retweet_count, reply_count, quote_count, .after = id) %&gt;% \n  mutate(id = as.integer(id))\n## Warning: There was 1 warning in `mutate()`.\n## ℹ In argument: `id = as.integer(id)`.\n## Caused by warning:\n## ! NAs introduced by coercion to integer range\n\nLeider müssen wir id in Integer umwandeln, das wir dies im Rezept auch so gemacht hatten. Dabei geht die Spalte kaputt, bzw. die Daten werden NA, da die resultierende Integerzahl zu groß für R ist. Aber nicht so schlimm: Wir fügen sie später wieder hinzu.\nSpaltennamen mal anschauen:\n\nnames(d_predict2)[1:33]\n##  [1] \"dataset\"                             \"id\"                                 \n##  [3] \"author_id\"                           \"created_at\"                         \n##  [5] \"text\"                                \"retweet_count\"                      \n##  [7] \"reply_count\"                         \"quote_count\"                        \n##  [9] \"profane_n\"                           \"emo_words_n\"                        \n## [11] \"emojis_n\"                            \"wild_emojis_n\"                      \n## [13] \"textfeature_text_copy_n_words\"       \"textfeature_text_copy_n_uq_words\"   \n## [15] \"textfeature_text_copy_n_charS\"       \"textfeature_text_copy_n_uq_charS\"   \n## [17] \"textfeature_text_copy_n_digits\"      \"textfeature_text_copy_n_hashtags\"   \n## [19] \"textfeature_text_copy_n_uq_hashtags\" \"textfeature_text_copy_n_mentions\"   \n## [21] \"textfeature_text_copy_n_uq_mentions\" \"textfeature_text_copy_n_commas\"     \n## [23] \"textfeature_text_copy_n_periods\"     \"textfeature_text_copy_n_exclaims\"   \n## [25] \"textfeature_text_copy_n_extraspaces\" \"textfeature_text_copy_n_caps\"       \n## [27] \"textfeature_text_copy_n_lowers\"      \"textfeature_text_copy_n_urls\"       \n## [29] \"textfeature_text_copy_n_uq_urls\"     \"textfeature_text_copy_n_nonasciis\"  \n## [31] \"textfeature_text_copy_n_puncts\"      \"textfeature_text_copy_politeness\"   \n## [33] \"textfeature_text_copy_first_person\""
  },
  {
    "objectID": "070-hatespeech2.html#vorhersagen",
    "href": "070-hatespeech2.html#vorhersagen",
    "title": "\n11  Fallstudie Hatespeech\n",
    "section": "\n11.5 Vorhersagen",
    "text": "11.5 Vorhersagen\nWir beziehen uns auf das Modell von Kapitel 10.8.4.\n\nfit3 &lt;- read_rds(\"/Users/sebastiansaueruser/github-repos/datascience-text/objects/chap_classific_fit3.rds\")\n\nfit3_final_train &lt;- read_rds(\"/Users/sebastiansaueruser/datasets/Twitter/hate-classific/fit3_final_train.rds\")\n\nUnd nutzen dann die predict-Methode von tidymodels:\n\ntic()\nd_predicted_values &lt;- predict(fit3_final_train, d_predict2)\ntoc()\nbeep()\n\nPuh, hier ist mein Rechner abgestürzt, als ich es mit ca. 2 Millionen Tweets versucht habe!\nBesser, wir probieren erstmal mit einem winzigen Teil der Daten, ob unsere Funktion “im Prinzip” oder “grundsätzlich” funktioniert:\n\nd_predicted_values_tiny &lt;- predict(fit3_final_train, head(d_predict2))\n## Error:\n## ! Can't convert `data$id` &lt;integer&gt; to match type of `id` &lt;character&gt;.\n\nd_predicted_values_tiny\n## Error in eval(expr, envir, enclos): object 'd_predicted_values_tiny' not found\n\nFunktioniert! Gut! Also weiter.\nPasst!"
  },
  {
    "objectID": "070-hatespeech2.html#ergebnisse",
    "href": "070-hatespeech2.html#ergebnisse",
    "title": "\n11  Fallstudie Hatespeech\n",
    "section": "\n11.6 Ergebnisse",
    "text": "11.6 Ergebnisse\n\n11.6.1 Hass-Proxis pro Politiki insgesamt\n\nres_summary1 &lt;- \nd_predict2 %&gt;% \n  group_by(dataset) %&gt;% \n  summarise(emo_words_n_mean = mean(emo_words_n),\n            profane_words_count_mean = mean(profane_n),\n            wild_emojis_n_mean = mean(wild_emojis_n),\n            exclaims_n_mean = mean(textfeature_text_copy_n_exclaims))\n\n\nres_summary1_long &lt;-\n  res_summary1 %&gt;% \n    pivot_longer(-dataset, names_to = \"hate_proxy\", values_to = \"prop\")\n\n\nres_summary1_long %&gt;% \n  ggplot(aes(x = prop, y = hate_proxy)) +\n  geom_col() +\n  facet_wrap(~ dataset)\n\n\n\n\n\n11.6.2 Hass-Proxis pro Politiki im Zeitverlauf\n\nres_summary2 &lt;- \nd_predict2 %&gt;%\n  select(created_at, profane_n, dataset, emo_words_n, wild_emojis_n, textfeature_text_copy_n_exclaims) %&gt;% \n  mutate(month = ymd_hms(created_at) %&gt;% round_date(unit = \"month\")) %&gt;% \n  group_by(month, dataset) %&gt;% \n  summarise(emo_words_n_mean = mean(emo_words_n),\n            profane_words_count_mean = mean(profane_n),\n            wild_emojis_n_mean = mean(wild_emojis_n),\n            exclaims_n_mean = mean(textfeature_text_copy_n_exclaims)) %&gt;% \n  rowwise() %&gt;% \n  mutate(hate_proxy = mean(c_across(emo_words_n_mean:exclaims_n_mean))) %&gt;% \n  ungroup()\n## `summarise()` has grouped output by 'month'. You can override using the\n## `.groups` argument.\n  \nres_summary2 %&gt;% \n  head()\n\n\n\n  \n\n\n\nLangifizieren fürs Plotten:\n\nres_summary2_long &lt;- \n  res_summary2 %&gt;% \n  pivot_longer(emo_words_n_mean:hate_proxy)\n\nres_summary2_long %&gt;% \n  head()\n\n\n\n  \n\n\n\n\nres_summary2_long %&gt;% \n  count(month)\n\n\n\n  \n\n\n\n\nres_summary2_long %&gt;% \n  ggplot() +\n  aes(x = month, y = value) +\n  facet_grid(dataset  ~ name) +\n  geom_point() +\n  geom_line(group=1, alpha = .7)\n## Warning: Removed 5 rows containing missing values (`geom_point()`).\n## Warning: Removed 5 rows containing missing values (`geom_line()`)."
  },
  {
    "objectID": "100-nn-quickstart.html#quick-start",
    "href": "100-nn-quickstart.html#quick-start",
    "title": "\n12  Einstieg in Neuronale Netze\n",
    "section": "\n12.1 Quick-Start",
    "text": "12.1 Quick-Start\n\n12.1.1 Quick Start mit R\nWir halten uns an das Tutorial von TensforFlow for R, “Hello, World!”.\n\n12.1.1.1 Setup\nWir starten die benötigten Pakete:\n\nlibrary(keras)  # TensorFlow API\nlibrary(tensorflow)  # TensorFlow pur\nlibrary(tidyverse)  # Datenjudo\nlibrary(tictoc)  # Zeitmessung\n\nDas Installieren von TensorFlow bzw. Keras kann Schwierigkeiten bereiten. Tipp: Stellen Sie in RStudio sicher, dass Sie die richtige Python-Version verwenden.\n\nmnist &lt;- dataset_mnist()\nX_train &lt;- mnist$train$x\nX_test &lt;- mnist$test$x\ny_train &lt;- mnist$train$y\ny_test &lt;- mnist$test$y\n\nIn Kurzform kann man synonym schreiben:\n\nc(c(x_train, y_train), c(x_test, y_test)) %&lt;-% keras::dataset_mnist()\n\n\n12.1.1.2 Visualisieren\nWählen wir ein Bild aus; das schauen wir uns näher an Quelle.\n\nimage_id &lt;- 2\nmy_image &lt;- mnist$train$x[image_id, 1:28, 1:28] %&gt;%\n                as_tibble()\n## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n## `.name_repair` is omitted as of tibble 2.0.0.\n## ℹ Using compatibility `.name_repair`.\n\nmy_image\n\n\n\n  \n\n\n\n\nmy_image_prepared &lt;- \n  my_image |&gt; \n  rownames_to_column(var = 'y') %&gt;% \n  pivot_longer(names_to = \"x\", values_to = \"val\", V1:V28) %&gt;%\n  mutate(x = str_replace(x, 'V', '')) %&gt;% \n  mutate(x = as.numeric(x),\n         y = as.numeric(y)) %&gt;% \n  mutate(y = 28-y) \n\n\nhead(my_image_prepared)\n\n\n\n  \n\n\n\nSo, genug der Vorarbeiten, jetzt plotten:\n\nmy_image_prepared %&gt;%\n  ggplot(aes(x, y))+\n  geom_tile(aes(fill = val + 1))+\n  coord_fixed()+\n  theme_void()+\n  theme(legend.position=\"none\") +\n  scale_fill_viridis_c()\n\n\n\n\n\n12.1.1.3 Neuronales Netz 1\nFür unser Netzwerk wollen wir Werte zwischen 0 und 1, daher teilen wir durch den Max-Wert, d.i. 255:\n\nx_train &lt;- X_train / 255\nx_test &lt;-  X_test / 255\n\n\nmodel &lt;- keras_model_sequential(input_shape = c(28, 28)) %&gt;%\n  layer_flatten() %&gt;%\n  layer_dense(128, activation = \"relu\") %&gt;%\n  layer_dropout(0.2) %&gt;%\n  layer_dense(10)\n\nHier ist eine Beschreibung des Modells:\n\nmodel\n## Model: \"sequential\"\n## ________________________________________________________________________________\n##  Layer (type)                       Output Shape                    Param #     \n## ================================================================================\n##  flatten (Flatten)                  (None, 784)                     0           \n##  dense_1 (Dense)                    (None, 128)                     100480      \n##  dropout (Dropout)                  (None, 128)                     0           \n##  dense (Dense)                      (None, 10)                      1290        \n## ================================================================================\n## Total params: 101770 (397.54 KB)\n## Trainable params: 101770 (397.54 KB)\n## Non-trainable params: 0 (0.00 Byte)\n## ________________________________________________________________________________\n\nDann definieren wir eine Fehlerfunktion:\n\nloss_fn &lt;- loss_sparse_categorical_crossentropy(from_logits = TRUE)\n\nBevor wir das Modul trainieren, konfigurieren wir es und kompilieren wir es in Maschinencode:\n\nmodel %&gt;% compile(\n  optimizer = \"adam\",\n  loss = loss_fn,\n  metrics = \"accuracy\"\n)\n\nJetzt ist Trainingszeit, das besorgt die fit-Methode:\n\ntic()\nmodel %&gt;% fit(x_train, y_train, epochs = 5)\n## Epoch 1/5\n## 1875/1875 - 6s - loss: 0.2933 - accuracy: 0.9140 - 6s/epoch - 3ms/step\n## Epoch 2/5\n## 1875/1875 - 5s - loss: 0.1415 - accuracy: 0.9579 - 5s/epoch - 3ms/step\n## Epoch 3/5\n## 1875/1875 - 5s - loss: 0.1065 - accuracy: 0.9680 - 5s/epoch - 3ms/step\n## Epoch 4/5\n## 1875/1875 - 5s - loss: 0.0869 - accuracy: 0.9737 - 5s/epoch - 3ms/step\n## Epoch 5/5\n## 1875/1875 - 5s - loss: 0.0763 - accuracy: 0.9751 - 5s/epoch - 3ms/step\ntoc()\n## 27.897 sec elapsed\n\nDie Modellgüte überprüfen wir natürlich im Test-Set:\n\nmodel %&gt;% evaluate(x_test,  y_test, verbose = 2)\n## 313/313 - 1s - loss: 0.0700 - accuracy: 0.9785 - 646ms/epoch - 2ms/step\n##       loss   accuracy \n## 0.06995549 0.97850001\n\nNicht schlecht: Knapp 98% Trefferquote.\nUnd hier sind die Vorhersagen für die ersten zwei Bilder:\n\npredictions &lt;- predict(model, x_test[1:2, , ])\n## 1/1 - 0s - 92ms/epoch - 92ms/step\n\npredictions\n##           [,1]       [,2]      [,3]      [,4]      [,5]       [,6]       [,7]\n## [1,] -3.905925 -10.611533 -1.400373  4.933351 -16.82393 -8.1285248 -18.969732\n## [2,] -7.280436   5.272494 11.744059 -3.961411 -19.05402 -0.2386339  -2.736672\n##            [,8]      [,9]     [,10]\n## [1,]   9.691772 -4.551620  -1.77038\n## [2,] -20.078672 -0.959926 -14.45026\n\nHm, das sind ja keine Wahrscheinlichkeiten? Stimmt! Es sind Logits. Daher müssen wir noch konvertieren:\n\ntf$nn$softmax(predictions)\n## tf.Tensor(\n## [[1.23274579e-06 1.50891550e-09 1.51015303e-05 8.50593665e-03\n##   3.02450936e-12 1.80726467e-08 3.53789682e-13 9.91466632e-01\n##   6.46325897e-07 1.04310670e-05]\n##  [5.45872712e-09 1.54439918e-03 9.98445655e-01 1.50843240e-07\n##   4.20620588e-14 6.24176197e-06 5.13361230e-07 1.50969060e-14\n##   3.03426854e-06 4.20024149e-12]], shape=(2, 10), dtype=float64)\n\nOb ihr wirklich richtig steht, seht ihr, wenn das Licht angeht:\n\ny_test[1:2]\n## [1] 7 2\n\nSieht gut aus!\n\n12.1.1.4 Best-Bet-Digit\nMöchte man ein Modell, das gleich die “Best-Bet-Digit” nennt, kann man das so machen:\n\npredictions &lt;- model %&gt;%\n  predict(X_test[1:5, , ]) %&gt;%  # nur die ersten 5\n  k_argmax()\n## 1/1 - 0s - 65ms/epoch - 65ms/step\n\npredictions$numpy()\n## [1] 7 2 1 0 4\n\nOder ein eigenes, dazu passendes Modell bauen:\n\nprobability_model &lt;- \n  keras_model_sequential() %&gt;%\n  model() %&gt;%\n  layer_activation_softmax() %&gt;%\n  layer_lambda(tf$argmax)\n\nHier sind die Vorhersagen:\n\nprobability_model(x_test[1:5, , ])\n## tf.Tensor([3 2 1 0 4 2 2 0 2 4], shape=(10), dtype=int64)\n\n\ny_test[1:5]\n## [1] 7 2 1 0 4\n\n\n\n\n\n12.1.2 Quick-Start mit Python und Colab\nAm einfachsten ist der Einstieg mit Google Colab, wo Python voreingestellt ist. Beginnen Sie mit dem [MNIST-Tutorial](https://www.tensorflow.org/tutorials/quickstart/beginner."
  },
  {
    "objectID": "100-nn-quickstart.html#vertiefung",
    "href": "100-nn-quickstart.html#vertiefung",
    "title": "\n12  Einstieg in Neuronale Netze\n",
    "section": "\n12.2 Vertiefung",
    "text": "12.2 Vertiefung\nDie TensforFlow-Docs bieten einen guten Einstieg in Keras und TensorFlow."
  },
  {
    "objectID": "100-nn-quickstart.html#fallstudien",
    "href": "100-nn-quickstart.html#fallstudien",
    "title": "\n12  Einstieg in Neuronale Netze\n",
    "section": "\n12.3 Fallstudien",
    "text": "12.3 Fallstudien\n\nMNIST einen Schritt weiter\nFashion-MNIST"
  },
  {
    "objectID": "200-projektmgt.html#pipeline-management",
    "href": "200-projektmgt.html#pipeline-management",
    "title": "\n13  Projektmanagement\n",
    "section": "\n13.1 Pipeline-Management",
    "text": "13.1 Pipeline-Management\n\n13.1.1 Am Anfang\nSie haben Großes vor! Naja, zumindest planen Sie ein neues Data-Science-Projekt.\nUnd, schlau wie Sie sind, stürzen Sie nicht sofort an die Tastatur, um sich einige Modelle berechnen zu lassen. Nein! Sie denken erst einmal nach. Zum Beispiel, wie die einzelnen Analyseschritte aussehen, worin sie bestehen, und in welcher Abfolge sie zu berechnen sind, s. Abbildung 13.1.\n\n\nAbbildung 13.1: So könnte Ihr Projektplan am Anfang aussehen, man spricht auch von einer Pipeline\n\n\n\n\n\n\n\nHinweis\n\n\n\nDen Graph der einzelnen Analyseschritte in ihrer Abhängigkeit bezeichnet man als *Pipeline.\n\n\n\n13.1.2 Sie träumen von einem Werkzeug\nNach einiger Zeit überlegen Sie sich, dass Sie ein System bräuchten, das Ihre Skizze umsetzt in tatsächliche Berechnungen. Und zwar suchen Sie ein Projektmanagement-System das folgendes Desiderata erfüllt:\n\nEs führt die einzelnen Schritte Ihres Projekt, die “Pipeline” in der richtigen Reihenfolge\nEs aktualisiert veraltete Objekte, aber es berechnet nicht Modelle neu, die unverändert sind\nEs ist gut zu debuggen\n\nJa, von so einem Werkzeug träumen Sie.\nUnd tatsächlich, Ihr Traum geht in Erfüllung. Dieses System existiert. Genau genommen gibt es viele Systeme, die sich anschicken, Ihre Wünsche zu erfüllen. Wir schauen uns eines näher an, das speziell für R gemacht ist. Das R-Paket targets.\n\n13.1.3 Targets\nEs lohnt sich, an dieser Stelle den “Walkthrough” aus dem Benutzerhandbuch von Targets durchzuarbeiten.\nFür ein Projekt ähnlich zu den, die wir in diesem Buch bearbeiten, ist folgende _targets.R-Datei ein guter Start.\n\nlibrary(targets)\n\n\n# Funktionen einlesen:\n#purrr::walk(list.files(path = \"funs\", pattern = \".R\", full.names = TRUE), source)\nsource(\"funs/def-recipe.R\")\nsource(\"funs/read-train-data.R\")\nsource(\"funs/read-test-data.R\")\n\n# Optionen, z.B. allgemein verfügbare Pakete in den Targets:tar_option_set(packages = c(\"readr\", \n                            \"dplyr\", \n                            \"ggplot2\", \n                            \"purrr\", \n                            \"easystats\", \n                            \"tidymodels\", \n                            \"textrecipes\"))\n\n# Definition der Pipeline:\nlist(\n  tar_target(data_train, read_train_data()),\n  tar_target(data_test, read_test_data()),\n  tar_target(recipe1, def_recipe(data_train)\n  ),\n  tar_target(model1,\n             logistic_reg(penalty = tune(), mixture = 1) %&gt;%\n               set_mode(\"classification\") %&gt;%\n               set_engine(\"glmnet\")\n             ),\n  tar_target(workflow1,\n             workflow() %&gt;% add_recipe(recipe1) %&gt;% add_model(model1)\n             ),\n  tar_target(grid1,\n             grid_regular(penalty(), levels = 3)\n             ),\n  tar_target(grid_fitted,\n             tune_grid(workflow1, \n                       resamples = vfold_cv(data_train, v = 2),\n                       grid = grid1)\n  ),\n  tar_target(best_hyperparams,\n             select_by_one_std_err(grid_fitted, metric = \"roc_auc\", penalty)\n             ),\n  tar_target(fit1,\n             workflow1 %&gt;% finalize_workflow(best_hyperparams) %&gt;% fit(data_train)),\n  tar_target(preds,\n             fit1 %&gt;% \n               predict(data_test) %&gt;% \n               bind_cols(data_test) %&gt;% \n               mutate(c1 = factor(c1))),\n  tar_target(metrics1,\n             preds %&gt;% metrics(truth = c1, .pred_class))\n)\n\nDann kann man auf den Play-Button drücken und die ganze Pipeline wird berechnet:\n\ntar_make()\n\nWenn die Pipeline aktuell ist, und nichts berechnet werden muss (und daher auch schon fehlerfrei durchgelaufen ist), sieht die Ausgabe so aus:\n✔ skip target grid1\n✔ skip target model1\n✔ skip target data_train\n✔ skip target data_test\n✔ skip target recipe1\n✔ skip target workflow1\n✔ skip target grid_fitted\n✔ skip target best_hyperparams\n✔ skip target fit1\n✔ skip target preds\n✔ skip target metrics1\n✔ skip pipeline [0.121 seconds]\nDie Pipeline kann man sich als DAG bzw. als Abhängigkeitsgraph visualisieren lassen:\n\ntar_visnetwork()\n\n\n\nAbhängigkeitsgraph der Pipeline\n\nEinzelne Objekte kann man sich komfortabel anschauen mit tar_load(objekt), z.B. tar_load(fit1) usw.\n\n13.1.4 Eine Pipeline als Spielwiese\nDieses Github-Repo stellt Ihnen eine “Spielwiese” zur Verfügung, wo Sie sich mit Pipleines à la Targets vertraut machen können."
  },
  {
    "objectID": "200-projektmgt.html#zeit-sparen",
    "href": "200-projektmgt.html#zeit-sparen",
    "title": "\n13  Projektmanagement\n",
    "section": "\n13.2 Zeit sparen",
    "text": "13.2 Zeit sparen\nEiner Maschine etwas beizubringen kann dauern … Ein einfaches Rechenbeispiel dazu:\n\nSie haben eine Kreuzvalidierung mit 10 Faltungen\nund 3 Wiederholungen\nund 3 Tuningparameter\nmit je 10 Werten\n\nDas sind 1033*10=900 Wiederholungen.\nLeider haben Sie noch in den ersten 10 Versuchen jeweils einen Bug, so dass sich die Rechenzeit noch einmal um den Faktor 10 erhöht…\nDie Rechenzeit kann also schnell ins astronomische steigen. Es braucht also Methoden, um Rechenzeit zu sparen.1 Einige Methoden zum Rechenzeit sparen sind:\n\n\nCloud: Cloud-Dienste in Anspruch nehmen (faktisch mietet man damit schnelle Rechner)\n\nParallelisierung: Mehrere Kerne des eigenen Computers nutzen\n\nUpgrade: Kaufen Sie sich einen schnelleren Rechner…\n\nCleveres Grid-Search: Methoden wie ANOVA Racing können die Rechenzeit - was das Tuning - betrifft - deutlich verringern.\n\nDieser Post gibt einen Überblick zu Rechenzeiten bei verschiedenen Tuningparameter-Optionen mit Tidymodels.\nNatürlich ist die (mit Abstand) beste Methode: guten Code schreiben. Denn “guter Code” verringert die Wahrscheinlichkeit von Bugs, und damit die Gefahr, dass die ganze schöne Rechenzeit für die Katz war.\n“Guter Code” ist vielleicht primär von zwei Dingen abhängig: erstens einen guten Plan zu haben bevor man das Programmieren anfängt und zweitens gute Methoden des Projektmanagements. Hunt und Thomas (2000) präsentieren eine weithin anerkannte Umsetzung, was “guter” Code bedeuten könnte."
  },
  {
    "objectID": "200-projektmgt.html#publizieren",
    "href": "200-projektmgt.html#publizieren",
    "title": "\n13  Projektmanagement\n",
    "section": "\n13.3 Publizieren",
    "text": "13.3 Publizieren\nSie haben eine super Analyse geschrieben, eine schicke Pipeline, und jetzt soll die Welt davon erfahren? Es gibt einige komfortable Möglichkeiten, Ihre Arbeit zu publizieren, z.B. als Blog mit Quarto.\nDieses Video zeigt Ihnen wie man einen Quarto-Blog in RStudio erstellt und ihn bei Netlify publiziert.\n\nDas Hosten bzw. Deployen bei Netlify ist kostenlos (in der Basis-Variante).\nSie können alternativ Github Pages als Hosting-Dienst verwenden. Dieses Video gibt dazu eine Anleitung."
  },
  {
    "objectID": "200-projektmgt.html#komplexitätsmanagement",
    "href": "200-projektmgt.html#komplexitätsmanagement",
    "title": "\n13  Projektmanagement\n",
    "section": "\n13.4 Komplexitätsmanagement",
    "text": "13.4 Komplexitätsmanagement\nProgrammieren ist faszinierend. Vor allem, wenn das Programm funktioniert. Genau genommen ist es eigentlich nur dann faszinierend, ansonsten wird es anstrengend? aufregend? süchtig? faszinierend? nervig? Wie auch immer: Bugs treten auf und mit steigender Komplexität Ihrer Software steigen die Bugs nicht linear, sondern eher quadratisch oder gar exponentiell an.\nEs gibt viele Ansätze, sich gegen die Komplexität zu “wehren”. Der beste ist vielleicht: Die Software so einfach wie möglich zu halten - und nur so komplex wie nötig. Sozusagen: Das beste Feature ist das, das Sie nicht implementieren.\n\n13.4.1 Geben Sie gute Namen\nDaraus leitet sich ab, dass die zentralen Methoden, um der Fehler Herr zu werden im Komplexitätsmanagement liegen. Den Variablen (Objekten) gute, “sprechende” aber prägnante Namen zu geben, ist in diesem Lichte auch als Komplexitätsmanagement (Reduktion) zu verstehen.\nEin typischer Fehler, der mir immer mal wieder passiert, ist: Ich ändere den Namen eines Objekts, aber vergesse, an allen Stellen im Code den Namen anzupassen. Glücklicherweise gibt es hier eine einfache Abhilfe: Replace-All.\n\n13.4.2 Portionieren\nEine andere, zentrale Maßnahme ist es, den Code in handlichen “Häppchen” zu verpacken. Statt einer Skriptdatei mit zich Tausend Zeilen, wünschen Sie sich doch sicher ein Skript der Art:\nmache_1()\nmache_2()\nmache_3()\ngratuliere_fertig()\nSchaut man dann in mache_1() rein, sieht man wiederum übersichtlichen Code.\nFunktionales Programmieren ist eine Umsetzung davon: Jedes Häppchen, jeder Schritt ist eine Funktion. Eine Funktion hat Input und Output; der Output ist dann der Input für die Funktion des nächsten Schrittes. targets ist eine Umsetzung dieser Idee.\n\n13.4.3 Debugging mit einem Logger\nWenn das Kind in dem Brunnen gefallen ist, hilft nur Heulen und Wehklagen Das Problem finden und lösen. Mit einem Logger kann man sich das Entwanzen, das Finden der Fehler, erleichtern. Ein Logger schreibt Zwischenschritte in eine Log-Datei.\nHier ist ein Beispiel mit dem futile Logger:. Mein Problem war, dass ich eine dynamische Aufgabe für eine Statistik-Klausur programmiert hatte, aber leider gab es einen Bug, den ich nicht gefunden habe2.\nDie Lösung brachte ein Logger, mit dem ich den Wert zentraler Variablen im Verlauf des Durchlaufens des Codes - bis eben der Laufzeitfehler aufkam3.\nHier ist ein Ausschnitt der Syntax. Zuerst initialisiert man den Logger mit einer Datei, hier exams.log. Neue Logging-Inhalte sollen an die bestehenden Logs angehängt werden (appender).\n\nlibrary(futile.logger)\nflog.appender(appender.file(\"/Users/sebastiansaueruser/github-repos/rexams-exams/exams.log\"))\n\nDann gebe ich eine Loggings vom Typ “Info” zum Protokoll:\n\nflog.info(paste0(\"Ex: post-uncertainty1\"))\nflog.info(msg = paste0(\"Data set: \", d_name))\nflog.info(paste0(\"Preds chosen: \", stringr::str_c(preds_chosen, collapse = \", \")))\nflog.info(paste0(\"Output var: \", av))\n\nDie Ergebnisse kann man dann in der Logging-Datei anschauen:\nNFO [2023-01-05 11:27:51] Rhats: 1.004503053029\nINFO [2023-01-05 11:27:51] Sol: 0.18\nINFO [2023-01-05 11:27:51] Sol typeof: double\nINFO [2023-01-05 11:27:52] Ex: post-uncertainty1\nINFO [2023-01-05 11:27:52] Data set: tips\nINFO [2023-01-05 11:27:52] Preds chosen: size, total_bill\nINFO [2023-01-05 11:27:52] Output var: tip\nINFO [2023-01-05 11:27:53] Rhats: 0.999004883794722\nINFO [2023-01-05 11:27:53] Rhats: 1.00021605674421\nINFO [2023-01-05 11:27:53] Rhats: 1.00091357638756\nINFO [2023-01-05 11:27:53] Sol: 0.32\nINFO [2023-01-05 11:27:53] Sol typeof: double\nINFO [2023-01-05 11:27:54] Ex: post-uncertainty1\nINFO [2023-01-05 11:27:54] Data set: TeachingRatings\nINFO [2023-01-05 11:27:54] Preds chosen: prof, beauty\nINFO [2023-01-05 11:27:54] Output var: eval\nINFO [2023-01-05 11:27:55] Rhats: 0.999060308710712\nINFO [2023-01-05 11:27:55] Rhats: 0.999032305267221\nINFO [2023-01-05 11:27:55] Rhats: 0.999229003550072\nINFO [2023-01-05 11:27:55] Sol: 0\nINFO [2023-01-05 11:27:55] Sol typeof: double\nINFO [2023-01-05 11:27:56] Ex: post-uncertainty1\nINFO [2023-01-05 11:27:56] Data set: gtcars\nINFO [2023-01-05 11:27:56] Preds chosen: mpg_c, year\nINFO [2023-01-05 11:27:56] Output var: msrp\nINFO [2023-01-05 11:28:00] Rhats: 0.99913061005524\nINFO [2023-01-05 11:28:00] Rhats: 0.998999786100339\nINFO [2023-01-05 11:28:00] Rhats: 0.999130286784586\nINFO [2023-01-05 11:28:01] Sol: 21959.35\nINFO [2023-01-05 11:28:01] Sol typeof: double\nJa, das sieht nicht schön aus. Aber es brachte mir die Lösung: Mir fiel auf, dass der Fehler nur auftrat, wenn sol einen großen Wert hatte (1000 oder mehr). Danke, Logger!\n\n\n\n\nHunt, Andrew, und David Thomas. 2000. The Pragmatic Programmer from Journeyman to Master. Reading, Mass.: Addison-Wesley."
  },
  {
    "objectID": "200-projektmgt.html#footnotes",
    "href": "200-projektmgt.html#footnotes",
    "title": "\n13  Projektmanagement\n",
    "section": "",
    "text": "Allerdings haben lange Rechenzeiten auch Vorteile, wie dieses XKCD-Cartoon zeigt.↩︎\nStackOverflow hat mich dann gerettet↩︎\nERROR!↩︎"
  },
  {
    "objectID": "300-fallstudien.html#quellen-für-textdaten",
    "href": "300-fallstudien.html#quellen-für-textdaten",
    "title": "14  Fallstudien",
    "section": "14.1 Quellen für Textdaten",
    "text": "14.1 Quellen für Textdaten\nDer MonkeyLearn Blog liefert eine Reihe von Quellen zu API, die Textdaten bereitstellen."
  }
]