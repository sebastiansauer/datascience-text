[
  {
    "objectID": "120-transformer.html#vorab",
    "href": "120-transformer.html#vorab",
    "title": "\n16¬† Transformer\n",
    "section": "\n16.1 Vorab",
    "text": "16.1 Vorab\n\n16.1.1 Lernziele\n\nSie k√∂nnen die grundlegende Architektur eines Transformer-Modells beschreiben.\nSie k√∂nnen Transformer-Modelle mit der API von Hugging-Face berechnen.\n\n16.1.2 Begleitliteratur\nDer Blogpost von Jay Alammar gibt einen illustrierten √úberblick √ºber Transformer.\n\n16.1.3 Ben√∂tigte Software\nWir ben√∂tigen Python, R sowei einige im Folgenden aufgef√ºhrte Python-Module.\n\nimport pandas as pd\nimport os\n\nF√ºr den Sch√ºleraustausch von R nach Python nutzen wir das R-Paket reticulate:\n\nlibrary(reticulate)\n\nAu√üerdem starte ich die ‚Äúrichtige‚Äù Python-Version, wo die ben√∂tigten Pakete (in der richtigen Version) installiert sind:\n\n#use_virtualenv(\"r-tensorflow\")\n\nCheck:\n\npy_available()\n## [1] TRUE\n\nWelche Python-Version nutzt reticulate gerade?\n\npy_config()\n## python:         /Users/sebastiansaueruser/.virtualenvs/r-tensorflow/bin/python\n## libpython:      /Users/sebastiansaueruser/.pyenv/versions/3.8.16/lib/libpython3.8.dylib\n## pythonhome:     /Users/sebastiansaueruser/.virtualenvs/r-tensorflow:/Users/sebastiansaueruser/.virtualenvs/r-tensorflow\n## version:        3.8.16 (default, Sep 15 2023, 17:53:02)  [Clang 14.0.3 (clang-1403.0.22.14.1)]\n## numpy:          /Users/sebastiansaueruser/.virtualenvs/r-tensorflow/lib/python3.8/site-packages/numpy\n## numpy_version:  1.24.3\n## \n## NOTE: Python version was forced by VIRTUAL_ENV",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Transformer</span>"
    ]
  },
  {
    "objectID": "120-transformer.html#√ºberblick",
    "href": "120-transformer.html#√ºberblick",
    "title": "\n16¬† Transformer\n",
    "section": "\n16.2 √úberblick",
    "text": "16.2 √úberblick\nTransformer sind eine Architekturvariante neuronaler Netze. Sie stellen die Grundlage vieler aktueller gro√üer Sprachmodelle1; da sie einige Vorz√ºge gegen√ºber Vorg√§ngermodellen aufweisen, haben sie einen zentralen Platz f√ºr verschiedenen Aufgaben des NLP eingenommen.\nIm Jahr 2017 erschien ein Paper auf Arxive mit dem Titel ‚ÄúAttention is all you need‚Äù, Vaswani u.¬†a. (2023)2. Transformer basieren auf einer bestimmten Art von ‚ÄúAufmerksamkeit‚Äù, genannt Selbst-Aufmerksamkeit (self-attention). Nat√ºrlich ist damit eine bestimmte Architektur im neuronalen Netzwerk gemeint, kein kognitivpsychologiches Konstruktr; allerdings lehnt sich die Methode an Konzepte der Kognitionspsychologie vage an.\nSelf-Attention weist zwei gro√üe Verteile auf: Erstens erlaubt es parallele Verarbeitung, was viele Vorg√§ngermodelle nicht erlaubten. Zweitens kann es den Kontext eines Tokens, also den Text um ein bestimmtes Wort herum, deutlich besser ‚Äúim Blick‚Äù (oder in der Aufmerksamkeit) behalten als viele Vorg√§ngermodelle.\nGerade f√ºr Daten mit sequenziellem Charakter, wie Text oder Sprache, sind Transformer-Modelle gut geeignet3.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Transformer</span>"
    ]
  },
  {
    "objectID": "120-transformer.html#grundkonzepte",
    "href": "120-transformer.html#grundkonzepte",
    "title": "\n16¬† Transformer\n",
    "section": "\n16.3 Grundkonzepte",
    "text": "16.3 Grundkonzepte",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Transformer</span>"
    ]
  },
  {
    "objectID": "120-transformer.html#einf√ºhrung-in-hugging-face",
    "href": "120-transformer.html#einf√ºhrung-in-hugging-face",
    "title": "\n16¬† Transformer\n",
    "section": "\n16.4 Einf√ºhrung in Hugging Face ü§ó",
    "text": "16.4 Einf√ºhrung in Hugging Face ü§ó\nDieser Abschnitt orientiert sich an Tunstall u.¬†a. (2022). Die Syntax zu allen Kapiteln des Buchs findet sich praktischerweise in diesem Github-Repo.\nBei ü§ó liegt der Schwerpunkt klar bei Python, nicht bei R. Allerdings erlaubt RStudio ein einfaches Wechseln zwischen R und Python: Funktionen und Daten aus Python k√∂nnen einfach mit dem $-Operator angesprochen werden. In diesem Post wirds das demonstriert.\nSchauen wir uns das einf√ºhrende Beispiel aus Tunstall u.¬†a. (2022). an.\n\n16.4.1 Hugging Face mit R\nHier ein ein Text-Schnipsel, dessen Sentiment wir detektieren wollen:\n\ntext &lt;- (\"Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\")\n\nUnd hier in der Python-Version:\n\ntext_py = r.text\n\nDann importieren wir die n√∂tigen Module:\n\n\nPython\nR\n\n\n\n\n#import tensorflow\nfrom transformers import pipeline\n\nNat√ºrlich m√ºssen Python-Module installiert sein, bevor man sie nutzen kann, genau so wie R-Pakete.\n\n\nMan kann die die Python-Module auch √ºber R starten:\n\ntransformers &lt;- reticulate::import(\"transformers\")\n\n\n\n\n\n16.4.2 Einfache Pipeline\n{.panel-tabset}\n\n16.4.3 Python\nWir bereiten das Modell vor; im Default wird distilbert-base-uncased-finetuned-sst-2-english verwendet.\n\nclassifier = pipeline(\"text-classification\")\n## No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n## Using a pipeline without specifying a model name and revision in production is not recommended.\n## All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n## \n## All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n## If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Transformer</span>"
    ]
  },
  {
    "objectID": "120-transformer.html#germeval-out-of-the-box-mit-hugging-face",
    "href": "120-transformer.html#germeval-out-of-the-box-mit-hugging-face",
    "title": "\n16¬† Transformer\n",
    "section": "\n16.5 Germeval Out-of-the-Box mit Hugging Face",
    "text": "16.5 Germeval Out-of-the-Box mit Hugging Face\nZuert importieren wir die Daten.\n\n\nR\nPython\n\n\n\n\ndata(germeval_train, package = \"pradadata\")\ntext &lt;- germeval_train$text[1:2]\ntext[1:2]\n## [1] \"@corinnamilborn Liebe Corinna, wir w√ºrden dich gerne als Moderatorin f√ºr uns gewinnen! W√§rst du begeisterbar?\"                                 \n## [2] \"@Martin28a Sie haben ja auch Recht. Unser Tweet war etwas missverst√§ndlich. Dass das BVerfG Sachleistungen nicht ausschlie√üt, kritisieren wir.\"\n\n\n\n\ngermeval_train_py = r.text\n\n\n\n\n\n16.5.1 Standard-Pipeline\n\nclassifier = pipeline(\"text-classification\")\n## No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n## Using a pipeline without specifying a model name and revision in production is not recommended.\n## All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n## \n## All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n## If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\noutputs2 = classifier(germeval_train_py)\noutputs2\n## [{'label': 'NEGATIVE', 'score': 0.9950070381164551}, {'label': 'NEGATIVE', 'score': 0.9954568147659302}]\n\nTja, vielleicht sollten wir ein Modell verwenden, das die deutsche Sprache versteht?\n\n16.5.2 Man spricht Deutsh\nAuf Hugging Face gibt es eine Menge von Modellen. Welches nehm ich nur? DISTILBERT oder BERT-Varianten d√ºrfte kein schlechter Start sein.\n\n#classifier = pipeline(\"text-classification\", model=\"distilbert-base-german-cased\")\n\n\nclassifier = pipeline(\n  \"text-classification\", model=\"oliverguhr/german-sentiment-bert\")\n## All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n## \n## All the weights of TFBertForSequenceClassification were initialized from the PyTorch model.\n## If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n\n\noutputs3 = classifier(germeval_train_py)\ndf = pd.DataFrame(outputs3)    \ndf.head()\n##       label     score\n## 0   neutral  0.987253\n## 1  negative  0.918047\n\n\ndf_r &lt;- py$pd\nhead(df_r)",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Transformer</span>"
    ]
  },
  {
    "objectID": "120-transformer.html#openai-api",
    "href": "120-transformer.html#openai-api",
    "title": "\n16¬† Transformer\n",
    "section": "\n16.6 OpenAI-API",
    "text": "16.6 OpenAI-API\n\n\n\n\n\n\nWichtig\n\n\n\nDer API-Aufruf von ChatGPT kostet Geld üí∏. \\(\\square\\)\n\n\n\n16.6.1 Authentifizierung\nWir m√ºssen uns bei der API anmelden:\n\n\nR\nPython\n\n\n\n\nopenai_key_r &lt;- Sys.getenv(\"OPENAI_API_KEY\")\n\n\n\n\nopenai_key_py = os.environ.get(\"OPENAI_API_KEY\")\n\n\n\n\n\n\n\n\n\n\nVorsicht\n\n\n\nSpeichern Sie keine sensiblen Daten in geteilten Ordner/Repos. Achten Sie auf Log-Dateien wir .Rhistory, in der u.U. Ihre sensiblen Daten enthalten sein k√∂nnen. \\(\\square\\)\n\n\nEine sichere Variante als das unverschl√ºsselte Speichenr von Passw√∂rtern ist es, sensible Daten mit einem Passwort zu sch√ºtzen. Dazu kann man z.B. in R das Paket keyring nutzen.\n\nlibrary(keyring)\nopenai_key_r &lt;- key_get(\"OPENAI_API_KEY\")\n\n\n16.6.2 Setup\n\nsentiment_scores = []\nsentiment_analysis = []\ntext = '@Martin28a Sie haben ja auch Recht. Unser Tweet war etwas missverst√§ndlich. Dass das BVerfG Sachleistungen nicht ausschlie√üt, kritisieren wir.'\n\n\n16.6.3 Anfrage an die API\n\nprompt = f\"Analysiere das Sentiment des folgenden Texts: \\n{text}\"\n\nresponse = openai.Completion.create(\n        prompt=prompt,\n        engine=\"davinci\",\n        max_tokens=100,\n        temperature=0.5,\n    )",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Transformer</span>"
    ]
  },
  {
    "objectID": "120-transformer.html#vertiefung",
    "href": "120-transformer.html#vertiefung",
    "title": "\n16¬† Transformer\n",
    "section": "\n16.7 Vertiefung",
    "text": "16.7 Vertiefung\nDer Originalartikel von Vaswani u.¬†a. (2023) gibt einen guten Einblick in die Konzepte; der Anspruch ist auf mittlerem Niveau. Von den Hugging-Face-Machern gibt es ein Buch, das - ebenfalls auf mittlerem Niveau - einen Einblick in Transformer-Modelle im Hugging-Face-√ñkosystem gew√§hrt (Tunstall u.¬†a. 2022). Rothman (2022) scheint gute Freunde bei Google zu haben, wenn man sein Buch √ºber Transformer liest, jedenfalls sind die Modelle jener Firma in dem Buch gut gefeatured. G√©ron (2023a) Standardwerk zu Scikit-Learn bietet auch einen Einblick in Attention-Konzepte (Kap. 16). √úbrigens ist das Buch (3. Auflage) jetzt auch in deutscher Sprache erh√§ltlich (G√©ron 2023b).\n\n\n\n\nG√©ron, Aur√©lien. 2023a. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. Third edition. Beijing Boston Farnham Sebastopol Tokyo: O‚ÄôReilly.\n\n\n‚Äî‚Äî‚Äî. 2023b. Praxiseinstieg Machine Learning mit Scikit-Learn, Keras und TensorFlow: Konzepte, Tools und Techniken f√ºr intelligente Systeme. √úbersetzt von Kristian Rother und Thomas Demmig. 3., aktualisierte und erweiterte Auflage. Heidelberg: O‚ÄôReilly.\n\n\nRothman, Denis. 2022. Transformers for Natural Language Processing: Build, Train, and Fine-Tune Deep Neural Network Architectures for NLP with Python, Hugging Face, and OpenAI¬¥s GPT3, ChatGPT, and GPT-4. Second edition. Expert Insight. Birmingham Mumbai: Packt.\n\n\nTunstall, Lewis, Leandro von Werra, Thomas Wolf, und Aur√©lien G√©ron. 2022. Natural Language Processing with Transformers: Building Language Applications with Hugging Face. Revised edition. Sebastopol: O‚ÄôReilly.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, und Illia Polosukhin. 2023. ‚ÄûAttention Is All You Need‚Äú. 1. August 2023. https://doi.org/10.48550/arXiv.1706.03762.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Transformer</span>"
    ]
  },
  {
    "objectID": "200-projektmgt.html#pipeline-management",
    "href": "200-projektmgt.html#pipeline-management",
    "title": "\n17¬† Projektmanagement\n",
    "section": "\n17.1 Pipeline-Management",
    "text": "17.1 Pipeline-Management\n\n17.1.1 Am Anfang\nSie haben Gro√ües vor! Naja, zumindest planen Sie ein neues Data-Science-Projekt.\nUnd, schlau wie Sie sind, st√ºrzen Sie nicht sofort an die Tastatur, um sich einige Modelle berechnen zu lassen. Nein! Sie denken erst einmal nach. Zum Beispiel, wie die einzelnen Analyseschritte aussehen, worin sie bestehen, und in welcher Abfolge sie zu berechnen sind, s. Abbildung¬†17.1.\n\n\n\n\n\nAbbildung¬†17.1: So k√∂nnte Ihr Projektplan am Anfang aussehen, man spricht auch von einer Pipeline\n\n\n\n\n\n\n\n\nHinweis\n\n\n\nDen Graph der einzelnen Analyseschritte in ihrer Abh√§ngigkeit bezeichnet man als *Pipeline.\n\n\n\n17.1.2 Sie tr√§umen von einem Werkzeug\nNach einiger Zeit √ºberlegen Sie sich, dass Sie ein System br√§uchten, das Ihre Skizze umsetzt in tats√§chliche Berechnungen. Und zwar suchen Sie ein Projektmanagement-System das folgendes Desiderata erf√ºllt:\n\nEs f√ºhrt die einzelnen Schritte Ihres Projekt, die ‚ÄúPipeline‚Äù in der richtigen Reihenfolge\nEs aktualisiert veraltete Objekte, aber es berechnet nicht Modelle neu, die unver√§ndert sind\nEs ist gut zu debuggen\n\nJa, von so einem Werkzeug tr√§umen Sie.\nUnd tats√§chlich, Ihr Traum geht in Erf√ºllung. Dieses System existiert. Genau genommen gibt es viele Systeme, die sich anschicken, Ihre W√ºnsche zu erf√ºllen. Wir schauen uns eines n√§her an, das speziell f√ºr R gemacht ist. Das R-Paket targets.\n\n17.1.3 Targets\nEs lohnt sich, an dieser Stelle den ‚ÄúWalkthrough‚Äù aus dem Benutzerhandbuch von Targets durchzuarbeiten.\nF√ºr ein Projekt √§hnlich zu den, die wir in diesem Buch bearbeiten, ist folgende _targets.R-Datei ein guter Start.\n\nlibrary(targets)\n\n\n# Funktionen einlesen:\n#purrr::walk(list.files(path = \"funs\", pattern = \".R\", full.names = TRUE), source)\nsource(\"funs/def-recipe.R\")\nsource(\"funs/read-train-data.R\")\nsource(\"funs/read-test-data.R\")\n\n# Optionen, z.B. allgemein verf√ºgbare Pakete in den Targets:tar_option_set(packages = c(\"readr\", \n                            \"dplyr\", \n                            \"ggplot2\", \n                            \"purrr\", \n                            \"easystats\", \n                            \"tidymodels\", \n                            \"textrecipes\"))\n\n# Definition der Pipeline:\nlist(\n  tar_target(data_train, read_train_data()),\n  tar_target(data_test, read_test_data()),\n  tar_target(recipe1, def_recipe(data_train)\n  ),\n  tar_target(model1,\n             logistic_reg(penalty = tune(), mixture = 1) %&gt;%\n               set_mode(\"classification\") %&gt;%\n               set_engine(\"glmnet\")\n             ),\n  tar_target(workflow1,\n             workflow() %&gt;% add_recipe(recipe1) %&gt;% add_model(model1)\n             ),\n  tar_target(grid1,\n             grid_regular(penalty(), levels = 3)\n             ),\n  tar_target(grid_fitted,\n             tune_grid(workflow1, \n                       resamples = vfold_cv(data_train, v = 2),\n                       grid = grid1)\n  ),\n  tar_target(best_hyperparams,\n             select_by_one_std_err(grid_fitted, metric = \"roc_auc\", penalty)\n             ),\n  tar_target(fit1,\n             workflow1 %&gt;% finalize_workflow(best_hyperparams) %&gt;% fit(data_train)),\n  tar_target(preds,\n             fit1 %&gt;% \n               predict(data_test) %&gt;% \n               bind_cols(data_test) %&gt;% \n               mutate(c1 = factor(c1))),\n  tar_target(metrics1,\n             preds %&gt;% metrics(truth = c1, .pred_class))\n)\n\nDann kann man auf den Play-Button dr√ºcken und die ganze Pipeline wird berechnet:\n\ntar_make()\n\nWenn die Pipeline aktuell ist, und nichts berechnet werden muss (und daher auch schon fehlerfrei durchgelaufen ist), sieht die Ausgabe so aus:\n‚úî skip target grid1\n‚úî skip target model1\n‚úî skip target data_train\n‚úî skip target data_test\n‚úî skip target recipe1\n‚úî skip target workflow1\n‚úî skip target grid_fitted\n‚úî skip target best_hyperparams\n‚úî skip target fit1\n‚úî skip target preds\n‚úî skip target metrics1\n‚úî skip pipeline [0.121 seconds]\nDie Pipeline kann man sich als DAG bzw. als Abh√§ngigkeitsgraph visualisieren lassen:\n\ntar_visnetwork()\n\n\n\nAbh√§ngigkeitsgraph der Pipeline\n\nEinzelne Objekte kann man sich komfortabel anschauen mit tar_load(objekt), z.B. tar_load(fit1) usw.\n\n17.1.4 Eine Pipeline als Spielwiese\nDieses Github-Repo stellt Ihnen eine ‚ÄúSpielwiese‚Äù zur Verf√ºgung, wo Sie sich mit Pipleines √† la Targets vertraut machen k√∂nnen.",
    "crumbs": [
      "Anwendung",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Projektmanagement</span>"
    ]
  },
  {
    "objectID": "200-projektmgt.html#zeit-sparen",
    "href": "200-projektmgt.html#zeit-sparen",
    "title": "\n17¬† Projektmanagement\n",
    "section": "\n17.2 Zeit sparen",
    "text": "17.2 Zeit sparen\nEiner Maschine etwas beizubringen kann dauern ‚Ä¶ Ein einfaches Rechenbeispiel dazu:\n\nSie haben eine Kreuzvalidierung mit 10 Faltungen\nund 3 Wiederholungen\nund 3 Tuningparameter\nmit je 10 Werten\n\nDas sind 1033*10=900 Wiederholungen.\nLeider haben Sie noch in den ersten 10 Versuchen jeweils einen Bug, so dass sich die Rechenzeit noch einmal um den Faktor 10 erh√∂ht‚Ä¶\nDie Rechenzeit kann also schnell ins astronomische steigen. Es braucht also Methoden, um Rechenzeit zu sparen.1 Einige Methoden zum Rechenzeit sparen sind:\n\n\nCloud: Cloud-Dienste in Anspruch nehmen (faktisch mietet man damit schnelle Rechner)\n\nParallelisierung: Mehrere Kerne des eigenen Computers nutzen\n\nUpgrade: Kaufen Sie sich einen schnelleren Rechner‚Ä¶\n\nCleveres Grid-Search: Methoden wie ANOVA Racing k√∂nnen die Rechenzeit - was das Tuning - betrifft - deutlich verringern.\n\nDieser Post gibt einen √úberblick zu Rechenzeiten bei verschiedenen Tuningparameter-Optionen mit Tidymodels.\nNat√ºrlich ist die (mit Abstand) beste Methode: guten Code schreiben. Denn ‚Äúguter Code‚Äù verringert die Wahrscheinlichkeit von Bugs, und damit die Gefahr, dass die ganze sch√∂ne Rechenzeit f√ºr die Katz war.\n‚ÄúGuter Code‚Äù ist vielleicht prim√§r von zwei Dingen abh√§ngig: erstens einen guten Plan zu haben bevor man das Programmieren anf√§ngt und zweitens gute Methoden des Projektmanagements. Hunt und Thomas (2000) pr√§sentieren eine weithin anerkannte Umsetzung, was ‚Äúguter‚Äù Code bedeuten k√∂nnte.\n\n\nQuelle: imgflip.com",
    "crumbs": [
      "Anwendung",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Projektmanagement</span>"
    ]
  },
  {
    "objectID": "200-projektmgt.html#publizieren",
    "href": "200-projektmgt.html#publizieren",
    "title": "\n17¬† Projektmanagement\n",
    "section": "\n17.3 Publizieren",
    "text": "17.3 Publizieren\nSie haben eine super Analyse geschrieben, eine schicke Pipeline, und jetzt soll die Welt davon erfahren? Es gibt einige komfortable M√∂glichkeiten, Ihre Arbeit zu publizieren, z.B. als Blog mit Quarto.\nDieses Video zeigt Ihnen wie man einen Quarto-Blog in RStudio erstellt und ihn bei Netlify publiziert.\n\nDas Hosten bzw. Deployen bei Netlify ist kostenlos (in der Basis-Variante).\nSie k√∂nnen alternativ Github Pages als Hosting-Dienst verwenden. Dieses Video gibt dazu eine Anleitung.",
    "crumbs": [
      "Anwendung",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Projektmanagement</span>"
    ]
  },
  {
    "objectID": "200-projektmgt.html#komplexit√§tsmanagement",
    "href": "200-projektmgt.html#komplexit√§tsmanagement",
    "title": "\n17¬† Projektmanagement\n",
    "section": "\n17.4 Komplexit√§tsmanagement",
    "text": "17.4 Komplexit√§tsmanagement\nProgrammieren ist faszinierend. Vor allem, wenn das Programm funktioniert. Genau genommen ist es eigentlich nur dann faszinierend, ansonsten wird es anstrengend? aufregend? s√ºchtig? faszinierend? nervig? Wie auch immer: Bugs treten auf und mit steigender Komplexit√§t Ihrer Software steigen die Bugs nicht linear, sondern eher quadratisch oder gar exponentiell an.\nEs gibt viele Ans√§tze, sich gegen die Komplexit√§t zu ‚Äúwehren‚Äù. Der beste ist vielleicht: Die Software so einfach wie m√∂glich zu halten - und nur so komplex wie n√∂tig. Sozusagen: Das beste Feature ist das, das Sie nicht implementieren.\n\n17.4.1 Geben Sie gute Namen\nDaraus leitet sich ab, dass die zentralen Methoden, um der Fehler Herr zu werden im Komplexit√§tsmanagement liegen. Den Variablen (Objekten) ‚Äúgute‚Äù, ‚Äúsprechende‚Äù, aber pr√§gnante Namen zu geben, ist in diesem Lichte auch als Komplexit√§tsmanagement (Reduktion) zu verstehen.\nEin typischer Fehler, der mir immer mal wieder passiert, ist: Ich √§ndere den Namen eines Objekts, aber vergesse, an allen Stellen im Code den Namen anzupassen. Gl√ºcklicherweise gibt es hier eine einfache Abhilfe: Replace-All.\nDer zwar einfache Weg, mehrere √§hnliche Objekte durchzunummerieren (workflow2a, recipe1, ‚Ä¶). ist zwar einfach, aber insgesamt nicht zu empfehlen: Es ist nicht leicht, immer genau zu wissen, was der Inhalt hinter der Nummer 2a etc. ist. √Ñndert man au√üerdem die Reihenfolge (oder schiebt ein Objekt dazwischen ein), macht die Nummerierung keinen Sinn mehr (oder man muss m√ºhselig die Nummern √§ndern, was fehleranf√§llig und nervig ist).\n\n17.4.2 Portionieren\nEine andere, zentrale Ma√ünahme ist es, den Code in handlichen ‚ÄúH√§ppchen‚Äù zu verpacken. Statt einer Skriptdatei mit zich Tausend Zeilen, w√ºnschen Sie sich doch sicher ein Skript der Art:\nmache_1()\nmache_2()\nmache_3()\ngratuliere_fertig()\nSchaut man dann in mache_1() rein, sieht man wiederum √ºbersichtlichen Code.\nFunktionales Programmieren ist eine Umsetzung davon: Jedes H√§ppchen, jeder Schritt ist eine Funktion. Eine Funktion hat Input und Output; der Output ist dann der Input f√ºr die Funktion des n√§chsten Schrittes. targets ist eine Umsetzung dieser Idee.\n\n17.4.3 Debugging mit einem Logger\nWenn das Kind in dem Brunnen gefallen ist, hilft nur Heulen und Wehklagen Das Problem finden und l√∂sen. Mit einem Logger kann man sich das Entwanzen, das Finden der Fehler, erleichtern. Ein Logger schreibt Zwischenschritte in eine Log-Datei.\nHier ist ein Beispiel mit dem futile Logger:. Mein Problem war, dass ich eine dynamische Aufgabe f√ºr eine Statistik-Klausur programmiert hatte, aber leider gab es einen Bug, den ich nicht gefunden habe2.\nDie L√∂sung brachte ein Logger, mit dem ich den Wert zentraler Variablen im Verlauf des Durchlaufens des Codes - bis eben der Laufzeitfehler aufkam3.\nHier ist ein Ausschnitt der Syntax. Zuerst initialisiert man den Logger mit einer Datei, hier exams.log. Neue Logging-Inhalte sollen an die bestehenden Logs angeh√§ngt werden (appender).\n\nlibrary(futile.logger)\nflog.appender(appender.file(\"/Users/sebastiansaueruser/github-repos/rexams-exams/exams.log\"))\n\nDann gebe ich eine Loggings vom Typ ‚ÄúInfo‚Äù zum Protokoll:\n\nflog.info(paste0(\"Ex: post-uncertainty1\"))\nflog.info(msg = paste0(\"Data set: \", d_name))\nflog.info(paste0(\"Preds chosen: \", stringr::str_c(preds_chosen, collapse = \", \")))\nflog.info(paste0(\"Output var: \", av))\n\nDie Ergebnisse kann man dann in der Logging-Datei anschauen:\nNFO [2023-01-05 11:27:51] Rhats: 1.004503053029\nINFO [2023-01-05 11:27:51] Sol: 0.18\nINFO [2023-01-05 11:27:51] Sol typeof: double\nINFO [2023-01-05 11:27:52] Ex: post-uncertainty1\nINFO [2023-01-05 11:27:52] Data set: tips\nINFO [2023-01-05 11:27:52] Preds chosen: size, total_bill\nINFO [2023-01-05 11:27:52] Output var: tip\nINFO [2023-01-05 11:27:53] Rhats: 0.999004883794722\nINFO [2023-01-05 11:27:53] Rhats: 1.00021605674421\nINFO [2023-01-05 11:27:53] Rhats: 1.00091357638756\nINFO [2023-01-05 11:27:53] Sol: 0.32\nINFO [2023-01-05 11:27:53] Sol typeof: double\nINFO [2023-01-05 11:27:54] Ex: post-uncertainty1\nINFO [2023-01-05 11:27:54] Data set: TeachingRatings\nINFO [2023-01-05 11:27:54] Preds chosen: prof, beauty\nINFO [2023-01-05 11:27:54] Output var: eval\nINFO [2023-01-05 11:27:55] Rhats: 0.999060308710712\nINFO [2023-01-05 11:27:55] Rhats: 0.999032305267221\nINFO [2023-01-05 11:27:55] Rhats: 0.999229003550072\nINFO [2023-01-05 11:27:55] Sol: 0\nINFO [2023-01-05 11:27:55] Sol typeof: double\nINFO [2023-01-05 11:27:56] Ex: post-uncertainty1\nINFO [2023-01-05 11:27:56] Data set: gtcars\nINFO [2023-01-05 11:27:56] Preds chosen: mpg_c, year\nINFO [2023-01-05 11:27:56] Output var: msrp\nINFO [2023-01-05 11:28:00] Rhats: 0.99913061005524\nINFO [2023-01-05 11:28:00] Rhats: 0.998999786100339\nINFO [2023-01-05 11:28:00] Rhats: 0.999130286784586\nINFO [2023-01-05 11:28:01] Sol: 21959.35\nINFO [2023-01-05 11:28:01] Sol typeof: double\nJa, das sieht nicht sch√∂n aus. Aber es brachte mir die L√∂sung: Mir fiel auf, dass der Fehler nur auftrat, wenn sol einen gro√üen Wert hatte (1000 oder mehr). Danke, Logger!\n\n\n\n\nHunt, Andrew, und David Thomas. 2000. The Pragmatic Programmer from Journeyman to Master. Reading, Mass.: Addison-Wesley.",
    "crumbs": [
      "Anwendung",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Projektmanagement</span>"
    ]
  },
  {
    "objectID": "300-fallstudien.html#quellen-f√ºr-textdaten",
    "href": "300-fallstudien.html#quellen-f√ºr-textdaten",
    "title": "18¬† Fallstudien",
    "section": "18.1 Quellen f√ºr Textdaten",
    "text": "18.1 Quellen f√ºr Textdaten\nDer MonkeyLearn Blog liefert eine Reihe von Quellen zu API, die Textdaten bereitstellen.",
    "crumbs": [
      "Anwendung",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Fallstudien</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Alkomah, Fatimah, and Xiaogang Ma. 2022. ‚ÄúA Literature\nReview of Textual Hate Speech Detection Methods and\nDatasets.‚Äù Information 13 (6, 6): 273. https://doi.org/10.3390/info13060273.\n\n\nAlmeida, Felipe, and Geraldo Xex√©o. 2019. ‚ÄúWord\nEmbeddings: A Survey.‚Äù ArXiv,\nJanuary. https://www.semanticscholar.org/paper/Word-Embeddings%3A-A-Survey-Almeida-Xex%C3%A9o/e28e81a8cb6655aebb72357538f7b7a360366a29.\n\n\nCamacho-Collados, Jose, and Mohammad Taher Pilehvar. 2020.\n‚ÄúEmbeddings in Natural Language Processing.‚Äù\nIn Proceedings of the 28th International Conference on\nComputational Linguistics: Tutorial\nAbstracts, 10‚Äì15. Barcelona, Spain (Online):\nInternational Committee for Computational Linguistics. https://doi.org/10.18653/v1/2020.coling-tutorials.2.\n\n\nCasta√±o-Pulgar√≠n, Sergio Andr√©s, Natalia Su√°rez-Betancur, Luz Magnolia\nTilano Vega, and Harvey Mauricio Herrera L√≥pez. 2021. ‚ÄúInternet,\nSocial Media and Online Hate Speech. Systematic\nReview.‚Äù Aggression and Violent Behavior 58 (May):\n101608. https://doi.org/10.1016/j.avb.2021.101608.\n\n\nChollet, Fran√ßois. 2021. Deep Learning with\nPython. Second edition. Shelter Island,\nNY: Manning.\n\n\nChollet, Fran√ßois, Tomasz Kalinowski, and J. J. Allaire. 2022a. Deep\nLearning with R. Second edition. Shelter Island,\nNY: Manning.\n\n\n‚Äî‚Äî‚Äî. 2022b. Deep Learning with R. Second edition.\nShelter Island, NY: Manning Publications Co.\n\n\nGallatin, Kyle, and Chris Albon. 2023. Machine Learning with\nPython Cookbook: Practical Solutions from Preprocessing to\nDeep Learning. Beijing Boston Farnham Sebastopol\nTokyo: O‚ÄôReilly Media.\n\n\nGeorge, Alexandra. 2022. Python Text Mining: Perform Text\nProcessing, Word Embedding, Text Classification and Machine\nTranslation. Delhi: BPB Publications.\n\n\nG√©ron, Aur√©lien. 2023. Praxiseinstieg Machine Learning mit\nScikit-Learn, Keras und TensorFlow: Konzepte, Tools und Techniken f√ºr\nintelligente Systeme. Translated by Kristian Rother and Thomas\nDemmig. 3., aktualisierte und erweiterte Auflage.\nHeidelberg: O‚ÄôReilly.\n\n\nHunt, Andrew, and David Thomas. 2000. The Pragmatic Programmer from\nJourneyman to Master. Reading, Mass.:\nAddison-Wesley.\n\n\nHvitfeldt, Emil, and Julia Silge. 2021. Supervised Machine\nLearning for Text Analysis in R.\n1st ed. Boca Raton: Chapman and Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2021. An Introduction to Statistical Learning: With Applications in\nR. Second edition. Springer Texts in Statistics.\nNew York: Springer. https://link.springer.com/book/10.1007/978-1-0716-1418-1.\n\n\nK√∂nig, Tim, Wolf J. Sch√ºnemann, Alexander Brand, Julian Freyberg, and\nMichael Gertz. 2022. ‚ÄúThe EPINetz Twitter Politicians\nDataset 2021. A New Resource for the Study of the German\nTwittersphere and Its Application for the 2021 Federal\nElections.‚Äù Politische Vierteljahresschrift 63 (3):\n529‚Äì47. https://doi.org/10.1007/s11615-022-00405-7.\n\n\nKulkarni, Akshay, and Adarsha Shivananda. 2021. Natural Language\nProcessing Recipes: Unlocking Text Data with Machine Learning and Deep\nLearning Using Python. Second edition. New\nYork: Apress.\n\n\nKurz, A. Solomon. 2021. Statistical Rethinking with Brms, Ggplot2,\nand the Tidyverse: Second Edition. https://bookdown.org/content/4857/.\n\n\nLex, Alexander, Nils Gehlenborg, Hendrik Strobelt, Romain Vuillemot, and\nHanspeter Pfister. 2014. ‚ÄúUpSet:\nVisualization of Intersecting Sets.‚Äù IEEE\nTransactions on Visualization and Computer Graphics 20 (12):\n1983‚Äì92. https://doi.org/10.1109/TVCG.2014.2346248.\n\n\nLiu, Zhiyuan, Yankai Lin, and Maosong Sun, eds. 2023. Representation\nLearning for Natural Language Processing.\nSingapore: Springer Nature Singapore. https://doi.org/10.1007/978-981-99-1600-9.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A\nBayesian Course with Examples in R and\nStan. 2nd ed. CRC Texts in Statistical\nScience. Boca Raton: Taylor and Francis, CRC\nPress.\n\n\nMikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.\n‚ÄúEfficient Estimation of Word\nRepresentations in Vector Space.‚Äù September\n6, 2013. https://doi.org/10.48550/arXiv.1301.3781.\n\n\nPennington, Jeffrey, Richard Socher, and Christopher Manning. 2014.\n‚ÄúGloVe: Global Vectors for Word\nRepresentation.‚Äù In Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), 1532‚Äì43. Doha, Qatar:\nAssociation for Computational Linguistics. https://doi.org/10.3115/v1/D14-1162.\n\n\nPilehvar, Mohammad Taher, and Jose Camacho-Collados. 2021.\nEmbeddings in Natural Language Processing:\nTheory and Advances in Vector\nRepresentations of Meaning. Synthesis\nLectures on Human Language Technologies.\nCham: Springer International Publishing. https://doi.org/10.1007/978-3-031-02177-0.\n\n\nRemus, Robert, Uwe Quasthoff, and Gerhard Heyer. 2010.\n‚ÄúSentiWS - a Publicly Available German-Language\nResource for Sentiment Analysis.‚Äù Proceedings of the 7th\nInternational Language Ressources and Evaluation (LREC‚Äô10),\n1168‚Äì71.\n\n\nRhys, Hefin. 2020. Machine Learning with\nR, the Tidyverse, and Mlr. Shelter Island,\nNY: Manning publications.\n\n\nRisch, Julian, Anke Stoll, Lena Wilms, and Michael Wiegand. 2021.\n‚ÄúOverview of the GermEval 2021 Shared Task on the\nIdentification of Toxic, Engaging, and Fact-Claiming Comments.‚Äù\nIn Proceedings of the GermEval 2021 Shared Task on the\nIdentification of Toxic, Engaging, and Fact-Claiming Comments,\n1‚Äì12. Duesseldorf, Germany: Association for\nComputational Linguistics. https://aclanthology.org/2021.germeval-1.1.\n\n\nShannon, C. E. 1948. ‚ÄúA Mathematical Theory of\nCommunication.‚Äù Bell System Technical Journal 27 (3):\n379‚Äì423. https://doi.org/10.1002/j.1538-7305.1948.tb01338.x.\n\n\nSiegel, Melanie, and Melpomeni Alexa. 2020. Sentiment-Analyse\ndeutschsprachiger Meinungs√§u√üerungen: Grundlagen, Methoden und\npraktische Umsetzung. Wiesbaden: Springer\nFachmedien Wiesbaden. https://doi.org/10.1007/978-3-658-29699-5.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with\nR: A Tidy Approach. First edition. Beijing ;\nBoston: O‚ÄôReilly. https://www.tidytextmining.com/.\n\n\nStone, James V. 2019. ‚ÄúInformation Theory: A\nTutorial Introduction.‚Äù June 13, 2019. http://arxiv.org/abs/1802.05968.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data\nScience: Visualize, Model,\nTransform, Tidy, and Import\nData. O‚ÄôReilly Media. https://r4ds.had.co.nz/index.html.\n\n\nWiegand, Michael. 2019a. ‚ÄúGermEval-2018 Corpus\n(DE).‚Äù heiDATA. https://doi.org/10.11588/data/0B5VML.\n\n\n‚Äî‚Äî‚Äî. 2019b. ‚ÄúGermEval-2018 Corpus\n(DE).‚Äù heiDATA. https://doi.org/10.11588/data/0B5VML.\n\n\n‚Äî‚Äî‚Äî. 2019c. ‚ÄúGermEval-2018-Data-master.‚Äù In\nGermEval-2018 Corpus (DE).\nheiDATA. https://doi.org/10.11588/data/0B5VML/XIUWJ7.\n\n\n‚ÄúWord Embeddings in NLP: A Complete\nGuide.‚Äù 2023. Turing. 2023. https://www.turing.com/kb/guide-on-word-embeddings-in-nlp.\n\n\nYamada, Ikuya, and Hiroyuki Shindo. 2019. ‚ÄúNeural Attentive\nBag-of-Entities Model for Text Classification.‚Äù In\nProceedings of the 23th SIGNLL Conference on\nComputational Natural Language Learning, 563‚Äì73. Association\nfor Computational Linguistics.",
    "crumbs": [
      "Abschluss",
      "References"
    ]
  }
]