---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Klassifikation von Hatespeech





## Vorab


### Lernziele


- Sie k√∂nnen grundlegende Verfahren zur Klassifikation von Hatespeech einsetzen und erkl√§ren








### Ben√∂tigte R-Pakete

```{r}
#| message: false
library(tidyverse)
library(rio)
library(tidymodels)
library(tidytext)
library(textrecipes)
library(lsa)  # stopwords
library(discrim)  # naive bayes classification
library(naivebayes)
library(tictoc)  # Zeitmessung
library(fastrtext)  # Worteinbettungen
```


## Daten


F√ºr Maschinenlernen brauchen wir Trainingsdaten,
Daten also, bei denen wir pro Beobachtung der Wert der Zielvariablen kennen.
Man spricht auch von "gelabelten" Daten.

Wir nutzen die Daten von @wiegand_germeval bzw. @wiegand-data.
Die Daten sind unter CC-By-4.0 Int. lizensiert.

```{r}
d_raw <- 
  import("data/germeval2018.training.txt",
         header = FALSE)
```


Da die Daten keine Spaltenk√∂pfe haben, informieren wir die Funktion dazu mit `header = FALSE`.

Benennen wir die die Spalten um:

```{r}
names(d_raw) <- c("text", "c1", "c2")
```

Dabei soll `c1` und `c2` f√ºr die 1. bzw. 2. Klassifikation stehen.


In `c1` finden sich diese Werte:

```{r}
d_raw %>% 
  count(c1)
```

Hier wurde klassifiziert,
ob beleidigende Sprache (offensive language) vorlag oder nicht [@isch-etal-2021-overview, S. 2]:


>   Task 1 was to decide whether a tweet includes some form of offensive language or not. The tweets had to be classiÔ¨Åed into the two classes OFFENSE and OTHER. The OFFENSE category covered abusive language, insults, as well as merely profane statements.


Und in `c2` finden sich folgende Auspr√§gungen:

```{r}
d_raw %>% 
  count(c2)
```


In `c2` ging es um eine feinere Klassifikation beleidigender Sprache [@isch-etal-2021-overview, S. 2]:

>   The second task involved four categories, a nonoffensive OTHER class and three sub-categories of what is OFFENSE in Task 1. In the case of PROFANITY, profane words are used, however, the tweet does not want to insult anyone. This typically concerns the usage of swearwords (Schei√üe, Fuck etc.) and cursing (Zur H√∂lle! Verdammt! etc.). This can be often found in youth language. Swearwords and cursing may, but need not, co-occur with insults or abusive speech. Profane language may in fact be used in tweets with positive sentiment to express emphasis. Whenever profane words are not directed towards a speciÔ¨Åc person or group of persons and there are no separate cues of INSULT or ABUSE, then tweets are labeled as simple cases of PROFANITY.




Sind Texte, die als `OFFENSE` klassifiziert sind,
auch (fast) immer als `ABUSE`, `INSULT` oder `PROFANITY` klassifiziert?


```{r}
d_raw %>% 
  filter(c1 == "OTHER", c2 == "OTHER") %>% 
  nrow() / nrow(d_raw)
```

In ca. 2/3 der F√§lle wurden in beiden Klassifikation `OTHER` klassifiziert.

```{r}
d_raw %>% 
  filter(c1 != "OTHER", c2 != "OTHER") %>% 
  nrow() / nrow(d_raw)
```

Entsprechend in ca. 1/3 der F√§lle wurde jeweils nicht mit `OTHER` klassifiziert.


Wir begn√ºgen uns hier mit der ersten, gr√∂beren Klassifikation.


## Feature Engineering


Reichern wir die Daten mit weiteren Features an,
in der Hoffnung, damit eine bessere Klassifikation erzielen zu k√∂nnen.


### Textl√§nge




```{r}
d2 <-
  d_raw %>% 
  mutate(text_length = str_length(text)) %>% 
  mutate(id = 1:nrow(.))

head(d2)
```



### Sentimentanalyse

Wir nutzen dazu `SentiWS` [@Remus2010].

```{r}
sentiws <- read_csv("https://osf.io/x89wq/?action=download")
```




```{r}
d2_long <-
  d2 %>% 
  unnest_tokens(input = text, output = token)

head(d2_long)
```

Jetzt filtern wir unsere Textdaten so,
dass nur W√∂rter mit Sentimentwert √ºbrig bleiben:

```{r}
d2_long_senti <- 
  d2_long %>%  
  inner_join(sentiws %>% select(-inflections), by = c("token" = "word"))

head(d2_long)
```


Schlie√ülich berechnen wir die Sentimentwert pro Polarit√§t und pro Tweet:

```{r}
d2_sentis <-
  d2_long_senti %>% 
  group_by(id, neg_pos) %>% 
  summarise(senti_avg = mean(value))

head(d2_sentis)
```


Diese Tabelle bringen wir wieder eine breitere Form,
um sie dann wieder mit den Hauptdaten zu vereinigen.


```{r}
d2_sentis_wide <-
  d2_sentis %>% 
  pivot_wider(names_from = "neg_pos", values_from = "senti_avg")

d2_sentis_wide %>% head()
```


```{r}
d3 <-
  d2 %>% 
  full_join(d2_sentis_wide)

head(d3)
```


:::callout-note
Die Sentimentanalyse hier vernachl√§ssigt Flexionen der W√∂rter. 
Der  Autor f√ºhlt den Drang zu schreiben: "Left as an exercise for the reader" :-)
:::


### Schimpfw√∂rter


Z√§hlen wir die Schimpfw√∂rter pro Text.
Dazu nutzen wir die Daten von [LDNOOBW](https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/blob/master/LICENSE), lizensiert nach CC-BY-4.0-Int.




```{r}
schimpf1 <- import("https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/de", format = ",", header = FALSE)
```


L√§nger aber noch ist die Liste aus dem [InsultWiki](https://www.insult.wiki/schimpfwort-liste), lizensiert CC0.


```{r}
schimpf2 <- 
  import("data/insult-de.txt", header = FALSE) %>% 
  mutate_all(str_to_lower)
```


Binden wir die Listen zusammen:

```{r}
schimpf <-
  schimpf1 %>% 
  bind_rows(schimpf2) %>% 
  distinct() %>% 
  rename(word = "V1")

nrow(schimpf)
```


Um die Lesis vor (unn√∂tiger?) Kopfverschmutzung zu bewahren,
sind diese Schimpfw√∂rter hier nicht abgedruckt.

Jetzt z√§hlen wir, ob unsere Tweets/Texte solcherlei W√∂rter enthalten.


```{r}
d_schimpf <- 
d2_long %>% 
  select(id, token) %>% 
  mutate(schimpf = token %in% schimpf$word)
  
d_schimpf %>% 
  filter(schimpf)
```


Wie viele Schimpfw√∂rter haben wir gefunden?

```{r}
d_schimpf %>% 
  count(schimpf)
```


Etwa ein Prozent der W√∂rter sind Schimpfw√∂rter in unserem Corpus.


```{r}
d_schimpf2 <-
  d_schimpf %>% 
  group_by(id) %>% 
  summarise(schimpf_n = sum(schimpf))

head(d_schimpf2)
```


```{r}
d_main <-
  d3 %>% 
  full_join(d_schimpf2)
```

### Worteinbettungen

Hier k√∂nnte ein sch√∂ner Abschnitt stehen √ºber Worteinbettungen in deutscher Sprache,
allerdings ist dieses Schmankerl au√üen vorgelassen zum √úbungsnutzen der Lesis 
üèãÔ∏èüôÇ.





## Datenaufbereitung


### Worth√§ufigkeiten



## Modell 1: Naive-Bayes


### Dummy-Rezept


Hier ist ein einfaches Beispiel,
um die Textvorbereitung mit `{textrecipes}` zu verdeutlichen.

Wir erstellen uns einen Dummy-Text:

```{r}
dummy <- 
  tibble(text = c("Ich gehe heim und der die das nicht in ein and the"))
```


Dann tokenisieren wir den Text:

```{r}
rec_dummy <-
  recipe(text ~ 1, data = dummy) %>% 
  step_tokenize(text)
  
rec_dummy
```


Die Tokens kann man sich so zeigen lassen:

```{r}
show_tokens(rec_dummy, text)
```


Jetzt entfernen wir die Stopw√∂rter deutscher Sprache;
daf√ºr nutzen wir die Stopwort-Quelle `snowball`:


```{r}
rec_dummy <-
  recipe(text ~ 1, data = dummy) %>% 
  step_tokenize(text) %>% 
  step_stopwords(text, language = "de", stopword_source = "snowball")

rec_dummy
```


Pr√ºfen wir die Tokens; 
sind die Stopw√∂rter wirklich entfernt?

```{r}
show_tokens(rec_dummy, text)
```


Ja, die deutschen Stopw√∂rter sind entfernt. Die englischen nicht;
das macht Sinn!


### Datenaufteilung


```{r}
d_split <- initial_split(d_main, strata = c1)

d_train <- training(d_split)
d_test <- testing(d_split)
```



### Rezept 1


Rezept definieren:

```{r}
rec1 <- 
  recipe(c1 ~ ., data = select(d_train, text, c1, id)) %>% 
  update_role(id, new_role = "id") %>% 
  step_tokenize(text) %>% 
  step_stopwords(text, language = "de", stopword_source = "snowball") %>% 
  step_stem(text) %>% 
  step_tokenfilter(text, max_tokens = 1e2) %>% 
  step_tfidf(text)

rec1
```


Preppen:

```{r}
rec1_prepped <- prep(rec1)
```

Und backen:

```{r}
d_rec1 <- bake(rec1_prepped, new_data = NULL)

head(d_rec1)
```



### Modellspezifikation 1

Wir definiere einen Naive-Bayes-Algorithmus:

```{r}
nb_spec <- naive_Bayes() %>%
  set_mode("classification") %>%
  set_engine("naivebayes")

nb_spec
```



Und setzen auf die klassische zehnfache Kreuzvalidierung.


```{r}
set.seed(42)
folds1 <- vfold_cv(d_train)

folds1
```



### Workflow 1


```{r}
wf1 <-
  workflow() %>% 
  add_recipe(rec1) %>% 
  add_model(nb_spec)

wf1
```


### Fitting 1


```{r}
fit1 <-
  fit_resamples(
    wf1,
    folds1,
    control = control_resamples(save_pred = TRUE)
  )
```


M√∂chte man sich die Zeit sparen, die Syntax wieder durchlaufen zu lassen,
kann man das Objekt speichern. 
Aber vorsicht: Dabei kann es passieren, dass man mit veralteten Objekten arbeitet.



```{r}
#| eval: false
write_rds(fit1, "objects/chap_classific_fit1.rds")
```



### Performanz 1

```{r}
wf1_performance <-
  collect_metrics(fit1)

wf1_performance
```



```{r}
wf_preds <-
  collect_predictions(fit1)

wf_preds %>% 
  group_by(id) %>% 
  roc_curve(truth = c1, .pred_OFFENSE) %>% 
  autoplot()
```


```{r}
conf_mat_resampled(fit1, tidy = FALSE) %>% 
  autoplot(type = "heatmap")
```




## Nullmodell


```{r}
null_classification <- 
  parsnip::null_model() %>%
  set_engine("parsnip") %>%
  set_mode("classification")

null_rs <- workflow() %>%
  add_recipe(rec1) %>%
  add_model(null_classification) %>%
  fit_resamples(
    folds1
  )
```


Hier ist die Performanz des Nullmodells.

```{r}
null_rs %>%
  collect_metrics()
```





## Modell 2: Lasso



```{r}
lasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

lasso_spec
```



Wir definieren die Auspr√§gungen von `penalty`, 
die wir ausprobieren wollen:


```{r}
lambda_grid <- grid_regular(penalty(), levels = 30)
```



```{r}
wf2 <-
  workflow() %>% 
  add_recipe(rec1) %>% 
  add_model(lasso_spec)

wf2
```


Tunen und Fitten:

```{r}
set.seed(42)

fit2 <-
  tune_grid(
    wf2,
    folds1,
    grid = lambda_grid,
    control = control_resamples(save_pred = TRUE)
  )

fit2
```





```{r}
#| eval: false
write_rds(fit2, "objects/chap_classific_fit2.rds")
```


Hier ist die Performanz:

```{r}
collect_metrics(fit2)
```


```{r}
autoplot(fit2)
```



```{r}
fit2 %>% 
  show_best("roc_auc")
```



```{r}
chosen_auc <- 
  fit2 %>%
  select_by_one_std_err(metric = "roc_auc", -penalty)
```



Finalisieren:


```{r}
wf2_final <-
  finalize_workflow(wf2, chosen_auc)

wf2_final
```



```{r}
fit2_final_train <-
  fit(wf2_final, d_train)
```


```{r}
fit2_final_train %>% 
  extract_fit_parsnip() %>% 
  tidy() %>% 
  arrange(-abs(estimate))
  
```



```{r}
fit2_final_test <-
  last_fit(wf2_final, d_split)

collect_metrics(fit2_final_test)
```



### Vorhersage


### Vohersagedaten

Pfad zu den Daten:

```{r}
tweet_data_path <- "/Users/sebastiansaueruser/github-repos/hate-speech/data/"
```



```{r}
tweet_data_files_names <- list.files(path = tweet_data_path,
                                     pattern  = "tweets-to-.*\\.rds$")
head(tweet_data_files_names)
```


Wie viele Dateien sind es?

```{r}
length(tweet_data_files_names)
```


Wir geben den Elementen des Vektors g√§ngige Namen,
das hilft uns gleich bei `map`:


```{r}
names(tweet_data_files_names) <- str_remove(tweet_data_files_names, "\\.rds")
```





OK, weiter: So k√∂nnen wir *eine* der Datendateien einlesen:

```{r}
d_raw <-
  read_rds(file = paste0(tweet_data_path, tweet_data_files_names[1])) 

d <- 
  d_raw %>% 
  select(id, author_id, created_at, public_metrics) %>% 
  unnest_wider(public_metrics)

head(d)
```


Und so lesen wir alle ein:


Zun√§chst erstellen wir uns eine Helper-Funktion:


```{r}
read_and_select <- function(file_name, path_to_tweet_data = tweet_data_path) {
  
  out <- 
    read_rds(file = paste0(path_to_tweet_data, file_name)) %>% 
    select(id, author_id, created_at, text, public_metrics) %>% 
    unnest_wider(public_metrics)
  
  cat("Data file was read.\n")
  
  return(out)
}
```

Testen:

```{r}
d1 <- read_and_select(tweet_data_files_names[1])

head(d)
```



Die Funktion `read_and_select`  mappen wir auf alle Datendateien:


```{r}
#| eval: false
tic()
ds <-
  tweet_data_files_names %>% 
  map_dfr(read_and_select, .id = "dataset")
toc()
```


`214.531 sec elapsed`

Da wir den Elementen von `tweet_data_files_names` Namen gegeben haben, 
finden wir diese Namen praktischerweise wieder in `ds`:

```{r}
head(ds)
```

```{r}
#| echo: false
#| eval: false
write_rds(ds, file = paste0(tweet_data_path, "ds.rds"))
```



```{r}
#| echo: false
ds_short <- read_rds("/Users/sebastiansaueruser/github-repos/hate-speech/data/ds_short.rds")
```


Vielleicht ist es zum Entwickeln besser,
mit einem kleineren Datensatz einstweilen zu arbeiten:

```{r}
#| eval: false
ds_short <- slice_sample(ds, prop = .05)
```



```{r}
#| echo: false
#| eval: false
write_rds(ds_short, "/Users/sebastiansaueruser/github-repos/hate-speech/data/ds_short.rds")
ds <- read_rds(file = "/Users/sebastiansaueruser/datasets/Twitter/ds.rds")
```




### Vokabular erstellen


```{r}
#| eval: false
ds_long <-
  ds %>% 
  select(text) %>% 
  unnest_tweets(input = text, output = word)
```

Puh, das hat gedauert!

Speichern wir uns diese Daten daher auf die Festplatte:

```{r}
#| eval: false
write_rds(ds_long, file = paste0(tweet_data_path, "ds_long.rds"))
```

Entfernen wir daraus die Duplikate,
um uns ein Vokabular zu erstellen:

```{r}
#| eval: false
ds_voc <-
  ds_long %>% 
  #slice_head(n = 10) %>% 
  distinct(word)
```

Und das resultierende Objekt speichern wir wieder ab:


```{r}
#| eval: false
write_rds(ds_voc, file = paste0(tweet_data_path, "ds_voc.rds"))
```


```{r}
#| echo: false
ds_voc <- read_rds("/Users/sebastiansaueruser/github-repos/hate-speech/data/ds_voc.rds")
```





### Worteinbettungen erstellen


```{r}
#| eval: false
texts <- ds %>% pull(text)
texts <- tolower(texts)
out_file_txt <- "/Users/sebastiansaueruser/datasets/Twitter/twitter-polit-model.vec"
out_file_model <- "/Users/sebastiansaueruser/datasets/Twitter/twitter-polit-model.bin"
writeLines(text = texts, con = out_file_txt)
execute(commands = c("skipgram", "-input", tmp_file_txt, "-output", out_file_model, "-verbose", 1))
```


```
Read 22M words
Number of words:  130328
Number of labels: 0
Progress: 100.0% words/sec/thread:   49218 lr:  0.000000 avg.loss:  1.720812 ETA:   0h 0m 0s
```

Jetzt laden wir das Modell von der Festplatte:

```{r}
model <- load_model(out_file_model)
dict <- get_dictionary(model)
```


Schauen wir uns einige Begriffe aus dem Vokabular an:

```{r}
print(head(dict, 10))
```

Hier ist der Vektor f√ºr `menschen`:


```{r}
print(get_word_vectors(model, c("menschen")))
```


```{r}
get_word_vectors(model, words = dict[1:100])
```


Erstellen wir uns einen Tibble, der 
als erste Spalte das Vokabular und in den √ºbrigen 100 Spalten die Dimensionen enth√§lt:

```{r}
word_embedding_twitter <-
  tibble(
    word = dict
  )
```


```{r}
words_vecs_twitter <-
  get_word_vectors(model)
```



```{r}
word_embedding_twitter <-
  word_embedding_twitter %>% 
  bind_cols(words_vecs_twitter)
```

Namen versch√∂nern:

```{r}
names(word_embedding_twitter) <- c("word", paste0("v", sprintf("%03d", 1:100)))
```


Und als Worteinbettungs-Datei abspeichern:

```{r}
#| eval: false:
write_rds(word_embedding_twitter, file = paste0(tweet_data_path, "word_embedding_twitter.rds"))
```



### Aufbereiten

Am besten nur die Spalten behalten,
die wir zum Modellieren nutzen:

```{r}
ds_short2 <-
  ds_short %>% 
  select(text, id)
```


Dann backen wir die Daten mit dem vorhandenen Rezept:



```{r}
ds_baked <- bake(rec1_prepped, new_data = ds_short2)

head(ds_baked)
```


Ist das nicht komfortabel?
Das Textrezept √ºbernimmt die Arbeit f√ºr uns,
mit den richtigen Features zu arbeiten,
die tf-idfs f√ºr die richtigen Tokens zu berechnen.

Wer dem Frieden nicht traut,
dem sei geraten, nachzupr√ºfen :-)






