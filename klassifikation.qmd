---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Klassifikation von Hatespeech





## Vorab


### Lernziele


- Sie k√∂nnen grundlegende Verfahren zur Klassifikation von Hatespeech einsetzen und erkl√§ren








### Ben√∂tigte R-Pakete

```{r}
#| message: false
library(tidyverse)
library(easystats)
library(tidymodels)
library(tidytext)
library(textrecipes)
library(lsa)  # stopwords
library(discrim)  # naive bayes classification
library(naivebayes)
library(tictoc)  # Zeitmessung
library(fastrtext)  # Worteinbettungen
library(remoji)  # Emojis
library(tokenizers)  # Vektoren tokenisieren
```


## Daten


F√ºr Maschinenlernen brauchen wir Trainingsdaten,
Daten also, bei denen wir pro Beobachtung der Wert der Zielvariablen kennen.
Man spricht auch von "gelabelten" Daten.

Wir nutzen die Daten von @wiegand_germeval bzw. @wiegand-data.
Die Daten sind unter CC-By-4.0 Int. lizensiert.

```{r import-heidelberg-data}
#| message: false
d_raw <- 
  data_read("data/germeval2018.training.txt",
         header = FALSE)
```


Die Daten finden sich auch im Paket [pradadata](https://github.com/sebastiansauer/pradadata).

Da die Daten keine Spaltenk√∂pfe haben, informieren wir die Funktion dazu mit `header = FALSE`.

Benennen wir die die Spalten um:

```{r}
names(d_raw) <- c("text", "c1", "c2")
```

Dabei soll `c1` und `c2` f√ºr die 1. bzw. 2. Klassifikation stehen.


In `c1` finden sich diese Werte:

```{r}
d_raw %>% 
  count(c1)
```

Hier wurde klassifiziert,
ob beleidigende Sprache (offensive language) vorlag oder nicht [@isch-etal-2021-overview, S. 2]:


>   Task 1 was to decide whether a tweet includes some form of offensive language or not. The tweets had to be classiÔ¨Åed into the two classes OFFENSE and OTHER. The OFFENSE category covered abusive language, insults, as well as merely profane statements.


Und in `c2` finden sich folgende Auspr√§gungen:

```{r}
d_raw %>% 
  count(c2)
```


In `c2` ging es um eine feinere Klassifikation beleidigender Sprache [@isch-etal-2021-overview, S. 2]:

>   The second task involved four categories, a nonoffensive OTHER class and three sub-categories of what is OFFENSE in Task 1. In the case of PROFANITY, profane words are used, however, the tweet does not want to insult anyone. This typically concerns the usage of swearwords (Schei√üe, Fuck etc.) and cursing (Zur H√∂lle! Verdammt! etc.). This can be often found in youth language. Swearwords and cursing may, but need not, co-occur with insults or abusive speech. Profane language may in fact be used in tweets with positive sentiment to express emphasis. Whenever profane words are not directed towards a speciÔ¨Åc person or group of persons and there are no separate cues of INSULT or ABUSE, then tweets are labeled as simple cases of PROFANITY.




Sind Texte, die als `OFFENSE` klassifiziert sind,
auch (fast) immer als `ABUSE`, `INSULT` oder `PROFANITY` klassifiziert?


```{r}
d_raw %>% 
  filter(c1 == "OTHER", c2 == "OTHER") %>% 
  nrow() / nrow(d_raw)
```

In ca. 2/3 der F√§lle wurden in beiden Klassifikation `OTHER` klassifiziert.

```{r}
d_raw %>% 
  filter(c1 != "OTHER", c2 != "OTHER") %>% 
  nrow() / nrow(d_raw)
```

Entsprechend in ca. 1/3 der F√§lle wurde jeweils nicht mit `OTHER` klassifiziert.


Wir begn√ºgen uns hier mit der ersten, gr√∂beren Klassifikation.



F√ºgen wir abschlie√üend noch eine ID-Variable hinzu:

```{r}
d1 <-
  d_raw %>% 
  mutate(id = as.character(1:nrow(.)))
```


Die ID-Variable definieren als Text (nicht als Integer),
da die Twitter-IDs zwar nat√ºrliche Zahlen sind,
aber zu gro√ü, um von R als Integer verarbeitet zu werden.
Faktisch sind sie f√ºr uns auch nur nominal skalierte Variablen,
so dass wir keinen Informationsverlust haben.


```{r}
#| eval: false
#write_rds(d1, "objects/d1.rds")
```



## Feature Engineering


Reichern wir die Daten mit weiteren Features an,
in der Hoffnung, damit eine bessere Klassifikation erzielen zu k√∂nnen.


### Textl√§nge




```{r d2}
d2 <-
  d_raw %>% 
  mutate(text_length = str_length(text))

head(d2)
```



### Sentimentanalyse

Wir nutzen dazu `SentiWS` [@Remus2010].

```{r read-sentiws}
sentiws <- read_csv("https://osf.io/x89wq/?action=download")
```




```{r d2-long}
d2_long <-
  d2 %>% 
  unnest_tokens(input = text, output = token)

head(d2_long)
```

Jetzt filtern wir unsere Textdaten so,
dass nur W√∂rter mit Sentimentwert √ºbrig bleiben:

```{r d2-senti-long}
d2_long_senti <- 
  d2_long %>%  
  inner_join(sentiws %>% select(-inflections), by = c("token" = "word"))

head(d2_long)
```


Schlie√ülich berechnen wir die Sentimentwert pro Polarit√§t und pro Tweet:

```{r d2-sentis}
d2_sentis <-
  d2_long_senti %>% 
  group_by(id, neg_pos) %>% 
  summarise(senti_avg = mean(value))

head(d2_sentis)
```


Diese Tabelle bringen wir wieder eine breitere Form,
um sie dann wieder mit den Hauptdaten zu vereinigen.


```{r d2-sentis-wide}
d2_sentis_wide <-
  d2_sentis %>% 
  pivot_wider(names_from = "neg_pos", values_from = "senti_avg")

d2_sentis_wide %>% head()
```


```{r d3}
d3 <-
  d2 %>% 
  full_join(d2_sentis_wide)

head(d3)
```


:::callout-note
Die Sentimentanalyse hier vernachl√§ssigt Flexionen der W√∂rter. 
Der  Autor f√ºhlt den Drang zu schreiben: "Left as an exercise for the reader" :-)
:::


### Schimpfw√∂rter


Z√§hlen wir die Schimpfw√∂rter pro Text.
Dazu nutzen wir die Daten von [LDNOOBW](https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/blob/master/LICENSE), lizensiert nach CC-BY-4.0-Int.




```{r schimpf1}
schimpf1 <- import("https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/de", format = ",", header = FALSE)
```


L√§nger aber noch ist die Liste aus dem [InsultWiki](https://www.insult.wiki/schimpfwort-liste), lizensiert CC0.


```{r schimpf2}
schimpf2 <- 
  import("data/insult-de.txt", header = FALSE) %>% 
  mutate_all(str_to_lower)
```



Die Daten finden sich auch im Paket [pradadata](https://github.com/sebastiansauer/pradadata).

Binden wir die Listen zusammen:

```{r schimpf}
schimpf <-
  schimpf1 %>% 
  bind_rows(schimpf2) %>% 
  distinct() %>% 
  rename(word = "V1")

nrow(schimpf)
```


Um die Lesis vor (unn√∂tiger?) Kopfverschmutzung zu bewahren,
sind diese Schimpfw√∂rter hier nicht abgedruckt.

Jetzt z√§hlen wir, ob unsere Tweets/Texte solcherlei W√∂rter enthalten.


```{r d_schimpf}
d_schimpf <- 
d2_long %>% 
  select(id, token) %>% 
  mutate(schimpf = token %in% schimpf$word)
```


Wie viele Schimpfw√∂rter haben wir gefunden?

```{r}
d_schimpf %>% 
  count(schimpf)
```


Etwa ein Prozent der W√∂rter sind Schimpfw√∂rter in unserem Corpus.


```{r d-schimpf2}
d_schimpf2 <-
  d_schimpf %>% 
  group_by(id) %>% 
  summarise(schimpf_n = sum(schimpf))

head(d_schimpf2)
```


```{r d_main}
d_main <-
  d3 %>% 
  full_join(d_schimpf2)
```


:::callout-important
Namen wie `final`, `main` oder `result` sind gef√§hrlich,
da es unter Garantie ein "final-final geben wird, oder der "Haupt-Datensat" pl√∂tzlich nicht mehr so wichtig erscheint und so weiter.
:::



### Emojis


```{r get-emoji-list}
emj <- emoji(list_emoji(), pad = FALSE)

head(emj)
```

Diese Liste umfasst knapp 900 Emojis, 
das sind allerdings noch nicht alle, die es gibt.
[Diese Liste](https://unicode.org/emoji/charts/full-emoji-list.html) umfasst mit gut 1800 Emojis
gut das Doppelte.


Selbstkuratierte Liste an "wilden" Emoji;
diese Liste ist inspiriert von [emojicombos.com](https://emojicombos.com/disgust).

```{r wild-emojis}
wild_emojis <- 
  c(
    emoji(find_emoji("gun")),
    emoji(find_emoji("bomb")),
    emoji(find_emoji("fist")),
    emoji(find_emoji("knife"))[1],
    emoji(find_emoji("ambulance")),
    emoji(find_emoji("fist")),
    emoji(find_emoji("skull")),
    "‚ò†Ô∏è",     "üóë",       "üò†",    "üëπ",    "üí©" ,
    "üñï",    "üëéÔ∏è",
    emoji(find_emoji("middle finger")),    "üò°",    "ü§¢",    "ü§Æ",  
    "üòñ",    "üò£",    "üò©",    "üò®",    "üòù",    "üò≥",    "üò¨",    "üò±",    "üòµ",
       "üò§",    "ü§¶‚Äç‚ôÄÔ∏è",    "ü§¶‚Äç"
  )
```


```{r}
wild_emojis_df <-
  tibble(emoji = wild_emojis)

save(wild_emojis_df, file = "data/wild_emojis.RData")
```



Auf dieser Basis k√∂nnen wir einen Pr√§diktor erstellen,
der z√§hlt, ob ein Tweet einen oder mehrere der "wilden" Emojis enth√§lt.


## Workflow 1: Rezept 1 + Naive-Bayes


### Dummy-Rezept


Hier ist ein einfaches Beispiel,
um die Textvorbereitung mit `{textrecipes}` zu verdeutlichen.

Wir erstellen uns einen Dummy-Text:

```{r}
dummy <- 
  tibble(text = c("Ich gehe heim und der die das nicht in ein and the"))
```


Dann tokenisieren wir den Text:

```{r rec-dummy1}
rec_dummy <-
  recipe(text ~ 1, data = dummy) %>% 
  step_tokenize(text)
  
rec_dummy
```


Die Tokens kann man sich so zeigen lassen:

```{r}
show_tokens(rec_dummy, text)
```


Jetzt entfernen wir die Stopw√∂rter deutscher Sprache;
daf√ºr nutzen wir die Stopwort-Quelle `snowball`:


```{r rec-dummy2}
rec_dummy <-
  recipe(text ~ 1, data = dummy) %>% 
  step_tokenize(text) %>% 
  step_stopwords(text, language = "de", stopword_source = "snowball")

rec_dummy
```


Pr√ºfen wir die Tokens; 
sind die Stopw√∂rter wirklich entfernt?

```{r}
show_tokens(rec_dummy, text)
```


Ja, die deutschen Stopw√∂rter sind entfernt. Die englischen nicht;
das macht Sinn!


### Datenaufteilung


```{r d-split2}
d_split <- initial_split(d_main, strata = c1)

d_train <- training(d_split)
d_test <- testing(d_split)
```





### Rezept 1


Rezept definieren:

```{r rec1}
rec1 <- 
  recipe(c1 ~ ., data = select(d_train, text, c1, id)) %>% 
  update_role(id, new_role = "id") %>% 
  step_tokenize(text) %>% 
  step_stopwords(text, language = "de", stopword_source = "snowball") %>% 
  step_stem(text) %>% 
  step_tokenfilter(text, max_tokens = 1e2) %>% 
  step_tfidf(text) %>% 
  step_normalize(all_numeric_predictors())

rec1
```


Preppen:

```{r rec1-prepped}
rec1_prepped <- prep(rec1)
```

Und backen:

```{r rec1-baked}
d_rec1 <- bake(rec1_prepped, new_data = NULL)

head(d_rec1)
```



### Modellspezifikation 1

Wir definiere einen Naive-Bayes-Algorithmus:

```{r nb-spec}
nb_spec <- naive_Bayes() %>%
  set_mode("classification") %>%
  set_engine("naivebayes")

nb_spec
```



Und setzen auf die klassische zehnfache Kreuzvalidierung.


```{r folds1}
set.seed(42)
folds1 <- vfold_cv(d_train)
```



### Workflow 1


```{r wf1}
wf1 <-
  workflow() %>% 
  add_recipe(rec1) %>% 
  add_model(nb_spec)

wf1
```


### Fitting 1


```{r fit1}
#| eval: false
fit1 <-
  fit_resamples(
    wf1,
    folds1,
    control = control_resamples(save_pred = TRUE)
  )
```

Die Vorhersagen speichern wir ab,
um die Performanz in den Faltungen des Hold-out-Samples zu berechnen.


M√∂chte man sich die Zeit sparen, die Syntax wieder durchlaufen zu lassen,
kann man das Objekt speichern. 
Aber Vorsicht: Dabei kann es passieren, dass man mit veralteten Objekten arbeitet.



```{r write-fit1}
#| eval: false
#write_rds(fit1, "objects/chap_classific_fit1.rds")
```


```{r read-fit1}
#| echo: false
fit1 <- read_rds("objects/chap_classific_fit1.rds")
```



### Performanz 1

```{r wf1-perf}
#| eval: false
wf1_performance <-
  collect_metrics(fit1)

wf1_performance
```



```{r wf1-preds}
wf_preds <-
  collect_predictions(fit1)

wf_preds %>% 
  group_by(id) %>% 
  roc_curve(truth = c1, .pred_OFFENSE) %>% 
  autoplot()
```


```{r}
conf_mat_resampled(fit1, tidy = FALSE) %>% 
  autoplot(type = "heatmap")
```




## Nullmodell


```{r nullmodel}
#| eval: false
null_classification <- 
  parsnip::null_model() %>%
  set_engine("parsnip") %>%
  set_mode("classification")

null_rs <- workflow() %>%
  add_recipe(rec1) %>%
  add_model(null_classification) %>%
  fit_resamples(
    folds1
  )
```


```{r}
#| eval: false
#| echo: false
#write_rds(null_rs, file = "/Users/sebastiansaueruser/github-repos/datascience-text/objects/chap_classific_fit_nullmodel.rds")
```


```{r}
#| echo: false
null_rs <- read_rds("/Users/sebastiansaueruser/github-repos/datascience-text/objects/chap_classific_fit3.rds")
```



Hier ist die Performanz des Nullmodells.

```{r}
#| eval: false
null_rs %>%
  collect_metrics()
```


```{r}
show_best(null_rs)
```




## Workflow 2: Rezept 1 + Lasso



```{r lasso-spec}
lasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

lasso_spec
```



Wir definieren die Auspr√§gungen von `penalty`, 
die wir ausprobieren wollen:


```{r lambda-grid}
lambda_grid <- grid_regular(penalty(), levels = 30)
```



```{r wf2}
wf2 <-
  workflow() %>% 
  add_recipe(rec1) %>% 
  add_model(lasso_spec)

wf2
```


Tunen und Fitten:

```{r fit2}
#| eval: false
set.seed(42)

fit2 <-
  tune_grid(
    wf2,
    folds1,
    grid = lambda_grid,
    control = control_resamples(save_pred = TRUE)
  )

fit2
```


Vorsicht beim Abspeichern.

```{r}
#| eval: false
#write_rds(fit2, "objects/chap_classific_fit2.rds")
```


```{r read-fit2}
#| echo: false
fit2 <- read_rds("objects/chap_classific_fit2.rds")
```



Hier ist die Performanz:

```{r}
#| eval: false
collect_metrics(fit2) %>% 
  filter(.metric == "roc_auc") %>% 
  slice_max(mean, n = 3)
```


```{r}
autoplot(fit2)
```



```{r}
fit2 %>% 
  show_best("roc_auc")
```



```{r chosen-auc-fit2}
chosen_auc <- 
  fit2 %>%
  select_by_one_std_err(metric = "roc_auc", -penalty)
```



Finalisieren:


```{r wf2-final}
wf2_final <-
  finalize_workflow(wf2, chosen_auc)

wf2_final
```



```{r fit2-final-train}
fit2_final_train <-
  fit(wf2_final, d_train)
```


```{r}
fit2_final_train %>% 
  extract_fit_parsnip() %>% 
  tidy() %>% 
  arrange(-abs(estimate)) %>% 
  head()
```



```{r fit2-final-test}
fit2_final_test <-
  last_fit(wf2_final, d_split)

collect_metrics(fit2_final_test)
```



### Vorhersage


### Vohersagedaten

Pfad zu den Daten:

```{r}
tweet_data_path <- "/Users/sebastiansaueruser/github-repos/hate-speech/data/"
```



```{r}
tweet_data_files_names <- list.files(path = tweet_data_path,
                                     pattern  = "tweets-to-.*\\.rds$")
head(tweet_data_files_names)
```


Wie viele Dateien sind es?

```{r}
length(tweet_data_files_names)
```


Wir geben den Elementen des Vektors g√§ngige Namen,
das hilft uns gleich bei `map`:


```{r}
names(tweet_data_files_names) <- str_remove(tweet_data_files_names, "\\.rds")
```





OK, weiter: So k√∂nnen wir *eine* der Datendateien einlesen:

```{r read-tweets}
d_raw <-
  read_rds(file = paste0(tweet_data_path, tweet_data_files_names[1])) 

d <- 
  d_raw %>% 
  select(id, author_id, created_at, public_metrics) %>% 
  unnest_wider(public_metrics)

head(d)
```


Und so lesen wir alle ein:


Zun√§chst erstellen wir uns eine Helper-Funktion:


```{r fun-read-and-select}
read_and_select <- function(file_name, path_to_tweet_data = tweet_data_path) {
  
  out <- 
    read_rds(file = paste0(path_to_tweet_data, file_name)) %>% 
    select(id, author_id, created_at, text, public_metrics) %>% 
    unnest_wider(public_metrics)
  
  cat("Data file was read.\n")
  
  return(out)
}
```

Testen:

```{r read-and-seelct-test}
#| eval: false
d1 <- read_and_select(tweet_data_files_names[1])

head(d1)
```



Die Funktion `read_and_select`  mappen wir auf alle Datendateien:


```{r map-read-and-select}
#| eval: false
tic()
ds <-
  tweet_data_files_names %>% 
  map_dfr(read_and_select, .id = "dataset")
toc()
```


`214.531 sec elapsed`

Da wir den Elementen von `tweet_data_files_names` Namen gegeben haben, 
finden wir diese Namen praktischerweise wieder in `ds`.


```{r}
#| echo: false
#| eval: false
#write_rds(ds, file = paste0(tweet_data_path, "ds.rds"))
```



```{r read-ds}
#| echo: false
ds <- read_rds(file = "/Users/sebastiansaueruser/datasets/Twitter/ds.rds")
```




Vielleicht ist es zum Entwickeln besser,
mit einem kleineren Datensatz einstweilen zu arbeiten:

```{r ds-short}
ds_short <- slice_sample(ds, prop = .05)
```



```{r}
#| echo: false
#| eval: false
#write_rds(ds_short, "/Users/sebastiansaueruser/github-repos/hate-speech/data/ds_short.rds")
```




### Vokabular erstellen


```{r ds-long}
#| eval: false
ds_long <-
  ds %>% 
  select(text) %>% 
  unnest_tweets(input = text, output = word)
```

Puh, das hat gedauert!

Speichern wir uns diese Daten daher auf die Festplatte:

```{r}
#| eval: false
#write_rds(ds_long, file = paste0(tweet_data_path, "ds_long.rds"))
```

Entfernen wir daraus die Duplikate,
um uns ein Vokabular zu erstellen:

```{r ds-voc}
#| eval: false
ds_voc <-
  ds_long %>% 
  #slice_head(n = 10) %>% 
  distinct(word)
```

Und das resultierende Objekt speichern wir wieder ab:


```{r}
#| eval: false
#write_rds(ds_voc, file = paste0(tweet_data_path, "ds_voc.rds"))
```


```{r}
#| echo: false
ds_voc <- read_rds("/Users/sebastiansaueruser/github-repos/hate-speech/data/ds_voc.rds")
```





## Worteinbettungen erstellen


### FastText-Modell {#sec-fasttext}

Definiere die Konstanten f√ºr das fastText-Modell:

```{r fastText-constants}
#| eval: false
texts <- ds %>% pull(text)
texts <- tolower(texts)
```


```{r fastText-filenames}
out_file_txt <- "/Users/sebastiansaueruser/datasets/Twitter/twitter-polit-model.vec"
out_file_model <- "/Users/sebastiansaueruser/datasets/Twitter/twitter-polit-model.bin"
```


```{r fasttext-modell}
#| eval: false
#writeLines(text = texts, con = out_file_txt)
#execute(commands = c("skipgram", "-input", tmp_file_txt, "-output", out_file_model, "-verbose", 1))
```


```
Read 22M words
Number of words:  130328
Number of labels: 0
Progress: 100.0% words/sec/thread:   49218 lr:  0.000000 avg.loss:  1.720812 ETA:   0h 0m 0s
```

Jetzt laden wir das Modell von der Festplatte:

```{r twitter-fasttext-model}
twitter_fasttext_model <- load_model(out_file_model)
dict <- get_dictionary(twitter_fasttext_model)
```


Schauen wir uns einige Begriffe aus dem Vokabular an:

```{r}
print(head(dict, 10))
```

Hier sind die ersten paar Elemente des Vektors f√ºr `menschen`:


```{r vector-menschen}
#| eval: false
get_word_vectors(twitter_fasttext_model, c("menschen")) %>% `[`(1:10)
```


```
 [1]  0.14156282  0.44875699  0.23911817 -0.02580349  0.29811972  0.03870077
 [7]  0.06518744  0.22527063  0.28198120  0.39931887
 ```


Erstellen wir uns einen Tibble, der 
als erste Spalte das Vokabular und in den √ºbrigen 100 Spalten die Dimensionen enth√§lt:

```{r}
word_embedding_twitter <-
  tibble(
    word = dict
  )
```


```{r words-vec-twitter}
#| eval: false
words_vecs_twitter <-
  get_word_vectors(twitter_fasttext_model)
```



```{r df-word-embedding-twitter}
#| eval: false
word_embedding_twitter <-
  word_embedding_twitter %>% 
  bind_cols(words_vecs_twitter)

names(word_embedding_twitter) <- c("word", paste0("v", sprintf("%03d", 1:100)))  # Namen versch√∂nern
```


Und als Worteinbettungs-Datei abspeichern:

```{r}
#| eval: false
#write_rds(word_embedding_twitter, file = paste0(tweet_data_path, "word_embedding_twitter.rds"))
```


```{r}
#| echo: false
word_embedding_twitter <- read_rds(file = "/Users/sebastiansaueruser/datasets/Twitter/word_embedding_twitter.rds")
```



### Aufbereiten

Am besten nur die Spalten behalten,
die wir zum Modellieren nutzen:

```{r ds-short2}
ds_short2 <-
  ds_short %>% 
  select(text, id)
```


Dann backen wir die Daten mit dem vorhandenen Rezept:



```{r ds-baked}
ds_baked <- bake(rec1_prepped, new_data = ds_short2)
```


Ist das nicht komfortabel?
Das Textrezept √ºbernimmt die Arbeit f√ºr uns,
mit den richtigen Features zu arbeiten,
die tf-idfs f√ºr die richtigen Tokens zu berechnen.

Wer dem Frieden nicht traut,
dem sei geraten, nachzupr√ºfen :-)



## Workflow 3: Rezept 2 + Lasso

### Daten aufteilen



```{r split-data3}
d_split <- initial_split(d2, strata = c1)

d_train <- training(d_split)
d_test <- testing(d_split)
```



### Hilfsfunktionen



```{r}
source("funs/helper-funs-recipes.R")
```



Testen wir die Funktionen:

```{r}
dummy <- c("hallo", "baby", "fatal")

count_profane(dummy) 

count_emo_words(dummy)

dummy <- c("baby", "und", "üÜó", "üñï")

count_emojis(dummy)

count_wild_emojis(dummy) 
```


### Rezept mit Worteinbettungen

```{r rec2}
rec2 <- 
  recipe(c1 ~ ., data = select(d_train, text, c1, id)) %>% 
  update_role(id, new_role = "id") %>% 
  step_text_normalization(text) %>% 
  step_mutate(text_copy = text,
              profane_n = map_int(text, count_profane),
              emo_words_n = map_int(text, count_emo_words),
              emojis_n = map_int(text, count_emojis),
              wild_emojis_n = map_int(text, count_wild_emojis)
  ) %>% 
  step_textfeature(text_copy) %>% 
  step_tokenize(text, token = "tweets") %>% 
  step_stopwords(text, language = "de", stopword_source = "snowball") %>% 
  step_word_embeddings(text, embeddings = word_embedding_twitter)
 
rec2
```




Jetzt preppen:


```{r rec2-prepped}
rec2_prepped <- prep(rec2)
```


Vielleicht macht es Sinn, sich das Objekt zur sp√§teren
Verwendung abzuspeichern.^[Aber Vorsicht beim Abspeichern, man k√∂nnte versehentlich mit einer veralteten Version weiterarbeiten.]
`Feather` verarbeitet nur Dataframes,
daher nutzen wir hier RDS.


```{r}
#| eval: false
#write_rds(rec2_prepped, file = "~/datasets/Twitter/klassifik-rec2-prepped.rds")
```



Das Element `rec2_prepped` ist recht gro√ü:


```{r}
format(object.size(rec2_prepped), units  = "Mb")
```

Jetzt k√∂nnen wir das pr√§parierte ("gepreppte") Rezept "backen":


```{r rec2-prepped-baked}
rec2_baked <- bake(rec2_prepped, new_data = NULL)
```


```{r}
rec2_baked %>% 
  select(1:15) %>% 
  glimpse()
```



### Fitting 3 {#sec-klassifik-fit3}




```{r wf3}
wf3 <-
  workflow() %>% 
  add_recipe(rec2) %>% 
  add_model(lasso_spec)

wf3
```





Tunen und Fitten:

```{r wf3-fit}
#| eval: false
set.seed(42)

tic()
fit3 <-
  tune_grid(
    wf3,
    folds1,
    grid = lambda_grid,
    control = control_resamples(save_pred = TRUE)
  )
(toc)
fit3
```





```{r}
#| eval: false
#write_rds(fit3, "objects/chap_classific_fit3.rds")
```


```{r}
#| echo: false
fit3 <- read_rds("/Users/sebastiansaueruser/github-repos/datascience-text/objects/chap_classific_fit3.rds")
```



Hier ist die Performanz:

```{r}
#| eval: false
collect_metrics(fit3)
```


```{r}
autoplot(fit3)
```



```{r}
fit3 %>% 
  show_best("roc_auc")
```



```{r chosenauc-fit3}
chosen_auc_fit3 <- 
  fit3 %>%
  select_by_one_std_err(metric = "roc_auc", -penalty)
```



Finalisieren:


```{r wf3-final}
wf3_final <-
  finalize_workflow(wf3, chosen_auc_fit3)

wf3_final
```



```{r fit3-final-train}
fit3_final_train <-
  fit(wf3_final, d_train)
```


```{r}
#| echo: false
write_rds(fit3_final_train, file = "fit3_final_triain.rds")
```



```{r fit3-final-train2}
fit3_final_train %>% 
  extract_fit_parsnip() %>% 
  tidy() %>% 
  arrange(-abs(estimate)) %>% 
  head()
```



```{r fit3-final-test}
fit3_final_test <-
  last_fit(wf3_final, d_split)

collect_metrics(fit3_final_test)
```


Am Ende so eines Arbeitsganges,
bei dem man wieder (und wieder) die gleichen Funktionen kopiert,
und nur aufpassen muss, aus `fit2` an der richtigen Stelle `fit3` zu machen:
Da blickt man jedem Umbau dieses Codes zu einer Funktion freudig ins Gesicht.

Ein anderes Problem,
f√ºr das hier keine elegante L√∂sung vorliegt,
sind die langen Berechnungszeiten, die, wenn man Pecht hat, auch noch
mehrfach wiederholt werden m√ºssen.

Zu diesen Punkten sp√§ter mehr.




