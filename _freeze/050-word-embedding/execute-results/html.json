{
  "hash": "3d667c9857a3557515a2ff4d06d924b9",
  "result": {
    "engine": "knitr",
    "markdown": "# Word Embedding\n\n\n![Text als Datenbasis prädiktiver Modelle](img/text-mining-1476780_640.png){width=10%}\nBild von <a href=\"https://pixabay.com/de/users/mcmurryjulie-2375405/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1476780\">mcmurryjulie</a> auf <a href=\"https://pixabay.com/de//?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1476780\">Pixabay</a>\n\n\n\n## Vorab\n\n\n### Lernziele\n\n\n\n<!-- - Die grundlegenden Konzepte der Informationstheorie erklären können -->\n- Die Erstellung von Word-Embeddings anhand grundlegender R-Funktionen erläutern können.\n\n\n\n### Begleitliteratur \n\n\nArbeiten Sie @smltar, [Kap. 5](https://smltar.com/embeddings.html) vor dem Unterricht durch als Vorbereitung.\n\n\n\n\n\n\n### Benötigte R-Pakete\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-1_0ab0a52e64f4186cf6df82e7ac754775'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(easystats)  # Komfort für deskriptive Statistiken, wie `describe_distribution`\nlibrary(tidytext)\nlibrary(hcandersenr)   # Textdaten\nlibrary(slider)  # slide\nlibrary(widyr)  # pairwise_pmi\nlibrary(furrr)  # mehrere Kerne gleichzeitig nutzen\nlibrary(textdata)  # Worteinbettungen wie Glove6b, vorgekocht\nlibrary(entropy)  # Entropie berechnen\nlibrary(widyr)  # Ähnlichkeit berechnen mit widyr_svd\nlibrary(furrr)  # Mehrere Kerne gleichzeitig\n```\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Daten\n\n\n### Complaints-Datensatz\n\nDer Datensatz `complaints` stammt aus [dieser Quelle](https://www.consumerfinance.gov/data-research/consumer-complaints/).\n\nDen Datensatz `complaints` kann man [hier](https://github.com/EmilHvitfeldt/smltar/raw/master/data/complaints.csv.gz) herunterladen. Im Buch ist die Webseite nicht (direkt?) angegeben. Die Datei ist mit `gz` gepackt; `read_csv` sollte das automatisch entpacken. Achtung: Die Datei ist recht groß.\n\n\n::: {.cell hash='050-word-embedding_cache/html/read-complaints-data_a59ac7326c2d25213916ee3cc3bbd2e0'}\n\n```{.r .cell-code}\nd_path <- \"https://github.com/EmilHvitfeldt/smltar/raw/master/data/complaints.csv.gz\"\ncomplaints <- read_csv(d_path)\n```\n:::\n\n\nGeschickter als jedes Mal die Datei herunterzuladen, ist es, sie einmal herunterzuladen, und dann lokal zu speichern,\netwa im Unterordner `data` des RStudio-Projektordners.\n\n\nNach dem Importieren wird der Datensatz in eine Tidy-Form gebracht (mit `unnest_tokens`) und dann verschachtelt, mit `nest`.\n\n\n### Complaints verkürzt und geschachtelt\n\nUm ein Herumprobieren zu erleichtern, ist hier der Datensatz `complaints` in zwei verkürzten Formen bereitgestellt:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-2_7e413a6168a7a56b8cb6ddd5f2c1c5f6'}\n\n```{.r .cell-code}\nnested_words2_path <- \"https://github.com/sebastiansauer/datascience-text/raw/main/data/nested_words2.rds\"\nnested_words3_path <- \"https://github.com/sebastiansauer/datascience-text/raw/main/data/nested_words3.rds\"\n```\n:::\n\n\n\n`nested_words2` enthält die ersten 10% des Datensatz `nested_words`und ist gut 4 MB groß (mit `gz` gezippt); er besteht aus ca. 11 Tausend Beschwerden.\n`nested_words3` enthält nur die ersten 10 Beschwerden, was ihn gut zum Handhaben macht.\n\nBeide sind verschachtelt und aus `tidy_complaints` (s. [Kap. 5.1](https://smltar.com/embeddings.html#motivatingsparse)) hervorgegangen.\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/read-rds-nested-data_b64b44a82bd7d5c50be2f2945123c60e'}\n\n```{.r .cell-code}\nnested_words3 <- read_rds(nested_words3_path)\n```\n:::\n\n\n\nDas sieht dann so aus:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-3_ebf9a0184eb5ccc7869c19a66fc45744'}\n\n```{.r .cell-code}\nnested_words3 %>% \n  head(3)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"complaint_id\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"words\"],\"name\":[2],\"type\":[\"list\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"3384392\",\"2\":\"<tibble[,1]>\"},{\"1\":\"3417821\",\"2\":\"<tibble[,1]>\"},{\"1\":\"3433198\",\"2\":\"<tibble[,1]>\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nWerfen wir einen Blick in den TExt der ersten Beschwerde des Datensatzes mit der ID `nested_words3_path$complaint_id[1]`.\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-4_b81267965b7dd781efe5f6d305c2056f'}\n\n```{.r .cell-code}\nbeschwerde1_text <- nested_words3$words[[1]]\n```\n:::\n\n\nDas ist ein Tibble mit einer Spalte und 17 Wörtern; \nda wir schon auf Unigramme aufgeteilt haben, ist jedes Wort ein Element des Vektors `word`: \n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-5_b6a29093bb55aed422c0c5fbac9646aa'}\n\n```{.r .cell-code}\nbeschwerde1_text %>% \n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"word\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"systems\"},{\"1\":\"inc\"},{\"1\":\"is\"},{\"1\":\"trying\"},{\"1\":\"to\"},{\"1\":\"collect\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-6_2c74f7e02e620de6188defa9d40f86e7'}\n\n```{.r .cell-code}\nbeschwerde1_text$word\n##  [1] \"systems\"    \"inc\"        \"is\"         \"trying\"     \"to\"        \n##  [6] \"collect\"    \"a\"          \"debt\"       \"that\"       \"is\"        \n## [11] \"not\"        \"mine\"       \"not\"        \"owed\"       \"and\"       \n## [16] \"is\"         \"inaccurate\"\n```\n:::\n\n\n\n\n## Wordembeddings selber erstellen\n\n\n<!-- TODO -->\n<!-- nette Illustrationen: https://entwickler.de/python/grundsatze-moderner-textklassifizierung-fur-machine-learning-word-embeddings -->\n\n\n\n### PMI berechnen\n\n\nRufen Sie sich die Definition der PMI ins Gedächtnis, s. @eq-pmi.\n\nMit R kann man die PMI z.B. so berechnen, s. `? pairwise_pmi` aus dem Paket `{widyr}`.\n\n\nZum Paket `widyr` von Robinson und Silge:\n\n>   This package wraps the pattern of un-tidying data into a wide matrix, performing some processing, then turning it back into a tidy form. This is useful for several mathematical operations such as co-occurrence counts, correlations, or clustering that are best done on a wide matrix.\n\n\n[Quelle](https://juliasilge.github.io/widyr/)\n\nErzeugen wir uns Dummy-Daten:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-7_7fc4ce5027dff04820fb41377d770767'}\n\n```{.r .cell-code}\ndat <- tibble(feature = rep(1:5, each = 2),\n              item = c(\"a\", \"b\",\n                       \"a\", \"c\",\n                       \"a\", \"c\",\n                       \"b\", \"e\",\n                       \"b\", \"f\"))\n\ndat\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"feature\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"item\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"1\",\"2\":\"a\"},{\"1\":\"1\",\"2\":\"b\"},{\"1\":\"2\",\"2\":\"a\"},{\"1\":\"2\",\"2\":\"c\"},{\"1\":\"3\",\"2\":\"a\"},{\"1\":\"3\",\"2\":\"c\"},{\"1\":\"4\",\"2\":\"b\"},{\"1\":\"4\",\"2\":\"e\"},{\"1\":\"5\",\"2\":\"b\"},{\"1\":\"5\",\"2\":\"f\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nAus der Hilfe der Funktion:\n\n>   Find pointwise mutual information of pairs of items in a column, based on a \"feature\" column that links them together. This is an example of the spread-operate-retidy pattern.\n\nDie Argumente der Funktion sind:\n\n*item*\n\nItem to compare; will end up in item1 and item2 columns\n\n*feature*\t\n\nColumn describing the feature that links one item to others\n\n\nManche Berechnungen (Operationen) lassen sich vielleicht leichter nicht in der Tidy-Form (Langform), sondern in der \"breiten\" oder Matrixform ausführen.\nWandeln wir mal `dat` von der Langform in die Breitform um:\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-8_5bcde7bbf8be00d20b0881fcbd8ee0fa'}\n\n```{.r .cell-code}\ntable(dat$item, dat$feature)\n##    \n##     1 2 3 4 5\n##   a 1 1 1 0 0\n##   b 1 0 0 1 1\n##   c 0 1 1 0 0\n##   e 0 0 0 1 0\n##   f 0 0 0 0 1\n```\n:::\n\n\nSilge und Robinson verdeutlichen das Prinzip von `widyr` so, s. @fig-widyr.\n\n\n![Die Funktionsweise von widyr, Quelle: Silge und Robinson](img/widyr.jpeg){#fig-widyr}\n\n(Vgl. auch die [Erklärung hier](https://bookdown.org/Maxine/tidy-text-mining/counting-and-correlating-pairs-of-words-with-widyr.html).)\n\nBauen wir das mal von Hand nach.\n\n\n\nRandwahrscheinlichkeiten von `a` und `c` sowie deren Produkt, `p_a_p_c`:\n\n\n::: {.cell hash='050-word-embedding_cache/html/p_a_und_p_c_66f24bb2041e530eefdd037fa67b9f16'}\n\n```{.r .cell-code}\np_a <- 3/5\np_c <- 2/5\n\np_a_p_c <- p_a * p_c\np_a_p_c\n## [1] 0.24\n```\n:::\n\n\n\nGemeinsame Wahrscheinlichkeit von `a` und `c`:\n\n\n::: {.cell hash='050-word-embedding_cache/html/p_ac_7df88f5285fbc01e3aa9c2670ba06c1e'}\n\n```{.r .cell-code}\np_ac <- 2/5\n```\n:::\n\n\n\nPMI von Hand berechnet:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-9_e6d94978df16605c832e9e159f337d83'}\n\n```{.r .cell-code}\nlog(p_ac/p_a_p_c)\n## [1] 0.5108256\n```\n:::\n\n\nMan beachte, dass hier als Basis $e$, der natürliche Logarithmus, verwendet wurde (nicht 2).\n\nJetzt berechnen wir die PMI mit `pairwise_pmi`.\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-10_a0f1c3605882757f7f5b2f443463b76a'}\n\n```{.r .cell-code}\npairwise_pmi(dat, item = item, feature = feature)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"item1\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"item2\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"pmi\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"b\",\"2\":\"a\",\"3\":\"-0.5877867\"},{\"1\":\"c\",\"2\":\"a\",\"3\":\"0.5108256\"},{\"1\":\"a\",\"2\":\"b\",\"3\":\"-0.5877867\"},{\"1\":\"e\",\"2\":\"b\",\"3\":\"0.5108256\"},{\"1\":\"f\",\"2\":\"b\",\"3\":\"0.5108256\"},{\"1\":\"a\",\"2\":\"c\",\"3\":\"0.5108256\"},{\"1\":\"b\",\"2\":\"e\",\"3\":\"0.5108256\"},{\"1\":\"b\",\"2\":\"f\",\"3\":\"0.5108256\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nWie man sieht, entspricht unserer Berechnung von Hand der Berechnung mit `pairwise_pmi`. \n\n\n\n\n\n### Sliding\n\nSliding ist ein interessantes Konzept, aber man braucht vielleicht etwas Zeit, \num sein Hirn um das Konzept zu wickeln...\n\nHier eine Illustration:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-11_875ec98ae1993840457cbae6b5672f80'}\n\n```{.r .cell-code}\ntxt_vec <- \"Das ist ein Test, von dem nicht viel zu erwarten ist\"\n\nslider::slide(txt_vec, ~ .x, .before = 2)\n## [[1]]\n## [1] \"Das ist ein Test, von dem nicht viel zu erwarten ist\"\n```\n:::\n\n\n\nOh, da passiert nichts?! Kaputt? Nein, wir müssen jedes Wort als *ein Element* des Vektors auffassen.\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-12_05fee5da369bd67ef578cdd468a238c7'}\n\n```{.r .cell-code}\ntxt_df <-\n  tibble(txt = txt_vec) %>% \n  unnest_tokens(input = txt, output = word)\n\nhead(txt_df)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"word\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"das\"},{\"1\":\"ist\"},{\"1\":\"ein\"},{\"1\":\"test\"},{\"1\":\"von\"},{\"1\":\"dem\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-13_f58fcf814eb3ef5b8951e33903b281d7'}\n\n```{.r .cell-code}\nslider::slide(txt_df$word, ~ .x, .before = 2)\n## [[1]]\n## [1] \"das\"\n## \n## [[2]]\n## [1] \"das\" \"ist\"\n## \n## [[3]]\n## [1] \"das\" \"ist\" \"ein\"\n## \n## [[4]]\n## [1] \"ist\"  \"ein\"  \"test\"\n## \n## [[5]]\n## [1] \"ein\"  \"test\" \"von\" \n## \n## [[6]]\n## [1] \"test\" \"von\"  \"dem\" \n## \n## [[7]]\n## [1] \"von\"   \"dem\"   \"nicht\"\n## \n## [[8]]\n## [1] \"dem\"   \"nicht\" \"viel\" \n## \n## [[9]]\n## [1] \"nicht\" \"viel\"  \"zu\"   \n## \n## [[10]]\n## [1] \"viel\"     \"zu\"       \"erwarten\"\n## \n## [[11]]\n## [1] \"zu\"       \"erwarten\" \"ist\"\n```\n:::\n\n\n\nAh!\n\n\nDas Aufteilen in einzelne Wörter pro Element des Vektors könnte man auch so erreichen:\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/slide2_0379451ed97fa6185c2743556cba099e'}\n\n```{.r .cell-code}\ntxt_vec2 <- str_split(txt_vec, pattern = boundary(\"word\")) %>% \n  simplify() \n\nslide(txt_vec2, .f = ~.x, .before = 2)\n## [[1]]\n## [1] \"Das\"\n## \n## [[2]]\n## [1] \"Das\" \"ist\"\n## \n## [[3]]\n## [1] \"Das\" \"ist\" \"ein\"\n## \n## [[4]]\n## [1] \"ist\"  \"ein\"  \"Test\"\n## \n## [[5]]\n## [1] \"ein\"  \"Test\" \"von\" \n## \n## [[6]]\n## [1] \"Test\" \"von\"  \"dem\" \n## \n## [[7]]\n## [1] \"von\"   \"dem\"   \"nicht\"\n## \n## [[8]]\n## [1] \"dem\"   \"nicht\" \"viel\" \n## \n## [[9]]\n## [1] \"nicht\" \"viel\"  \"zu\"   \n## \n## [[10]]\n## [1] \"viel\"     \"zu\"       \"erwarten\"\n## \n## [[11]]\n## [1] \"zu\"       \"erwarten\" \"ist\"\n```\n:::\n\n\n\n\n\nIn unserem Beispiel mit den Beschwerden:\n\n\n::: {.cell hash='050-word-embedding_cache/html/slide3_93f158057d60dd2b224b00cf3616ded4'}\n\n```{.r .cell-code}\nslide(beschwerde1_text$word,  ~.x, .before = 2)\n## [[1]]\n## [1] \"systems\"\n## \n## [[2]]\n## [1] \"systems\" \"inc\"    \n## \n## [[3]]\n## [1] \"systems\" \"inc\"     \"is\"     \n## \n## [[4]]\n## [1] \"inc\"    \"is\"     \"trying\"\n## \n## [[5]]\n## [1] \"is\"     \"trying\" \"to\"    \n## \n## [[6]]\n## [1] \"trying\"  \"to\"      \"collect\"\n## \n## [[7]]\n## [1] \"to\"      \"collect\" \"a\"      \n## \n## [[8]]\n## [1] \"collect\" \"a\"       \"debt\"   \n## \n## [[9]]\n## [1] \"a\"    \"debt\" \"that\"\n## \n## [[10]]\n## [1] \"debt\" \"that\" \"is\"  \n## \n## [[11]]\n## [1] \"that\" \"is\"   \"not\" \n## \n## [[12]]\n## [1] \"is\"   \"not\"  \"mine\"\n## \n## [[13]]\n## [1] \"not\"  \"mine\" \"not\" \n## \n## [[14]]\n## [1] \"mine\" \"not\"  \"owed\"\n## \n## [[15]]\n## [1] \"not\"  \"owed\" \"and\" \n## \n## [[16]]\n## [1] \"owed\" \"and\"  \"is\"  \n## \n## [[17]]\n## [1] \"and\"        \"is\"         \"inaccurate\"\n```\n:::\n\n\n\n\n### Funktion `slide_windows`\n\n\nDie Funktion `slide_windows` im [Kapitel 5.2](https://smltar.com/embeddings.html#understand-word-embeddings-by-finding-them-yourself) ist recht kompliziert. \nIn solchen Fällen ist es hilfreich, sich jeden Schritt einzeln ausführen zu lassen. \nDas machen wir jetzt mal.\n\nHier ist die Syntax der Funktion `slide_windows`: \n\n\n\n::: {.cell hash='050-word-embedding_cache/html/fun-slide-win_c0ab7a4744b0894bf8d593b3e2d4e689'}\n\n```{.r .cell-code}\nslide_windows <- function(tbl, window_size) {\n  skipgrams <- slider::slide(\n    tbl, \n    ~.x,  # Syntax ähnlich zu purrr::map()\n    .after = window_size - 1, \n    .step = 1, \n    .complete = TRUE\n  )\n  \n  safe_mutate <- safely(mutate)\n  \n  out <- map2(skipgrams,\n              1:length(skipgrams),\n              ~ safe_mutate(.x, window_id = .y))\n  \n  out %>%\n    transpose() %>%\n    pluck(\"result\") %>%\n    compact() %>%\n    bind_rows()\n}\n```\n:::\n\n\n\n\n\nErschwerend kommt eine große Datenmenge und eine lange Berechnungszeit dazu, was das Debuggen (Nachvollziehen und Durchdenken) der Schritte zusätzlich erschwert.\nIn solchen Fällen hilft die goldene Regel: Mach es dir so einfach wie möglich (aber nicht einfacher).\nWir nutzen also den stark verkleinerten Datensatz `nested_words3`, den wir oben importiert haben.\n\n\nZuerst erlauben wir mal, \ndasss unsere R-Session mehrere Kerne benutzen darf.\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-14_f3bed33e0f71fa0513c31b286a6ee558'}\n\n```{.r .cell-code}\nplan(multisession)  ## for parallel processing\n```\n:::\n\n\nDie Funktion `slide_windows` ist recht kompliziert.\nEs hilft oft, sich mit `debug(fun)` eine Funktion Schritt für Schritt anzuschauen.\n\n\n\nGehen wir Schritt für Schritt durch die Syntax von `slide_windows`.\n\n\n\nWerfen wir einen Blick in `words`, erstes Element (ein Tibble mit einer Spalte). \nDenn `die einzelnen Elemente von `words` werden an die Funktion `slide_windows` als \"Futter\" übergeben.\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-15_a2417d01674a2791129d97f08470a34e'}\n\n```{.r .cell-code}\nfutter1 <- nested_words3[[\"words\"]][[1]]\nfutter1\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"word\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"systems\"},{\"1\":\"inc\"},{\"1\":\"is\"},{\"1\":\"trying\"},{\"1\":\"to\"},{\"1\":\"collect\"},{\"1\":\"a\"},{\"1\":\"debt\"},{\"1\":\"that\"},{\"1\":\"is\"},{\"1\":\"not\"},{\"1\":\"mine\"},{\"1\":\"not\"},{\"1\":\"owed\"},{\"1\":\"and\"},{\"1\":\"is\"},{\"1\":\"inaccurate\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nDas ist der Text der ersten Beschwerde.\n\nOkay, also dann geht's los durch die einzelnen Schritte der Funktion `slide_windows`.\n\n\nZunächst holen wir uns die \"Fenster\" oder \"Skipgrams\":\n\n\n::: {.cell hash='050-word-embedding_cache/html/skipgrams1_9a58d04e50ec9670dedbcfb420ab3d55'}\n\n```{.r .cell-code}\nskipgrams1 <- slider::slide(\n   futter1, \n    ~.x, \n    .after = 3, \n    .step = 1, \n    .complete = TRUE\n  )\n```\n:::\n\n\n\nBei `slide(tbl, ~.x)` geben wir die Funktion an, die auf `tbl` angewendet werden soll. \nDaher auch die Tilde, die uns von `purrr::map()` her bekannt ist.\nIn unserem Fall wollen wir nur die Elemente auslesen;\nElemente auslesen erreicht man,\nin dem man sie mit Namen anspricht,\nin diesem Fall mit dem Platzhalter `.x`.\n\n\nJedes Element von `skipgrams1` ist ein 4*1-Tibble und ist ein Skripgram.\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-16_8824c79cfdbfebb2d5432dc4ff428a38'}\n\n```{.r .cell-code}\nskipgrams1 %>% str()\n## List of 17\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"systems\" \"inc\" \"is\" \"trying\"\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"inc\" \"is\" \"trying\" \"to\"\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"is\" \"trying\" \"to\" \"collect\"\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"trying\" \"to\" \"collect\" \"a\"\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"to\" \"collect\" \"a\" \"debt\"\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"collect\" \"a\" \"debt\" \"that\"\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"a\" \"debt\" \"that\" \"is\"\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"debt\" \"that\" \"is\" \"not\"\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"that\" \"is\" \"not\" \"mine\"\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"is\" \"not\" \"mine\" \"not\"\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"not\" \"mine\" \"not\" \"owed\"\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"mine\" \"not\" \"owed\" \"and\"\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"not\" \"owed\" \"and\" \"is\"\n##  $ : tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"owed\" \"and\" \"is\" \"inaccurate\"\n##  $ : NULL\n##  $ : NULL\n##  $ : NULL\n```\n:::\n\n\n\nDas zweite Skipgram von `skipgrams1` enthält, naja, das zweite Skipgram.\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-17_1c786c0b43e382f97887f51a44ae011d'}\n\n```{.r .cell-code}\nskipgrams1[[2]] %>% str()\n## tibble [4 × 1] (S3: tbl_df/tbl/data.frame)\n##  $ word: chr [1:4] \"inc\" \"is\" \"trying\" \"to\"\n```\n:::\n\n\nUnd so weiter.\n\n\n\n\nOkay, weiter im Programm. Jetzt mappen wir das Erstellen der Skipgrams \n\n\n::: {.cell hash='050-word-embedding_cache/html/skipgrams2_85deb39de11a4e8fbd05f509d2c8efcd'}\n\n```{.r .cell-code}\nsafe_mutate <- safely(mutate)\n  \nout1 <- map2(skipgrams1,\n             1:length(skipgrams1),\n             ~ safe_mutate(.x, window_id = .y))\n  \nout1 %>% \n  head(2) %>% \n  str()\n## List of 2\n##  $ :List of 2\n##   ..$ result: tibble [4 × 2] (S3: tbl_df/tbl/data.frame)\n##   .. ..$ word     : chr [1:4] \"systems\" \"inc\" \"is\" \"trying\"\n##   .. ..$ window_id: int [1:4] 1 1 1 1\n##   ..$ error : NULL\n##  $ :List of 2\n##   ..$ result: tibble [4 × 2] (S3: tbl_df/tbl/data.frame)\n##   .. ..$ word     : chr [1:4] \"inc\" \"is\" \"trying\" \"to\"\n##   .. ..$ window_id: int [1:4] 2 2 2 2\n##   ..$ error : NULL\n```\n:::\n\n\n`out1` ist eine Liste mit 17 Elementen; jedes Element  mit jeweils zwei Elementen: Den Ergebnissen und ob es einen Fehler gab bei `safe_mutate`.\nDie 10 Elemente entsprechen den 10 Skipgrams.\nWir können aber `out1` auch \"drehen\", transponieren genauer gesagt.\nso dass wir eine Liste mit *zwei* Elementen bekommen: \ndas erste Element hat die (zehn) Ergebnisse (nämlich die Skipgrams) und das zweite Elemente, ob es Fehler gab.\n\nDas Prinzip des Transponierens ist in @fig-Matrix-transpose dargestellt.\n\n![Transponieren einer Matrix (\"Tabelle\")](img/Matrix_transpose.gif){#fig-Matrix-transpose}\n\n\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-18_7db516c3139ca97464bf36e0930215b4'}\n\n```{.r .cell-code}\nout2 <-\nout1 %>%\n  transpose() \n```\n:::\n\n\n\nPuh, das ist schon anstrengendes Datenyoga...\n\n\nAber jetzt ist es einfach. \nWir ziehen das erste der beiden Elemente, die Ergebnisse heraus (`pluck`), \nentfernen leere Elemente (`compact`) und machen einen Tibble daraus (`bind_rows`):\n\n\n::: {.cell hash='050-word-embedding_cache/html/skipgrams3_e18ad3a3a9378fb608ae7da530e15b8b'}\n\n```{.r .cell-code}\nout2 %>% \n  pluck(\"result\") %>%\n  compact() %>%\n  bind_rows() %>% \n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"word\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"window_id\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"systems\",\"2\":\"1\"},{\"1\":\"inc\",\"2\":\"1\"},{\"1\":\"is\",\"2\":\"1\"},{\"1\":\"trying\",\"2\":\"1\"},{\"1\":\"inc\",\"2\":\"2\"},{\"1\":\"is\",\"2\":\"2\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nGeschafft!\n\n\n### Ähnlichkeit berechnen\n\n\nNachdem wir jetzt `slide_windows` kennen, schauen wir uns die nächsten Schritte an:\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/tidy-pmi1_366e085a07f4ced89213c51d4ecd790a'}\n\n```{.r .cell-code}\ntidy_pmi1 <- nested_words3 %>%  # <--- Kleiner Datensatz!\n  mutate(words = future_map(  # Schleife (mit mehreren Kernen) über ...\n    words,   #  alle Wörter\n    slide_windows,  # wobei jedes Mal diese Funtion angewendet wird\n    4L  # Parameter an `slide_windows`: Window-Größe ist 4 (L wie \"Long\", für Integer)\n  ))  \n```\n:::\n\n\nWir werden `slide_windows` auf die Liste `words` an,\ndie die Beschwerden enthält. \nFür jede Beschwerde erstellen wir die Skipgrams;\ndiese Schleife wird realisiert über `map` bzw. `future_map`,\ndie uns erlaubt, mehrere Kerne des Rechners gleichzeitig zu nutzen,\ndamit es schneller geht.\n\nHier sehen wir z.B. die Skipgram-IDs der ersten Beschwerde.\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-19_a2ea99525c016d294a5a6eca8e7c9a36'}\n\n```{.r .cell-code}\ntidy_pmi1[[\"words\"]][[1]] %>% \n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"word\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"window_id\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"systems\",\"2\":\"1\"},{\"1\":\"inc\",\"2\":\"1\"},{\"1\":\"is\",\"2\":\"1\"},{\"1\":\"trying\",\"2\":\"1\"},{\"1\":\"inc\",\"2\":\"2\"},{\"1\":\"is\",\"2\":\"2\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\nGenestet siehst es so aus:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-20_a442a77d2f64fecb72a2ccb340fdbdcc'}\n\n```{.r .cell-code}\ntidy_pmi1 %>% \n  head(1)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"complaint_id\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"words\"],\"name\":[2],\"type\":[\"list\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"3384392\",\"2\":\"<tibble[,2]>\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\nDie Listenspalte entschachteln wir mal:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-21_442654d2c204f83422bb3525343fb1b5'}\n\n```{.r .cell-code}\ntidy_pmi2 <- tidy_pmi1 %>% \n  unnest(words)  # entschachtele\n\ntidy_pmi2 %>% \n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"complaint_id\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"word\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"window_id\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"3384392\",\"2\":\"systems\",\"3\":\"1\"},{\"1\":\"3384392\",\"2\":\"inc\",\"3\":\"1\"},{\"1\":\"3384392\",\"2\":\"is\",\"3\":\"1\"},{\"1\":\"3384392\",\"2\":\"trying\",\"3\":\"1\"},{\"1\":\"3384392\",\"2\":\"inc\",\"3\":\"2\"},{\"1\":\"3384392\",\"2\":\"is\",\"3\":\"2\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nZum Berechnen der Ähnlichkeit brauchen wir eineindeutige IDs, \nnach dem Prinzip \"1. Skipgram der 1. Beschwerde\" etc:\n\n\n::: {.cell hash='050-word-embedding_cache/html/tidy-pmi3_c094f46bfe2059133f95884d65af8e85'}\n\n```{.r .cell-code}\ntidy_pmi3 <- tidy_pmi2 %>% \n  unite(window_id, complaint_id, window_id)  # führe Spalten zusammen\n\ntidy_pmi3 %>% \n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"window_id\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"word\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"3384392_1\",\"2\":\"systems\"},{\"1\":\"3384392_1\",\"2\":\"inc\"},{\"1\":\"3384392_1\",\"2\":\"is\"},{\"1\":\"3384392_1\",\"2\":\"trying\"},{\"1\":\"3384392_2\",\"2\":\"inc\"},{\"1\":\"3384392_2\",\"2\":\"is\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nSchließlich berechnen wir die Ähnlichkeit mit `pairwise_pmi`,\ndas hatten wir uns oben schon mal näher angeschaut:\n\n\n::: {.cell hash='050-word-embedding_cache/html/tidy-pmi4_2d5406115227464c1e9a41fcc9aef6f4'}\n\n```{.r .cell-code}\ntidy_pmi4 <- tidy_pmi3 %>% \n  pairwise_pmi(word, window_id)  # berechne Ähnlichkeit\n\ntidy_pmi <- tidy_pmi4  # mit dem Objekt arbeiten wir dann weiter\n\ntidy_pmi %>% \n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"item1\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"item2\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"pmi\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"inc\",\"2\":\"systems\",\"3\":\"5.728498\"},{\"1\":\"is\",\"2\":\"systems\",\"3\":\"2.838126\"},{\"1\":\"trying\",\"2\":\"systems\",\"3\":\"5.035351\"},{\"1\":\"systems\",\"2\":\"inc\",\"3\":\"5.728498\"},{\"1\":\"is\",\"2\":\"inc\",\"3\":\"2.838126\"},{\"1\":\"trying\",\"2\":\"inc\",\"3\":\"5.035351\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n\n\n### SVD \n\n\nDie *Singulärwertzerlegung* (singular value decomposition, SVD) ist ein Verfahren zur Verringerung der Spaltenzahl (Dimension), vergleichbar zur Faktorenanalyse.\nZur Anschaulichkeit - und ohne substanziellen Hintergrund - sei folgendes Beispiel genannt:\nDie Verben \"gehen\", \"rennen\", \"laufen\", \"schwimmen\", \"fahren\", \"rutschen\" könnten zu einer gemeinsamen Dimension, etwa \"fortbewegen\" reduziert werden.\nJedes einzelne der eingehenden Verben erhält eine Zahl von 0 bis 1, \ndas die konzeptionelle Nähe des Verbs\nzur \"dahinterliegenden\" Dimension (fortbewegen) quantifiziert; \ndie Zahl nennt man auch die \"Ladung\" des Items (Worts) auf die Dimension.\nSagen wir, wir identifizieren 10 Dimensionen.\nMan erhält dann für jedes unique Wort im Corpus einen Vektor mit den Ladungen auf die Dimensionen.\nIm genannten Beispiel wäre es ein 10-stelliger Vektor.\nSo wie ein 3-stelliger Vektor die Position einer Biene im Raum beschreibt^[Man könnte ergänzen: plus eine 4. Dimension für Zeit, plus noch ein paar Weitere für die Beschleunigung in verschiedene Richtungen...],\nbeschreibt hier unser 10-stelliger Vektor die \"Position\" eines Worts in unserem *Einbettungsvektor*.\n\n\nDie Syntax dazu ist dieses Mal einfach:\n\n\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/widely-svd_471d984af6e1856c781ad468b219d4fe'}\n\n```{.r .cell-code}\ntidy_word_vectors <- \n  tidy_pmi %>%\n  widely_svd(\n    item1, item2, pmi,\n    nv = 100\n  )\n\ntidy_word_vectors %>% \n  (head)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"item1\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"dimension\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"value\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"inc\",\"2\":\"1\",\"3\":\"0.03789628\"},{\"1\":\"is\",\"2\":\"1\",\"3\":\"0.11320691\"},{\"1\":\"trying\",\"2\":\"1\",\"3\":\"0.05127645\"},{\"1\":\"systems\",\"2\":\"1\",\"3\":\"0.03333321\"},{\"1\":\"to\",\"2\":\"1\",\"3\":\"0.12034339\"},{\"1\":\"collect\",\"2\":\"1\",\"3\":\"0.05542110\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\nMit `nv = 100` haben wir die Anzahl (`n`) der Dimensionen (Variablen, `v`) auf 100 bestimmt.\n\n\n\n### Wortähnlichkeit\n\n\nJetzt, da wir mit der SVD jedes Wort in einem Koordinatensystem verortet haben, können wir die Abstände der Wörter im Koordinatensystem bestimmen.\nDas geht mit Hilfe des alten Pythagoras, s. @fig-euklid-distance.\nDer Abstand, den man mit Hilfe des Satz des Pythagoras berechnet, nennt man auch *euklidische Distanz*.\n\n\n![Euklidische Distanz in 2D, Quelle: Wikipedia, CC BY4.0, Kmhkmh](img/euklid-distance.png){#fig-euklid-distance width=50%}\n\nOkay, wir sind in einem Raum mit vielen Dimensionen, was mein Hirn nicht mitmacht, [aber der Algebra ist das egal](https://mathworld.wolfram.com/Distance.html). \nPythagoras' Satz lässt sich genauso anwenden, wenn es mehr als Dimensionen sind.\n\n\n\n\n\nDie Autoren basteln sich selber eine Funktion in [Kap. 5.3](https://smltar.com/embeddings.html#exploring-cfpb-word-embeddings),\naber der Einfachheit halber nehme ich (erstmal) die entsprechende Funktion aus `widyr`:\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/pairwise-dist_4118ca68baa871dfc0af7459269e405e'}\n\n```{.r .cell-code}\nword_neighbors <- \ntidy_word_vectors %>% \n  pairwise_dist(item1, dimension, value)\n\nword_neighbors %>% \n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"item1\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"item2\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"distance\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"is\",\"2\":\"inc\",\"3\":\"1.0220147\"},{\"1\":\"trying\",\"2\":\"inc\",\"3\":\"0.9332845\"},{\"1\":\"systems\",\"2\":\"inc\",\"3\":\"0.4161259\"},{\"1\":\"to\",\"2\":\"inc\",\"3\":\"1.0913876\"},{\"1\":\"collect\",\"2\":\"inc\",\"3\":\"0.5221787\"},{\"1\":\"a\",\"2\":\"inc\",\"3\":\"1.0309535\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n\n\n\n\nSchauen wir uns ein Beispiel an.\nWas sind die Nachbarn von \"inaccurate\"?\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-22_80346be309e1baf7556f8311772d02fd'}\n\n```{.r .cell-code}\nword_neighbors %>% \n  filter(item1 == \"inaccurate\") %>% \n  arrange(distance) %>% \n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"item1\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"item2\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"distance\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"inaccurate\",\"2\":\"mine\",\"3\":\"0.5248875\"},{\"1\":\"inaccurate\",\"2\":\"score\",\"3\":\"0.5310127\"},{\"1\":\"inaccurate\",\"2\":\"oh\",\"3\":\"0.5400913\"},{\"1\":\"inaccurate\",\"2\":\"ny\",\"3\":\"0.5400913\"},{\"1\":\"inaccurate\",\"2\":\"dob\",\"3\":\"0.5801282\"},{\"1\":\"inaccurate\",\"2\":\"cell\",\"3\":\"0.6093669\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nHier ist die Datenmenge zu klein, um vernünftige Schlüsse zu ziehen.\nAber \"incorrectly\", \"correct\", \"balance\" sind wohl plausible Nachbarn von \"inaccurate\".\n\n\n### Cosinus-Ähnlichkeit\n\n\nDie Nähe zweier Vektoren lässt sich, neben der euklidischen Distanz, auch z.B. über die [Cosinus-Ähnlichkeit](https://en.wikipedia.org/wiki/Cosine_similarity) (Cosine similarity) berechnen, vgl. auch @fig-dotproduct:\n\n![Die Cosinus-Ähnlichkeit zweier Vektoren](img/dotproduct.png){#fig-dotproduct}\n\n[Quelle:  Mazin07, Lizenz: PD](https://en.wikipedia.org/wiki/Dot_product#/media/File:Dot_Product.svg)\n\n\n$${\\displaystyle {\\text{Cosinus-Ähnlichkeit}}=S_{C}(A,B):=\\cos(\\theta )={\\mathbf {A} \\cdot \\mathbf {B}  \\over \\|\\mathbf {A} \\|\\|\\mathbf {B} \\|}={\\frac {\\sum \\limits _{i=1}^{n}{A_{i}B_{i}}}{{\\sqrt {\\sum \\limits _{i=1}^{n}{A_{i}^{2}}}}{\\sqrt {\\sum \\limits _{i=1}^{n}{B_{i}^{2}}}}}},}$$\n\nwobei $A$ und $B$  zwei Vektoren sind und $\\|\\mathbf {A} \\|$ das Skalarprodukt von A (und B genauso).\nDas [Skalarprodukt](https://en.wikipedia.org/wiki/Dot_product) von $\\color {red} {a =  {\\displaystyle [a_{1},a_{2},\\cdots ,a_{n}]}}$ und $\\color {blue} {b =  {\\displaystyle [b_{1},b_{2},\\cdots ,b_{n}]}}$ ist so definiert:\n\n\n$${\\displaystyle \\mathbf {\\color {red}a} \\cdot \\mathbf {\\color {blue}b} =\\sum _{i=1}^{n}{\\color {red}a}_{i}{\\color {blue}b}_{i}={\\color {red}a}_{1}{\\color {blue}b}_{1}+{\\color {red}a}_{2}{\\color {blue}b}_{2}+\\cdots +{\\color {red}a}_{n}{\\color {blue}b}_{n}}$$\n\n\nEntsprechend ist die Funktion `nearest_neighbors` zu verstehen aus [Kap. 5.3](https://smltar.com/embeddings.html#exploring-cfpb-word-embeddings):\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-23_d86532b56b27a00d8929c8350a9ff2ee'}\n\n```{.r .cell-code}\nnearest_neighbors <- function(df, token) {\n  df %>%\n    widely(\n      ~ {\n        y <- .[rep(token, nrow(.)), ]\n        res <- rowSums(. * y) / \n          (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))\n        \n        matrix(res, ncol = 1, dimnames = list(x = names(res)))\n      },\n      sort = TRUE\n    )(item1, dimension, value) %>%\n    select(-item2)\n}\n```\n:::\n\n\nWobei mit `widely` zuerst noch von der Langform in die Breitform umformatiert wird,\nda die Breitform von der Formel verlangt wird bzw. das Rechnen einfacher macht.\n\nDer eine Vektor ist das Embedding des Tokens,\nder andere Vektor ist das *mittlere* Embedding über alle Tokens des Corpus.\nWenn die Anzahl der Elemente konstant bleibt,\nkann man sich das Teilen durch $n$ schenken,\nwenn man einen Mittelwert berechnen;\nso hält es auch die Syntax von `nearest_neighbors`.\n\nEin nützlicher Post zur Cosinus-Ähnlichkeit findet sich [hier](https://towardsdatascience.com/cosine-similarity-how-does-it-measure-the-similarity-maths-behind-and-usage-in-python-50ad30aad7db).\n[Dieses Bild](https://datascience-enthusiast.com/figures/cosine_sim.png) zeigt das\nKonzept der Cosinus-Ähnlichkeit anschaulich.\n\nZur Erinnerung: Der Cosinus eines Winkels ist definiert als Verhältnis der Länge der Ankathete (das ist jene Kathete, die einen Schenkel des Winkels bildet) zur Länge der Hypotenuse^[Quelle: https://de.wikipedia.org/wiki/Sinus_und_Kosinus] in einem rechtwinkligen, vgl. @fig-dreieck.\n\n![Ein rechtwinkliges Dreieck als Grundlage der trigonometrischen Funktionen](img/dreieck.png){#fig-dreieck}\n\nAlso: ${\\displaystyle \\cos \\alpha ={\\frac {b}{c}}}$\n\n\n\n[Quelle: PaterSigmund, Wikipedia,dre CC-BY-SA 2.5](https://de.wikipedia.org/wiki/Sinus_und_Kosinus#/media/Datei:RechtwinkligesDreieck.svg)\n\n\nHilfreich ist auch [die Visualisierung von Sinus und Cosinus am Einheitskreis](https://de.wikipedia.org/wiki/Sinus_und_Kosinus#/media/Datei:Sinus_und_Kosinus_am_Einheitskreis_1.svg); gerne [animiert](https://upload.wikimedia.org/wikipedia/commons/f/f3/Sinus_und_Cosinus_am_Einheitskreis.gif) betrachten.\n\n\n## Word-Embeddings vorgekocht\n\n### Glove6B\n\nIn [Kap. 5.4](https://smltar.com/embeddings.html#glove) schreiben die Autoren:\n\n>   If your data set is too small, you typically cannot train reliable word embeddings.\n\nEin paar Millionen Wörter sollte der Corpus schon enthalten, so die Autoren.\nDa solche \"Worteinbettungen\" (word embedings) aufwändig zu erstellen sind, \nkann man fertige, \"vorgekochte\" Produkte nutzen.\n\nGlove6B wurde anhand von Wikipedia und anderen Datenquellen erstellt [@pennington_glove_2014].\n\n:::callout-note\nDie zugehörigen Daten sind recht groß; für [`glove6b`](https://nlp.stanford.edu/projects/glove/) [@pennington_glove_2014] ist fast ein Gigabyte fällig.\nSie sollten sich die Daten in einem ruhigen Moment (mit stabiler Internetverbindung) herunterladen und in einem Verzeichnis meiner Wahl abgespeichert (`datasets`).\nDa bei mir Download abbrach, als ich `embedding_glove6b(dimensions = 100)` aufrief, habe ich die Daten manuell heruntergeladen, s.u.\n:::\n\n\nWie immer, Hilfe für eine Funktion bekommt man mit `?fun_name` oder interaktiv z.B. in RStudio.\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/load-glove_25547664596a3428a08225336fed918e'}\n\n```{.r .cell-code}\nglove6b <- \n  embedding_glove6b(dir = \"~/datasets\",  # aus dem Paket `textdata`\n                    dimensions = 50,  # mit nur 50 Dimensionen\n                    manual_download = TRUE)  # ist der Datensatz schon manuell heruntergeladen?\n```\n:::\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-24_e46fd328a2430c2ae2342a5082b9cf8b'}\n\n:::\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-25_3abe0c0d76f6707e600d4198f18eb895'}\n\n```{.r .cell-code}\nglove6b %>% \n  select(1:5) %>% \n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"token\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"d1\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"d2\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"d3\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"d4\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"the\",\"2\":\"0.418000\",\"3\":\"0.249680\",\"4\":\"-0.41242\",\"5\":\"0.121700\"},{\"1\":\",\",\"2\":\"0.013441\",\"3\":\"0.236820\",\"4\":\"-0.16899\",\"5\":\"0.409510\"},{\"1\":\".\",\"2\":\"0.151640\",\"3\":\"0.301770\",\"4\":\"-0.16763\",\"5\":\"0.176840\"},{\"1\":\"of\",\"2\":\"0.708530\",\"3\":\"0.570880\",\"4\":\"-0.47160\",\"5\":\"0.180480\"},{\"1\":\"to\",\"2\":\"0.680470\",\"3\":\"-0.039263\",\"4\":\"0.30186\",\"5\":\"-0.177920\"},{\"1\":\"and\",\"2\":\"0.268180\",\"3\":\"0.143460\",\"4\":\"-0.27877\",\"5\":\"0.016257\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n\nDie ersten paar Tokens sind:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-26_28530680bcc5628d1a12120723128674'}\n\n```{.r .cell-code}\nglove6b$token %>% head(20)\n##  [1] \"the\"  \",\"    \".\"    \"of\"   \"to\"   \"and\"  \"in\"   \"a\"    \"\\\"\"   \"'s\"  \n## [11] \"for\"  \"-\"    \"that\" \"on\"   \"is\"   \"was\"  \"said\" \"with\" \"he\"   \"as\"\n```\n:::\n\n\n\n\nIn eine Tidyform bringen:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-27_8275f0535dbb712b37c5099560291bc6'}\n\n```{.r .cell-code}\ntidy_glove <- \n  glove6b %>%\n  pivot_longer(contains(\"d\"),\n               names_to = \"dimension\") %>%\n  rename(item1 = token)\n\nhead(tidy_glove)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"item1\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"dimension\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"value\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"the\",\"2\":\"d1\",\"3\":\"0.418000\"},{\"1\":\"the\",\"2\":\"d2\",\"3\":\"0.249680\"},{\"1\":\"the\",\"2\":\"d3\",\"3\":\"-0.412420\"},{\"1\":\"the\",\"2\":\"d4\",\"3\":\"0.121700\"},{\"1\":\"the\",\"2\":\"d5\",\"3\":\"0.345270\"},{\"1\":\"the\",\"2\":\"d6\",\"3\":\"-0.044457\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\nGanz schön groß:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-28_6ff1b7a0a904b8b394c5c0080917aaa5'}\n\n```{.r .cell-code}\ndim(glove6b)\n## [1] 400000     51\n```\n:::\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-29_fc98dce55158813840e3b3b927e95dc7'}\n\n```{.r .cell-code}\nobject.size(tidy_glove)\n## 503834736 bytes\n```\n:::\n\n\nIn Megabyte^[$1024 \\cdot 1024$ Byte, und $1024 =2^{10}$, daher $2^{10} \\cdot 2^{10} = 2^{20}$]\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-30_0d746724a1ef5826c9de5d6cb700a12a'}\n\n```{.r .cell-code}\nobject.size(tidy_glove) / 2^20\n## 480.5 bytes\n```\n:::\n\n\nEinfacher und genauer geht es so:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-31_0a80d721d6b20d2db98f8cc39736c006'}\n\n```{.r .cell-code}\npryr::object_size(tidy_glove)\n## 503.83 MB\n```\n:::\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-32_abbda65bef7a208336648e41697c93ef'}\n\n```{.r .cell-code}\npryr::mem_used()\n## 858 MB\n```\n:::\n\n\n\n\nUm Speicher zu sparen, könnte man `glove6b` wieder direkt löschen, wenn man nur mit der Tidyform weiterarbeitet.\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-33_05483203e5b9a25944666a1414447e0f'}\n\n```{.r .cell-code}\nrm(glove6b)\n```\n:::\n\n\n\n\nJetzt können wir wieder nach Nachbarn fragen, im euklidischen Sinne, wie oben.\nProbieren wir aus, welche Wörter nah zu \"inaccurate\" stehen.\n\n\n:::callout-note\nWie wir oben gesehen haben, ist der Datensatz riesig^[zugegeben, ein subjektiver Ausdruck],\nwas die Berechnungen (zeitaufwändig) und damit nervig machen können.\nDarüber hinaus kann es nötig sein, dass Sie mehr Speicher auf Ihrem Computer zur Verfügung stellen müssen^[Kaufen...].\nWir müssen noch `maximum_size = NULL`, um das Jonglieren mit riesigen Matrixen zu erlauben.\nMöge der Gott der RAMs und Arbeitsspeicher uns gnädig sein!\n:::\n\n\n\n\n\nMit `pairwise_dist` dauert die Berechnung sehr lange und braucht wohl sehr viel Speicher.\nMitunter kam folgender Fehler auf: \"R error: vector memory exhausted (limit reached?)\".\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-34_524f773014ed953c81dd36d80efe1362'}\n\n```{.r .cell-code}\nword_neighbors_glove6b <- \ntidy_glove %>% \n  slice_head(prop = .1) %>% \n  pairwise_dist(item1, dimension, value, maximum_size = NULL)\n\nhead(word_neighbors_glove6b)\n\ntidy_glove %>% \n  filter(item1 == \"inaccurate\") %>% \n  arrange(-value) %>% \n  slice_head(n = 5)\n```\n:::\n\n\n\n\nDeswegen probieren wir doch die Funktion `nearest_neighbors`, so wie es im Buch vorgeschlagen wird, s. [Kap 5.3](https://smltar.com/embeddings.html#exploring-cfpb-word-embeddings).\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/fun-nearest-neighbors2_5fd3dc01fa8d285f56193e37aa4c29ba'}\n\n```{.r .cell-code}\nnearest_neighbors <- function(df, token) {\n  df %>%\n    widely(\n      ~ {\n        y <- .[rep(token, nrow(.)), ]\n        res <- rowSums(. * y) / \n          (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))\n        matrix(res, ncol = 1, dimnames = list(x = names(res)))\n      },\n      sort = TRUE,\n      maximum_size = NULL\n    )(item1, dimension, value) %>%\n    select(-item2)\n}\n```\n:::\n\n::: {.cell hash='050-word-embedding_cache/html/tidy-glove-nearest-neighbors_fa2c0fa6962e7230cbc2a2e87dacc901'}\n\n```{.r .cell-code}\ntidy_glove %>%\n  # slice_head(prob = .1) %>% \n  nearest_neighbors(\"error\") %>% \n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"item1\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"value\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"error\",\"2\":\"1.0000000\"},{\"1\":\"errors\",\"2\":\"0.8462532\"},{\"1\":\"correct\",\"2\":\"0.7349197\"},{\"1\":\"mistake\",\"2\":\"0.7174614\"},{\"1\":\"accurate\",\"2\":\"0.7063602\"},{\"1\":\"precise\",\"2\":\"0.7035035\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\nEntschachteln wir unsere Daten zu `complaints`: \n\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnest-tidy-complaints3_60ee389ba2152157b24e164be3693cd8'}\n\n```{.r .cell-code}\ntidy_complaints3 <-\n  nested_words3 %>% \n  unnest(words)\n```\n:::\n\n\n\n\nDann erstellen wir uns eine Tabelle, in der nur die Schnittmenge der Wörter aus den Beschwerden und Glove vorkommen.\nDazu nutzen winr einen [inneren Join](https://github.com/gadenbuie/tidyexplain/blob/main/images/inner-join.gif)\n\n![Inner Join, Quelle: Garrick Adenbuie](img/inner-join.gif)\n\n[Quelle](https://github.com/gadenbuie/tidyexplain)\n\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/join-complaints-glove_f4f9f39118501f69e6348786089f0a6d'}\n\n```{.r .cell-code}\ncomplaints_glove <- \ntidy_complaints3 %>% \n  inner_join(by = \"word\", \n  tidy_glove %>% \n  distinct(item1) %>% \n  rename(word = item1)) \n\nhead(complaints_glove)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"complaint_id\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"word\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"3384392\",\"2\":\"systems\"},{\"1\":\"3384392\",\"2\":\"inc\"},{\"1\":\"3384392\",\"2\":\"is\"},{\"1\":\"3384392\",\"2\":\"trying\"},{\"1\":\"3384392\",\"2\":\"to\"},{\"1\":\"3384392\",\"2\":\"collect\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nWie viele unique (distinkte) Wörter gibt es in unserem Corpus?\n\n\n::: {.cell hash='050-word-embedding_cache/html/tidy_complaints3_distinct_words_n_4c2ba336f309dd585efb40f944220f5f'}\n\n```{.r .cell-code}\ntidy_complaints3_distinct_words_n <- \ntidy_complaints3 %>% \n  distinct(word) %>% \n  nrow()\n\ntidy_complaints3_distinct_words_n\n## [1] 222\n```\n:::\n\n\n\nIn `tidy_complaints` gibt es übrigens 222 verschiedene Wörter.\n\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/word_matrix_5814af2f71076aac8bec03f940273cd5'}\n\n```{.r .cell-code}\nword_matrix <- tidy_complaints3 %>%\n  inner_join(by = \"word\",\n             tidy_glove %>%\n               distinct(item1) %>%\n               rename(word = item1)) %>%\n  count(complaint_id, word) %>%\n  cast_sparse(complaint_id, word, n)\n\n#word_matrix\n```\n:::\n\n\n`word_matrix` zählt für jede der 10 Beschwerden, welche Wörter (und wie häufig) vorkommen.\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-35_6c409cd8f03afcb0de7658383dffb577'}\n\n```{.r .cell-code}\ndim(word_matrix)\n## [1]  10 222\n```\n:::\n\n\n10 Beschwerden (Dokumente) und 222 unique Wörter.\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/glove_matrix_aa71f52cb314ed9633768250ba520e0d'}\n\n```{.r .cell-code}\nglove_matrix <- tidy_glove %>%\n  inner_join(by = \"item1\",\n             tidy_complaints3 %>%\n               distinct(word) %>%\n               rename(item1 = word)) %>%\n  cast_sparse(item1, dimension, value)\n\n#glove_matrix\n```\n:::\n\n\n\n`glove_matrix` gibt für jedes unique Wort den Einbettungsvektor an.\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-36_6234824dda04e19d14e32629e01e8963'}\n\n```{.r .cell-code}\ndim(glove_matrix)\n## [1] 222  50\n```\n:::\n\n\nDas sind 222 unique Wörter und 50 Dimensionen des Einbettungsvektors.\n\n\n\nJetzt können wir noch pro Dokument (10 in diesem Beispiel) die mittlere \"Position\" jedes Dokuments im Einbettungsvektor ausrechnen.\nBildlich gesprochen: Was ist der mittlere Raumpunkt (Zentroid) des Bienenschwarms, wobei eine Biene ein Dokument darstellt und die Raumachsen die Dimensionen des Einbettungsvektors.\n\nDazu gewichten wir jedes Wort eines Dokuments mit den Ladungen des Einbettungsvektor und summieren diese Terme.\nEs resultiert eine Matrix mit einem Einbettungsvektor pro Dokument.\nDiese Matrix können wir jetzt als Prädiktorenmatrix hernehmen.\n\n\n::: {.cell hash='050-word-embedding_cache/html/doc_matrix_8ac7db28475be6072fcb91a76e695cfb'}\n\n```{.r .cell-code}\ndoc_matrix <- word_matrix %*% glove_matrix\n#doc_matrix %>% head()\n```\n:::\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-37_cdb06327327393c29b81fd32298bd1fe'}\n\n```{.r .cell-code}\ndim(doc_matrix)\n## [1] 10 50\n```\n:::\n\n\n\nDie Anzahl der Dokumente ist 10 und die Anzahl der Dimensionen (des Einbettungsvektors) ist 50.\n\n\n### Wordembeddings für die deutsche Sprache\n\n\nIn dem [Github-Projekt \"Wikipedia2Vec\"](https://wikipedia2vec.github.io/wikipedia2vec/) finden sich die Materialien für ein deutsches Wordembedding [@yamada2019neural].  \n\n\n\n## Fazit\n\nWorteinbettungen sind eine aufwändige Angelegenheit. \nPositiv gesprochen kann ein Analysti die Muskeln spielen lassen und zeigen, was sie oder er so alles drauf hat.\nIst ja schon cooles Zeugs, die Word Embeddings.\nEs besteht die Chance, dass man mit dieser Methode bessere Vorhersagen erreicht, als mit anderen, einfachen\nAnsätzen wir Worthäufigkeiten oder tf-idf.\nAuf der anderen Seite ist es oft sinnvoll, mit einfachen Ansätzen zu starten,\nund zu sehen, wie weit man kommt.\nVielleicht ja weit genug.\nNatürlich haben Modelle auf Basis von Worthäufigkeiten auch ihre Schwächen: Vor allem ist ihre prädiktive Nützlichkeit nicht immer gegeben und die Datensätze werden schnell sehr groß (viele Spalten).\n\n\n\n\n\n\n## Fallstudie \n\n[Hacker-News-Einbettungs-Fallstudie](https://juliasilge.com/blog/tidy-word-vectors/) (was für ein Word!) -- Eine praktische Darstellung der Erstellung von Word-Vektoren rein auf Basis der linearen Algebra.\n\n\n## Vertiefung\n\nWorteinbettungen sind ein zentrales Thema im NLP; für moderne Sprachmodelle sind sie von zentraler Bedeutung. Entsprechend findet sich viel Literatur, z.B. [@liu_representation_2023; @george_python_2022; @noauthor_word_2023; @mikolov_efficient_2013; @camacho-collados_embeddings_2020; @almeida_word_2019; @pilehvar_embeddings_2021].\n\n\n\nEin Online-Python-Tutorial findet sich z.B. bei [TensorFlow](https://www.tensorflow.org/text/guide/word_embeddings).\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}