{
  "hash": "330976da3acffc76777b17644dff3315",
  "result": {
    "engine": "knitr",
    "markdown": "# Transformer\n\n\n\n## Vorab\n\n\n### Lernziele\n\n\n\n- Sie k√∂nnen die grundlegende Architektur eines Transformer-Modells beschreiben.\n- Sie k√∂nnen Transformer-Modelle mit der API von Hugging-Face berechnen.\n\n\n\n### Begleitliteratur \n\n\nDer [Blogpost von Jay Alammar](https://jalammar.github.io/illustrated-transformer/) gibt einen illustrierten √úberblick √ºber Transformer.\n\n\n\n\n\n\n### Ben√∂tigte Software\n\nWir ben√∂tigen Python, R sowei einige im Folgenden aufgef√ºhrte Python-Module.\n\n\n::: {.cell hash='120-transformer_cache/html/unnamed-chunk-1_eb5522239ca02bbf481e0cbe07555ce3'}\n\n```{.python .cell-code}\nimport pandas as pd\nimport os\n```\n:::\n\n\n\nF√ºr den Sch√ºleraustausch von R nach Python nutzen wir das R-Paket `reticulate`:\n\n\n::: {.cell hash='120-transformer_cache/html/unnamed-chunk-2_35550288e552b1b191097962ee5c1174'}\n\n```{.r .cell-code}\nlibrary(reticulate)\n```\n:::\n\n\n\nAu√üerdem starte ich die \"richtige\" Python-Version, wo die ben√∂tigten Pakete (in der richtigen Version) installiert sind:\n\n\n::: {.cell hash='120-transformer_cache/html/use-virtualenv_4d7a1d376bc9840cf39d671b66e69907'}\n\n```{.r .cell-code}\n#use_virtualenv(\"r-tensorflow\")\n```\n:::\n\n\nCheck:\n\n\n::: {.cell hash='120-transformer_cache/html/unnamed-chunk-3_f2b1eb32aed8c2ec3bf20ea1a3c7dcd3'}\n\n```{.r .cell-code}\npy_available()\n## [1] TRUE\n```\n:::\n\n\nWelche Python-Version nutzt `reticulate` gerade?\n\n\n::: {.cell hash='120-transformer_cache/html/unnamed-chunk-4_c767e9652e114455729de854513e1e54'}\n\n```{.r .cell-code}\npy_config()\n## python:         /Users/sebastiansaueruser/.virtualenvs/r-tensorflow/bin/python\n## libpython:      /Users/sebastiansaueruser/.pyenv/versions/3.8.16/lib/libpython3.8.dylib\n## pythonhome:     /Users/sebastiansaueruser/.virtualenvs/r-tensorflow:/Users/sebastiansaueruser/.virtualenvs/r-tensorflow\n## version:        3.8.16 (default, Sep 15 2023, 17:53:02)  [Clang 14.0.3 (clang-1403.0.22.14.1)]\n## numpy:          /Users/sebastiansaueruser/.virtualenvs/r-tensorflow/lib/python3.8/site-packages/numpy\n## numpy_version:  1.24.3\n## \n## NOTE: Python version was forced by VIRTUAL_ENV\n```\n:::\n\n\n\n\n## √úberblick\n\nTransformer sind eine Architekturvariante neuronaler Netze. Sie stellen die Grundlage vieler aktueller gro√üer Sprachmodelle^[Man spricht von *Large Language Models*, LLM]; da sie einige Vorz√ºge gegen√ºber Vorg√§ngermodellen aufweisen, haben sie einen zentralen Platz f√ºr verschiedenen Aufgaben des NLP eingenommen.\n\nIm Jahr 2017 erschien ein Paper auf Arxive mit dem Titel \"Attention is all you need\", @vaswani_attention_2023^[Da die Autoren immer wieder Updates bei Arxive eingestellt haben, ist hier die aktuellste Version, V7 aus 2023, zitiert.].\nTransformer basieren auf einer bestimmten Art von \"Aufmerksamkeit\", genannt Selbst-Aufmerksamkeit (self-attention).\nNat√ºrlich ist damit eine bestimmte Architektur im neuronalen Netzwerk gemeint, kein kognitivpsychologiches Konstruktr; allerdings lehnt sich die Methode an Konzepte der Kognitionspsychologie vage an.\n\nSelf-Attention weist zwei gro√üe Verteile auf: Erstens erlaubt es parallele Verarbeitung, was viele Vorg√§ngermodelle nicht erlaubten. Zweitens kann es den Kontext eines Tokens, also den Text um ein bestimmtes Wort herum, deutlich besser \"im Blick\" (oder in der Aufmerksamkeit) behalten als viele Vorg√§ngermodelle.\n\nGerade f√ºr Daten mit sequenziellem Charakter, wie Text oder Sprache, sind Transformer-Modelle gut geeignet^[relativ zu anderen, bisherigen Modellen].\n\n\n## Grundkonzepte\n\n\n{{< video https://youtu.be/4Bdc55j80l8?si=t3ku0MxhWDD7z2TG >}}\n\n\n\n\n\n## Einf√ºhrung in Hugging Face ü§ó\n\nDieser Abschnitt orientiert sich an @tunstall_natural_2022.\nDie Syntax zu allen Kapiteln des [Buchs](https://transformersbook.com/) findet sich praktischerweise [in diesem Github-Repo](https://github.com/nlp-with-transformers/notebooks).\n\n\n\nBei ü§ó liegt der Schwerpunkt klar bei Python, nicht bei R.\nAllerdings erlaubt RStudio ein einfaches Wechseln zwischen R und Python:\nFunktionen und Daten aus Python k√∂nnen einfach mit dem `$`-Operator angesprochen werden.\n[In diesem Post](https://rpubs.com/eR_ic/transfoRmers) wirds das demonstriert.\n\n\nSchauen wir uns das einf√ºhrende Beispiel aus @tunstall_natural_2022. an.\n\n\n### Hugging Face mit R\n\nHier ein ein Text-Schnipsel,\ndessen Sentiment wir detektieren wollen:\n\n\n::: {.cell fenced='true' hash='120-transformer_cache/html/unnamed-chunk-5_95d09ba21a704a2735f48dbc4ba9d85b'}\n\n```{.r .cell-code}\ntext <- (\"Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\")\n```\n:::\n\n\nUnd hier in der Python-Version:\n\n\n::: {.cell fenced='true' hash='120-transformer_cache/html/unnamed-chunk-6_0e7c583f5bcee2db69e76e2c8b2b5ff5'}\n\n```{.python .cell-code}\ntext_py = r.text\n```\n:::\n\n\n\nDann importieren wir die n√∂tigen Module:\n\n\n:::{.panel-tabset}\n\n### Python\n\n\n::: {.cell hash='120-transformer_cache/html/unnamed-chunk-7_a2c72ceaa16dc163faa09b3630c685f0'}\n\n```{.python .cell-code}\n#import tensorflow\nfrom transformers import pipeline\n```\n:::\n\n\n\nNat√ºrlich m√ºssen Python-Module installiert sein, bevor man sie nutzen kann, genau so wie R-Pakete.\n\n\n### R\n\nMan kann die die Python-Module auch √ºber R starten:\n\n\n\n\n::: {.cell hash='120-transformer_cache/html/unnamed-chunk-8_3597af7d2cc08eeb280320f609764490'}\n\n```{.r .cell-code}\ntransformers <- reticulate::import(\"transformers\")\n```\n:::\n\n:::\n\n\n\n### Einfache Pipeline\n\n\n{.panel-tabset}\n\n### Python\n\nWir bereiten das Modell vor; im Default wird \n`distilbert-base-uncased-finetuned-sst-2-english` verwendet.\n\n\n::: {.cell hash='120-transformer_cache/html/unnamed-chunk-9_19b0c983288162e83428ef1882926a20'}\n\n```{.python .cell-code}\nclassifier = pipeline(\"text-classification\")\n## No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n## Using a pipeline without specifying a model name and revision in production is not recommended.\n## All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n## \n## All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n## If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n```\n:::\n\n\n\n\n\n## Germeval Out-of-the-Box mit Hugging Face\n\nZuert importieren wir die Daten.\n\n:::{.panel-tabset}\n\n\n### R\n\n\n::: {.cell hash='120-transformer_cache/html/unnamed-chunk-10_7b78c45530d01dcdfbd619c199d37c19'}\n\n```{.r .cell-code}\ndata(germeval_train, package = \"pradadata\")\ntext <- germeval_train$text[1:2]\ntext[1:2]\n## [1] \"@corinnamilborn Liebe Corinna, wir w√ºrden dich gerne als Moderatorin f√ºr uns gewinnen! W√§rst du begeisterbar?\"                                 \n## [2] \"@Martin28a Sie haben ja auch Recht. Unser Tweet war etwas missverst√§ndlich. Dass das BVerfG Sachleistungen nicht ausschlie√üt, kritisieren wir.\"\n```\n:::\n\n\n\n### Python\n\n\n::: {.cell hash='120-transformer_cache/html/unnamed-chunk-11_1716652a69bb408125e1275d5a807d51'}\n\n```{.python .cell-code}\ngermeval_train_py = r.text\n```\n:::\n\n\n:::\n\n\n### Standard-Pipeline\n\n\n::: {.cell hash='120-transformer_cache/html/unnamed-chunk-12_bc5e6039e7bd537e3ad450cdc9f587ab'}\n\n```{.python .cell-code}\nclassifier = pipeline(\"text-classification\")\n## No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n## Using a pipeline without specifying a model name and revision in production is not recommended.\n## All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n## \n## All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n## If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\noutputs2 = classifier(germeval_train_py)\noutputs2\n## [{'label': 'NEGATIVE', 'score': 0.9950070381164551}, {'label': 'NEGATIVE', 'score': 0.9954568147659302}]\n```\n:::\n\n\n\nTja, vielleicht sollten wir ein Modell verwenden, das die deutsche Sprache versteht?\n\n\n### Man spricht Deutsh\n\nAuf Hugging Face gibt es eine Menge von Modellen. Welches nehm ich nur? [DISTILBERT](https://huggingface.co/distilbert-base-german-cased) oder [BERT](https://huggingface.co/bert-base-uncased)-Varianten d√ºrfte kein schlechter Start sein.\n\n\n\n\n::: {.cell hash='120-transformer_cache/html/unnamed-chunk-13_f4ee29bf9dfe02feb26565ddfb993984'}\n\n```{.python .cell-code}\n#classifier = pipeline(\"text-classification\", model=\"distilbert-base-german-cased\")\n```\n:::\n\n::: {.cell hash='120-transformer_cache/html/german-sentiment-bert_08f74b5e05f48b0ad6930e2acc2e3490'}\n\n```{.python .cell-code}\nclassifier = pipeline(\n  \"text-classification\", model=\"oliverguhr/german-sentiment-bert\")\n## All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n## \n## All the weights of TFBertForSequenceClassification were initialized from the PyTorch model.\n## If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n```\n:::\n\n::: {.cell hash='120-transformer_cache/html/class-preds_e799cf56515e36351696237984f35b87'}\n\n```{.python .cell-code}\noutputs3 = classifier(germeval_train_py)\ndf = pd.DataFrame(outputs3)    \ndf.head()\n##       label     score\n## 0   neutral  0.987253\n## 1  negative  0.918047\n```\n:::\n\n::: {.cell hash='120-transformer_cache/html/head-df_b129ce1b2c8affd401b5422fdd5cebcb'}\n\n```{.r .cell-code}\ndf_r <- py$pd\nhead(df_r)\n```\n:::\n\n\n\n\n## OpenAI-API\n\n::: callout-important\n\nDer API-Aufruf von ChatGPT kostet Geld üí∏. $\\square$\n:::\n\n\n### Authentifizierung\n\n\nWir m√ºssen uns bei der API anmelden:\n\n\n:::{.panel-tabset}\n\n\n### R\n\n\n::: {.cell hash='120-transformer_cache/html/unnamed-chunk-14_e25abeee46ce452b4214b2d7ae519d2b'}\n\n```{.r .cell-code}\nopenai_key_r <- Sys.getenv(\"OPENAI_API_KEY\")\n```\n:::\n\n\n\n### Python\n\n\n::: {.cell hash='120-transformer_cache/html/unnamed-chunk-15_68b492206ae6700ee202b655d64c2e7f'}\n\n```{.python .cell-code}\nopenai_key_py = os.environ.get(\"OPENAI_API_KEY\")\n```\n:::\n\n\n\n:::\n\n:::callout-caution\nSpeichern Sie keine sensiblen Daten in geteilten Ordner/Repos. Achten Sie auf Log-Dateien wir `.Rhistory`, in der u.U. Ihre sensiblen Daten enthalten sein k√∂nnen. $\\square$\n:::\n\nEine sichere Variante als das unverschl√ºsselte Speichenr von Passw√∂rtern ist es, sensible Daten mit einem Passwort zu sch√ºtzen. \nDazu kann man z.B. in R das Paket `keyring` nutzen.\n\n\n::: {.cell hash='120-transformer_cache/html/unnamed-chunk-16_4bc1caa73cd3f7536c1b3cb3454aa68b'}\n\n```{.r .cell-code}\nlibrary(keyring)\nopenai_key_r <- key_get(\"OPENAI_API_KEY\")\n```\n:::\n\n\n\n\n### Setup\n\n\n::: {.cell hash='120-transformer_cache/html/unnamed-chunk-17_45c3074000d6395990ce0489e1a19a2a'}\n\n```{.python .cell-code}\nsentiment_scores = []\nsentiment_analysis = []\ntext = '@Martin28a Sie haben ja auch Recht. Unser Tweet war etwas missverst√§ndlich. Dass das BVerfG Sachleistungen nicht ausschlie√üt, kritisieren wir.'\n```\n:::\n\n\n\n### Anfrage an die API\n\n\n\n::: {.cell hash='120-transformer_cache/html/openai1_0fb15a85e6e5ca76de8c1f91ab460462'}\n\n```{.python .cell-code}\nprompt = f\"Analysiere das Sentiment des folgenden Texts: \\n{text}\"\n\nresponse = openai.Completion.create(\n        prompt=prompt,\n        engine=\"davinci\",\n        max_tokens=100,\n        temperature=0.5,\n    )\n```\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n## Vertiefung\n\nDer Originalartikel von @vaswani_attention_2023 gibt einen guten Einblick in die Konzepte; der Anspruch ist auf mittlerem Niveau.\nVon den Hugging-Face-Machern gibt es ein Buch, das - ebenfalls auf mittlerem Niveau - einen Einblick in Transformer-Modelle im Hugging-Face-√ñkosystem gew√§hrt [@tunstall_natural_2022].\n@rothman_transformers_2022 scheint gute Freunde bei Google zu haben, wenn man sein Buch √ºber Transformer liest, jedenfalls sind die Modelle jener Firma in dem Buch gut gefeatured. \n@geron_hands-machine_2023 Standardwerk zu Scikit-Learn bietet auch einen Einblick in Attention-Konzepte (Kap. 16).\n√úbrigens ist das Buch (3. Auflage) jetzt auch in deutscher Sprache erh√§ltlich [@geron_praxiseinstieg_2023-1].\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}