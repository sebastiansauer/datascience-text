{
  "hash": "7a180fa97806ce8fa42f1816a92d3b5a",
  "result": {
    "engine": "knitr",
    "markdown": "---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n# Transformer\n\n\n\n## Vorab\n\n\n### Lernziele\n\n\n\n- Sie k√∂nnen die grundlegende Architektur eines Transformer-Modells beschreiben.\n- Sie k√∂nnen Transformer-Modelle mit der API von Hugging-Face berechnen.\n\n\n\n### Begleitliteratur \n\n\nDer [Blogpost von Jay Alammar](https://jalammar.github.io/illustrated-transformer/) gibt einen illustrierten √úberblick √ºber Transformer.\n\n\n\n\n\n\n### Ben√∂tigte Software (Python+R)\n\nWir ben√∂tigen Python, R sowie einige im Folgenden aufgef√ºhrte Python-Module.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd  # Data Frames\nimport os  # zB f√ºr Environment-Variablen\nfrom transformers import pipeline  # Hugging Face Modelle\nimport time  # Rechenzeit\nfrom sklearn.metrics import accuracy_score  # Modellg√ºte\n```\n:::\n\n\n\nF√ºr den Sch√ºleraustausch von R nach Python (und retour) nutzen wir das R-Paket `reticulate`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate)\n```\n:::\n\n\n\nAu√üerdem starte ich die \"richtige\" Python-Version, wo die ben√∂tigten Pakete (in der richtigen Version) installiert sind:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#use_virtualenv(\"r-tensorflow\")\n```\n:::\n\n\nCheck:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npy_available()\n## [1] TRUE\n```\n:::\n\n\nWelche Python-Version nutzt `reticulate` gerade?\n\n\n::: {.cell}\n\n```{.r .cell-code}\npy_config()\n## python:         /Users/sebastiansaueruser/.virtualenvs/r-tensorflow/bin/python\n## libpython:      /Users/sebastiansaueruser/.pyenv/versions/3.8.16/lib/libpython3.8.dylib\n## pythonhome:     /Users/sebastiansaueruser/.virtualenvs/r-tensorflow:/Users/sebastiansaueruser/.virtualenvs/r-tensorflow\n## version:        3.8.16 (default, Sep 15 2023, 17:53:02)  [Clang 14.0.3 (clang-1403.0.22.14.1)]\n## numpy:          /Users/sebastiansaueruser/.virtualenvs/r-tensorflow/lib/python3.8/site-packages/numpy\n## numpy_version:  1.24.3\n## \n## NOTE: Python version was forced by VIRTUAL_ENV\n```\n:::\n\n\n\n\n## √úberblick\n\nTransformer sind eine Architekturvariante neuronaler Netze. Sie stellen die Grundlage vieler aktueller gro√üer Sprachmodelle^[Man spricht von *Large Language Models*, LLM]; da sie einige Vorz√ºge gegen√ºber Vorg√§ngermodellen aufweisen, haben sie einen zentralen Platz f√ºr verschiedenen Aufgaben des NLP eingenommen.\n\nIm Jahr 2017 erschien ein Paper auf Arxive mit dem Titel \"Attention is all you need\", @vaswani_attention_2023^[Da die Autoren immer wieder Updates bei Arxive eingestellt haben, ist hier die aktuellste Version, V7 aus 2023, zitiert.].\nTransformer basieren auf einer bestimmten Art von \"Aufmerksamkeit\", genannt Selbst-Aufmerksamkeit (self-attention).\nNat√ºrlich ist damit eine bestimmte Architektur im neuronalen Netzwerk gemeint, kein kognitivpsychologiches Konstruktr; allerdings lehnt sich die Methode an Konzepte der Kognitionspsychologie vage an.\n\nSelf-Attention weist zwei gro√üe Verteile auf: Erstens erlaubt es parallele Verarbeitung, was viele Vorg√§ngermodelle nicht erlaubten. Zweitens kann es den Kontext eines Tokens, also den Text um ein bestimmtes Wort herum, deutlich besser \"im Blick\" (oder in der Aufmerksamkeit) behalten als viele Vorg√§ngermodelle.\n\nGerade f√ºr Daten mit sequenziellem Charakter, wie Text oder Sprache, sind Transformer-Modelle gut geeignet^[relativ zu anderen, bisherigen Modellen].\n\n\n## Grundkonzepte\n\n\n{{< video https://youtu.be/4Bdc55j80l8?si=t3ku0MxhWDD7z2TG >}}\n\n\n\n\n\n## Einf√ºhrung in Hugging Face ü§ó\n\nDieser Abschnitt orientiert sich an @tunstall_natural_2022.\nDie Syntax zu allen Kapiteln des [Buchs](https://transformersbook.com/) findet sich praktischerweise [in diesem Github-Repo](https://github.com/nlp-with-transformers/notebooks).\n\n\n\nBei ü§ó liegt der Schwerpunkt klar bei Python, nicht bei R.\nAllerdings erlaubt RStudio ein einfaches Wechseln zwischen R und Python:\nFunktionen und Daten aus Python k√∂nnen einfach mit dem `$`-Operator angesprochen werden.\n[In diesem Post](https://rpubs.com/eR_ic/transfoRmers) wirds das demonstriert.\n\n\nSchauen wir uns das einf√ºhrende Beispiel aus @tunstall_natural_2022. an.\n\n\n### Hugging Face mit R\n\nHier ein ein Text-Schnipsel,\ndessen Sentiment wir detektieren wollen:\n\n\n::: {.cell fenced='true'}\n\n```{.r .cell-code}\ntext <- (\"Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\")\n```\n:::\n\n\nUnd hier in der Python-Version:\n\n\n::: {.cell fenced='true'}\n\n```{.python .cell-code}\ntext_py = r.text\n```\n:::\n\n\n\nDann importieren wir die n√∂tigen Module:\n\n\n:::{.panel-tabset}\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n#import tensorflow\nfrom transformers import pipeline\n```\n:::\n\n\n\nNat√ºrlich m√ºssen Python-Module installiert sein, bevor man sie nutzen kann, genau so wie R-Pakete.\n\n\n### R\n\nMan kann die die Python-Module auch √ºber R starten:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntransformers <- reticulate::import(\"transformers\")\n```\n:::\n\n\n:::\n\n\n\n### Einfache Pipeline\n\n\n\n\n### Python\n\nWir bereiten das Modell vor; im Default wird \n`distilbert-base-uncased-finetuned-sst-2-english` verwendet.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nclassifier = pipeline(\"text-classification\")\n## No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n## Using a pipeline without specifying a model name and revision in production is not recommended.\n## All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n## \n## All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n## If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n```\n:::\n\n\n\n\n\n## Germeval Out-of-the-Box mit Hugging Face\n\nZuerst importieren wir die Daten.\n\n:::{.panel-tabset}\n\n\n### Ein bisschen R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(germeval_train, package = \"pradadata\")\ntext <- germeval_train$text[1:2]\ntext[1:2]\n## [1] \"@corinnamilborn Liebe Corinna, wir w√ºrden dich gerne als Moderatorin f√ºr uns gewinnen! W√§rst du begeisterbar?\"                                 \n## [2] \"@Martin28a Sie haben ja auch Recht. Unser Tweet war etwas missverst√§ndlich. Dass das BVerfG Sachleistungen nicht ausschlie√üt, kritisieren wir.\"\n```\n:::\n\n\n\n### Ein bisschen Python Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\ngermeval_train_py = r.text\n```\n:::\n\n\n:::\n\n\n### Standard-Pipeline\n\n\nHugging Face bietet eine sehr einfache Oberfl√§che: Im einfachsten Fall kann man mit `pipeline()` das Ziel der Analyse (wie Textklassifikation) oder das zu verwendende Modell angegeben. Dann wird das entsprechende Modell heruntergeladen und vorbereitet.\nMit `classifier()` wird ein Datensatz dann entsprechend klassifiziert.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nclassifier = pipeline(\"text-classification\")  \n## No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n## Using a pipeline without specifying a model name and revision in production is not recommended.\n## All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n## \n## All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n## If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\noutputs2 = classifier(germeval_train_py)\noutputs2\n## [{'label': 'NEGATIVE', 'score': 0.9950070381164551}, {'label': 'NEGATIVE', 'score': 0.9954568147659302}]\n```\n:::\n\n\n\nTja, vielleicht sollten wir ein Modell verwenden, das die deutsche Sprache versteht?\n\n\n### Man spricht Deutsh\n\nAuf Hugging Face gibt es eine Menge von Modellen. Welches nehm ich nur? [DISTILBERT](https://huggingface.co/distilbert-base-german-cased) oder [BERT](https://huggingface.co/bert-base-uncased)-Varianten d√ºrfte kein schlechter Start sein.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n#classifier = pipeline(\"text-classification\", model=\"distilbert-base-german-cased\")\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nclassifier = pipeline(\n  \"text-classification\", model=\"oliverguhr/german-sentiment-bert\")\n## All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n## \n## All the weights of TFBertForSequenceClassification were initialized from the PyTorch model.\n## If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\noutputs3 = classifier(germeval_train_py)\ndf = pd.DataFrame(outputs3)    \ndf.head()\n##       label     score\n## 0   neutral  0.987253\n## 1  negative  0.918047\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_r <- py$pd\nhead(df_r)\n```\n:::\n\n\n\n## Fine-Tuning\n\n### Grundlagen\n\n:::{#def-finetuning}\n### Fine-Tuning\nUnter Fine-Tuning^[\"Weitertraininieren\" ist ein Versuch, den Term \"Fine-Tuning\" auf Deutsch zu √ºbersetzen.] versteht man das Anpassen der Gewichte eines gro√üen (neuronalen) Sprachmodells (Large Language Models (LLM); Foundational Model) an einen spezifischen Datensatz. $\\square$\n:::\n\n\nFine-Tuning ist eine Art \"Trick\", wie man die Power eines gro√üen Sprachmodells an die Spezifika eines bestimmten (Ihres!) Datensatzes anzupassen. Insofern k√∂nnte man sagen, dass man mit Fine-Tuning die Vorteile eines LLM nutzen kann, auch wenn man einen kleinen Datensatz hat.\n\n\n:::callout-tip\nNutzen Sie Fine-Tuning, wo immer m√∂glich. Sie sparen nicht nur Energie und Rechenzeit und verbessern damit Ihren √∂kologischen Fu√üabdruck (als Nutzer von LLM haben Sie (wir!) ganz sch√∂n viel Energie verbraucht). Sie verbessern mit etwas Gl√ºck auch die pr√§diktive Leistung Ihres Modells. \n:::\n\n\n:::{#def-zeroshotlearning}\n### Zero-Shot-Learning\nNutzt man ein LLM ohne Fine-Tuning, etwa indem man das Modell mittels eines Prompts zu einer Sentiment-Klassifikation auffordert, so spricht man von Zero-Shot-Learning. In diesem Fall lernt das Modell ohne (spezifisches) Train-Sample. $\\square$\n:::\n\n\n### Fine-Tuning vorgekocht\n\nNat√ºrlich kann man ein Modell selber an einen spezifischen Datensatz fitten. In dem Fall werden anstelle von Zufallsgewichten im neuronalen Netz die Gewichte des Modells als Ausgangspunkt genommen.\nAllerdings kann es auch sein, dass es auf einem Hub wie Hugging Face schon vortrainierte (\"gefinetunte\"?) Modelle gibt, so dass man sich die Arbeit des selber Fine-Tunings sparen kann.\n\n\n:::{#exm-hatespeech}\nIn [dieser Sammlung](https://huggingface.co/collections/sebastiansauer/hate-speech-detection-655e66e27b44c113b821423d) finden sich LLMs, die an deutschen Hate-Speech-Datasets weitertrainiert wurden. $\\square$\n:::\n\n\n\nWir holen uns ein an deutschem Hate-Speech-Daten vortrainiertes Modell von Hugging Face:\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\npipe_bert_germeval = pipeline(\"text-classification\", model=\"deepset/bert-base-german-cased-hatespeech-GermEval18Coarse\")\n## All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n## \n## All the weights of TFBertForSequenceClassification were initialized from the PyTorch model.\n## If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n```\n:::\n\n\n:::callout-note\nWenn man ein Modell zum ersten Mal anfragt, wird das Modell heruntergeladen; das kann ggf. etwas dauern (und braucht ewtas Speicherplatz). $\\square$\n:::\n\n\nHier ist ein Beispiel-Satz:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntext = '@Martin28a Sie haben ja auch Recht. Unser Tweet war etwas missverst√§ndlich. Dass das BVerfG Sachleistungen nicht ausschlie√üt, kritisieren wir.'\n```\n:::\n\n\n\nUnd dann lassen wir uns die Vorhersage des Modells ausgeben:\n\n\n::: {.cell}\n\n```{.python .cell-code}\noutputs = pipe_bert_germeval(text)\npd.DataFrame(outputs)\n##    label     score\n## 0  OTHER  0.994407\n```\n:::\n\n\n\n:::{.callout-attention}\nDieses Modell wurde explizit am Datensatz [germeval2018](https://github.com/uds-lsv/GermEval-2018-Data) (Coarse Classification) trainiert. Eine hohe Klassifikationsg√ºte ist daher vorprogrammiert.\nBliebe noch zu pr√ºfen, ob auch das Test-Sample zum Training verwendet wurde. $\\square$\n:::\n\n\n### Fallbeispiel \n\n\n\nHier ist unser Germeval-Datensatz:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncsv_file_path_test = 'https://github.com/sebastiansauer/pradadata/raw/master/data-raw/germeval_test.csv'\n\ngermeval_test = pd.read_csv(csv_file_path_test)\n```\n:::\n\n\n\nNachdem der Datensatz als DataFrame vorliegt, konvertieren wir ihn noch zu einer Liste:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntweets = germeval_test[\"text\"].tolist()\n```\n:::\n\n\nZu Testzwecken ist es oft sinnvoll, sich einen \"Toy-Datensatz\" zu erstellen:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntweets_head = germeval_test[\"text\"].head(2).tolist()\n```\n:::\n\n\n\n\nUnd dann kommt das Vorhersagen.\nZuerst, zum Testen, mit dem kleinen Spielzeug-Datensatz:\n\n\n::: {.cell}\n\n```{.python .cell-code}\noutputs = pipe_bert_germeval(tweets_head)\npd.DataFrame(outputs)\n##    label     score\n## 0  OTHER  0.971582\n## 1  OTHER  0.571138\n```\n:::\n\n\n\nSchein zu klappen. \nDann wagen wir uns also an den ganzen GermEval-(Test-)Datensatz:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nstart_time = time.time()\n\noutputs = pipe_bert_germeval(tweets)\npreds = pd.DataFrame(outputs)\n\nend_time = time.time()\nend_time - start_time\n```\n:::\n\n\n`1250.577404975891`\n\nDa es einige Zeit gedauert hat, speichern wir uns die Predictions als CSV-Datei:\n\n\n::: {.cell}\n\n```{.python .cell-code}\npreds.to_csv(\"data/pipe_bert_germeval_preds.csv\")\n```\n:::\n\n\nUnd wenn man sie gespeichert hat, kann man sie wieder importieren:\n\n\n::: {.cell}\n\n```{.python .cell-code}\npreds = pd.read_csv(\"data/pipe_bert_germeval_preds.csv\")\n```\n:::\n\n\n\nZ√§hlen wir, wie oft jede Klasse vorhergesagt wurde:\n\n\n::: {.cell}\n\n```{.python .cell-code}\npreds[\"label\"].value_counts()\n## label\n## OTHER      2442\n## OFFENSE    1090\n## Name: count, dtype: int64\n```\n:::\n\n\n\nWir konvertieren die Label-Spalte der Vorhersagen in eine Python-Liste, da die Accuracy-Funktion dies verlangt:\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\npreds_list = preds[\"label\"].to_list()\n```\n:::\n\n\n\n\nAls N√§chstes bewerten wir die Modellg√ºte^[Performance, Scoring] im Test-Set.\n\n\n\n\nHier ist die *Liste* der wahren Werte (die sich in der Spalte `c1` finden lassen):\n\n\n::: {.cell}\n\n```{.python .cell-code}\ny = germeval_test[\"c1\"].values.tolist()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\naccuracy = accuracy_score(y, preds_list)\nprint(\"Accuracy:\", accuracy)\n## Accuracy: 0.7814269535673839\n```\n:::\n\n\n\n\n:::{.callout-attention}\n### Overfitting?\nEs ist nicht klar, ob unser Modell den GermEval-Test-Datensatz als Trainingsinput gesehen hat. \nIn dem Fall w√§re nat√ºrlich die Modellg√ºte von massivem Overfitting betroffen, also \"too good to be true\";\nk√ºnftige Vorhersagen m√ºssten mal also mit deutlich geringer G√ºte erwarten. $\\square$\n:::\n\n\n## Vertiefung\n\nDer Originalartikel von @vaswani_attention_2023 gibt einen guten Einblick in die Konzepte; der Anspruch ist auf mittlerem Niveau.\nVon den Hugging-Face-Machern gibt es ein Buch, das - ebenfalls auf Einstiegs- bis mittlerem Niveau - einen Einblick in Transformer-Modelle im Hugging-Face-√ñkosystem gew√§hrt [@tunstall_natural_2022].\n@rothman_transformers_2022 scheint gute Freunde bei Google zu haben, wenn man sein Buch √ºber Transformer liest, jedenfalls sind die Modelle jener Firma in dem Buch gut gefeatured. \n@geron_hands-machine_2023 Standardwerk zu Scikit-Learn bietet auch einen Einblick in Attention-Konzepte (Kap. 16).\n√úbrigens ist das Buch (3. Auflage) jetzt auch in deutscher Sprache erh√§ltlich [@geron_praxiseinstieg_2023-1].\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}