{
  "hash": "9fdacc4c8743b1e3a0769f38fd162a11",
  "result": {
    "engine": "knitr",
    "markdown": "\n# Textmining 1\n\n\n\n![Text als Datenbasis pr√§diktiver Modelle](img/text-mining-1476780_640.png){width=10%}\nBild von <a href=\"https://pixabay.com/de/users/mcmurryjulie-2375405/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1476780\">mcmurryjulie</a> auf <a href=\"https://pixabay.com/de//?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1476780\">Pixabay</a>\n\n\n\n## Vorab\n\n\n### Lernziele\n\n\n- Die vorgestellten Techniken des Textminings mit R anwenden k√∂nnen\n\n\n\n### Begleitliteratur\n\nLesen Sie in @smltar Kap. 2 zur Vorbereitung.\n\n\n\n\n\n\n### Ben√∂tigte R-Pakete\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-1_0a9ab126099cdf2496d800006661a382'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tokenizers)  # Tokenisieren\nlibrary(tidymodels)  # Rezepte f√ºr Textverarbeitung\nlibrary(tidyverse)\nlibrary(tidytext)  # Textanalyse-Tools\nlibrary(hcandersenr)  # Textdaten: M√§rchen von H.C. Andersen\nlibrary(SnowballC)  # Stemming\nlibrary(lsa)  # Stopw√∂rter\nlibrary(easystats)  # Komfort f√ºr deskriptive Statistiken, wie `describe_distribution`\nlibrary(textclean)  # Emojis ersetzen\nlibrary(wordcloud)  # un√ºbersichtlich, aber manche m√∂gen es\n```\n:::\n\n\n\n\n## Einfache Methoden des Textminings\n\n\n:::{#def-nlp}\n### Natural Language Processing\nDie Analyse von Texten (nat√ºrlicher Sprache) mit Hilfe von Methoden des Maschinellen Lernens bezeichnet man (auch) als *Natural Language Processing* (NLP). $\\square$\n:::\n\n\n\n### Tokenisierung\n\nErarbeiten Sie dieses Kapitel: @smltar, [Kap. 2](https://smltar.com/tokenization.html#tokenization)\n\nWie viele Zeilen hat das M√§rchen \"The Fir tree\" (in der englischen Fassung?)\n\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-2_34852d6b1f7d7a9b8e37f7e928d1c3d9'}\n\n```{.r .cell-code}\nhcandersen_en %>% \n  filter(book == \"The fir tree\") %>% \n  nrow()\n## [1] 253\n```\n:::\n\n\n\n### Stopw√∂rter entfernen\n\n\nErarbeiten Sie dieses Kapitel: s. @smltar, [Kap. 3](https://smltar.com/stopwords.html#stopwords)\n\n\n\nEine alternative Quelle von Stopw√∂rtern - in verschiedenen Sprachen - \nbiwetet das Paket `quanteda`:\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-3_6029d3a37a4a1b0737bf2b8b0c723065'}\n\n```{.r .cell-code}\nstop2 <-\n  tibble(word = quanteda::stopwords(\"german\"))\n\nhead(stop2)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"word\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"aber\"},{\"1\":\"alle\"},{\"1\":\"allem\"},{\"1\":\"allen\"},{\"1\":\"aller\"},{\"1\":\"alles\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nEs bestehst (in der deutschen Version) aus 231 W√∂rtern.\n\n\n\n\n### W√∂rter z√§hlen {#sec-woerterzaehlen}\n\nIst der Text tokenisiert, kann man einfach mit \"Bordmitteln\" die W√∂rter z√§hlen (Bordmittel aus dem Tidyverse, in diesem Fall).\n\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-4_a67059e6280e843c309b8812ac65ed07'}\n\n```{.r .cell-code}\nhc_andersen_count <- \n  hcandersen_de %>% \n  filter(book == \"Das Feuerzeug\") %>% \n  unnest_tokens(output = word, input = text) %>% \n  anti_join(stop2) %>% \n  count(word, sort = TRUE) \n## Joining with `by = join_by(word)`\n\nhc_andersen_count %>% \n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"word\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"soldat\",\"2\":\"35\"},{\"1\":\"sagte\",\"2\":\"28\"},{\"1\":\"hund\",\"2\":\"23\"},{\"1\":\"prinzessin\",\"2\":\"17\"},{\"1\":\"hexe\",\"2\":\"16\"},{\"1\":\"feuerzeug\",\"2\":\"14\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n\nZur Visualisierung eignen sich Balkendiagramme, s. @fig-hcandersen-count.\n\n\n\n::: {.cell hash='030-textmining1_cache/html/fig-hc-andersen-count_9e05f3f47b5e58d965bebebd0a4813aa'}\n\n```{.r .cell-code}\nhc_andersen_count %>% \n  slice_max(order_by = n, n = 10) %>% \n  mutate(word = factor(word)) %>% \n  ggplot() +\n  aes(y = reorder(word, n), x = n) +\n  geom_col()\n  \n```\n\n::: {.cell-output-display}\n![Die h√§ufigsten W√∂rter in H.C. Anderssens Feuerzeug](030-textmining1_files/figure-html/fig-hc-andersen-count-1.png){#fig-hc-andersen-count width=672}\n:::\n:::\n\n\n\nDabei macht es Sinn, aus `word` einen Faktor zu machen,\ndenn Faktorstufen kann man sortieren,\nzumindest ist das die einfachste L√∂sung in `ggplot2` (wenn auch nicht super komfortabel).\n\n\nEine (beliebte?) Methode, um Worth√§ufigkeiten in Corpora darzustellen, \nsind *Wortwolken*, s. @fig-wordcloud1.\nEs sei hinzugef√ºgt, dass solche Wortwolken nicht gerade optimale\nperzeptorische Qualit√§ten aufweisen.\n\n\n::: {.cell hash='030-textmining1_cache/html/fig-wordcloud1_6ca8bf25f87c173b76bd6064b6579940'}\n\n```{.r .cell-code}\nwordcloud(words = hc_andersen_count$word,\n          freq = hc_andersen_count$n,\n          max.words = 50,\n          rot.per = 0.35,\n          colors = brewer.pal(8, \"Dark2\"))\n```\n\n::: {.cell-output-display}\n![Eine Wortwolke zu den h√§ufigsten W√∂rtern in H.C. Andersens Feuerzeug](030-textmining1_files/figure-html/fig-wordcloud1-1.png){#fig-wordcloud1 width=672}\n:::\n:::\n\n\n\n### tf-idf\n\n#### Grundlagen\n\nWas sind \"h√§ufige\" W√∂rter? W√∂rter wie \"und\" oder \"der\" sind sehr h√§ufig, aber sie nicht spezifisch f√ºr bestimmte Texte. Sie sind sozusagen \"Allerweltsw√∂rter\".\nMan k√∂nnte argumentieren, was wir suchen, sind nicht einfach h√§ufige W√∂rter, sondern W√∂rter, die *relevant* sind.\nUnter \"relevant\" k√∂nnte man verstehen \"h√§ufig aber spezifisch\".\n\n\nDa kommt `tf-idf` ins Spiel (*term frequency‚Äìinverse document frequency*). \n`tf-idf` ist eine M√∂glichkeit, W√∂rter die *h√§ufig* und gleichzeitig *spezifisch* f√ºr einen Text sind, aufzufinden.\n\nBetrachten wir als Beispiel @tbl-shakespeare ([Quelle](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)).\nDas Wort \"Romeo\" ist hoch spezifisch in dem Sinne, dass sein `idf` hoch ist (relativ zu den anderen W√∂rtern).\nIm Gegensatz dazu ist das Wort \"good\" nicht spezifisch f√ºr ein bestimmtes St√ºck von Shakespeare - der IDF-Wert ist sehr klein.\n\n\n::: {#tbl-shakespeare .cell tbl-cap='Einige W√∂rter aus Shakespeares Werken' hash='030-textmining1_cache/html/tbl-shakespeare_076d8da59765f8130b0d3869083cbd09'}\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Word\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"tf\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"idf\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tf_idf\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Romeo\",\"2\":\"1\",\"3\":\"1.570\",\"4\":\"1.570\"},{\"1\":\"salad\",\"2\":\"2\",\"3\":\"1.270\",\"4\":\"2.540\"},{\"1\":\"Falstaff\",\"2\":\"4\",\"3\":\"0.967\",\"4\":\"3.868\"},{\"1\":\"forest\",\"2\":\"12\",\"3\":\"0.489\",\"4\":\"5.868\"},{\"1\":\"battle\",\"2\":\"21\",\"3\":\"0.246\",\"4\":\"5.166\"},{\"1\":\"wit\",\"2\":\"34\",\"3\":\"0.037\",\"4\":\"1.258\"},{\"1\":\"fool\",\"2\":\"36\",\"3\":\"0.012\",\"4\":\"0.432\"},{\"1\":\"good\",\"2\":\"37\",\"3\":\"0.000\",\"4\":\"0.000\"},{\"1\":\"sweet\",\"2\":\"37\",\"3\":\"0.000\",\"4\":\"0.000\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n:::{#def-tf}\n### Text Frequency (Worth√§ufigkeit)\nDie Worth√§ufigkeit (Vorkommensh√§ufigkeit) eines Wortes (oder Terms, Tokens) $h_w$ in einem Dokument $d$ (oder Corpus) ist definiert als die H√§ufigkeit des Wortes geteilt durch die Anzahl aller W√∂rter des Dokuments, $h_d$:\n\n$tf(w,d) = \\frac{h_w}{h_d} \\square$\n:::\n\nManchmal wird auch eine logarithmierte Version verwendet.\n\n\n:::{#def-idf}\n### Inverse Document Frequency (Inverse Dokumenth√§ufigkeit)\nDie IDF eines Wortes h√§ngt erstens von der Gesamtzahl der *Dokumente* (nicht W√∂rter) im Corpus, $N$, ab.\nZweitens h√§ngt IDF davon ab, wie viele Dokumente das Wort enthalten, $N_w$. \n\n$idf(w,c) = log \\frac{N}{N_w} \\square$\n:::\n\n\n:::{#def-tfidf}\n### TF-IDF\nTF-IDF ist ein Ma√ü zur Beurteilung der *Relevanz* von W√∂rtern eines Corpus.\n\n$\\text{tf-idf} = tf \\cdot idf \\qquad \\square$\n:::\n\n\n\n#### Beispiel zur tf-idf\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-8_712667ec8d073389070f9db0e7a54c53'}\n\n```{.r .cell-code}\n#library(hcandersenr)\ndata(hcandersen_de)\n\nhca_count <- \nhcandersen_de |> \n  unnest_tokens(word, text) |> \n  count(book, word, sort = TRUE)  |> # Z√§hle W√∂rter pro Buch\n  ungroup()  # um gleich nur nach B√ºcher zu gruppieren\n\nhead(hca_count)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"book\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"word\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"n\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Die Eisjungfrau\",\"2\":\"und\",\"3\":\"644\"},{\"1\":\"Die Schneek√∂nigin\",\"2\":\"und\",\"3\":\"586\"},{\"1\":\"Moork√∂nigs Tochter\",\"2\":\"und\",\"3\":\"581\"},{\"1\":\"Eine Geschichte aus den Sandd√ºnen\",\"2\":\"und\",\"3\":\"566\"},{\"1\":\"Die Eisjungfrau\",\"2\":\"die\",\"3\":\"541\"},{\"1\":\"Moork√∂nigs Tochter\",\"2\":\"die\",\"3\":\"502\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nDie Gesamtzahl der W√∂rter pro Buch:\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-9_91dddeecf1887164874af8dbeae4cf23'}\n\n```{.r .cell-code}\nwords_total <-\n  hca_count |> \n  group_by(book) |> \n  summarise(total = sum(n))\n\nwords_total |> \n  head()  # die ersten paar von 150 B√ºchern\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"book\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"total\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"\\\"Tanze, tanze, P√ºppchen mein!\\\"\",\"2\":\"352\"},{\"1\":\"Alles am rechten Platz\",\"2\":\"2947\"},{\"1\":\"Am √§u√üersten Meer\",\"2\":\"723\"},{\"1\":\"Anne Lisbeth\",\"2\":\"4036\"},{\"1\":\"Aufgeschoben ist nicht aufgehoben\",\"2\":\"1016\"},{\"1\":\"Das ABC Buch\",\"2\":\"1135\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nDann f√ºgen wir die Spalte mit der Gesamt-Wortzahl des jeweiligen Buches zur Tabelle `hca_count` hinzu:\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-10_650465f95777e685b1c35e71e49fbbf4'}\n\n```{.r .cell-code}\nhca_count <-\n  hca_count |> \n  left_join(words_total)\n## Joining with `by = join_by(book)`\n\nhead(hca_count)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"book\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"word\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"n\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"total\"],\"name\":[4],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Die Eisjungfrau\",\"2\":\"und\",\"3\":\"644\",\"4\":\"17240\"},{\"1\":\"Die Schneek√∂nigin\",\"2\":\"und\",\"3\":\"586\",\"4\":\"11163\"},{\"1\":\"Moork√∂nigs Tochter\",\"2\":\"und\",\"3\":\"581\",\"4\":\"13059\"},{\"1\":\"Eine Geschichte aus den Sandd√ºnen\",\"2\":\"und\",\"3\":\"566\",\"4\":\"12635\"},{\"1\":\"Die Eisjungfrau\",\"2\":\"die\",\"3\":\"541\",\"4\":\"17240\"},{\"1\":\"Moork√∂nigs Tochter\",\"2\":\"die\",\"3\":\"502\",\"4\":\"13059\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-11_b8663b73ecd06d7cee186f762c20631e'}\n\n```{.r .cell-code}\nhca_count <-\n  hca_count |> \n  bind_tf_idf(term = word, document = book, n = n)\n\nhca_count |> \n  arrange(-tf_idf) |> \n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"book\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"word\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"n\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"total\"],\"name\":[4],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"tf\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"idf\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tf_idf\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Der Halskragen\",\"2\":\"halskragen\",\"3\":\"20\",\"4\":\"801\",\"5\":\"0.02496879\",\"6\":\"5.010635\",\"7\":\"0.12510950\"},{\"1\":\"Frag die Gr√ºnwarenfrau!\",\"2\":\"mohrr√ºbe\",\"3\":\"5\",\"4\":\"220\",\"5\":\"0.02272727\",\"6\":\"5.010635\",\"7\":\"0.11387807\"},{\"1\":\"V√§n√∂ und Gl√§n√∂\",\"2\":\"gl√§n√∂\",\"3\":\"14\",\"4\":\"630\",\"5\":\"0.02222222\",\"6\":\"5.010635\",\"7\":\"0.11134745\"},{\"1\":\"Das Liebespaar (Kreisel und Ball)\",\"2\":\"kreisel\",\"3\":\"16\",\"4\":\"782\",\"5\":\"0.02046036\",\"6\":\"5.010635\",\"7\":\"0.10251939\"},{\"1\":\"Der kleine Klaus und der gro√üe Klaus\",\"2\":\"klaus\",\"3\":\"101\",\"4\":\"4290\",\"5\":\"0.02354312\",\"6\":\"4.317488\",\"7\":\"0.10164716\"},{\"1\":\"Zwei Jungfern\",\"2\":\"jungfer\",\"3\":\"15\",\"4\":\"574\",\"5\":\"0.02613240\",\"6\":\"3.218876\",\"7\":\"0.08411696\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n\n### Stemming (Wortstamm finden)\n\nErarbeiten Sie dieses Kapitel: @smltar, [Kap. 4](https://smltar.com/stemming.html#stemming)\n\n\nVertiefende Hinweise zum *UpSet plot* finden Sie [hier](https://ieeexplore.ieee.org/document/6876017), @lex_upset_2014.\n\n\nF√ºr welche Sprachen gibt es Stemming im Paket `SnowballC`?\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-12_f027acc06d57c507fcd58d909da2de64'}\n\n```{.r .cell-code}\nlibrary(SnowballC)\ngetStemLanguages()\n##  [1] \"arabic\"     \"basque\"     \"catalan\"    \"danish\"     \"dutch\"     \n##  [6] \"english\"    \"finnish\"    \"french\"     \"german\"     \"greek\"     \n## [11] \"hindi\"      \"hungarian\"  \"indonesian\" \"irish\"      \"italian\"   \n## [16] \"lithuanian\" \"nepali\"     \"norwegian\"  \"porter\"     \"portuguese\"\n## [21] \"romanian\"   \"russian\"    \"spanish\"    \"swedish\"    \"tamil\"     \n## [26] \"turkish\"\n```\n:::\n\n\n\nEinfacher Test: Suchen wir den Wordstamm f√ºr das Wort \"wissensdurstigen\", wie in \"die wissensdurstigen Studentis l√∂cherten dis armi Professi\"^[[Gender-i](https://gender-i.de/#mit-bestimmtem-artikel)].\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-13_a4932a52ba29627ba939843f3397f733'}\n\n```{.r .cell-code}\nwordStem(\"wissensdurstigen\", language = \"german\")\n## [1] \"wissensdurst\"\n```\n:::\n\n\n\nWerfen Sie mal einen Blick in das Handbuch von [SnowballC](https://cran.r-project.org/web/packages/SnowballC/SnowballC.pdf).\n\n\n\n### Fallstudie AfD-Parteiprogramm\n\n\n\nDaten einlesen:\n\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-14_d2976d465546ea45170af7694936ce11'}\n\n```{.r .cell-code}\nd_link <- \"https://raw.githubusercontent.com/sebastiansauer/pradadata/master/data-raw/afd_2022.csv\"\nafd <- read_csv(d_link, show_col_types = FALSE)\n```\n:::\n\n\nWie viele Seiten hat das Dokument?\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-15_833478e1f5fe5bd11a952045de634366'}\n\n```{.r .cell-code}\nnrow(afd)\n## [1] 190\n```\n:::\n\n\nUnd wie viele W√∂rter?\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-16_430d59b81c4b20eeab63989079c44e94'}\n\n```{.r .cell-code}\nstr_count(afd$text, pattern = \"\\\\w\") %>% sum(na.rm = TRUE)\n## [1] 179375\n```\n:::\n\n\n\n\nAus breit mach lang, oder: wir tokenisieren (nach W√∂rtern): \n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-17_556be62bfeda7ffacad8e1e71ffaaab7'}\n\n```{.r .cell-code}\nafd %>% \n  unnest_tokens(output = token, input = text) %>% \n  filter(str_detect(token, \"[a-z]\")) -> afd_long\n```\n:::\n\n\n\nStopw√∂rter entfernen:\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-18_f7220ca180e7cac282aabe6ccc1fd43a'}\n\n```{.r .cell-code}\ndata(stopwords_de, package = \"lsa\")\n\nstopwords_de <- tibble(word = stopwords_de)\n\n# F√ºr das Joinen werden gleiche Spaltennamen ben√∂tigt:\nstopwords_de <- stopwords_de %>% \n  rename(token = word)  \n\nafd_long %>% \n  anti_join(stopwords_de) -> afd_no_stop\n## Joining with `by = join_by(token)`\n```\n:::\n\n\n\nW√∂rter z√§hlen:\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-19_2df2e8760cfcf32179d26d552c2c0305'}\n\n```{.r .cell-code}\nafd_no_stop %>% \n  count(token, sort = TRUE) -> afd_count\n\nhead(afd_count)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"token\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"afd\",\"2\":\"174\"},{\"1\":\"deutschland\",\"2\":\"113\"},{\"1\":\"wollen\",\"2\":\"66\"},{\"1\":\"euro\",\"2\":\"60\"},{\"1\":\"b√ºrger\",\"2\":\"57\"},{\"1\":\"eu\",\"2\":\"54\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\nW√∂rter trunkieren:\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-20_48b49439eeb40efe3123e8ca2da6e3cc'}\n\n```{.r .cell-code}\nafd_no_stop %>% \n  mutate(token_stem = wordStem(token, language = \"de\")) %>% \n  count(token_stem, sort = TRUE) -> afd_count_stemmed\n\nhead(afd_no_stop)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"page\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"token\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"1\",\"2\":\"programm\"},{\"1\":\"1\",\"2\":\"deutschland\"},{\"1\":\"1\",\"2\":\"grundsatzprogramm\"},{\"1\":\"1\",\"2\":\"alternative\"},{\"1\":\"1\",\"2\":\"deutschland\"},{\"1\":\"2\",\"2\":\"inhaltsverzeichnis\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n\n### Stringverarbeitung\n\n\n\nErarbeiten Sie dieses Kapitel: @r4ds, [Kap. 14](https://r4ds.had.co.nz/strings.html)\n\n\n\n#### Regul√§rausdr√ºcke {#regex}\n\nDas `\"[a-z]\"` in der Syntax oben steht f√ºr \"alle Buchstaben von a-z\". D\niese flexible Art von \"String-Verarbeitung mit Jokern\" nennt man *Regul√§rausdr√ºcke* (regular expressions; regex). \nEs gibt eine ganze Reihe von diesen Regul√§rausdr√ºcken, die die Verarbeitung von Texten erleichert. \nMit dem Paket `stringr` geht das - mit etwas √úbung - gut von der Hand. \nNehmen wir als Beispiel den Text eines Tweets:\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-21_5fd9b269c0f011ba2d22e2779733c77e'}\n\n```{.r .cell-code}\nstring <- \"Correlation of unemployment and #AfD votes at #btw17: ***r = 0.18***\\n\\nhttps://t.co/YHyqTguVWx\"  \n```\n:::\n\n\nM√∂chte man Ziffern identifizieren, so hilft der Reul√§rausdruck `[:digit:]`:\n\n\"Gibt es mindestens eine Ziffer in dem String?\"\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-22_b3189829c06f8173f1254ff965a0fe1d'}\n\n```{.r .cell-code}\nstr_detect(string, \"[:digit:]\")\n## [1] TRUE\n```\n:::\n\n\n\"Finde die Position der ersten Ziffer! Welche Ziffer ist es?\"\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-23_a12e5de54ee2e668c197a89bc9ff5eb6'}\n\n```{.r .cell-code}\nstr_locate(string, \"[:digit:]\")\n##      start end\n## [1,]    51  51\nstr_extract(string, \"[:digit:]\")\n## [1] \"1\"\n```\n:::\n\n\n\"Finde alle Ziffern!\"\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-24_42d5c896e21108a9d41b5fe775580825'}\n\n```{.r .cell-code}\nstr_extract_all(string, \"[:digit:]\")\n## [[1]]\n## [1] \"1\" \"7\" \"0\" \"1\" \"8\"\n```\n:::\n\n\n\n\"Finde alle Stellen an denen genau 2 Ziffern hintereinander folgen!\"\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-25_ce83ce51236d2cb55032a6b61892fcd2'}\n\n```{.r .cell-code}\nstr_extract_all(string, \"[:digit:]{2}\")\n## [[1]]\n## [1] \"17\" \"18\"\n```\n:::\n\n\nDer Quantit√§tsoperator `{n}` findet alle Stellen, in der der der gesuchte Ausdruck genau $n$ mal auftaucht.\n\n\n\"Zeig die Hashtags!\"\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-26_806242bc9ff9b406c64daac662b4f746'}\n\n```{.r .cell-code}\nstr_extract_all(string, \"#[:alnum:]+\")\n## [[1]]\n## [1] \"#AfD\"   \"#btw17\"\n```\n:::\n\n\nDer Operator `[:alnum:]` steht f√ºr \"alphanumerischer Charakter\" - also eine Ziffer oder ein Buchstabe; synonym h√§tte man auch `\\\\w` schreiben k√∂nnen (w wie word). Warum werden zwei Backslashes gebraucht? Mit `\\\\w` wird signalisiert, dass nicht der Buchstabe *w*, sondern etwas Besonderes, eben der Regex-Operator `\\w` gesucht wird. \n\n\"Zeig die URLs!\"\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-27_9d35cde10d42923e9e434b125b273d51'}\n\n```{.r .cell-code}\nstr_extract_all(string, \"https?://[:graph:]+\")\n## [[1]]\n## [1] \"https://t.co/YHyqTguVWx\"\n```\n:::\n\n\nDas Fragezeichen `?` ist eine Quantit√§tsoperator, der einen Treffer liefert, wenn das vorherige Zeichen (hier *s*) null oder einmal gefunden wird. `[:graph:]` ist die Summe von `[:alpha:]` (Buchstaben, gro√ü und klein), `[:digit:]` (Ziffern) und `[:punct:]` (Satzzeichen u.√§.).\n\n\"Z√§hle die W√∂rter im String!\"\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-28_18d433de408063fa601297ba5cb77946'}\n\n```{.r .cell-code}\nstr_count(string, boundary(\"word\"))\n## [1] 13\n```\n:::\n\n\n\n\"Liefere nur Buchstaben*folgen* zur√ºck, l√∂sche alles √ºbrige\"\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-29_58b5604b8c3d76a3427413107c8b43f7'}\n\n```{.r .cell-code}\nstr_extract_all(string, \"[:alpha:]+\")\n## [[1]]\n##  [1] \"Correlation\"  \"of\"           \"unemployment\" \"and\"          \"AfD\"         \n##  [6] \"votes\"        \"at\"           \"btw\"          \"r\"            \"https\"       \n## [11] \"t\"            \"co\"           \"YHyqTguVWx\"\n```\n:::\n\n\nDer Quantit√§tsoperator `+` liefert alle Stellen zur√ºck, in denen der gesuchte Ausdruck *einmal oder h√§ufiger*  vorkommt. Die Ergebnisse werden als Vektor von W√∂rtern zur√ºckgegeben. Ein anderer Quantit√§tsoperator ist `*`, der f√ºr 0 oder mehr Treffer steht. M√∂chte man einen Vektor, der aus Stringen-Elementen besteht zu einem Strring zusammenf√ºngen, hilft `paste(string)` oder `str_c(string, collapse = \" \")`.\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-30_be3e61db22f4b6683d13f56735899bd3'}\n\n```{.r .cell-code}\nstr_replace_all(string, \"[^[:alpha:]+]\", \"\")\n## [1] \"CorrelationofunemploymentandAfDvotesatbtwrhttpstcoYHyqTguVWx\"\n```\n:::\n\n\nMit dem Negationsoperator `[^x]` wird der Regul√§rausrck `x` negiert; die Syntax oben hei√üt also \"ersetze in `string` alles au√üer Buchstaben durch Nichts\". Mit \"Nichts\" sind hier Strings der L√§nge Null gemeint; ersetzt man einen belieibgen String durch einen String der L√§nge Null, so hat man den String gel√∂scht.\n\nDas Cheatsheet zur Strings bzw zu `stringr` von RStudio gibt einen guten √úberblick √ºber Regex; im Internet finden sich viele Beispiele.\n\n\n\n\n\n\n\n\n\n\n#### Regex im Texteditor\n\n\nEinige Texteditoren unterst√ºtzen Regex, so auch RStudio.\n\nDas ist eine praktische Sache. \nEin Beispiel: Sie haben eine Liste mit Namen der Art:\n\n- Nachname1, Vorname1\n- Nachname2, Vorname2\n- Nachname3, Vorname3\n\n\nUnd Sie m√∂chten jetzt aber die Liste mit Stil *Vorname Nachname* sortiert haben.\n\nRStudio mit Regex macht's m√∂glich, s. @fig-vorher-regex.\n\n\n::: {#fig-regrex-rstudio}\n\n![Vorher; mit Regex-Syntax](img/regex1.png){#fig-vorher-regex}\n![Vorher; mit Regex-Syntax](img/regex2.png){#fig-nacher-regex}\n\n:::\n\n\n\n\n\n### Emoji-Analyse\n\nEine einfache Art, Emojis in einer Textmining-Analyse zu verarbeiten, \nbietet das Paket `textclean`:\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-31_bae0315a6ef4a277950f7c257b7e7c4e'}\n\n```{.r .cell-code}\nfls <- system.file(\"docs/emoji_sample.txt\", package = \"textclean\")\nx <- readLines(fls)[1]\nx\n## [1] \"Proin üòç ut maecenas üòè condimentum üòî purus eget. Erat, üòÇvitae nunc elit. Condimentum üò¢ semper iaculis bibendum sed tellus. Ut suscipit interdumüòë in. Faucibüòû us nunc quis a vitae posuere. üòõ Eget amet sit condimentum non. Nascetur vitae ‚òπ et. Auctor ornare ‚ò∫ vestibulum primis justo congue üòÄurna ac magna. Quam üò• pharetra üòü eros üòífacilisis ac lectus nibh est üòôvehicula üòê ornare! Vitae, malesuada üòé erat sociosqu urna, üòè nec sed ad aliquet üòÆ .\"\n```\n:::\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-32_3580d2f2a16788ca63b666dd58cec5a9'}\n\n```{.r .cell-code}\nreplace_emoji(x)\n## [1] \"Proin d??? ut maecenas d??? condimentum d??? purus eget. Erat, d???vitae nunc elit. Condimentum d??c semper iaculis bibendum sed tellus. Ut suscipit interdumd??? in. Faucibd??? us nunc quis a vitae posuere. d??? Eget amet sit condimentum non. Nascetur vitae ^a?^1 et. Auctor ornare ^a?o vestibulum primis justo congue d???urna ac magna. Quam d??yen pharetra d??? eros d???facilisis ac lectus nibh est d???vehicula d??? ornare! Vitae, malesuada d??? erat sociosqu urna, d??? nec sed ad aliquet d??(R) .\"\nreplace_emoji_identifier(x)\n## [1] \"Proin d??? ut maecenas d??? condimentum d??? purus eget. Erat, d???vitae nunc elit. Condimentum d??c semper iaculis bibendum sed tellus. Ut suscipit interdumd??? in. Faucibd??? us nunc quis a vitae posuere. d??? Eget amet sit condimentum non. Nascetur vitae ^a?^1 et. Auctor ornare ^a?o vestibulum primis justo congue d???urna ac magna. Quam d??yen pharetra d??? eros d???facilisis ac lectus nibh est d???vehicula d??? ornare! Vitae, malesuada d??? erat sociosqu urna, d??? nec sed ad aliquet d??(R) .\"\n```\n:::\n\n\n\n\n\n### Text aufr√§umen\n\nEine Reihe generischer Tests bietet das Paket `textclean` von [Tyler Rinker](https://github.com/trinker/textclean):\n\n\nHier ist ein \"unaufger√§umeter\" Text:\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-33_fdb06b1bb060321c0a4096784b4c1492'}\n\n```{.r .cell-code}\nx <- c(\"i like\", \"<p>i want. </p>. thet them ther .\", \"I am ! that|\", \"\", NA, \n    \"&quot;they&quot; they,were there\", \".\", \"   \", \"?\", \"3;\", \"I like goud eggs!\", \n    \"bi\\xdfchen Z\\xfcrcher\", \"i 4like...\", \"\\\\tgreat\",  \"She said \\\"yes\\\"\")\n```\n:::\n\n\n\n\nLassen wir uns dazu ein paar Diagnostiken ausgeben.\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-34_d38708e1de7c2e9c6e4f0ad07c94f2c5'}\n\n```{.r .cell-code}\nEncoding(x) <- \"latin1\"\nx <- as.factor(x)\ncheck_text(x)\n## \n## =============\n## NON CHARACTER\n## =============\n## \n## The text variable is not a character column (likely `factor`):\n## \n## \n## *Suggestion: Consider using `as.character` or `stringsAsFactors = FALSE` when reading in\n##              Also, consider rerunning `check_text` after fixing\n## \n## \n## =====\n## DIGIT\n## =====\n## \n## The following observations contain digits/numbers:\n## \n## 10, 13\n## \n## This issue affected the following text:\n## \n## 10: 3;\n## 13: i 4like...\n## \n## *Suggestion: Consider using `replace_number`\n## \n## \n## ========\n## EMOTICON\n## ========\n## \n## The following observations contain emoticons:\n## \n## 6\n## \n## This issue affected the following text:\n## \n## 6: &quot;they&quot; they,were there\n## \n## *Suggestion: Consider using `replace_emoticons`\n## \n## \n## =====\n## EMPTY\n## =====\n## \n## The following observations contain empty text cells (all white space):\n## \n## 1\n## \n## This issue affected the following text:\n## \n## 1: i like\n## \n## *Suggestion: Consider running `drop_empty_row`\n## \n## \n## =======\n## ESCAPED\n## =======\n## \n## The following observations contain escaped back spaced characters:\n## \n## 14\n## \n## This issue affected the following text:\n## \n## 14: \\tgreat\n## \n## *Suggestion: Consider using `replace_white`\n## \n## \n## ====\n## HTML\n## ====\n## \n## The following observations contain HTML markup:\n## \n## 2, 6\n## \n## This issue affected the following text:\n## \n## 2: <p>i want. </p>. thet them ther .\n## 6: &quot;they&quot; they,were there\n## \n## *Suggestion: Consider running `replace_html`\n## \n## \n## ==========\n## INCOMPLETE\n## ==========\n## \n## The following observations contain incomplete sentences (e.g., uses ending punctuation like '...'):\n## \n## 13\n## \n## This issue affected the following text:\n## \n## 13: i 4like...\n## \n## *Suggestion: Consider using `replace_incomplete`\n## \n## \n## =============\n## MISSING VALUE\n## =============\n## \n## The following observations contain missing values:\n## \n## 5\n## \n## *Suggestion: Consider running `drop_NA`\n## \n## \n## ========\n## NO ALPHA\n## ========\n## \n## The following observations contain elements with no alphabetic (a-z) letters:\n## \n## 4, 7, 8, 9, 10\n## \n## This issue affected the following text:\n## \n## 4: \n## 7: .\n## 8:    \n## 9: ?\n## 10: 3;\n## \n## *Suggestion: Consider cleaning the raw text or running `filter_row`\n## \n## \n## ==========\n## NO ENDMARK\n## ==========\n## \n## The following observations contain elements with missing ending punctuation:\n## \n## 1, 3, 4, 6, 8, 10, 12, 14, 15\n## \n## This issue affected the following text:\n## \n## 1: i like\n## 3: I am ! that|\n## 4: \n## 6: &quot;they&quot; they,were there\n## 8:    \n## 10: 3;\n## 12: bi√üchen Z√ºrcher\n## 14: \\tgreat\n## 15: She said \"yes\"\n## \n## *Suggestion: Consider cleaning the raw text or running `add_missing_endmark`\n## \n## \n## ====================\n## NO SPACE AFTER COMMA\n## ====================\n## \n## The following observations contain commas with no space afterwards:\n## \n## 6\n## \n## This issue affected the following text:\n## \n## 6: &quot;they&quot; they,were there\n## \n## *Suggestion: Consider running `add_comma_space`\n## \n## \n## =========\n## NON ASCII\n## =========\n## \n## The following observations contain non-ASCII text:\n## \n## 12\n## \n## This issue affected the following text:\n## \n## 12: bi√üchen Z√ºrcher\n## \n## *Suggestion: Consider running `replace_non_ascii`\n## \n## \n## ==================\n## NON SPLIT SENTENCE\n## ==================\n## \n## The following observations contain unsplit sentences (more than one sentence per element):\n## \n## 2, 3\n## \n## This issue affected the following text:\n## \n## 2: <p>i want. </p>. thet them ther .\n## 3: I am ! that|\n## \n## *Suggestion: Consider running `textshape::split_sentence`\n```\n:::\n\n\n\n\n\n### Diverse Wortlisten\n\n\n[Tyler Rinker](https://github.com/trinker/lexicon) stellt mit dem Paket `lexicon` eine Zusammenstellung von Wortlisten zu diversen Zwecken zur Verf√ºgung.\nAllerding nur f√ºr die englische Sprache.\n\n\n\n### One-hot-Enkodierung (Dummy-Variablen)\n\nViele Methoden des Maschinellen Lernens k√∂nnen keine nominalen Variablen verarbeiten. Daher m√ºssen nominale Variablen vorab h√§ufig erst in numerische Variablen umgewandelt werden.\nIn der Textanalyse\n\nNehmen wir als einfaches Beispiel den ber√ºhmten Iris-Datensatz:\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-35_4d68d0dc1adb2cbfcd2e5e4c126ef1ee'}\n\n```{.r .cell-code}\ndata(iris)\niris$id <- 1:nrow(iris)  # ID-Variable hinzuf√ºgen\n\niris_mini <-\n  iris |> \n  filter(id %in% c(1, 51, 101))  # von jeder Species ein Exemplar\n```\n:::\n\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell tbl-cap='Tabelle im Lang-Format' hash='030-textmining1_cache/html/unnamed-chunk-36_bd2bb5d0f16a7bf02c80c69057d5c986'}\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"id\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Species\"],\"name\":[2],\"type\":[\"fct\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"1\",\"2\":\"setosa\"},{\"1\":\"51\",\"2\":\"versicolor\"},{\"1\":\"101\",\"2\":\"virginica\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell tbl-cap='Tabelle mit Spalte \\'Species\\' im One-Hot-Format' hash='030-textmining1_cache/html/unnamed-chunk-37_a0a0c85b345230680032fa5fd0d1b811'}\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"id\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Species_setosa\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Species_versicolor\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Species_virginica\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"1\",\"3\":\"0\",\"4\":\"0\"},{\"1\":\"51\",\"2\":\"0\",\"3\":\"1\",\"4\":\"0\"},{\"1\":\"101\",\"2\":\"0\",\"3\":\"0\",\"4\":\"1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n:::\n\n::::\n\n\n\n#### Tidymodels\n\nTidymodels bieten eine recht komfortable Methode zur One-Hot-Transformation:\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-38_316a80d4da73ffbd89c7d74bef06dc84'}\n\n```{.r .cell-code}\niris_rec <-\n  recipe( ~ ., data = iris) |> \n  step_dummy(Species, one_hot = TRUE)\n```\n:::\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-39_f19a43024c01932aff862ae9e4d15db1'}\n\n```{.r .cell-code}\niris_rec_prepped <-\n  iris_rec |> prep()\n\niris_baked <-\n  iris_rec_prepped |> bake(new_data = NULL)\n```\n:::\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-40_521bc97e2c03be081c7768bcaad7e8de'}\n\n```{.r .cell-code}\niris_baked |> \n  select(starts_with(\"Species\")) |> \n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Species_setosa\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Species_versicolor\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Species_virginica\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"0\",\"3\":\"0\"},{\"1\":\"1\",\"2\":\"0\",\"3\":\"0\"},{\"1\":\"1\",\"2\":\"0\",\"3\":\"0\"},{\"1\":\"1\",\"2\":\"0\",\"3\":\"0\"},{\"1\":\"1\",\"2\":\"0\",\"3\":\"0\"},{\"1\":\"1\",\"2\":\"0\",\"3\":\"0\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n \n`tidymodels` ist prim√§r f√ºr das professionelle Modellieren angelegt;\n\"mal eben\" eine One-Hot-Enkodierung durchzuf√ºhren, braucht ein paar Zeilen Code.\n\nOhne den Parameter `one_hot = TRUE` w√ºrden anstelle von $k$ Stufen nur $k-1$ Stufen zur√ºckgegeben werden.\n\nMal sehen, ob es auch schneller geht, also mit weniger Zeilen Code.\n\n\n#### Base R: model.matrix\n\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-41_e4d6df169a5155a361c404808fa61b57'}\n\n```{.r .cell-code}\niris_one_hot_matrix <- model.matrix(~ 0 + Species, data = iris_mini)\niris_one_hot_matrix\n##   Speciessetosa Speciesversicolor Speciesvirginica\n## 1             1                 0                0\n## 2             0                 1                0\n## 3             0                 0                1\n## attr(,\"assign\")\n## [1] 1 1 1\n## attr(,\"contrasts\")\n## attr(,\"contrasts\")$Species\n## [1] \"contr.treatment\"\n```\n:::\n\n\nDas `+ 0` sorgt daf√ºr, dass bei $k$ Stufen von Species nicht, wie bei linearen Modellen n√∂tig, $k-1$ Stufen zur√ºckgegeben werden, sondern $k$ Stufen.\n\n\n`model.matrix` gibt eine Matrix zur√ºck;\ndiese wandlen wir noch in einen Dataframe um:\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-42_dd0d873b5e7debf932007c75923a5590'}\n\n```{.r .cell-code}\niris_one_hot_matrix |> \n  as_tibble() |> \n  mutate(id = 1:n()) |> \n  select(id, everything())\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"id\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Speciessetosa\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Speciesversicolor\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Speciesvirginica\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"1\",\"3\":\"0\",\"4\":\"0\"},{\"1\":\"2\",\"2\":\"0\",\"3\":\"1\",\"4\":\"0\"},{\"1\":\"3\",\"2\":\"0\",\"3\":\"0\",\"4\":\"1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n#### Paket `fastDummies`\n\nDas Paket `fastDummies` bietet ebenfalls eine recht einfache Konvertierung zu One-Hot-Enkodierung:\n\n\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-43_bb8c7737d007ef7d4ff9315c64ba8772'}\n\n```{.r .cell-code}\nlibrary(fastDummies)\n\ndummy_cols(iris_mini, select_columns = \"Species\") |> \n  select(starts_with(\"Species\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Species\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Species_setosa\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Species_versicolor\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Species_virginica\"],\"name\":[4],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"setosa\",\"2\":\"1\",\"3\":\"0\",\"4\":\"0\"},{\"1\":\"versicolor\",\"2\":\"0\",\"3\":\"1\",\"4\":\"0\"},{\"1\":\"virginica\",\"2\":\"0\",\"3\":\"0\",\"4\":\"1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n\n### Sentimentanalyse {#sec-sentimentanalyse}\n\n\n#### Einf√ºhrung\n\n\nEine weitere interessante Analyse ist, die \"Stimmung\" oder \"Emotionen\" (Sentiments) eines Textes auszulesen. \nDie Anf√ºhrungszeichen deuten an, dass hier ein Ma√ü an Verst√§ndnis suggeriert wird, welches nicht (unbedingt) von der Analyse eingehalten wird. \nJedenfalls ist das Prinzip der Sentiment-Analyse im einfachsten Fall so: \n\n\n\n\n\n1. Schau dir jeden Token aus dem Text an.  \n2. Pr√ºfe, ob sich das Wort im Lexikon der Sentiments wiederfindet.  \n3. Wenn ja, dann addiere den Sentimentswert dieses Tokens zum bestehenden Sentiments-Wert.  \n4. Wenn nein, dann gehe weiter zum n√§chsten Wort.  \n5. Liefere zum Schluss die Summenwerte pro Sentiment zur√ºck.  \n\n\n\n\n     \nEs gibt Sentiment-Lexika, die lediglich einen Punkt f√ºr \"positive Konnotation\" bzw. \"negative Konnotation\" geben; andere Lexiko weisen differenzierte Gef√ºhlskonnotationen auf. Wir nutzen hier das *deutsche* Sentimentlexikon `sentiws` [@Remus2010]. Sie k√∂nnen das Lexikon als CSV hier herunterladen:\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-44_b37c31f41071e59f4c0fbfd0398bf1a2'}\n\n```{.r .cell-code}\nsentiws <- read_csv(\"https://osf.io/x89wq/?action=download\")\n```\n:::\n\n\n\nDen Volltext zum Paper finden Sie z.B. [hier](http://www.lrec-conf.org/proceedings/lrec2010/pdf/490_Paper.pdf).\n\nAlternativ k√∂nnen Sie die Daten aus dem Paket `pradadata` laden. Allerdings m√ºssen Sie dieses Paket von Github installieren:\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-45_8f406ca0672b5cf028e214039feeb268'}\n\n```{.r .cell-code}\ninstall.packages(\"devtools\", dep = TRUE)\ndevtools::install_github(\"sebastiansauer/pradadata\")\n```\n:::\n\n::: {.cell hash='030-textmining1_cache/html/parse-sentiment-dics_e0db8761ce03f26a88fa548d5a8c45f5'}\n\n```{.r .cell-code}\ndata(sentiws, package = \"pradadata\")\n```\n:::\n\n\n@tbl-afdcount zeigt einen Ausschnitt aus dem Sentiment-Lexikon *SentiWS*.\n\n\n::: {#tbl-afdcount .cell tbl-cap='Auszug aus SentiWS' hash='030-textmining1_cache/html/tbl-afdcount_ca7a62ac0665bfb20477f473e5d8dcc2'}\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"neg_pos\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"word\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"value\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"inflections\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"neg\",\"2\":\"Abbau\",\"3\":\"-0.0580\",\"4\":\"Abbaus,Abbaues,Abbauen,Abbaue\"},{\"1\":\"neg\",\"2\":\"Abbruch\",\"3\":\"-0.0048\",\"4\":\"Abbruches,Abbr√ºche,Abbruchs,Abbr√ºchen\"},{\"1\":\"neg\",\"2\":\"Abdankung\",\"3\":\"-0.0048\",\"4\":\"Abdankungen\"},{\"1\":\"neg\",\"2\":\"Abd√§mpfung\",\"3\":\"-0.0048\",\"4\":\"Abd√§mpfungen\"},{\"1\":\"neg\",\"2\":\"Abfall\",\"3\":\"-0.0048\",\"4\":\"Abfalles,Abf√§lle,Abfalls,Abf√§llen\"},{\"1\":\"neg\",\"2\":\"Abfuhr\",\"3\":\"-0.3367\",\"4\":\"Abfuhren\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n#### Ungewichtete Sentiment-Analyse\n\nNun k√∂nnen wir jedes Token des Textes mit dem Sentiment-Lexikon abgleichen; \ndabei z√§hlen wir die Treffer f√ºr positive bzw. negative Terme. \nZuvor m√ºssen wir aber noch die Daten (`afd_long`) mit dem Sentimentlexikon zusammenf√ºhren (joinen). \nDas geht nach bew√§hrter Manier mit `inner_join`; \"inner\" sorgt dabei daf√ºr, dass nur Zeilen behalten werden, die in beiden Dataframes vorkommen. Tabelle @tbl-afdsenti zeigt Summe, Anzahl und Anteil der Emotionswerte.\n\n\nWir nutzen die Tabelle `afd_long`,  die wir oben definiert haben.\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-46_8b9254f85750d076c7bf750a486f7a9e'}\n\n```{.r .cell-code}\nafd_long %>% \n  inner_join(sentiws, by = c(\"token\" = \"word\")) %>% \n  select(-inflections) -> afd_senti  # die Spalte brauchen wir nicht\n## Warning in inner_join(., sentiws, by = c(token = \"word\")): Detected an unexpected many-to-many relationship between `x` and `y`.\n## ‚Ñπ Row 9101 of `x` matches multiple rows in `y`.\n## ‚Ñπ Row 3190 of `y` matches multiple rows in `x`.\n## ‚Ñπ If a many-to-many relationship is expected, set `relationship =\n##   \"many-to-many\"` to silence this warning.\n\nafd_senti %>% \n  group_by(neg_pos) %>% \n  summarise(polarity_sum = sum(value),\n            polarity_count = n()) %>% \n  mutate(polarity_prop = (polarity_count / sum(polarity_count)) %>% round(2)) ->\n  afd_senti_tab\n```\n:::\n\n::: {#tbl-afdsenti .cell tbl-cap='Zusammenfassung von SentiWS' hash='030-textmining1_cache/html/tbl-afdsenti_9e7b730e237b241604323a13af7cb000'}\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"neg_pos\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"polarity_sum\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"polarity_count\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"polarity_prop\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"neg\",\"2\":\"-48.5307\",\"3\":\"210\",\"4\":\"0.27\"},{\"1\":\"pos\",\"2\":\"30.6595\",\"3\":\"578\",\"4\":\"0.73\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\nDie Analyse zeigt, dass die emotionale Bauart des Textes durchaus interessant ist: \nEs gibt viel mehr positiv get√∂nte W√∂rter als negativ get√∂nte. \nAllerdings sind die negativen W√∂rter offenbar deutlich st√§rker emotional aufgeladen, \ndenn die Summe an Emotionswert der negativen W√∂rter ist (√ºberraschenderweise?) deutlich gr√∂√üer als die der positiven.\n\nBetrachten wir also die intensivsten negativ und positive konnotierten W√∂rter n√§her.\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-47_56721603637afe80cb704ecd77e9a434'}\n\n```{.r .cell-code}\nafd_senti %>% \n  distinct(token, .keep_all = TRUE) %>% \n  mutate(value_abs = abs(value)) %>% \n  top_n(20, value_abs) %>% \n  pull(token)\n##  [1] \"ungerecht\"    \"besonders\"    \"gef√§hrlich\"   \"√ºberfl√ºssig\"  \"behindern\"   \n##  [6] \"gelungen\"     \"brechen\"      \"unzureichend\" \"gemein\"       \"verletzt\"    \n## [11] \"zerst√∂ren\"    \"trennen\"      \"falsch\"       \"vermeiden\"    \"zerst√∂rt\"    \n## [16] \"schwach\"      \"belasten\"     \"sch√§dlich\"    \"t√∂ten\"        \"verbieten\"\n```\n:::\n\n\nDiese \"Hitliste\" wird zumeist (19/20) von negativ polarisierten Begriffen aufgef√ºllt, \nwobei \"besonders\" ein Intensivierwort ist, welches das Bezugswort verst√§rt (\"besonders gef√§hrlich\"). \nDas Argument `keep_all = TRUE` sorgt daf√ºr, dass alle Spalten zur√ºckgegeben werden, \nnicht nur die durchsuchte Spalte `token`. \nMit `pull` haben wir aus dem Dataframe, der von den dplyr-Verben √ºbergeben wird, \ndie Spalte `pull` \"herausgezogen\"; \nhier nur um Platz zu sparen bzw. der √úbersichtlichkeit halber.\n\n\n     \nNun k√∂nnte man noch den erzielten \"Netto-Sentimentswert\" des Corpus ins Verh√§ltnis setzen Sentimentswert des Lexikons:\nWenn es insgesamt im Sentiment-Lexikon sehr negativ zuginge,\nw√§re ein negativer Sentimentwer in einem beliebigen Corpus nicht √ºberraschend. `describe_distribution` aus `{easystats}` gibt uns einen √úberblick der √ºblichen deskriptiven Statistiken.\n     \n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-48_e73225e8b39e7b0db3e545dcda7400b6'}\n\n```{.r .cell-code}\nsentiws %>% \n  select(value, neg_pos) %>% \n  #group_by(neg_pos) %>% \n  describe_distribution()\n```\n:::\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-49_a72f8b34c79f66f3b3034a25184f3ab4'}\n::: {.cell-output-display}\n\n\n|Variable |  Mean |   SD |  IQR |         Range | Skewness | Kurtosis |    n | n_Missing |\n|:--------|:-----:|:----:|:----:|:-------------:|:--------:|:--------:|:----:|:---------:|\n|value    | -0.05 | 0.20 | 0.05 | (-1.00, 1.00) |    -0.68 |     2.36 | 3468 |         0 |\n\n\n:::\n:::\n\n\nInsgesamt ist das Lexikon ziemlich ausgewogen; negative Werte sind leicht in der √úberzahl im Lexikon. \nUnser Corpus hat eine √§hnliche mittlere emotionale Konnotation wie das Lexikon:\n\n\n::: {.cell hash='030-textmining1_cache/html/unnamed-chunk-50_2618d811c532b60dc054d489fd441388'}\n\n```{.r .cell-code}\nafd_senti %>% \n  summarise(senti_sum = mean(value) %>% round(2))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"senti_sum\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"-0.02\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n\n### Weitere Sentiment-Lexika\n\n[Tyler Rinker](https://github.com/trinker/sentimentr) stellt das Paket `sentimentr` zur Verf√ºgung.\n[Matthew Jockers](https://www.matthewjockers.net/2015/02/02/syuzhet/) stellt das Paket `Syushet` zur Verf√ºgung.\n\n\n\n### Google Trends\n\nEine weitere M√∂glichkeit, \"Worth√§ufigkeiten\" zu identifizieren ist [Google Trends](https://trends.google.com/trends/?geo=US).\nDieser Post zeigt Ihnen eine Einsatzm√∂glichkeit.\n\n\n\n\n## Aufgaben\n\n\n- [purrr-map01](https://datenwerk.netlify.app/posts/purrr-map01/purrr-map01.html)\n- [purrr-map02](https://datenwerk.netlify.app/posts/purrr-map02/purrr-map02.html)\n- [purrr-map03](https://datenwerk.netlify.app/posts/purrr-map03/purrr-map03.html)\n- [purrr-map04](https://datenwerk.netlify.app/posts/purrr-map04/purrr-map04.html)\n- [Regex-√úbungen](https://regexone.com/)\n- [Aufgaben zum Textmining von Tweets](https://datenwerk.netlify.app/#category=textmining)\n\n\n\n\n## Fallstudie Hate-Speech\n\n\n### Daten\n\nEs finden sich mehrere Datens√§tze zum Thema Hate-Speech im √∂ffentlichen Internet, eine Quelle ist [Hate Speech Data](https://ckan.hatespeechdata.com/), ein Repositorium, das mehrere Datens√§tze beinhaltet.\n\n\n\n- [Kaggle Hate Speech and Offensive Language Dataset](https://www.kaggle.com/datasets/mrmorj/hate-speech-and-offensive-language-dataset?select=labeled_data.csv)\n- [Bretschneider and Peters Prejudice on Facebook Dataset](https://ckan.hatespeechdata.com/dataset/bretschneider-and-peters-prejudice-on-facebook-dataset)\n- [Daten zum Fachartikel\"Large Scale Crowdsourcing and Characterization of Twitter Abusive Behavior\"](https://github.com/ENCASEH2020/hatespeech-twitter/blob/master/hatespeech_labels.csv)\n\n\nF√ºr Textmining kann eine Liste mit anst√∂√üigen (obsz√∂nen) W√∂rten n√ºtzlich sein,\nauch wenn man solche Dinge ungern anf√§sst, verst√§ndlicherweise.\n[Jenyay](https://github.com/Jenyay/Obscene-Words-List) bietet solche Listen in verschiedenen Sprachen an. Die Liste von [KDNOOBW](https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words) sieht sehr √§hnlich aus (zumindest die deutsche Version).\nEine lange Sammlung deutscher Schimpfw√∂rter findet sich im [insult.wiki](https://www.insult.wiki/schimpfwort-liste);\n√§hnlich bei [Hyperhero](http://www.hyperhero.com/de/insults.htm).\n\n\n\n\n\n\n\nTwitterdaten d√ºrfen nur in \"dehydrierter\" Form weitergegeben werden, so dass kein R√ºckschluss von ID zum Inhalt des Tweets m√∂glich ist. \nDaher werden √∂ffentlich nur die IDs der Tweets, als einzige Information zum Tweet, also ohne den eigentlichen Inhalt des Tweets, bereitgestellt.\n\n√úber die Twitter-API kann man sich, wie oben dargestellt, dann die Tweets wieder \"rehydrieren\", also wieder mit dem zugeh√∂rigen Tweet-Text (und sonstigen Infos des Tweets) zu versehen.\n\n\n\n\n### Grundlegendes Text Mining\n\n\nWenden Sie die oben aufgef√ºhrten Techniken des grundlegenden Textminings auf einen der oben dargestellten Hate-Speech-Datens√§tze an.\nErstellen Sie ein (HTML-Dokument) mit Ihren Ergebnissen. \nStellen Sie die Ergebnisse auf dem Github-Repo dieses Kurses ein.\nVergleichen Sie Ihre L√∂sung mit den L√∂sungen der anderen Kursmitglieder.\n\nWir nutzen noch nicht eigene Daten, die wir von Twitter ausgelesen haben, das heben wir uns f√ºr sp√§ter auf.\n\n\n\n\n\n\n\n## Vertiefung\n\n[Julia Silge](https://juliasilge.com/) bietet eine nette [Fallstudie](https://juliasilge.com/blog/taylor-swift/) zu den Themen in Taylor Swifts Liefern.\n\nApropos Themenanalyse: Alternativ zum klassischen, probabilistischen Themen-Modellierung kann man pretrainierte Wort-Einbettungen verwenden. [Dieses Paper](https://aclanthology.org/2020.emnlp-main.135/) gibt eine Einf√ºhrung.\n\n[Hauptkomponenten-Analyse (Principal Component Analysis, PCA)](https://juliasilge.com/blog/stack-overflow-pca/) ist eine zentrale Methode der Datenanalyse. Diese Fallstudie stellt eine einfache Anwendung - ohne tiefere theoretische Erl√§uterung - vor.\n\n@silge_text_2017 geben eine Einf√ºhrung in die Textanalyse mit Hilfe von Tidy-Prinzipien.\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}