{
  "hash": "ef32f8da22cad63d227c5b94f27b747d",
  "result": {
    "engine": "knitr",
    "markdown": "\n\n# Klassifikation von Hatespeech\n\n\n\n\n\n## Vorab\n\n\n### Lernziele\n\n\n- Sie können grundlegende Verfahren zur Klassifikation von Hatespeech einsetzen und erklären\n\n\n\n\n\n\n\n\n### Benötigte R-Pakete und Skripte\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(tidymodels)\nlibrary(tidytext)  # Textmiing\nlibrary(textrecipes)  # Textanalysen in Tidymodels-Rezepten\nlibrary(lsa)  # stopwords\nlibrary(discrim)  # naive bayes classification\nlibrary(naivebayes)\nlibrary(tictoc)  # Zeitmessung\nlibrary(fastrtext)  # Worteinbettungen\nlibrary(remoji)  # Emojis\nlibrary(tokenizers)  # Vektoren tokenisieren\nlibrary(tictoc)  # Zeitmessung\n\nsource(\"funs/helper-funs-recipes.R\")\n```\n:::\n\n\n\n## Daten\n\n\nFür Maschinenlernen brauchen wir Trainingsdaten,\nDaten also, bei denen wir pro Beobachtung der Wert der Zielvariablen kennen.\nMan spricht auch von \"gelabelten\" Daten.\n\nWir nutzen die Daten von @germeval bzw. @wiegand-data.\nDie Daten sind unter CC-By-4.0 Int. lizensiert.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_raw <- \n  data_read(\"data/germeval2018.training.txt\",\n         header = FALSE,\n         quote = \"\")\n```\n:::\n\n\n\nDie Daten finden sich auch im Paket [pradadata](https://github.com/sebastiansauer/pradadata).\n\nDa die Daten keine Spaltenköpfe haben, informieren wir die Funktion dazu mit `header = FALSE`.\n\nBenennen wir die die Spalten um:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(d_raw) <- c(\"text\", \"c1\", \"c2\")\n```\n:::\n\n\nDabei soll `c1` und `c2` für die 1. bzw. 2. Klassifikation stehen.\n\n\nIn `c1` finden sich diese Werte:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_raw %>% \n  count(c1)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"c1\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"OFFENSE\",\"2\":\"1688\"},{\"1\":\"OTHER\",\"2\":\"3321\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nMit `c1` wurde klassifiziert,\nob beleidigende Sprache (offensive language) vorlag oder nicht [@risch-etal-2021-overview, S. 2], also `OFFENSE` bzw. `OTHER`:\n\n\n\n>   Task 1 was to decide whether a tweet includes some form of offensive language or not. The tweets had to be classiﬁed into the two classes OFFENSE and OTHER. The OFFENSE category covered abusive language, insults, as well as merely profane statements.\n\n\nUnd in `c2` finden sich die vier Ausprägungen `ABUSE`, `INSULT`, `PROFANITY` und `OTHER`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_raw %>% \n  count(c2)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"c2\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"ABUSE\",\"2\":\"1022\"},{\"1\":\"INSULT\",\"2\":\"595\"},{\"1\":\"OTHER\",\"2\":\"3321\"},{\"1\":\"PROFANITY\",\"2\":\"71\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\nIn `c2` ging es um eine *feinere Klassifikation* beleidigender Sprache [@risch-etal-2021-overview, S. 2]:\n\n>   The second task involved four categories, a nonoffensive OTHER class and three sub-categories of what is OFFENSE in Task 1. In the case of PROFANITY, profane words are used, however, the tweet does not want to insult anyone. This typically concerns the usage of swearwords (Scheiße, Fuck etc.) and cursing (Zur Hölle! Verdammt! etc.). This can be often found in youth language. Swearwords and cursing may, but need not, co-occur with insults or abusive speech. Profane language may in fact be used in tweets with positive sentiment to express emphasis. Whenever profane words are not directed towards a speciﬁc person or group of persons and there are no separate cues of INSULT or ABUSE, then tweets are labeled as simple cases of PROFANITY.\n\n\n\n\nSind Texte, die als `OFFENSE` klassifiziert sind,\nauch (fast) immer nie als `OTHER`, sondern stattdessen als `ABUSE`, `INSULT` oder `PROFANITY` klassifiziert?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_raw %>% \n  filter(c1 == \"OFFENSE\", c2 != \"OTHER\") %>% \n  nrow() / nrow(d_raw)\n## [1] 0.3369934\n```\n:::\n\n\n\nIn ca. 2/3 der Fälle wurden in beiden Klassifikation `OTHER` klassifiziert:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_raw %>% \n  filter(c1 == \"OTHER\", c2 == \"OTHER\") %>% \n  nrow() / nrow(d_raw)\n## [1] 0.6630066\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nd_raw %>% \n  filter(c1 != \"OTHER\", c2 != \"OTHER\") %>% \n  nrow() / nrow(d_raw)\n## [1] 0.3369934\n```\n:::\n\n\nEntsprechend in ca. 1/3 der Fälle wurde jeweils nicht mit `OTHER` klassifiziert.\n\n\nWir begnügen uns hier mit der ersten, gröberen Klassifikation.\n\n\n\nFügen wir abschließend noch eine ID-Variable hinzu:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd1 <-\n  d_raw %>% \n  mutate(id = as.character(1:nrow(.)))\n```\n:::\n\n\n\nDie *ID-Variable* definieren als *Text* (nicht als Integer),\nda die Twitter-IDs zwar natürliche Zahlen sind,\naber zu groß, um von R als Integer verarbeitet zu werden.\nFaktisch sind sie für uns auch nur nominal skalierte Variablen,\nso dass wir keinen Informationsverlust haben.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#write_rds(d1, \"objects/d1.rds\")\n```\n:::\n\n\n\n\n## Feature Engineering\n\n\nReichern wir die Daten mit weiteren Features an,\nin der Hoffnung, damit eine bessere Klassifikation erzielen zu können.\n\n\n### Textlänge\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd2 <-\n  d1 %>% \n  mutate(text_length = str_length(text))\n\nhead(d2)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"text\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"c1\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"c2\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"id\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"text_length\"],\"name\":[5],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"@corinnamilborn Liebe Corinna, wir würden dich gerne als Moderatorin für uns gewinnen! Wärst du begeisterbar?\",\"2\":\"OTHER\",\"3\":\"OTHER\",\"4\":\"1\",\"5\":\"109\",\"_rn_\":\"1\"},{\"1\":\"@Martin28a Sie haben ja auch Recht. Unser Tweet war etwas missverständlich. Dass das BVerfG Sachleistungen nicht ausschließt, kritisieren wir.\",\"2\":\"OTHER\",\"3\":\"OTHER\",\"4\":\"2\",\"5\":\"142\",\"_rn_\":\"2\"},{\"1\":\"@ahrens_theo fröhlicher gruß aus der schönsten stadt der welt theo ⚓️\",\"2\":\"OTHER\",\"3\":\"OTHER\",\"4\":\"3\",\"5\":\"69\",\"_rn_\":\"3\"},{\"1\":\"@dushanwegner Amis hätten alles und jeden gewählt...nur Hillary wollten sie nicht und eine Fortsetzung von Obama-Politik erst recht nicht..!\",\"2\":\"OTHER\",\"3\":\"OTHER\",\"4\":\"4\",\"5\":\"140\",\"_rn_\":\"4\"},{\"1\":\"@spdde kein verläßlicher Verhandlungspartner. Nachkarteln nach den Sondierzngsgesprächen - schickt diese Stümper #SPD in die Versenkung.\",\"2\":\"OFFENSE\",\"3\":\"INSULT\",\"4\":\"5\",\"5\":\"136\",\"_rn_\":\"5\"},{\"1\":\"@Dirki_M Ja, aber wo widersprechen die Zahlen denn denen, die im von uns verlinkten Artikel stehen? In unserem Tweet geht es rein um subs. Geschützte. 2017 ist der gesamte Familiennachzug im Vergleich zu 2016 - die Zahlen, die Hr. Brandner bemüht - übrigens leicht rückläufig gewesen.\",\"2\":\"OTHER\",\"3\":\"OTHER\",\"4\":\"6\",\"5\":\"284\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n\n### Sentimentanalyse\n\nWir nutzen dazu `SentiWS` [@Remus2010].\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsentiws <- read_csv(\"https://osf.io/x89wq/?action=download\")\n## Rows: 3468 Columns: 4\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (3): neg_pos, word, inflections\n## dbl (1): value\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nd2_long <-\n  d2 %>% \n  unnest_tokens(input = text, output = token)\n\nhead(d2_long)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"c1\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"c2\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"id\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"text_length\"],\"name\":[4],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"token\"],\"name\":[5],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"OTHER\",\"2\":\"OTHER\",\"3\":\"1\",\"4\":\"109\",\"5\":\"corinnamilborn\",\"_rn_\":\"1\"},{\"1\":\"OTHER\",\"2\":\"OTHER\",\"3\":\"1\",\"4\":\"109\",\"5\":\"liebe\",\"_rn_\":\"2\"},{\"1\":\"OTHER\",\"2\":\"OTHER\",\"3\":\"1\",\"4\":\"109\",\"5\":\"corinna\",\"_rn_\":\"3\"},{\"1\":\"OTHER\",\"2\":\"OTHER\",\"3\":\"1\",\"4\":\"109\",\"5\":\"wir\",\"_rn_\":\"4\"},{\"1\":\"OTHER\",\"2\":\"OTHER\",\"3\":\"1\",\"4\":\"109\",\"5\":\"würden\",\"_rn_\":\"5\"},{\"1\":\"OTHER\",\"2\":\"OTHER\",\"3\":\"1\",\"4\":\"109\",\"5\":\"dich\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nJetzt filtern wir unsere Textdaten so,\ndass nur Wörter mit Sentimentwert übrig bleiben:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd2_long_senti <- \n  d2_long %>%  \n  inner_join(sentiws %>% select(-inflections), by = c(\"token\" = \"word\"))\n## Warning in inner_join(., sentiws %>% select(-inflections), by = c(token = \"word\")): Detected an unexpected many-to-many relationship between `x` and `y`.\n## ℹ Row 1559 of `x` matches multiple rows in `y`.\n## ℹ Row 2572 of `y` matches multiple rows in `x`.\n## ℹ If a many-to-many relationship is expected, set `relationship =\n##   \"many-to-many\"` to silence this warning.\n\nhead(d2_long)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"c1\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"c2\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"id\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"text_length\"],\"name\":[4],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"token\"],\"name\":[5],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"OTHER\",\"2\":\"OTHER\",\"3\":\"1\",\"4\":\"109\",\"5\":\"corinnamilborn\",\"_rn_\":\"1\"},{\"1\":\"OTHER\",\"2\":\"OTHER\",\"3\":\"1\",\"4\":\"109\",\"5\":\"liebe\",\"_rn_\":\"2\"},{\"1\":\"OTHER\",\"2\":\"OTHER\",\"3\":\"1\",\"4\":\"109\",\"5\":\"corinna\",\"_rn_\":\"3\"},{\"1\":\"OTHER\",\"2\":\"OTHER\",\"3\":\"1\",\"4\":\"109\",\"5\":\"wir\",\"_rn_\":\"4\"},{\"1\":\"OTHER\",\"2\":\"OTHER\",\"3\":\"1\",\"4\":\"109\",\"5\":\"würden\",\"_rn_\":\"5\"},{\"1\":\"OTHER\",\"2\":\"OTHER\",\"3\":\"1\",\"4\":\"109\",\"5\":\"dich\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\nSchließlich berechnen wir die Sentimentwert pro Polarität und pro Tweet:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd2_sentis <-\n  d2_long_senti %>% \n  group_by(id, neg_pos) %>% \n  summarise(senti_avg = mean(value))\n## `summarise()` has grouped output by 'id'. You can override using the `.groups`\n## argument.\n\nhead(d2_sentis)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"id\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"neg_pos\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"senti_avg\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"pos\",\"3\":\"0.0040\"},{\"1\":\"1012\",\"2\":\"neg\",\"3\":\"-0.2087\"},{\"1\":\"1013\",\"2\":\"neg\",\"3\":\"-0.0420\"},{\"1\":\"1015\",\"2\":\"neg\",\"3\":\"-0.0048\"},{\"1\":\"1021\",\"2\":\"pos\",\"3\":\"0.0845\"},{\"1\":\"1024\",\"2\":\"neg\",\"3\":\"-0.4787\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\nDiese Tabelle bringen wir wieder eine breitere Form,\num sie dann wieder mit den Hauptdaten zu vereinigen.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd2_sentis_wide <-\n  d2_sentis %>% \n  pivot_wider(names_from = \"neg_pos\", values_from = \"senti_avg\")\n\nd2_sentis_wide %>% head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"id\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"pos\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"neg\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"0.0040\",\"3\":\"NA\"},{\"1\":\"1012\",\"2\":\"NA\",\"3\":\"-0.2087\"},{\"1\":\"1013\",\"2\":\"NA\",\"3\":\"-0.0420\"},{\"1\":\"1015\",\"2\":\"NA\",\"3\":\"-0.0048\"},{\"1\":\"1021\",\"2\":\"0.0845\",\"3\":\"NA\"},{\"1\":\"1024\",\"2\":\"0.0040\",\"3\":\"-0.4787\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nd3 <-\n  d2 %>% \n  full_join(d2_sentis_wide)\n## Joining with `by = join_by(id)`\n\nhead(d3)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"text\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"c1\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"c2\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"id\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"text_length\"],\"name\":[5],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"pos\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"neg\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"@corinnamilborn Liebe Corinna, wir würden dich gerne als Moderatorin für uns gewinnen! Wärst du begeisterbar?\",\"2\":\"OTHER\",\"3\":\"OTHER\",\"4\":\"1\",\"5\":\"109\",\"6\":\"0.004\",\"7\":\"NA\",\"_rn_\":\"1\"},{\"1\":\"@Martin28a Sie haben ja auch Recht. Unser Tweet war etwas missverständlich. Dass das BVerfG Sachleistungen nicht ausschließt, kritisieren wir.\",\"2\":\"OTHER\",\"3\":\"OTHER\",\"4\":\"2\",\"5\":\"142\",\"6\":\"NA\",\"7\":\"-0.3466\",\"_rn_\":\"2\"},{\"1\":\"@ahrens_theo fröhlicher gruß aus der schönsten stadt der welt theo ⚓️\",\"2\":\"OTHER\",\"3\":\"OTHER\",\"4\":\"3\",\"5\":\"69\",\"6\":\"NA\",\"7\":\"NA\",\"_rn_\":\"3\"},{\"1\":\"@dushanwegner Amis hätten alles und jeden gewählt...nur Hillary wollten sie nicht und eine Fortsetzung von Obama-Politik erst recht nicht..!\",\"2\":\"OTHER\",\"3\":\"OTHER\",\"4\":\"4\",\"5\":\"140\",\"6\":\"NA\",\"7\":\"NA\",\"_rn_\":\"4\"},{\"1\":\"@spdde kein verläßlicher Verhandlungspartner. Nachkarteln nach den Sondierzngsgesprächen - schickt diese Stümper #SPD in die Versenkung.\",\"2\":\"OFFENSE\",\"3\":\"INSULT\",\"4\":\"5\",\"5\":\"136\",\"6\":\"NA\",\"7\":\"NA\",\"_rn_\":\"5\"},{\"1\":\"@Dirki_M Ja, aber wo widersprechen die Zahlen denn denen, die im von uns verlinkten Artikel stehen? In unserem Tweet geht es rein um subs. Geschützte. 2017 ist der gesamte Familiennachzug im Vergleich zu 2016 - die Zahlen, die Hr. Brandner bemüht - übrigens leicht rückläufig gewesen.\",\"2\":\"OTHER\",\"3\":\"OTHER\",\"4\":\"6\",\"5\":\"284\",\"6\":\"0.004\",\"7\":\"-0.2042\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n:::callout-note\nDie Sentimentanalyse hier vernachlässigt Flexionen der Wörter. \nDer  Autor fühlt den Drang zu schreiben: \"Left as an exercise for the reader\" :-)\n:::\n\n\n### Schimpfwörter\n\n\nZählen wir die Schimpfwörter pro Text.\nDazu nutzen wir die Daten von [LDNOOBW](https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/blob/master/LICENSE), lizensiert nach CC-BY-4.0-Int.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nschimpf1 <- read_csv(\"https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/de\", col_names = FALSE)\n## Rows: 66 Columns: 1\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (1): X1\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n\n\nLänger aber noch ist die Liste aus dem [InsultWiki](https://www.insult.wiki/schimpfwort-liste), lizensiert CC0.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nschimpf2 <- \n  data_read(\"data/insult-de.txt\", header = FALSE) %>% \n  mutate_all(str_to_lower)\n```\n:::\n\n\n\n\nDie Daten finden sich auch im Paket [pradadata](https://github.com/sebastiansauer/pradadata).\n\nBinden wir die Listen zusammen:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nschimpf <-\n  schimpf1 %>% \n  bind_rows(schimpf2) %>% \n  distinct() %>% \n  rename(word = \"V1\")\n\nnrow(schimpf)\n## [1] 6235\n```\n:::\n\n\n\nUm die Lesis vor (unnötiger?) Kopfverschmutzung zu bewahren,\nsind diese Schimpfwörter hier nicht abgedruckt.\n\nJetzt zählen wir, ob unsere Tweets/Texte solcherlei Wörter enthalten.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_schimpf <- \nd2_long %>% \n  select(id, token) %>% \n  mutate(schimpf = token %in% schimpf$word)\n```\n:::\n\n\n\nWie viele Schimpfwörter haben wir gefunden?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_schimpf %>% \n  count(schimpf)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"schimpf\"],\"name\":[1],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"FALSE\",\"2\":\"99105\"},{\"1\":\"TRUE\",\"2\":\"1112\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\nEtwa ein Prozent der Wörter (ca. 1k) sind Schimpfwörter in unserem Corpus.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_schimpf2 <-\n  d_schimpf %>% \n  group_by(id) %>% \n  summarise(schimpf_n = sum(schimpf))\n\nhead(d_schimpf2)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"id\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"schimpf_n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"0\"},{\"1\":\"10\",\"2\":\"0\"},{\"1\":\"100\",\"2\":\"0\"},{\"1\":\"1000\",\"2\":\"0\"},{\"1\":\"1001\",\"2\":\"0\"},{\"1\":\"1002\",\"2\":\"0\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nd_main <-\n  d3 %>% \n  full_join(d_schimpf2)\n## Joining with `by = join_by(id)`\n```\n:::\n\n\n\n:::callout-important\nNamen wie `final`, `main` oder `result` sind gefährlich,\nda es unter Garantie ein \"final-final geben wird, oder der \"Haupt-Datensatz\" plötzlich nicht mehr so wichtig erscheint und so weiter.\n:::\n\n\n\n### Emojis\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemj <- emoji(list_emoji(), pad = FALSE)\n\nhead(emj)\n## [1] \"😄\" \"😃\" \"😀\" \"😊\" \"☺️\"  \"😉\"\n```\n:::\n\n\nDiese Liste umfasst knapp 900 Emojis, \ndas sind allerdings noch nicht alle, die es gibt.\n[Diese Liste](https://unicode.org/emoji/charts/full-emoji-list.html) umfasst mit gut 1800 Emojis\ngut das Doppelte.\n\n\nSelbstkuratierte Liste an \"wilden\" Emoji;\ndiese Liste ist inspiriert von [emojicombos.com](https://emojicombos.com/disgust).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwild_emojis <- \n  c(\n    emoji(find_emoji(\"gun\")),\n    emoji(find_emoji(\"bomb\")),\n    emoji(find_emoji(\"fist\")),\n    emoji(find_emoji(\"knife\"))[1],\n    emoji(find_emoji(\"ambulance\")),\n    emoji(find_emoji(\"fist\")),\n    emoji(find_emoji(\"skull\")),\n    \"☠️\",     \"🗑\",       \"😠\",    \"👹\",    \"💩\" ,\n    \"🖕\",    \"👎️\",\n    emoji(find_emoji(\"middle finger\")),    \"😡\",    \"🤢\",    \"🤮\",  \n    \"😖\",    \"😣\",    \"😩\",    \"😨\",    \"😝\",    \"😳\",    \"😬\",    \"😱\",    \"😵\",\n       \"😤\",    \"🤦‍♀️\",    \"🤦‍\"\n  )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwild_emojis_df <-\n  tibble(emoji = wild_emojis)\n\n#save(wild_emojis_df, file = \"data/wild_emojis.RData\")\n```\n:::\n\n\n\n\nAuf dieser Basis können wir einen Prädiktor erstellen,\nder zählt, ob ein Tweet einen oder mehrere der \"wilden\" Emojis enthält.\n\n\n## Workflow 1: TF-IDF + Naive-Bayes\n\n\n### Dummy-Rezept\n\n\nHier ist ein einfaches Beispiel,\num die Textvorbereitung mit `{textrecipes}` zu verdeutlichen.\n\nWir erstellen uns einen Dummy-Text:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndummy <- \n  tibble(text = c(\"Ich gehe heim und der die das nicht in ein and the\"))\n```\n:::\n\n\n\nDann tokenisieren wir den Text:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_dummy <-\n  recipe(text ~ 1, data = dummy) %>% \n  step_tokenize(text)\n  \nrec_dummy\n## \n## ── Recipe ──────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome: 1\n## \n## ── Operations\n## • Tokenization for: text\n```\n:::\n\n\n\nDie Tokens kann man sich so zeigen lassen:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_tokens(rec_dummy, text)\n## [[1]]\n##  [1] \"ich\"   \"gehe\"  \"heim\"  \"und\"   \"der\"   \"die\"   \"das\"   \"nicht\" \"in\"   \n## [10] \"ein\"   \"and\"   \"the\"\n```\n:::\n\n\n\nJetzt entfernen wir die Stopwörter deutscher Sprache;\ndafür nutzen wir die Stopwort-Quelle `snowball`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_dummy <-\n  recipe(text ~ 1, data = dummy) %>% \n  step_tokenize(text) %>% \n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\")\n\nrec_dummy\n## \n## ── Recipe ──────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome: 1\n## \n## ── Operations\n## • Tokenization for: text\n## • Stop word removal for: text\n```\n:::\n\n\n\nPrüfen wir die Tokens; \nsind die Stopwörter wirklich entfernt?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_tokens(rec_dummy, text)\n## [[1]]\n## [1] \"gehe\" \"heim\" \"and\"  \"the\"\n```\n:::\n\n\n\nJa, die deutschen Stopwörter sind entfernt. Die englischen nicht;\ndas macht Sinn!\n\n\n### Datenaufteilung\n\nDen Datensatz `d_main` kann man sich auch hier importieren:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#d_main <- read_rds(file = \"objects/chap-klassifik/d_main.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nd_split <- initial_split(d_main, strata = c1)\n\nd_train <- training(d_split)\nd_test <- testing(d_split)\n```\n:::\n\n\n\n\n\n\n### Rezept\n\n\nRezept definieren:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec1 <- \n  recipe(c1 ~ ., data = select(d_train, text, c1, id)) %>% \n  update_role(id, new_role = \"id\") %>% \n  # step_mutate(text_copy = text) |> \n  step_tokenize(text) %>% \n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %>% \n  step_stem(text) %>% \n  step_tokenfilter(text, max_tokens = 1e2) %>%  # wir behalten nur die häufigsten Tokens\n  step_tfidf(text) \n\nrec1\n## \n## ── Recipe ──────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:   1\n## predictor: 1\n## id:        1\n## \n## ── Operations\n## • Tokenization for: text\n## • Stop word removal for: text\n## • Stemming for: text\n## • Text filtering for: text\n## • Term frequency-inverse document frequency with: text\n```\n:::\n\n\n\nPreppen. Und backen:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec1_prepped <- prep(rec1)\n\nd_rec1 <- bake(rec1_prepped, new_data = NULL)\n\nhead(d_rec1)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"id\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"c1\"],\"name\":[2],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"tfidf_text__macmik\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_2\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_ab\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_afd\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_amp\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_anna_iina\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_athinamala\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_beim\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_besser\"],\"name\":[11],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_bild\"],\"name\":[12],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_cdu\"],\"name\":[13],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_charlie_silv\"],\"name\":[14],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_csu\"],\"name\":[15],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_d\"],\"name\":[16],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_dafür\"],\"name\":[17],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_dank\"],\"name\":[18],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_dass\"],\"name\":[19],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_deutsch\"],\"name\":[20],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_deutschen\"],\"name\":[21],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_deutschland\"],\"name\":[22],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_dumm\"],\"name\":[23],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_einfach\"],\"name\":[24],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_ellibisathid\"],\"name\":[25],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_endlich\"],\"name\":[26],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_ennof_\"],\"name\":[27],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_erst\"],\"name\":[28],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_eu\"],\"name\":[29],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_europa\"],\"name\":[30],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_feldenfrizz\"],\"name\":[31],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_flüchtling\"],\"name\":[32],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_focusonlin\"],\"name\":[33],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_frage\"],\"name\":[34],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_frau\"],\"name\":[35],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_ganz\"],\"name\":[36],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_geht\"],\"name\":[37],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_gerad\"],\"name\":[38],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_gibt\"],\"name\":[39],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_gott\"],\"name\":[40],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_grünen\"],\"name\":[41],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_gt\"],\"name\":[42],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_gut\"],\"name\":[43],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_heut\"],\"name\":[44],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_immer\"],\"name\":[45],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_info2099\"],\"name\":[46],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_islam\"],\"name\":[47],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_ja\"],\"name\":[48],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_jahr\"],\"name\":[49],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_klar\"],\"name\":[50],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_kommt\"],\"name\":[51],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_krippmari\"],\"name\":[52],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_land\"],\"name\":[53],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_lassen\"],\"name\":[54],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_lbr\"],\"name\":[55],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_leben\"],\"name\":[56],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_leut\"],\"name\":[57],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_lifetrend\"],\"name\":[58],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_link\"],\"name\":[59],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_macht\"],\"name\":[60],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_machtjanix23\"],\"name\":[61],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_mal\"],\"name\":[62],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_md_franz\"],\"name\":[63],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_mehr\"],\"name\":[64],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_menschen\"],\"name\":[65],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_merkel\"],\"name\":[66],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_miriamozen\"],\"name\":[67],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_moslem\"],\"name\":[68],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_müssen\"],\"name\":[69],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_nancypeggymandi\"],\"name\":[70],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_nasanas\"],\"name\":[71],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_nie\"],\"name\":[72],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_noherrman\"],\"name\":[73],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_norbinator2403\"],\"name\":[74],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_petpanther0\"],\"name\":[75],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_politik\"],\"name\":[76],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_recht\"],\"name\":[77],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_richtig\"],\"name\":[78],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_schmiddiemaik\"],\"name\":[79],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_schon\"],\"name\":[80],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_schulz\"],\"name\":[81],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_seit\"],\"name\":[82],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_sollten\"],\"name\":[83],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_spd\"],\"name\":[84],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_tagesschau\"],\"name\":[85],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_thomasgbau\"],\"name\":[86],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_troll_putin\"],\"name\":[87],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_trump\"],\"name\":[88],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_tun\"],\"name\":[89],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_türken\"],\"name\":[90],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_u\"],\"name\":[91],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_unser\"],\"name\":[92],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_viel\"],\"name\":[93],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_volk\"],\"name\":[94],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_wäre\"],\"name\":[95],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_warum\"],\"name\":[96],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_welt\"],\"name\":[97],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_wer\"],\"name\":[98],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_willjrosenblatt\"],\"name\":[99],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_wohl\"],\"name\":[100],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_wurd\"],\"name\":[101],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_zeit\"],\"name\":[102],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"5\",\"2\":\"OFFENSE\",\"3\":\"0\",\"4\":\"0\",\"5\":\"0\",\"6\":\"0.000000\",\"7\":\"0\",\"8\":\"0\",\"9\":\"0\",\"10\":\"0\",\"11\":\"0\",\"12\":\"0\",\"13\":\"0\",\"14\":\"0\",\"15\":\"0\",\"16\":\"0\",\"17\":\"0\",\"18\":\"0\",\"19\":\"0\",\"20\":\"0.000000\",\"21\":\"0.000000\",\"22\":\"0\",\"23\":\"0\",\"24\":\"0\",\"25\":\"0\",\"26\":\"0\",\"27\":\"0\",\"28\":\"0\",\"29\":\"0\",\"30\":\"0\",\"31\":\"0\",\"32\":\"0\",\"33\":\"0\",\"34\":\"0\",\"35\":\"0\",\"36\":\"0\",\"37\":\"0.000000\",\"38\":\"0\",\"39\":\"0\",\"40\":\"0\",\"41\":\"0\",\"42\":\"0\",\"43\":\"0\",\"44\":\"0.000000\",\"45\":\"0.000000\",\"46\":\"0\",\"47\":\"0\",\"48\":\"0.000000\",\"49\":\"0\",\"50\":\"0\",\"51\":\"0\",\"52\":\"0\",\"53\":\"0\",\"54\":\"0\",\"55\":\"0.0000000\",\"56\":\"0\",\"57\":\"0\",\"58\":\"0\",\"59\":\"0\",\"60\":\"0\",\"61\":\"0\",\"62\":\"0\",\"63\":\"0\",\"64\":\"0\",\"65\":\"0\",\"66\":\"0\",\"67\":\"0\",\"68\":\"0\",\"69\":\"0\",\"70\":\"0\",\"71\":\"0\",\"72\":\"0\",\"73\":\"0\",\"74\":\"0\",\"75\":\"0\",\"76\":\"0.000000\",\"77\":\"0\",\"78\":\"0\",\"79\":\"0\",\"80\":\"0\",\"81\":\"0\",\"82\":\"0.00000\",\"83\":\"0\",\"84\":\"3.671899\",\"85\":\"0.000000\",\"86\":\"0\",\"87\":\"0\",\"88\":\"0\",\"89\":\"0\",\"90\":\"0\",\"91\":\"0\",\"92\":\"0\",\"93\":\"0\",\"94\":\"0\",\"95\":\"0\",\"96\":\"0\",\"97\":\"0\",\"98\":\"0\",\"99\":\"0\",\"100\":\"0\",\"101\":\"0\",\"102\":\"0.000000\"},{\"1\":\"7\",\"2\":\"OFFENSE\",\"3\":\"0\",\"4\":\"0\",\"5\":\"0\",\"6\":\"0.000000\",\"7\":\"0\",\"8\":\"0\",\"9\":\"0\",\"10\":\"0\",\"11\":\"0\",\"12\":\"0\",\"13\":\"0\",\"14\":\"0\",\"15\":\"0\",\"16\":\"0\",\"17\":\"0\",\"18\":\"0\",\"19\":\"0\",\"20\":\"0.000000\",\"21\":\"0.000000\",\"22\":\"0\",\"23\":\"0\",\"24\":\"0\",\"25\":\"0\",\"26\":\"0\",\"27\":\"0\",\"28\":\"0\",\"29\":\"0\",\"30\":\"0\",\"31\":\"0\",\"32\":\"0\",\"33\":\"0\",\"34\":\"0\",\"35\":\"0\",\"36\":\"0\",\"37\":\"1.174937\",\"38\":\"0\",\"39\":\"0\",\"40\":\"0\",\"41\":\"0\",\"42\":\"0\",\"43\":\"0\",\"44\":\"1.120382\",\"45\":\"0.000000\",\"46\":\"0\",\"47\":\"0\",\"48\":\"0.000000\",\"49\":\"0\",\"50\":\"0\",\"51\":\"0\",\"52\":\"0\",\"53\":\"0\",\"54\":\"0\",\"55\":\"0.0000000\",\"56\":\"0\",\"57\":\"0\",\"58\":\"0\",\"59\":\"0\",\"60\":\"0\",\"61\":\"0\",\"62\":\"0\",\"63\":\"0\",\"64\":\"0\",\"65\":\"0\",\"66\":\"0\",\"67\":\"0\",\"68\":\"0\",\"69\":\"0\",\"70\":\"0\",\"71\":\"0\",\"72\":\"0\",\"73\":\"0\",\"74\":\"0\",\"75\":\"0\",\"76\":\"0.000000\",\"77\":\"0\",\"78\":\"0\",\"79\":\"0\",\"80\":\"0\",\"81\":\"0\",\"82\":\"0.00000\",\"83\":\"0\",\"84\":\"0.000000\",\"85\":\"0.000000\",\"86\":\"0\",\"87\":\"0\",\"88\":\"0\",\"89\":\"0\",\"90\":\"0\",\"91\":\"0\",\"92\":\"0\",\"93\":\"0\",\"94\":\"0\",\"95\":\"0\",\"96\":\"0\",\"97\":\"0\",\"98\":\"0\",\"99\":\"0\",\"100\":\"0\",\"101\":\"0\",\"102\":\"1.412771\"},{\"1\":\"9\",\"2\":\"OFFENSE\",\"3\":\"0\",\"4\":\"0\",\"5\":\"0\",\"6\":\"1.687779\",\"7\":\"0\",\"8\":\"0\",\"9\":\"0\",\"10\":\"0\",\"11\":\"0\",\"12\":\"0\",\"13\":\"0\",\"14\":\"0\",\"15\":\"0\",\"16\":\"0\",\"17\":\"0\",\"18\":\"0\",\"19\":\"0\",\"20\":\"0.000000\",\"21\":\"0.000000\",\"22\":\"0\",\"23\":\"0\",\"24\":\"0\",\"25\":\"0\",\"26\":\"0\",\"27\":\"0\",\"28\":\"0\",\"29\":\"0\",\"30\":\"0\",\"31\":\"0\",\"32\":\"0\",\"33\":\"0\",\"34\":\"0\",\"35\":\"0\",\"36\":\"0\",\"37\":\"0.000000\",\"38\":\"0\",\"39\":\"0\",\"40\":\"0\",\"41\":\"0\",\"42\":\"0\",\"43\":\"0\",\"44\":\"0.000000\",\"45\":\"0.000000\",\"46\":\"0\",\"47\":\"0\",\"48\":\"0.000000\",\"49\":\"0\",\"50\":\"0\",\"51\":\"0\",\"52\":\"0\",\"53\":\"0\",\"54\":\"0\",\"55\":\"0.0000000\",\"56\":\"0\",\"57\":\"0\",\"58\":\"0\",\"59\":\"0\",\"60\":\"0\",\"61\":\"0\",\"62\":\"0\",\"63\":\"0\",\"64\":\"0\",\"65\":\"0\",\"66\":\"0\",\"67\":\"0\",\"68\":\"0\",\"69\":\"0\",\"70\":\"0\",\"71\":\"0\",\"72\":\"0\",\"73\":\"0\",\"74\":\"0\",\"75\":\"0\",\"76\":\"0.000000\",\"77\":\"0\",\"78\":\"0\",\"79\":\"0\",\"80\":\"0\",\"81\":\"0\",\"82\":\"0.00000\",\"83\":\"0\",\"84\":\"0.000000\",\"85\":\"2.196699\",\"86\":\"0\",\"87\":\"0\",\"88\":\"0\",\"89\":\"0\",\"90\":\"0\",\"91\":\"0\",\"92\":\"0\",\"93\":\"0\",\"94\":\"0\",\"95\":\"0\",\"96\":\"0\",\"97\":\"0\",\"98\":\"0\",\"99\":\"0\",\"100\":\"0\",\"101\":\"0\",\"102\":\"0.000000\"},{\"1\":\"10\",\"2\":\"OFFENSE\",\"3\":\"0\",\"4\":\"0\",\"5\":\"0\",\"6\":\"0.000000\",\"7\":\"0\",\"8\":\"0\",\"9\":\"0\",\"10\":\"0\",\"11\":\"0\",\"12\":\"0\",\"13\":\"0\",\"14\":\"0\",\"15\":\"0\",\"16\":\"0\",\"17\":\"0\",\"18\":\"0\",\"19\":\"0\",\"20\":\"3.397601\",\"21\":\"0.000000\",\"22\":\"0\",\"23\":\"0\",\"24\":\"0\",\"25\":\"0\",\"26\":\"0\",\"27\":\"0\",\"28\":\"0\",\"29\":\"0\",\"30\":\"0\",\"31\":\"0\",\"32\":\"0\",\"33\":\"0\",\"34\":\"0\",\"35\":\"0\",\"36\":\"0\",\"37\":\"0.000000\",\"38\":\"0\",\"39\":\"0\",\"40\":\"0\",\"41\":\"0\",\"42\":\"0\",\"43\":\"0\",\"44\":\"0.000000\",\"45\":\"0.000000\",\"46\":\"0\",\"47\":\"0\",\"48\":\"0.000000\",\"49\":\"0\",\"50\":\"0\",\"51\":\"0\",\"52\":\"0\",\"53\":\"0\",\"54\":\"0\",\"55\":\"0.0000000\",\"56\":\"0\",\"57\":\"0\",\"58\":\"0\",\"59\":\"0\",\"60\":\"0\",\"61\":\"0\",\"62\":\"0\",\"63\":\"0\",\"64\":\"0\",\"65\":\"0\",\"66\":\"0\",\"67\":\"0\",\"68\":\"0\",\"69\":\"0\",\"70\":\"0\",\"71\":\"0\",\"72\":\"0\",\"73\":\"0\",\"74\":\"0\",\"75\":\"0\",\"76\":\"0.000000\",\"77\":\"0\",\"78\":\"0\",\"79\":\"0\",\"80\":\"0\",\"81\":\"0\",\"82\":\"0.00000\",\"83\":\"0\",\"84\":\"0.000000\",\"85\":\"0.000000\",\"86\":\"0\",\"87\":\"0\",\"88\":\"0\",\"89\":\"0\",\"90\":\"0\",\"91\":\"0\",\"92\":\"0\",\"93\":\"0\",\"94\":\"0\",\"95\":\"0\",\"96\":\"0\",\"97\":\"0\",\"98\":\"0\",\"99\":\"0\",\"100\":\"0\",\"101\":\"0\",\"102\":\"0.000000\"},{\"1\":\"12\",\"2\":\"OFFENSE\",\"3\":\"0\",\"4\":\"0\",\"5\":\"0\",\"6\":\"0.000000\",\"7\":\"0\",\"8\":\"0\",\"9\":\"0\",\"10\":\"0\",\"11\":\"0\",\"12\":\"0\",\"13\":\"0\",\"14\":\"0\",\"15\":\"0\",\"16\":\"0\",\"17\":\"0\",\"18\":\"0\",\"19\":\"0\",\"20\":\"0.000000\",\"21\":\"0.000000\",\"22\":\"0\",\"23\":\"0\",\"24\":\"0\",\"25\":\"0\",\"26\":\"0\",\"27\":\"0\",\"28\":\"0\",\"29\":\"0\",\"30\":\"0\",\"31\":\"0\",\"32\":\"0\",\"33\":\"0\",\"34\":\"0\",\"35\":\"0\",\"36\":\"0\",\"37\":\"0.000000\",\"38\":\"0\",\"39\":\"0\",\"40\":\"0\",\"41\":\"0\",\"42\":\"0\",\"43\":\"0\",\"44\":\"0.000000\",\"45\":\"1.108688\",\"46\":\"0\",\"47\":\"0\",\"48\":\"1.073983\",\"49\":\"0\",\"50\":\"0\",\"51\":\"0\",\"52\":\"0\",\"53\":\"0\",\"54\":\"0\",\"55\":\"0.6122061\",\"56\":\"0\",\"57\":\"0\",\"58\":\"0\",\"59\":\"0\",\"60\":\"0\",\"61\":\"0\",\"62\":\"0\",\"63\":\"0\",\"64\":\"0\",\"65\":\"0\",\"66\":\"0\",\"67\":\"0\",\"68\":\"0\",\"69\":\"0\",\"70\":\"0\",\"71\":\"0\",\"72\":\"0\",\"73\":\"0\",\"74\":\"0\",\"75\":\"0\",\"76\":\"0.000000\",\"77\":\"0\",\"78\":\"0\",\"79\":\"0\",\"80\":\"0\",\"81\":\"0\",\"82\":\"0.00000\",\"83\":\"0\",\"84\":\"0.000000\",\"85\":\"0.000000\",\"86\":\"0\",\"87\":\"0\",\"88\":\"0\",\"89\":\"0\",\"90\":\"0\",\"91\":\"0\",\"92\":\"0\",\"93\":\"0\",\"94\":\"0\",\"95\":\"0\",\"96\":\"0\",\"97\":\"0\",\"98\":\"0\",\"99\":\"0\",\"100\":\"0\",\"101\":\"0\",\"102\":\"0.000000\"},{\"1\":\"17\",\"2\":\"OFFENSE\",\"3\":\"0\",\"4\":\"0\",\"5\":\"0\",\"6\":\"0.000000\",\"7\":\"0\",\"8\":\"0\",\"9\":\"0\",\"10\":\"0\",\"11\":\"0\",\"12\":\"0\",\"13\":\"0\",\"14\":\"0\",\"15\":\"0\",\"16\":\"0\",\"17\":\"0\",\"18\":\"0\",\"19\":\"0\",\"20\":\"0.000000\",\"21\":\"1.166537\",\"22\":\"0\",\"23\":\"0\",\"24\":\"0\",\"25\":\"0\",\"26\":\"0\",\"27\":\"0\",\"28\":\"0\",\"29\":\"0\",\"30\":\"0\",\"31\":\"0\",\"32\":\"0\",\"33\":\"0\",\"34\":\"0\",\"35\":\"0\",\"36\":\"0\",\"37\":\"0.000000\",\"38\":\"0\",\"39\":\"0\",\"40\":\"0\",\"41\":\"0\",\"42\":\"0\",\"43\":\"0\",\"44\":\"0.000000\",\"45\":\"0.000000\",\"46\":\"0\",\"47\":\"0\",\"48\":\"0.000000\",\"49\":\"0\",\"50\":\"0\",\"51\":\"0\",\"52\":\"0\",\"53\":\"0\",\"54\":\"0\",\"55\":\"0.0000000\",\"56\":\"0\",\"57\":\"0\",\"58\":\"0\",\"59\":\"0\",\"60\":\"0\",\"61\":\"0\",\"62\":\"0\",\"63\":\"0\",\"64\":\"0\",\"65\":\"0\",\"66\":\"0\",\"67\":\"0\",\"68\":\"0\",\"69\":\"0\",\"70\":\"0\",\"71\":\"0\",\"72\":\"0\",\"73\":\"0\",\"74\":\"0\",\"75\":\"0\",\"76\":\"1.163786\",\"77\":\"0\",\"78\":\"0\",\"79\":\"0\",\"80\":\"0\",\"81\":\"0\",\"82\":\"1.25166\",\"83\":\"0\",\"84\":\"0.000000\",\"85\":\"0.000000\",\"86\":\"0\",\"87\":\"0\",\"88\":\"0\",\"89\":\"0\",\"90\":\"0\",\"91\":\"0\",\"92\":\"0\",\"93\":\"0\",\"94\":\"0\",\"95\":\"0\",\"96\":\"0\",\"97\":\"0\",\"98\":\"0\",\"99\":\"0\",\"100\":\"0\",\"101\":\"0\",\"102\":\"0.000000\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n\n### Modellspezifikation\n\nWir definiere einen Naive-Bayes-Algorithmus:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnb_spec <- naive_Bayes() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"naivebayes\")\n\nnb_spec\n## Naive Bayes Model Specification (classification)\n## \n## Computational engine: naivebayes\n```\n:::\n\n\n\n\nUnd setzen auf die 5-fache Kreuzvalidierung.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nfolds1 <- vfold_cv(d_train, v = 5)\n```\n:::\n\n\n\n\n### Workflow\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf1 <-\n  workflow() %>% \n  add_recipe(rec1) %>% \n  add_model(nb_spec)\n\nwf1\n## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: naive_Bayes()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 5 Recipe Steps\n## \n## • step_tokenize()\n## • step_stopwords()\n## • step_stem()\n## • step_tokenfilter()\n## • step_tfidf()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Naive Bayes Model Specification (classification)\n## \n## Computational engine: naivebayes\n```\n:::\n\n\n\n### Fitting\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit1 <-\n  fit_resamples(\n    wf1,\n    folds1,\n    control = control_resamples(save_pred = TRUE)\n  )\n```\n:::\n\n\nDie Vorhersagen speichern wir ab,\num die Performanz in den Faltungen des Hold-out-Samples zu berechnen.\n\n\nMöchte man sich die Zeit sparen, die Syntax wieder durchlaufen zu lassen,\nkann man das Objekt speichern. \nAber Vorsicht: Dabei kann es passieren, dass man mit veralteten Objekten arbeitet.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#write_rds(fit1, \"objects/chap-klassifik/chap_classific_fit1.rds\")\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n### Performanz\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf1_performance <-\n  collect_metrics(fit1)\n\nwf1_performance\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_preds <-\n  collect_predictions(fit1)\n\nwf_preds %>% \n  group_by(id) %>% \n  roc_curve(truth = c1, .pred_OFFENSE) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](080-klassifikation_files/figure-html/wf1-preds-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_mat_resampled(fit1, tidy = FALSE) %>% \n  autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](080-klassifikation_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Nullmodell\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnull_classification <- \n  parsnip::null_model() %>%\n  set_engine(\"parsnip\") %>%\n  set_mode(\"classification\")\n\nnull_rs <- workflow() %>%\n  add_recipe(rec1) %>%\n  add_model(null_classification) %>%\n  fit_resamples(\n    folds1\n  )\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\nHier ist die Performanz des Nullmodells.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnull_rs %>%\n  collect_metrics()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(null_rs)\n## Warning: No value of `metric` was given; metric 'roc_auc' will be used.\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"penalty\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".metric\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimator\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"mean\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"n\"],\"name\":[5],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"std_err\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".config\"],\"name\":[7],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"0.0017433288\",\"2\":\"roc_auc\",\"3\":\"binary\",\"4\":\"0.8116297\",\"5\":\"10\",\"6\":\"0.008710636\",\"7\":\"Preprocessor1_Model22\"},{\"1\":\"0.0007880463\",\"2\":\"roc_auc\",\"3\":\"binary\",\"4\":\"0.8111630\",\"5\":\"10\",\"6\":\"0.008941393\",\"7\":\"Preprocessor1_Model21\"},{\"1\":\"0.0003562248\",\"2\":\"roc_auc\",\"3\":\"binary\",\"4\":\"0.8103737\",\"5\":\"10\",\"6\":\"0.009238082\",\"7\":\"Preprocessor1_Model20\"},{\"1\":\"0.0001610262\",\"2\":\"roc_auc\",\"3\":\"binary\",\"4\":\"0.8099159\",\"5\":\"10\",\"6\":\"0.009444783\",\"7\":\"Preprocessor1_Model19\"},{\"1\":\"0.0000000001\",\"2\":\"roc_auc\",\"3\":\"binary\",\"4\":\"0.8095907\",\"5\":\"10\",\"6\":\"0.009602864\",\"7\":\"Preprocessor1_Model01\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n\n\n## Workflow 2a: Textfeatures + Naive-Bayes\n\n\n\n\n### Rezept\n\n\nTesten wir die Funktionen, die wir im Folgenden nutzen, um Text-Features zu erzeugen:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndummy <- c(\"hallo\", \"baby\", \"fatal\")\n\ncount_profane(dummy) \n## [1] 1\n\ncount_emo_words(dummy)\n## [1] 1\n\ndummy <- c(\"baby\", \"und\", \"🆗\", \"🖕\")\n\ncount_emojis(dummy)\n## [1] 0\n\ncount_wild_emojis(dummy) \n## [1] 0\n```\n:::\n\n\nRezept definieren:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec2a <- \n  recipe(c1 ~ ., data = select(d_train, text, c1, id)) %>% \n  update_role(id, new_role = \"id\") %>% \n  step_text_normalization(text) %>%   # UTF8-Normalisiuerung\n  step_mutate(text_copy = text,\n              profane_n = map_int(text, count_profane),\n              emo_words_n = map_int(text, count_emo_words),\n              emojis_n = map_int(text, count_emojis),\n              wild_emojis_n = map_int(text, count_wild_emojis)\n  ) %>% \n  step_textfeature(text_copy) %>% \n  step_tokenize(text, token = \"words\") %>% \n  #step_stem(text) %>% \n  step_tokenfilter(text, max_tokens = 1e2) %>%  # wir behalten nur die häufigsten Tokens\n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") |> \n  step_tfidf(text)\nrec2a\n## \n## ── Recipe ──────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:   1\n## predictor: 1\n## id:        1\n## \n## ── Operations\n## • Text Normalization for: text\n## • Variable mutation for: text, map_int(text, count_profane), ...\n## • Text feature extraction for: text_copy\n## • Tokenization for: text\n## • Text filtering for: text\n## • Stop word removal for: text\n## • Term frequency-inverse document frequency with: text\n```\n:::\n\n\nEinige Hinweise zum Rezept:\n\n- Das Tokenisieren kommt erst nach den Textfeatures.\n- Da `textfeatures` die Spalte `text` \"aufisst\", legen wir eine Kopie der Spalte an.\n- Man darf nicht vergessen, jedem Step zu sagen, auf welche Spalten er sich beziehen soll (z.B. `text`).\n\nPreppen:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec2a_prepped <- prep(rec2a)\n```\n:::\n\n\nUnd backen:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_rec2a <- bake(rec2a_prepped, new_data = NULL)\n```\n:::\n\n\n\nOder importieren aus der Konserve:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# write_rds(d_rec2a, file = \"objects/chap-klassifik/d_rec2a.rds\")\nd_rec2a <- read_rds(file = \"objects/chap-klassifik/d_rec2a.rds\")\ndim(d_rec2a)\n## [1] 3756   58\nhead(d_rec2a)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"id\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"c1\"],\"name\":[2],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"profane_n\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"emo_words_n\"],\"name\":[4],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"emojis_n\"],\"name\":[5],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"wild_emojis_n\"],\"name\":[6],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"textfeature_text_copy_n_words\"],\"name\":[7],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"textfeature_text_copy_n_uq_words\"],\"name\":[8],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"textfeature_text_copy_n_charS\"],\"name\":[9],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"textfeature_text_copy_n_uq_charS\"],\"name\":[10],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"textfeature_text_copy_n_digits\"],\"name\":[11],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"textfeature_text_copy_n_hashtags\"],\"name\":[12],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"textfeature_text_copy_n_uq_hashtags\"],\"name\":[13],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"textfeature_text_copy_n_mentions\"],\"name\":[14],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"textfeature_text_copy_n_uq_mentions\"],\"name\":[15],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"textfeature_text_copy_n_commas\"],\"name\":[16],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"textfeature_text_copy_n_periods\"],\"name\":[17],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"textfeature_text_copy_n_exclaims\"],\"name\":[18],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"textfeature_text_copy_n_extraspaces\"],\"name\":[19],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"textfeature_text_copy_n_caps\"],\"name\":[20],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"textfeature_text_copy_n_lowers\"],\"name\":[21],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"textfeature_text_copy_n_urls\"],\"name\":[22],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"textfeature_text_copy_n_uq_urls\"],\"name\":[23],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"textfeature_text_copy_n_nonasciis\"],\"name\":[24],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"textfeature_text_copy_n_puncts\"],\"name\":[25],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"textfeature_text_copy_politeness\"],\"name\":[26],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"textfeature_text_copy_first_person\"],\"name\":[27],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"textfeature_text_copy_first_personp\"],\"name\":[28],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"textfeature_text_copy_second_person\"],\"name\":[29],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"textfeature_text_copy_second_personp\"],\"name\":[30],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"textfeature_text_copy_third_person\"],\"name\":[31],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"textfeature_text_copy_to_be\"],\"name\":[32],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"textfeature_text_copy_prepositions\"],\"name\":[33],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_afd\"],\"name\":[34],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_amp\"],\"name\":[35],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_d\"],\"name\":[36],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_dass\"],\"name\":[37],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_deutsche\"],\"name\":[38],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_deutschen\"],\"name\":[39],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_deutschland\"],\"name\":[40],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_geht\"],\"name\":[41],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_gibt\"],\"name\":[42],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_heute\"],\"name\":[43],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_immer\"],\"name\":[44],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_ja\"],\"name\":[45],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_krippmarie\"],\"name\":[46],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_land\"],\"name\":[47],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_lbr\"],\"name\":[48],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_mal\"],\"name\":[49],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_mehr\"],\"name\":[50],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_merkel\"],\"name\":[51],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_müssen\"],\"name\":[52],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_schon\"],\"name\":[53],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_spd\"],\"name\":[54],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_u\"],\"name\":[55],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_unsere\"],\"name\":[56],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_welt\"],\"name\":[57],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tfidf_text_wer\"],\"name\":[58],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"5\",\"2\":\"OFFENSE\",\"3\":\"1\",\"4\":\"0\",\"5\":\"0\",\"6\":\"0\",\"7\":\"16\",\"8\":\"16\",\"9\":\"121\",\"10\":\"31\",\"11\":\"0\",\"12\":\"1\",\"13\":\"1\",\"14\":\"1\",\"15\":\"1\",\"16\":\"0\",\"17\":\"2\",\"18\":\"0\",\"19\":\"0\",\"20\":\"8\",\"21\":\"108\",\"22\":\"0\",\"23\":\"0\",\"24\":\"0\",\"25\":\"3\",\"26\":\"0\",\"27\":\"0\",\"28\":\"0\",\"29\":\"0\",\"30\":\"0\",\"31\":\"0\",\"32\":\"0\",\"33\":\"0\",\"34\":\"0.000000\",\"35\":\"0\",\"36\":\"0\",\"37\":\"0\",\"38\":\"0.000000\",\"39\":\"0\",\"40\":\"0\",\"41\":\"0.000000\",\"42\":\"0\",\"43\":\"0.000000\",\"44\":\"0.000000\",\"45\":\"0.0000\",\"46\":\"0.000000\",\"47\":\"0\",\"48\":\"0.0000000\",\"49\":\"0\",\"50\":\"0\",\"51\":\"0\",\"52\":\"0\",\"53\":\"0\",\"54\":\"3.671899\",\"55\":\"0\",\"56\":\"0\",\"57\":\"0\",\"58\":\"0\"},{\"1\":\"7\",\"2\":\"OFFENSE\",\"3\":\"1\",\"4\":\"0\",\"5\":\"0\",\"6\":\"0\",\"7\":\"32\",\"8\":\"28\",\"9\":\"145\",\"10\":\"29\",\"11\":\"4\",\"12\":\"0\",\"13\":\"0\",\"14\":\"1\",\"15\":\"1\",\"16\":\"0\",\"17\":\"2\",\"18\":\"0\",\"19\":\"0\",\"20\":\"4\",\"21\":\"134\",\"22\":\"0\",\"23\":\"0\",\"24\":\"0\",\"25\":\"1\",\"26\":\"0\",\"27\":\"0\",\"28\":\"0\",\"29\":\"0\",\"30\":\"0\",\"31\":\"0\",\"32\":\"0\",\"33\":\"0\",\"34\":\"0.000000\",\"35\":\"0\",\"36\":\"0\",\"37\":\"0\",\"38\":\"0.000000\",\"39\":\"0\",\"40\":\"0\",\"41\":\"1.821262\",\"42\":\"0\",\"43\":\"1.673477\",\"44\":\"0.000000\",\"45\":\"0.0000\",\"46\":\"0.000000\",\"47\":\"0\",\"48\":\"0.0000000\",\"49\":\"0\",\"50\":\"0\",\"51\":\"0\",\"52\":\"0\",\"53\":\"0\",\"54\":\"0.000000\",\"55\":\"0\",\"56\":\"0\",\"57\":\"0\",\"58\":\"0\"},{\"1\":\"9\",\"2\":\"OFFENSE\",\"3\":\"0\",\"4\":\"1\",\"5\":\"0\",\"6\":\"0\",\"7\":\"12\",\"8\":\"12\",\"9\":\"66\",\"10\":\"29\",\"11\":\"0\",\"12\":\"1\",\"13\":\"1\",\"14\":\"1\",\"15\":\"1\",\"16\":\"0\",\"17\":\"1\",\"18\":\"0\",\"19\":\"0\",\"20\":\"9\",\"21\":\"53\",\"22\":\"0\",\"23\":\"0\",\"24\":\"0\",\"25\":\"3\",\"26\":\"0\",\"27\":\"0\",\"28\":\"0\",\"29\":\"0\",\"30\":\"0\",\"31\":\"0\",\"32\":\"0\",\"33\":\"0\",\"34\":\"3.361147\",\"35\":\"0\",\"36\":\"0\",\"37\":\"0\",\"38\":\"0.000000\",\"39\":\"0\",\"40\":\"0\",\"41\":\"0.000000\",\"42\":\"0\",\"43\":\"0.000000\",\"44\":\"0.000000\",\"45\":\"0.0000\",\"46\":\"0.000000\",\"47\":\"0\",\"48\":\"0.0000000\",\"49\":\"0\",\"50\":\"0\",\"51\":\"0\",\"52\":\"0\",\"53\":\"0\",\"54\":\"0.000000\",\"55\":\"0\",\"56\":\"0\",\"57\":\"0\",\"58\":\"0\"},{\"1\":\"10\",\"2\":\"OFFENSE\",\"3\":\"0\",\"4\":\"0\",\"5\":\"0\",\"6\":\"0\",\"7\":\"15\",\"8\":\"15\",\"9\":\"119\",\"10\":\"30\",\"11\":\"0\",\"12\":\"0\",\"13\":\"0\",\"14\":\"0\",\"15\":\"0\",\"16\":\"2\",\"17\":\"0\",\"18\":\"0\",\"19\":\"0\",\"20\":\"6\",\"21\":\"108\",\"22\":\"0\",\"23\":\"0\",\"24\":\"0\",\"25\":\"3\",\"26\":\"0\",\"27\":\"0\",\"28\":\"0\",\"29\":\"0\",\"30\":\"0\",\"31\":\"0\",\"32\":\"0\",\"33\":\"0\",\"34\":\"0.000000\",\"35\":\"0\",\"36\":\"0\",\"37\":\"0\",\"38\":\"3.604721\",\"39\":\"0\",\"40\":\"0\",\"41\":\"0.000000\",\"42\":\"0\",\"43\":\"0.000000\",\"44\":\"0.000000\",\"45\":\"0.0000\",\"46\":\"0.000000\",\"47\":\"0\",\"48\":\"0.0000000\",\"49\":\"0\",\"50\":\"0\",\"51\":\"0\",\"52\":\"0\",\"53\":\"0\",\"54\":\"0.000000\",\"55\":\"0\",\"56\":\"0\",\"57\":\"0\",\"58\":\"0\"},{\"1\":\"12\",\"2\":\"OFFENSE\",\"3\":\"0\",\"4\":\"0\",\"5\":\"0\",\"6\":\"0\",\"7\":\"26\",\"8\":\"25\",\"9\":\"134\",\"10\":\"39\",\"11\":\"2\",\"12\":\"0\",\"13\":\"0\",\"14\":\"1\",\"15\":\"1\",\"16\":\"0\",\"17\":\"3\",\"18\":\"0\",\"19\":\"0\",\"20\":\"12\",\"21\":\"111\",\"22\":\"0\",\"23\":\"0\",\"24\":\"0\",\"25\":\"6\",\"26\":\"0\",\"27\":\"0\",\"28\":\"0\",\"29\":\"0\",\"30\":\"0\",\"31\":\"0\",\"32\":\"0\",\"33\":\"0\",\"34\":\"0.000000\",\"35\":\"0\",\"36\":\"0\",\"37\":\"0\",\"38\":\"0.000000\",\"39\":\"0\",\"40\":\"0\",\"41\":\"0.000000\",\"42\":\"0\",\"43\":\"0.000000\",\"44\":\"1.122775\",\"45\":\"1.0823\",\"46\":\"0.000000\",\"47\":\"0\",\"48\":\"0.6210228\",\"49\":\"0\",\"50\":\"0\",\"51\":\"0\",\"52\":\"0\",\"53\":\"0\",\"54\":\"0.000000\",\"55\":\"0\",\"56\":\"0\",\"57\":\"0\",\"58\":\"0\"},{\"1\":\"27\",\"2\":\"OFFENSE\",\"3\":\"0\",\"4\":\"0\",\"5\":\"0\",\"6\":\"0\",\"7\":\"15\",\"8\":\"15\",\"9\":\"101\",\"10\":\"31\",\"11\":\"0\",\"12\":\"0\",\"13\":\"0\",\"14\":\"1\",\"15\":\"1\",\"16\":\"0\",\"17\":\"4\",\"18\":\"0\",\"19\":\"0\",\"20\":\"6\",\"21\":\"88\",\"22\":\"0\",\"23\":\"0\",\"24\":\"0\",\"25\":\"3\",\"26\":\"0\",\"27\":\"0\",\"28\":\"0\",\"29\":\"0\",\"30\":\"0\",\"31\":\"0\",\"32\":\"0\",\"33\":\"0\",\"34\":\"0.000000\",\"35\":\"0\",\"36\":\"0\",\"37\":\"0\",\"38\":\"0.000000\",\"39\":\"0\",\"40\":\"0\",\"41\":\"0.000000\",\"42\":\"0\",\"43\":\"0.000000\",\"44\":\"0.000000\",\"45\":\"0.0000\",\"46\":\"3.671899\",\"47\":\"0\",\"48\":\"0.0000000\",\"49\":\"0\",\"50\":\"0\",\"51\":\"0\",\"52\":\"0\",\"53\":\"0\",\"54\":\"0.000000\",\"55\":\"0\",\"56\":\"0\",\"57\":\"0\",\"58\":\"0\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n### Workflow\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf2a <-\n  workflow() %>% \n  add_recipe(rec2a) %>% \n  add_model(nb_spec)\n\nwf2a\n## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: naive_Bayes()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 7 Recipe Steps\n## \n## • step_text_normalization()\n## • step_mutate()\n## • step_textfeature()\n## • step_tokenize()\n## • step_tokenfilter()\n## • step_stopwords()\n## • step_tfidf()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Naive Bayes Model Specification (classification)\n## \n## Computational engine: naivebayes\n```\n:::\n\n\n\n### Fitting\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit2a <-\n  fit_resamples(\n    wf2a,\n    resamples = vfold_cv(d_train, v = 5),\n    control = control_resamples(save_pred = TRUE)\n  )\n```\n:::\n\n\nDie Vorhersagen speichern wir ab,\num die Performanz in den Faltungen des Hold-out-Samples zu berechnen.\n\n\n:::.callout-caution\nMöchte man sich die Zeit sparen, die Syntax wieder durchlaufen zu lassen,\nkann man das Objekt speichern. \nAber Vorsicht: Dabei kann es passieren, dass man mit veralteten Objekten arbeitet. $\\square$\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#write_rds(fit2a, \"objects/chap-klassifik/chap_classific_fit2a.rds\")\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n### Performanz\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf2a_performance <-\n  collect_metrics(fit2a)\n\nwf2a_performance\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_preds_2a <-\n  collect_predictions(fit2a)\n\nwf_preds_2a %>% \n  group_by(id) %>% \n  roc_curve(truth = c1, .pred_OFFENSE) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](080-klassifikation_files/figure-html/wf2a-preds-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_mat_resampled(fit2a, tidy = FALSE) %>% \n  autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](080-klassifikation_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Workflow 2b: Textfeatures + Lasso\n\n\n### Modell: Lasso\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"glmnet\")\n\nlasso_spec\n## Logistic Regression Model Specification (classification)\n## \n## Main Arguments:\n##   penalty = tune()\n##   mixture = 1\n## \n## Computational engine: glmnet\n```\n:::\n\n\n\n\nWir definieren die Ausprägungen von `penalty`, \ndie wir ausprobieren wollen:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlambda_grid <- grid_regular(penalty(), levels = 3)  # hier nur 3 Werte, um Rechenzeit zu sparen\n```\n:::\n\n\n\n### Workflow\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf2b <-\n  workflow() %>% \n  add_recipe(rec1) %>% \n  add_model(lasso_spec)\n\nwf2b\n## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: logistic_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 5 Recipe Steps\n## \n## • step_tokenize()\n## • step_stopwords()\n## • step_stem()\n## • step_tokenfilter()\n## • step_tfidf()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Logistic Regression Model Specification (classification)\n## \n## Main Arguments:\n##   penalty = tune()\n##   mixture = 1\n## \n## Computational engine: glmnet\n```\n:::\n\n\n\nTunen und Fitten:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\n\nfit2b <-\n  tune_grid(\n    wf2b,\n    folds1,\n    grid = lambda_grid,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nfit2b\n```\n:::\n\n\n\n:::.callout-caution\nVorsicht beim Abspeichern von Objekten: Da kann man leicht vergessen, zu updaten. $\\square$\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#write_rds(fit2b, \"objects/chap-klassifik/chap_classific_fit2b.rds\")\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\nHier ist die Performanz:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(fit2b) %>% \n  filter(.metric == \"roc_auc\") %>% \n  slice_max(mean, n = 3)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(fit2b)\n```\n\n::: {.cell-output-display}\n![](080-klassifikation_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit2b %>% \n  show_best(\"roc_auc\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"penalty\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".metric\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimator\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"mean\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"n\"],\"name\":[5],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"std_err\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".config\"],\"name\":[7],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"1e-10\",\"2\":\"roc_auc\",\"3\":\"binary\",\"4\":\"0.6433326\",\"5\":\"5\",\"6\":\"0.008664343\",\"7\":\"Preprocessor1_Model1\"},{\"1\":\"1e-05\",\"2\":\"roc_auc\",\"3\":\"binary\",\"4\":\"0.6433326\",\"5\":\"5\",\"6\":\"0.008664343\",\"7\":\"Preprocessor1_Model2\"},{\"1\":\"1e+00\",\"2\":\"roc_auc\",\"3\":\"binary\",\"4\":\"0.5000000\",\"5\":\"5\",\"6\":\"0.000000000\",\"7\":\"Preprocessor1_Model3\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nchosen_auc <- \n  fit2b %>%\n  select_by_one_std_err(metric = \"roc_auc\", -penalty)\n```\n:::\n\n\n\n\nFinalisieren:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf2b_final <-\n  finalize_workflow(wf2b, chosen_auc)\n\nwf2b_final\n## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: logistic_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 5 Recipe Steps\n## \n## • step_tokenize()\n## • step_stopwords()\n## • step_stem()\n## • step_tokenfilter()\n## • step_tfidf()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Logistic Regression Model Specification (classification)\n## \n## Main Arguments:\n##   penalty = 1e-05\n##   mixture = 1\n## \n## Computational engine: glmnet\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit2b_final_train <-\n  fit(wf2b_final, d_train)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit2b_final_train %>% \n  extract_fit_parsnip() %>% \n  tidy() %>% \n  arrange(-abs(estimate)) %>% \n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"term\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"estimate\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"penalty\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"tfidf_text_feldenfrizz\",\"2\":\"12.304611\",\"3\":\"1e-05\"},{\"1\":\"tfidf_text_schmiddiemaik\",\"2\":\"-11.823955\",\"3\":\"1e-05\"},{\"1\":\"tfidf_text__macmik\",\"2\":\"9.414012\",\"3\":\"1e-05\"},{\"1\":\"tfidf_text_md_franz\",\"2\":\"-4.398868\",\"3\":\"1e-05\"},{\"1\":\"tfidf_text_thomasgbau\",\"2\":\"-3.634176\",\"3\":\"1e-05\"},{\"1\":\"tfidf_text_athinamala\",\"2\":\"-3.184619\",\"3\":\"1e-05\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit2b_final_test <-\n  last_fit(wf2b_final, d_split)\n\ncollect_metrics(fit2b_final_test)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\".metric\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimator\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimate\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".config\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"accuracy\",\"2\":\"binary\",\"3\":\"0.6687949\",\"4\":\"Preprocessor1_Model1\"},{\"1\":\"roc_auc\",\"2\":\"binary\",\"3\":\"0.6469109\",\"4\":\"Preprocessor1_Model1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n\n### Vorhersage\n\n\n### Vohersagedaten\n\nPfad zu den Daten:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntweet_data_path <- \"/Users/sebastiansaueruser/github-repos/hate-speech-data/data-raw/tweets/\"\n```\n:::\n\n\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntweet_data_files_names <- list.files(path = tweet_data_path,\n                                     pattern  = \"tweets-to-.*\\\\.rds$\")\nhead(tweet_data_files_names)\n## [1] \"tweets-to-Janine_Wissler_2021.rds\" \"tweets-to-Janine_Wissler_2022.rds\"\n## [3] \"tweets-to-MAStrackZi_2021.rds\"     \"tweets-to-MAStrackZi_2022.rds\"    \n## [5] \"tweets-to-schirdewan_2021.rds\"     \"tweets-to-schirdewan_2022.rds\"\n```\n:::\n\n\n\nWie viele Dateien sind es?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlength(tweet_data_files_names)\n## [1] 6\n```\n:::\n\n\n\nWir geben den Elementen des Vektors gängige Namen,\ndas hilft uns gleich bei `map`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(tweet_data_files_names) <- \n  str_remove(tweet_data_files_names, \"\\\\.rds$\")  # Datei-Suffix entfernen \n```\n:::\n\n\n\n\n\n\nOK, weiter: So können wir *eine* der Datendateien einlesen:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_raw <-\n  read_rds(file = paste0(tweet_data_path, tweet_data_files_names[1])) \n\nd <- \n  d_raw %>% \n  select(id, author_id, created_at, public_metrics) %>% \n  unnest_wider(public_metrics)\n\nhead(d)\n```\n:::\n\n\n\nUnd so lesen wir alle ein:\n\n\nZunächst erstellen wir uns eine Helper-Funktion:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread_and_select <- function(file_name, path_to_tweet_data = tweet_data_path) {\n  \n  out <- \n    read_rds(file = paste0(path_to_tweet_data, file_name)) %>% \n    select(id, author_id, created_at, text, public_metrics) %>% \n    unnest_wider(public_metrics)\n  \n  cat(\"Data file was read.\\n\")\n  \n  return(out)\n}\n```\n:::\n\n\nTesten:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd1 <- read_and_select(tweet_data_files_names[1])\n\nhead(d1)\n```\n:::\n\n\n\n\nDie Funktion `read_and_select`  mappen wir auf alle Datendateien:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nds <-\n  tweet_data_files_names %>% \n  map_dfr(read_and_select, .id = \"dataset\")\ntoc()\n```\n:::\n\n\n\n`214.531 sec elapsed`\n\nDa wir den Elementen von `tweet_data_files_names` Namen gegeben haben, \nfinden wir diese Namen praktischerweise wieder in `ds`.\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n\nVielleicht ist es zum Entwickeln besser,\nmit einem kleineren Datensatz einstweilen zu arbeiten:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nds_short <- read_rds(file = \"/Users/sebastiansaueruser/github-repos/hate-speech-data/objects/ds_short.rds\")  # 300kb\n\nds <- ds_short\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n\n### Vokabular erstellen\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nds_long <-\n  ds %>% \n  select(text) %>% \n  unnest_tokens(input = text, output = word)\n```\n:::\n\n\nPuh, das hat gedauert!\n\nSpeichern wir uns diese Daten daher auf die Festplatte:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#write_rds(ds_long, file = paste0(tweet_data_path, \"ds_long.rds\"))\n```\n:::\n\n\nEntfernen wir daraus die Duplikate,\num uns ein Vokabular zu erstellen:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nds_voc <-\n  ds_long %>% \n  distinct(word)\n\nhead(ds_voc)\n```\n:::\n\n\nInsgesamt umfasst `nrow(ds_voc)` Wörter.\n\nUnd das resultierende Objekt speichern wir wieder ab:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#write_rds(ds_voc, file = paste0(\"objects/\", \"ds_voc.rds\"))\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n## Worteinbettungen erstellen\n\n>    👨‍🏫 Hey ChatGPT, beschreibe Das FastText-Modell für Worteinbettungen.\n\n>    🤖 Ja, Moment...\n\n\n\n### Was ist das FastText-Modell? {#sec-fasttext}\n\nDas [FastText-Modell](https://fasttext.cc/) für Worteinbettungen (auch als Word Embeddings bezeichnet) ist ein NLP-Modell, das von Facebook AI Research entwickelt wurde. Es ist eine Erweiterung des klassischen Word2Vec-Modells und zielt darauf ab, Worte in Vektoren umzuwandeln, um semantische Informationen über Wörter zu erfassen. Der Hauptunterschied zwischen FastText und Word2Vec besteht darin, wie sie Worte darstellen.\n\nHier sind einige wichtige Merkmale und Unterschiede des FastText-Modells:\n\nSubword-Informationen: Während Word2Vec auf ganzen Wörtern basiert, behandelt FastText Wörter als eine Sammlung von Subwörtern (n-Grammen). Diese Subwörter sind buchstäbliche Teile eines Wortes, die für die Analyse verwendet werden. Zum Beispiel, für das Wort \"apple\" könnte FastText die Subwörter \"ap,\" \"app,\" \"ppl,\" \"ple\" usw. berücksichtigen. Dies ermöglicht es dem Modell, semantische Informationen auf Buchstabenebene zu erfassen und seltene Wörter oder Wörter, die nicht im Trainingskorpus enthalten sind, besser zu behandeln.\n\n\n\nPre-trained Modelle: Es stehen vorab trainierte FastText-Modelle in vielen Sprachen zur Verfügung. Diese vortrainierten Modelle können in verschiedenen NLP-Anwendungen verwendet werden, ohne von Grund auf neu trainiert werden zu müssen.\n\nDer Trainingsprozess für ein FastText-Modell umfasst das Lernen von Wortdarstellungen anhand großer Textkorpora. Die resultierenden Wortvektoren können verwendet werden, um semantische Beziehungen zwischen Wörtern zu erkunden, Ähnlichkeiten zu berechnen oder als Eingabe für maschinelle Lernmodelle in verschiedenen natürlichsprachlichen Verarbeitungsaufgaben zu dienen. Insgesamt hat FastText aufgrund seiner Fähigkeit zur Berücksichtigung von Subwörtern und seiner Effizienz bei der Verarbeitung großer Vokabulare in der NLP-Gemeinschaft an Popularität gewonnen.^[Der Text dieses Abschnitts wurde von ChatGPT 3.5 erstellt, 2023-10-29.]\n\n\n### Ein Fast-Text-Modell trainieren\n\n\nDefiniere die Konstanten für das fastText-Modell:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntexts <- ds %>% pull(text)\ntexts <- tolower(texts)\n```\n:::\n\n\n\nWir können uns ein FastText-Modell für Worteinbettungen basierend auf unseren Daten berechnen lassen:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#writeLines(text = texts, con = out_file_txt)\n#execute(commands = c(\"skipgram\", \"-input\", tmp_file_txt, \"-output\", out_file_model, \"-verbose\", 1))  # execute berechnet ein FastText-Mdlell\n```\n:::\n\n\nIm Standard werden 100 Dimensionen berechnet.\n\nHier ist schon ein vorab gespeichertes Modell, basierend auf den Twitter-Daten:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_file_txt <- \"/Users/sebastiansaueruser/datasets/Twitter/twitter-polit-model.vec\"\nout_file_model <- \"/Users/sebastiansaueruser/datasets/Twitter/twitter-polit-model.bin\"\n\nfile.exists(out_file_txt)\n## [1] TRUE\nfile.exists(out_file_model)\n## [1] TRUE\n```\n:::\n\n\n\n\n```\nRead 22M words\nNumber of words:  130328\nNumber of labels: 0\nProgress: 100.0% words/sec/thread:   49218 lr:  0.000000 avg.loss:  1.720812 ETA:   0h 0m 0s\n```\n\nJetzt laden wir das Modell von der Festplatte:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntwitter_fasttext_model <- load_model(out_file_model)\ndict <- get_dictionary(twitter_fasttext_model)\n```\n:::\n\n\nDas Wörterbuch umfasst 130328 (verschiedene) Wörter.\n\n\nSchauen wir uns einige Begriffe aus dem Vokabular an:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(head(dict, 10))\n##  [1] \"</s>\"            \"die\"             \"und\"             \"der\"            \n##  [5] \"sie\"             \"das\"             \"nicht\"           \"in\"             \n##  [9] \"ist\"             \"@_friedrichmerz\"\n```\n:::\n\n\nHier sind die ersten paar Elemente des Vektors für `menschen`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvec1 <- get_word_vectors(twitter_fasttext_model, c(\"menschen\"))\nvec1[1:10]\n```\n:::\n\n\n\n\n```\n [1]  0.14156282  0.44875699  0.23911817 -0.02580349  0.29811972  0.03870077\n [7]  0.06518744  0.22527063  0.28198120  0.39931887\n ```\n\n\nErstellen wir uns einen Tibble, der \nals erste Spalte das Vokabular und in den übrigen 100 Spalten die Dimensionen enthält:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nword_embedding_twitter <-\n  tibble(\n    word = dict\n  )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwords_vecs_twitter <-\n  get_word_vectors(twitter_fasttext_model)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nword_embedding_twitter <-\n  word_embedding_twitter %>% \n  bind_cols(words_vecs_twitter)\n\nnames(word_embedding_twitter) <- \n  c(\"word\", \n    paste0(\"v\", sprintf(\"%03d\", 1:100)))  # Namen verschönern mit fortlaufender Nummer von 0 bis 100\n```\n:::\n\n\n\nUnd als Worteinbettungsdatei abspeichern:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#write_rds(word_embedding_twitter, file = paste0(tweet_data_path, \"word_embedding_twitter.rds\"))\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"word\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"v001\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"v002\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"v003\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"v004\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"<\\/s>\",\"2\":\"0.17330834\",\"3\":\"0.07820062\",\"4\":\"-0.05277666\",\"5\":\"0.0862241\"},{\"1\":\"die\",\"2\":\"0.48641139\",\"3\":\"0.03693734\",\"4\":\"-0.01204743\",\"5\":\"0.1565817\"},{\"1\":\"und\",\"2\":\"0.39090732\",\"3\":\"-0.02645574\",\"4\":\"0.11355034\",\"5\":\"0.1545031\"},{\"1\":\"der\",\"2\":\"0.37105647\",\"3\":\"-0.33681911\",\"4\":\"0.05018367\",\"5\":\"0.2029566\"},{\"1\":\"sie\",\"2\":\"0.45957077\",\"3\":\"0.04141690\",\"4\":\"0.23099238\",\"5\":\"0.1809430\"},{\"1\":\"das\",\"2\":\"0.07037771\",\"3\":\"-0.07423849\",\"4\":\"0.04087655\",\"5\":\"0.1506333\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n\n### Aufbereiten\n\nAm besten nur die Spalten (unseres Twitter-Datensatzes) behalten,\ndie wir zum Modellieren nutzen:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nds_short2 <-\n  ds_short %>% \n  select(text, id)\n```\n:::\n\n\n\nWir definieren ein Rezept `rec_word_embed` (basierend auf `rec1`):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_word_embed <- \n  recipe(c1 ~ ., data = select(d_train, text, c1, id)) %>% \n  update_role(id, new_role = \"id\") %>% \n  step_tokenize(text) %>% \n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %>% \n  step_stem(text) %>% \n  step_tokenfilter(text, max_tokens = 1e2) %>%  # wir behalten nur die häufigsten Tokens\n  step_word_embeddings(text, embeddings = word_embedding_twitter)\n```\n:::\n\n\n\nDann preppen und backen:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_word_embed_prepped <-\n  prep(rec_word_embed)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nds_baked_rec1a <- bake(rec_word_embed_prepped, new_data = ds_short2)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nds_baked_rec1a %>% `[`(1:10, 1:10)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"id\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"wordembed_text_v001\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"wordembed_text_v002\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"wordembed_text_v003\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"wordembed_text_v004\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"wordembed_text_v005\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"wordembed_text_v006\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"wordembed_text_v007\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"wordembed_text_v008\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"wordembed_text_v009\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"NA\",\"2\":\"0.23893653\",\"3\":\"0.014598664\",\"4\":\"-0.2222376\",\"5\":\"0.48163152\",\"6\":\"0.20162874\",\"7\":\"-0.12450162\",\"8\":\"0.23333058\",\"9\":\"-0.226192281\",\"10\":\"-0.027382970\"},{\"1\":\"NA\",\"2\":\"0.32377085\",\"3\":\"0.695485767\",\"4\":\"0.4336860\",\"5\":\"0.28993332\",\"6\":\"-0.17770833\",\"7\":\"-0.74807747\",\"8\":\"1.49294108\",\"9\":\"1.064348385\",\"10\":\"0.820613639\"},{\"1\":\"NA\",\"2\":\"0.21142043\",\"3\":\"0.783711389\",\"4\":\"-1.3797138\",\"5\":\"0.28518000\",\"6\":\"0.33366236\",\"7\":\"-0.24040961\",\"8\":\"0.19629040\",\"9\":\"0.084888689\",\"10\":\"-0.186157003\"},{\"1\":\"NA\",\"2\":\"0.71277493\",\"3\":\"-0.304109801\",\"4\":\"-0.4309491\",\"5\":\"0.05916093\",\"6\":\"0.04967718\",\"7\":\"0.10116307\",\"8\":\"0.34288836\",\"9\":\"0.004611025\",\"10\":\"0.875826895\"},{\"1\":\"NA\",\"2\":\"0.00000000\",\"3\":\"0.000000000\",\"4\":\"0.0000000\",\"5\":\"0.00000000\",\"6\":\"0.00000000\",\"7\":\"0.00000000\",\"8\":\"0.00000000\",\"9\":\"0.000000000\",\"10\":\"0.000000000\"},{\"1\":\"NA\",\"2\":\"0.00000000\",\"3\":\"0.000000000\",\"4\":\"0.0000000\",\"5\":\"0.00000000\",\"6\":\"0.00000000\",\"7\":\"0.00000000\",\"8\":\"0.00000000\",\"9\":\"0.000000000\",\"10\":\"0.000000000\"},{\"1\":\"NA\",\"2\":\"0.61656542\",\"3\":\"-0.006150134\",\"4\":\"-0.7271604\",\"5\":\"-0.10712643\",\"6\":\"0.96813375\",\"7\":\"-0.03373864\",\"8\":\"0.08099335\",\"9\":\"0.201609042\",\"10\":\"1.104397699\"},{\"1\":\"NA\",\"2\":\"0.04866463\",\"3\":\"-0.441774538\",\"4\":\"-0.3122953\",\"5\":\"0.05295798\",\"6\":\"0.78680488\",\"7\":\"0.09914146\",\"8\":\"0.01339768\",\"9\":\"-0.094183717\",\"10\":\"0.029520422\"},{\"1\":\"NA\",\"2\":\"0.00000000\",\"3\":\"0.000000000\",\"4\":\"0.0000000\",\"5\":\"0.00000000\",\"6\":\"0.00000000\",\"7\":\"0.00000000\",\"8\":\"0.00000000\",\"9\":\"0.000000000\",\"10\":\"0.000000000\"},{\"1\":\"NA\",\"2\":\"0.21170381\",\"3\":\"-0.291642219\",\"4\":\"-0.3079664\",\"5\":\"-0.17432338\",\"6\":\"0.52147198\",\"7\":\"-0.06467295\",\"8\":\"0.21021771\",\"9\":\"-0.425262958\",\"10\":\"0.006601942\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\nIst das nicht komfortabel?\nDas Textrezept übernimmt die Arbeit für uns,\nmit den richtigen Features zu arbeiten,\nund die Wort-Einbettungen für die richtigen Wörter bereitzustellen.\n\nWer dem Frieden nicht traut,\ndem sei geraten, nachzuprüfen 🤓.\n\n\n\n## Workflow 3: Worteinbettungen + Lasso\n\n### Daten aufteilen\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_split <- initial_split(d2, strata = c1)\n\nd_train <- training(d_split)\nd_test <- testing(d_split)\n```\n:::\n\n\n\n\n### Hilfsfunktionen\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"funs/helper-funs-recipes.R\")\n```\n:::\n\n\n\n\n\n\n### Rezept mit Worteinbettungen\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec2 <- \n  recipe(c1 ~ ., data = select(d_train, text, c1, id)) %>% \n  update_role(id, new_role = \"id\") %>% \n  step_text_normalization(text) %>% \n  step_mutate(text_copy = text,\n              profane_n = map_int(text, count_profane),\n              emo_words_n = map_int(text, count_emo_words),\n              emojis_n = map_int(text, count_emojis),\n              wild_emojis_n = map_int(text, count_wild_emojis)\n  ) %>% \n  step_textfeature(text_copy) %>% \n  step_tokenize(text, token = \"words\") %>% \n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %>% \n  step_word_embeddings(text, embeddings = word_embedding_twitter)\n \nrec2\n## \n## ── Recipe ──────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:   1\n## predictor: 1\n## id:        1\n## \n## ── Operations\n## • Text Normalization for: text\n## • Variable mutation for: text, map_int(text, count_profane), ...\n## • Text feature extraction for: text_copy\n## • Tokenization for: text\n## • Stop word removal for: text\n## • Word embeddings aggregated from: text\n```\n:::\n\n\n\n\n\nJetzt preppen:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec2_prepped <- prep(rec2)\n```\n:::\n\n\n\n\n\nVielleicht macht es Sinn, sich das Objekt zur späteren\nVerwendung abzuspeichern.^[Aber Vorsicht beim Abspeichern, man könnte versehentlich mit einer veralteten Version weiterarbeiten.]\n`Feather` verarbeitet nur Dataframes,\ndaher nutzen wir hier RDS.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#write_rds(rec2_prepped, file = \"~/datasets/Twitter/klassifik-rec2-prepped.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrec2_prepped <- read_rds(\"/Users/sebastiansaueruser/github-repos/hate-speech-data/objects/rec2_prepped.rds\")\n```\n:::\n\n\n\n\n\nDas Element `rec2_prepped` ist recht groß:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nformat(object.size(rec2_prepped), units  = \"Mb\")\n## [1] \"113.8 Mb\"\n```\n:::\n\n\nJetzt können wir das präparierte (\"gepreppte\") Rezept \"backen\":\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec2_baked <- bake(rec2_prepped, new_data = NULL)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrec2_baked %>% \n  select(1:15) %>% \n  glimpse()\n## Rows: 3,756\n## Columns: 15\n## $ id                                  <fct> 5, 9, 10, 17, 27, 33, 42, 44, 48, …\n## $ c1                                  <fct> OFFENSE, OFFENSE, OFFENSE, OFFENSE…\n## $ profane_n                           <int> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ emo_words_n                         <int> 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1…\n## $ emojis_n                            <int> 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0…\n## $ wild_emojis_n                       <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ textfeature_text_copy_n_words       <int> 16, 12, 15, 19, 15, 33, 30, 31, 6,…\n## $ textfeature_text_copy_n_uq_words    <int> 16, 12, 15, 17, 15, 32, 29, 29, 6,…\n## $ textfeature_text_copy_n_charS       <int> 121, 66, 119, 112, 101, 196, 171, …\n## $ textfeature_text_copy_n_uq_charS    <int> 31, 29, 30, 36, 31, 35, 42, 35, 23…\n## $ textfeature_text_copy_n_digits      <int> 0, 0, 0, 4, 0, 0, 0, 1, 0, 4, 2, 0…\n## $ textfeature_text_copy_n_hashtags    <int> 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ textfeature_text_copy_n_uq_hashtags <int> 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ textfeature_text_copy_n_mentions    <int> 1, 1, 0, 0, 1, 1, 5, 1, 0, 1, 1, 1…\n## $ textfeature_text_copy_n_uq_mentions <int> 1, 1, 0, 0, 1, 1, 5, 1, 0, 1, 1, 1…\n```\n:::\n\n\n\nDas Rezept beinhaltet also einige Textfeatures plus die FastText-Worteinbettungen.\n\n### Fitting 3 {#sec-klassifik-fit3}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf3 <-\n  workflow() %>% \n  add_recipe(rec2) %>% \n  add_model(lasso_spec)\n\nwf3\n## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: logistic_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 6 Recipe Steps\n## \n## • step_text_normalization()\n## • step_mutate()\n## • step_textfeature()\n## • step_tokenize()\n## • step_stopwords()\n## • step_word_embeddings()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Logistic Regression Model Specification (classification)\n## \n## Main Arguments:\n##   penalty = tune()\n##   mixture = 1\n## \n## Computational engine: glmnet\n```\n:::\n\n\n\n\n\n\nTunen und Fitten:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\n\ntic()\nfit3 <-\n  tune_grid(\n    wf3,\n    folds1,\n    grid = lambda_grid,\n    control = control_resamples(save_pred = TRUE)\n  )\n(toc)\nfit3\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#write_rds(fit3, \"objects/chap-klassifik/chap_classific_fit3.rds\")\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\nHier ist die Performanz:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(fit3)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(fit3)\n```\n\n::: {.cell-output-display}\n![](080-klassifikation_files/figure-html/unnamed-chunk-53-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit3 %>% \n  show_best(\"roc_auc\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"penalty\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".metric\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimator\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"mean\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"n\"],\"name\":[5],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"std_err\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".config\"],\"name\":[7],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"0.0017433288\",\"2\":\"roc_auc\",\"3\":\"binary\",\"4\":\"0.8116297\",\"5\":\"10\",\"6\":\"0.008710636\",\"7\":\"Preprocessor1_Model22\"},{\"1\":\"0.0007880463\",\"2\":\"roc_auc\",\"3\":\"binary\",\"4\":\"0.8111630\",\"5\":\"10\",\"6\":\"0.008941393\",\"7\":\"Preprocessor1_Model21\"},{\"1\":\"0.0003562248\",\"2\":\"roc_auc\",\"3\":\"binary\",\"4\":\"0.8103737\",\"5\":\"10\",\"6\":\"0.009238082\",\"7\":\"Preprocessor1_Model20\"},{\"1\":\"0.0001610262\",\"2\":\"roc_auc\",\"3\":\"binary\",\"4\":\"0.8099159\",\"5\":\"10\",\"6\":\"0.009444783\",\"7\":\"Preprocessor1_Model19\"},{\"1\":\"0.0000000001\",\"2\":\"roc_auc\",\"3\":\"binary\",\"4\":\"0.8095907\",\"5\":\"10\",\"6\":\"0.009602864\",\"7\":\"Preprocessor1_Model01\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nchosen_auc_fit3 <- \n  fit3 %>%\n  select_by_one_std_err(metric = \"roc_auc\", -penalty)\n```\n:::\n\n\n\n\nFinalisieren:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf3_final <-\n  finalize_workflow(wf3, chosen_auc_fit3)\n\nwf3_final\n## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: logistic_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 6 Recipe Steps\n## \n## • step_text_normalization()\n## • step_mutate()\n## • step_textfeature()\n## • step_tokenize()\n## • step_stopwords()\n## • step_word_embeddings()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Logistic Regression Model Specification (classification)\n## \n## Main Arguments:\n##   penalty = 0.00853167852417281\n##   mixture = 1\n## \n## Computational engine: glmnet\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#fit3_final_train <-  # die Berechnung kann dauern ...\n  fit(wf3_final, d_train)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit3_final_train %>% \n  extract_fit_parsnip() %>% \n  tidy() %>% \n  arrange(-abs(estimate)) %>% \n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"term\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"estimate\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"penalty\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"(Intercept)\",\"2\":\"1.3359975\",\"3\":\"0.008531679\"},{\"1\":\"profane_n\",\"2\":\"-0.5669547\",\"3\":\"0.008531679\"},{\"1\":\"wordembed_text_v055\",\"2\":\"0.2084922\",\"3\":\"0.008531679\"},{\"1\":\"textfeature_text_copy_n_exclaims\",\"2\":\"-0.1848394\",\"3\":\"0.008531679\"},{\"1\":\"wordembed_text_v089\",\"2\":\"-0.1844506\",\"3\":\"0.008531679\"},{\"1\":\"wordembed_text_v054\",\"2\":\"0.1759950\",\"3\":\"0.008531679\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit3_final_test <-\n  last_fit(wf3_final, d_split)  # dauert etwas\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\nUnd endlich: Wie gut ist die Performanz?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(fit3_final_test)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\".metric\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimator\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".estimate\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".config\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"accuracy\",\"2\":\"binary\",\"3\":\"0.7454110\",\"4\":\"Preprocessor1_Model1\"},{\"1\":\"roc_auc\",\"2\":\"binary\",\"3\":\"0.8055475\",\"4\":\"Preprocessor1_Model1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\nAm Ende so eines Arbeitsganges,\nbei dem man wieder (und wieder) die gleichen Funktionen kopiert,\nund nur aufpassen muss, aus `fit2` an der richtigen Stelle `fit3` zu machen:\nDa blickt man jedem Umbau dieses Codes zu einer Funktion freudig ins Gesicht.\n\nEin anderes Problem,\nfür das hier keine elegante Lösung präsentiert wird,\nsind die langen Berechnungszeiten, die, wenn man Pech hat, auch noch\nmehrfach wiederholt werden müssen.\n\nDie Gefahr mit dem Abspeichern via `write_rds` ist klar:\nMan riskiert, später ein veraltetes Objekt zu laden.\n\nZu diesen Punkten später mehr.\n\n\n\n## Fallstudien\n\n\n\nJulia Silge stellt eine schöne [Fallstudie](https://juliasilge.com/blog/spam-email/) zur Klassifikation von Spam-Mails bereit. In der Fallstudie geht es nicht um Feature Engineering, sondern um Modellierung.\n\n[ChatGPT generierten Text automatisch erkennen -- ](https://juliasilge.com/blog/gpt-detectors/) wie gut geht das? \nDie Maschine erkennt sich selbst, im besten (oder schlechtesten?) Fall. Die Fallstudie zeigt, naja, so mittel, zumindest in den Daten in der Fallstudie.\n\n\n[OpenAIs Word-Einbettungen](https://platform.openai.com/docs/guides/embeddings) werden in [dieser Fallstudie](https://juliasilge.com/blog/horror-embeddings/) verwendet. Sehr schön!\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}