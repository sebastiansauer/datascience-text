{
  "hash": "92b69cd8b50bdf857a71b11129168c61",
  "result": {
    "markdown": "# Informationstheorie\n\n\n\n### Lernziele\n\n\n- Die grundlegenden Konzepte der Informationstheorie erkl√§ren k√∂nnen\n\n\n\n### Vorbereitung\n\n- Lesen Sie [diesen Text](https://www.science4all.org/article/shannons-information-theory/) als Vorbereitung\n\n\n\n### Ben√∂tigte R-Pakete\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-1_ef9f77b3c46559acc4229417c02e5bf0'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(hcandersenr)   # Textdaten\nlibrary(easystats)  # Komfort f√ºr deskriptive Statistiken, wie `describe_distribution`\nlibrary(entropy)  # Entropie berechnen\n```\n:::\n\n\n\n\n\n\n## Grundlagen\n\n\nDie Informationstheorie ist eine der Sternstunden der Wissenschaft. [Manche sagen](http://www.science4all.org/article/shannons-information-theory/) dass Claude Shannon, der Autor der Theorie, auf einer Stufe mit Darwin und Einstein stehen sollte:\n\n>    In this single paper, Shannon introduced this new fundamental theory. He raised the right questions, which no one else even thought of asking. This would have been enough to make this contribution earthshaking. But amazingly enough, Shannon also provided most of the right answers with class and elegance. In comparison, it took decades for a dozen of top physicists to define the basics of quantum theory. Meanwhile, Shannon constructed something equivalent, all by himself, in a single paper.\nShannon‚Äôs theory has since transformed the world like no other ever had, from information technologies to telecommunications, from theoretical physics to economical globalization, from everyday life to philosophy. (...)\nI don‚Äôt think Shannon has had the credits he deserves. He should be right up there, near Darwin and Einstein, among the few greatest scientists mankind has ever had the chance to have. \n\n\nF√ºr die Statistik ist die Informationstheorie von hoher Bedeutung.\nIm Folgenden schauen wir uns einige Grundlagen an.\n\n\n\n\n### Shannon-Information\n\nMit der *Shannon-Information* (Information, Selbstinformation) quantifizieren wir, wie viel \"√úberraschung\" sich in einem Ereignis verbirgt [@shannon_1948].\n\nEin Ereignis mit ...\n\n- *geringer* Wahrscheinlichkeit: *Viel* √úberraschung (Information)\n- *hoher* Wahrscheinlichkeit: *Wenig* √úberraschung (Information)\n\n\n\nWenn wir also erfahren, dass ein unwahrscheinliches Ereignis eingetreten ist (Schnee im Sommer), sind wir √ºberraschter als wenn wir h√∂hen, dass ein wahrscheinliches Ereignis eingetreten ist (Schnee im Winter).\n\nDie Information eines Ereignis ist also eng verbunden mit seiner Wahrscheinlichkeit oder auch mit den Odds eines Ereignisses.\n\nDie Shannon-Information ist die einzige Gr√∂√üe, die einige w√ºnschenswerte Anforderungen^[Desiderata, sagt man] erf√ºllt:\n\n1. Stetig\n2. Je mehr Ereignisse in einem Zufallsexperiment m√∂glich sind, desto h√∂her die Information, wenn ein bestimmtes Ereignis eintritt\n3. Additiv: Die Summe der Information zweier Teilereignisse ist gleich der Information des Gesamtereignis\n\n\n\n:::{#def-info}\n\n## Shannon-Information\n\n\nDie Information ist so definiert:\n\n\n$$I(x) = - \\log_2 \\left( Pr(x) \\right)$$\n\n:::\n\nAndere Logaritmusbasen sind m√∂glich. Bei einem bin√§ren Logarithmus nennt man die Einheit *Bit*^[oder *shannon*].\n\n\nEin M√ºnwzurf^[wie immer, als fair angenommen, wenn sonst nichts anderes angegeben ist] hat *1 Bit* Information:\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-2_e072da104ce091cb1dbcd379596c3b74'}\n\n```{.r .cell-code}\n-log(1/2, base = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n\n\n\n\nDamit gilt: $I = \\frac{1}{Pr(x)}$\n\nDie Information ist eng verwandt mit den Odds bzw. Log-Odds (Logits):\n\n\n$\\text{log-odds}(x)=\\log \\left({\\frac {p(x)}{p(\\lnot x)}}\\right)$\n\nLogits k√∂nnen als Differenz zweier Shannon-Infos ausgedr√ºckt werden:\n\n\n$\\text{log-odds}(x)=I(\\lnot x)-I(x)$\n\n\n\nDie Information zweier unabh√§ngiger Ereignisse ist additiv.\n\nDie gemeinsame Wahrscheinlichkeit zweier unabh√§ngiger Ereignisse ist das Produkt der einzelnen Wahrscheinlichkeiten:\n\n$Pr(x,y) = Pr(x) \\cdot Pr(y)$\n\nDie *gemeinsame Information* ist dann\n\n$$\n{\\displaystyle {\\begin{aligned}\\operatorname {I}(x,y)&=-\\log _{2}\\left[p(x,y)\\right]=-\\log _{2}\\left[p(x)p(y)\\right]\\\\[5pt]&=-\\log _{2}\\left[p{(x)}\\right]-\\log _{2}\\left[p{(y)}\\right]\\\\[5pt]&=\\operatorname {I} (x)+\\operatorname {I} (y)\\end{aligned}}}\n$$\n\n\n\n\n\n:::{#exm-info1}\n\n\n## Information eines wahrscheinlichen Ereignisses\n\nDie Information eines fast sicheren Ereignisses ist gering.\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-3_e3bf136dd0052d4b7f4a47bc4125cbf0'}\n\n```{.r .cell-code}\n-log(99/100, base = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.01449957\n```\n:::\n:::\n\n\n\n:::\n\n\n\n\n\n:::{#exm-info2}\n\n\n## Information eines unwahrscheinlichen Ereignisses\n\nDie Information eines unwahrscheinlichen Ereignisses ist hoch.\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-4_52ebe5a8066f3720ea5ebff8223f4bb4'}\n\n```{.r .cell-code}\n-log(01/100, base = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6.643856\n```\n:::\n:::\n\n\n\n:::\n\n\n\n\n\n\n\n:::{#exm-info3}\n\n\n## Information eines W√ºrfelwurfs\n\n\nDie Wahrscheinlichkeitsfunktion eines W√ºrfel ist\n\n${\\displaystyle Pr(k)={\\begin{cases}{\\frac {1}{6}},&k\\in \\{1,2,3,4,5,6\\}\\\\0,&{\\text{ansonsten}}\\end{cases}}}$\n\n\n\nDie Wahrscheinlichkeit, eine 6 zu w√ºrfeln, ist $Pr(X=6) = \\frac{1}{6}$.\n\n\n\nDie Information von $X=6$ betr√§gt also\n\n\n$I(X=6) = -\\log_2 \\left( Pr(X=6) \\right) = -\\log_2(1/6) \\approx 2.585 \\, \\text{bits}$.\n\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-5_c916ec436f89446639eb597f39e7201e'}\n\n```{.r .cell-code}\n-log(1/6, base = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.584963\n```\n:::\n:::\n\n\n\n:::\n\n\n\n\n\n\n\n:::{#exm-info3}\n\n\n## Information zweier  W√ºrfelwurfe\n\n\n\n\n\n\n\nDie Wahrscheinlichkeit, mit zwei W√ºrfeln, $X$ und $Y$, jeweils *6* zu w√ºrfeln, \nbetr√§gt $Pr(X=6, Y=6) = \\frac{1}{36}$\n\n\n\nDie Information betr√§gt also\n\n\n$I(X=6, Y=6) = -\\log_2 \\left( Pr(6,6) \\right)$\n\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-6_d2e6db3315f4c4c9ff7446a8d8c53d0e'}\n\n```{.r .cell-code}\n-log(1/36, base = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.169925\n```\n:::\n:::\n\n\n\nAufgrund der Additivit√§t der Information gilt\n\n\n$I(6,6) = I(6) + I(6)$\n\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-7_d0d18766698cb48b034e0c7cbe78797f'}\n\n```{.r .cell-code}\n-log(1/6, base = 2) + -log(1/6, base = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.169925\n```\n:::\n:::\n\n\n\n\n:::\n\n\n\n\n\n\n\n\n\n### Entropie\n\n\n(Informations)entropie ist die Summe der Informationen der Ereignisse einer Zufallsvariablen, $X$.\n\n\n\n:::{#def-entropie}\n\n\n## Informationsentropie\n\n\n*Informationsentropie*  ist so definiert:\n\n$$H(p) = - \\text{E log} (p_i) = - \\sum_{i = 1}^n p_i \\text{log} (p_i) = E\\left[I(X) \\right]$$\n\n:::\n\nDie Informationsentropie ist also die \"mittlere\" oder \"erwartete Information einer Zufallsvariablen.\n\n\nDie Entropie eines M√ºnzwurf-Versuchs (Bernoulli-Experiment) ist dann maximal (1 bit), wenn die Trefferwahrscheinlichkeit 50% betr√§gt: $Pr(X=x) = 1/2$, s. Abb. @fig-bernoulli-entropy.\n\n\n![Der Zusammenhang von Wahrscheinlichkeit und Entropie bei einem Bernoulli-Versuch, Quelle: Wikipedia, CC-BY-SA, Brona, Rubber Duck](img/Binary_entropy_plot.svg.png){#fig-bernoulli-entropy width=\"30%\"}\n\n\n\n### Gemeinsame Information\n\nDie *gemeinsame Information* (mutual information, MI) zweier Zufallsvariablen $X$ und $Y$, $I(X,Y)$, quantifiziert die Informationsmenge, die man √ºber $Y$ erh√§lt, wenn man $X$ beobachtet. Mit anderen Worten: Die MI ist ein Ma√ü des Zusammenhangs zweier (nominaler) Variablen. Im Gegensatz zur Korrelation ist die MI nicht auf lineare Abh√§ngigkeiten beschr√§nkt.\n\n\n\nDie MI quantifiziert den Unterschied zwischen der gemeinsamen Verteilung $Pr(X,Y)$ und dem Produkt einer einzelnen^[auch als *marginalen* Wahrscheinlichkeiten oder *Randwahrscheinlichkeiten* bezeichnet] Wahrscheinlichkeitsverteilungen, d.h. $Pr(X)$ und $Pr(Y)$.\n\nWenn die beiden Variablen (stochastisch) unabh√§ngig^[F√ºr stochastische Unabh√§ngigkeit kann das Zeichen $\\bot$ verwendet werden] sind, ist ihre gemeinsame Information Null:\n\n$I(X,Y) = 0 \\quad \\text{gdw} \\quad \\bot(X,Y)$.\n\nDann gilt n√§mlich:\n\n$\\log \\left( \\frac{Pr(X,Y)} {Pr(X) \\cdot Pr(Y)} \\right) =\\log(1) = 0$.\n\n\nDas macht intuitiv Sinn: Sind zwei Variablen unabh√§ngig, so erf√§hrt man nichts √ºber die zweite, wenn man die erste kennt. So ist Kenntnis der Sternzeichens einer Person und ihrer K√∂rpergr√∂√üe unabh√§ngig.\n\nDas Gegenteil ist auch wahr: Sind zwei Variablen voneinander komplett abh√§ngig, so wei√ü man alles √ºber die zweite, wenn man die erste kennt.\n\n\nDie gemeinsame Information kann man sich als *Summe der einzelnen gemeinsamen Informationen* von $XY$ sehen (s. @tbl-mi1):\n\n\n::: {#tbl-mi1 .cell tbl-cap='Summe der punktweisen gemeinsamen Informationen' hash='Information_cache/html/tbl-mi1_22e5d6640ea6bab46c23e74f4d0a5f62'}\n\n```{.r .cell-code}\nd <- tibble::tribble(\n     ~x1,    ~x2,    ~x3,\n  \"x1y2\", \"x2y1\", \"x3y1\",\n  \"x2y1\", \"x2y2\", \"x3y2\",\n  \"x1y3\", \"x2y3\", \"x3y3\"\n  )\n```\n:::\n\n\n\n\n$I(X,Y) = \\Sigma_Y \\Sigma_y Pr(x,y) \\underbrace{\\log \\left( \\frac{Pr(X,Y)}{Pr(X) Pr(Y)} \\right)}_\\text{punktweise MI}$ \n\nDie Summanden der gemeinsamen Information bezeichnet man auch als *punktweise gemeinsame Information* (pointwise mutual information, PMI), entsprechend, s. @eq-pmi. MI ist also der Erwartungswert der PMI.\n\n\n$${\\displaystyle \\operatorname {PMI} (x,y)\\equiv \\log_{2}{\\frac {p(x,y)}{p(x)p(y)}}=\\log _{2}{\\frac {p(x|y)}{p(x)}}=\\log _{2}{\\frac {p(y|x)}{p(y)}}}\n$${#eq-pmi}\n\n\nAndere Basen als `log2` sind gebr√§uchlich, vor allem der nat√ºrliche Logarithmus.\n\n\n\n\n::: {.remark}\n\nDie zwei rechten Umformungen in @eq-pmi basieren auf der Umformung der gemeinsamen Wahrscheinlichkeit. \n\nZur Erinnerung: $p(x,y) = p(y)p(x|y) = p(x)p(y|x)$\n:::\n\n\n\n\n::: {#exm-pmi}\n\n## Interpretation der PMI\n\nSei $p(x) = p(y) = 1/10$ und $p(x,y) = 1/10$. W√§ren $x$ und $y$ unabh√§ngig, dann w√§re $p^{\\prime}(x,y) = p(x)p(y) = 1/100$.\nDas Verh√§ltnis der Produkte der einzelnen Wahrscheinlichkeit zur gemeinsamen Wahrscheinlichkeit w√§re dann 1 und der Logarithmus von 1 ist 0. Das Verh√§ltnis von 1 entspricht also der Unabh√§ngigkeit. Ist das Verh√§ltnis z.B. 5, so zeigt das eine gewisse Abh√§ngigkeit an.\nIm obigen Beispiel gilt: $\\frac{1/20}{1/100}=5$.\n\n:::\n\n\nDie MI wird auch √ºber die sog. *Kullback-Leibler-Divergenz* definiert,\ndie die Differenz zwischen gemeinsamer Wahrscheinlichkeitsfunktion bzw. -dichte und dem Produkt der jeweiligen Randwahrscheinlichkeiten bzw. -dichte angibt.\n\n\n\n### Maximumentropie\n\n\n:::{#def-maxent}\n\n\n## Maximumentropie\n\nDie Verteilungsform, f√ºr die es die meisten M√∂glichkeiten (Pfade im Baumdiagramm) gibt,\nhat die h√∂chste Informationsentropie.\n\n:::\n\n@fig-muenz3 zeigt ein Baumdiagramm f√ºr einen 3-fachen M√ºnzwurf.\nIn den \"Bl√§ttern\" (Endknoten) sind die Ergebnisse des Experiments dargestellt\nsowie die Zufallsvariable $X$, die die Anzahl der \"Treffer\" (Kopf) fasst.\nWie man sieht, vereinen einige Werte der Zufallsvariable mehr Pfade auf sich als andere:\nDer Wert $X=1$ vereinigt 3 Pfade (von 8) auf sich; der Wert $X=3$ nur 1 Pfad.\n\n![Pfade im Baumdiagramm: 3-facher M√ºnzwurf](img/muenz3.png){#fig-muenz3}\n\n\n\n#### Ilustration \n\n\nSagen wir, Sie stehen vor 5 Eimern und haben 10 Kieselsteine bei sich, die mit den Nummern 1 bis 10 beschriftet sind [@mcelreath_statistical_2020].\nWeil Sie nichts besseres zu tun haben, werfen Sie die Kiesel in die Eimer und zwar so, \ndass die Wahrscheinlichkeit f√ºr einen Kiesel in einen bestimmten Eimer zu landen f√ºr alle Eimer gleich ist.\nSie werfen also Ihre 10 Kiesel und betrachten das Ergebnis; die Kiesel sind jetzt in einem bestimmten (zuf√§lligen)\nArrangement auf die Eimer verteilt.\nJede Aufteilung (der 10 Kiesel in den 5 Eimern) ist gleich wahrscheinlich^[so √§hnlich wie mit den Lottozahlen] -- \ndie Wahrscheinlichkeit, dass alle 10 Kiesel in Eimer 1 landen ist also gleich hoch wie die Wahrscheinlichkeit,\ndass jeder Eimer einen Kiesel abkriegt.\nJetzt kommt's: Manche Arrangements k√∂nnen auf mehrere Arten erzielt werden als andere. \nSo gibt es nur *eine* Aufteilung f√ºr alle 10 Kiesel in einem Eimer (Teildiagramm a, in @fig-kiesel).\nAber es gibt 90 M√∂glichkeiten, die Kiesel so aufzuteilen, dass 2 in Eimer 2 landen, 8 in Eimer 4 und 2 in Eimer 4,\ns. Teildiagramm b in @fig-kiesel [@kurz_statistical_2021]. \nTeildiagramme c bis e zeigen, dass die Anzahl der Aufteilungen schnell astronomisch hoch wird,\nwenn sich die Kiesel \"gleichm√§√üiger\" auf die Eimer verteilen.\nDie gleichm√§√üigste Aufteilung (Diagramm e) hat die gr√∂√üte Zahl an m√∂glichen Anordnungen.\nEine Aufteilung der Kiesel auch als Pfad durch ein Baumdiagramm beschrieben werden.\n\nHier sind ein paar verschiedene Arrangements, deren Anzahl von Aufteilungen wir hier anschauen:\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-9_2f43b8f7651c14664feee0ba4a60bee5'}\n\n```{.r .cell-code}\nd <-\n  tibble(a = c(0, 0, 10, 0, 0),\n         b = c(0, 1, 8, 1, 0),\n         c = c(0, 2, 6, 2, 0),\n         d = c(1, 2, 4, 2, 1),\n         e = 2) \nd\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|  a|  b|  c|  d|  e|\n|--:|--:|--:|--:|--:|\n|  0|  0|  0|  1|  2|\n|  0|  1|  2|  2|  2|\n| 10|  8|  6|  4|  2|\n|  0|  1|  2|  2|  2|\n|  0|  0|  0|  1|  2|\n\n</div>\n:::\n:::\n\n::: {.cell hash='Information_cache/html/fig-kiesel_25dd73ea0e0816034ae929e95c0c7443'}\n::: {.cell-output-display}\n![Anzahl von Aufteilungen der 10 Kiesel in 5 Eimer](Information_files/figure-html/fig-kiesel-1.png){#fig-kiesel width=672}\n:::\n:::\n\n\nHier sind die Wahrscheinlichkeitsverteilungen der 5 Arrangements^[Ist das nicht eine elegante Syntax?! Solomon Kurz, der Autor der Syntax, hielt sich nicht an die aktuelle Version von `dplyr`; `mutate_all` ist eigentlich √ºberholt zugunsten von `mutate` mit `across`, aber die Pr√§gnanz der Syntax hier ist schon beeindruckend, wie ich finde.]:\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-11_088ae5ff207a594fc40206aac7e357df'}\n\n```{.r .cell-code}\nd %>% \n  mutate_all(~. / sum(.))\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|  a|   b|   c|   d|   e|\n|--:|---:|---:|---:|---:|\n|  0| 0.0| 0.0| 0.1| 0.2|\n|  0| 0.1| 0.2| 0.2| 0.2|\n|  1| 0.8| 0.6| 0.4| 0.2|\n|  0| 0.1| 0.2| 0.2| 0.2|\n|  0| 0.0| 0.0| 0.1| 0.2|\n\n</div>\n:::\n:::\n\n\n\nDann kann man die Wahrscheinlichkeit einfach in Entropie umrechnen^[Syntax aus @kurz_statistical_2021]:\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-12_941c1de2fa7fa9ac91fafd3ad2afd1c5'}\n\n```{.r .cell-code}\nd %>% \n  mutate_all(~ . / sum(.)) %>% \n  gather() %>% \n  group_by(key) %>% \n  summarise(h = -sum(ifelse(value == 0, 0, value * log(value))))\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|key |         h|\n|:---|---------:|\n|a   | 0.0000000|\n|b   | 0.6390319|\n|c   | 0.9502705|\n|d   | 1.4708085|\n|e   | 1.6094379|\n\n</div>\n:::\n:::\n\n\n\nDas `ifelse` dient nur dazu, eine Wahrscheinlichkeit von 0 eine Entropie von 0 zu verpassen^[Regel von L'Hopital], denn sonst w√ºrden wir ein Problem rennen, wenn wir $log(0)$ ausrechnen.\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-13_4542385575c1e35ab26589681fbfea05'}\n\n```{.r .cell-code}\nlog(0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -Inf\n```\n:::\n:::\n\n\n\n\n## Zufallstext erkennen\n\n\n### Entropie von Zufallstext\n\n\nKann man wohl Zufallstext maschinell erkennen? Sicher gibt es viele Ans√§tze,\num das Problem anzugehen.\nLassen Sie uns einen Ansatz erforschen. \nErforschen hei√üt, *wir* erforschen f√ºr *uns*, es handelt sich um eine didaktische √úbung, \ndas Ziel ist nicht, Neuland f√ºr die Menschheit zu betreten.\n\nAber zuerst m√ºssen wir √ºberlegen, was \"Zufallstext\" bedeuten soll.\n\n\nNehmen wir uns dazu zuerst einen richtigen Text, ein M√§rchen von H.C. Andersen zum Beispiel.\nNehmen wir das Erste aus der Liste in dem Tibble `hcandersen_de`, \"das Feuerzeug\".\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-14_4a86dad54c7fc181f2fd18083866086f'}\n\n```{.r .cell-code}\ndas_feuerzeug <-\n  hcandersen_de  %>% \n  filter(book == \"Das Feuerzeug\") %>% \n  unnest_tokens(input = text, output = word) %>% \n  pull(word) \n\nhead(das_feuerzeug)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"es\"         \"kam\"        \"ein\"        \"soldat\"     \"die\"       \n[6] \"landstra√üe\"\n```\n:::\n:::\n\n\n\nDas M√§rchen ist 2688 W√∂rter lang.\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-15_1eb3fdf036147b220915073f89a2d396'}\n\n```{.r .cell-code}\nwortliste <- \nhcandersen_de  %>% \n  filter(book == \"Das Feuerzeug\") %>% \n  unnest_tokens(output = word, input = text) %>% \n  pull(word) %>% \n  unique()\n\nhead(wortliste)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"es\"         \"kam\"        \"ein\"        \"soldat\"     \"die\"       \n[6] \"landstra√üe\"\n```\n:::\n:::\n\n\n\nJetzt ziehen wir Stichproben (mit Zur√ºcklegen) aus dieser Liste und erstellen damit eine Art Zufallstext.\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-16_a047c57bf1a6ae2e5f01e5d18f5cdfa7'}\n\n```{.r .cell-code}\nzufallstext <- \n  sample(x = wortliste, size = length(das_feuerzeug)*10^2, replace = TRUE)\nhead(zufallstext)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"oben\"      \"offiziere\" \"morgen\"    \"leg\"       \"soldaten\"  \"aber\"     \n```\n:::\n:::\n\n\n\nZ√§hlen wir, wie h√§ufig jedes Wort vorkommt:\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-17_1878fccdf4123b4723c5d20552c35565'}\n\n```{.r .cell-code}\nzufallstext_count <-\ntibble(zufallstext = zufallstext) %>% \n  count(zufallstext)\n\nhead(zufallstext_count)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|zufallstext |   n|\n|:-----------|---:|\n|ab          | 397|\n|abend       | 357|\n|aber        | 402|\n|abflog      | 382|\n|abschlagen  | 376|\n|acht        | 390|\n\n</div>\n:::\n:::\n\n\n\nDer H√§ufigkeitsvektor von `wortliste` besteht nur aus Einsen, \nso haben wir ja gerade die Wortliste definiert:\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-18_2f057cb0f26d44905767121c36d35dea'}\n\n```{.r .cell-code}\nwortliste_count <-\ntibble(wortliste = wortliste) %>% \n  count(wortliste)\n\nhead(wortliste_count)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|wortliste  |  n|\n|:----------|--:|\n|ab         |  1|\n|abend      |  1|\n|aber       |  1|\n|abflog     |  1|\n|abschlagen |  1|\n|acht       |  1|\n\n</div>\n:::\n:::\n\n\n\nDaher ist ihre Informationsentropy maximal.\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-19_51f6c5cd5528e4c2ca69d4d9de6e5722'}\n\n```{.r .cell-code}\nentropy(wortliste_count$n, unit = \"log2\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 9.47978\n```\n:::\n:::\n\n\n\n\nDie H√§ufigkeiten der W√∂rter in `zufallstext` hat eine hohe Entropie.\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-20_cfe7379fae9c1da504e2d35e820fd809'}\n\n```{.r .cell-code}\nentropy(zufallstext_count$n, unit = \"log2\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 9.477938\n```\n:::\n:::\n\n\n\nZ√§hlen wir die H√§ufigkeiten in der Geschichte \"Das Feuerzeug\".\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-21_d6a7b6ca5e8214dd36dc72274551bcec'}\n\n```{.r .cell-code}\ndas_feuerzeug_count <-\n  tibble(text = das_feuerzeug) %>% \n  count(text)\n\nhead(das_feuerzeug_count)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|text       |  n|\n|:----------|--:|\n|ab         |  2|\n|abend      |  3|\n|aber       | 21|\n|abflog     |  1|\n|abschlagen |  1|\n|acht       |  1|\n\n</div>\n:::\n:::\n\nUnd berechnen dann die Entropie:\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-22_a02e25cc2c768b2b1a5c51ba08c8aab2'}\n\n```{.r .cell-code}\nentropy(das_feuerzeug_count$n, unit = \"log2\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8.075194\n```\n:::\n:::\n\n\n\nDer Zufallstext hat also eine h√∂here Entropie als der echte M√§rchentext.\nDer Zufallstext ist also gleichverteilter in den Worth√§ufigkeiten.\n\n\nPro Bit weniger Entropie halbiert sich die Anzahl der M√∂glichkeiten einer H√§ufigkeitsverteilung.\n\n\n\n### MI von Zufallstext\n\n\n\nLeft as an exercises for the reader^[[Vgl. hier](https://academia.stackexchange.com/questions/20084/is-using-the-phrase-is-left-as-an-exercise-for-the-reader-considered-good)] ü•≥.\n\n\n\n\n\n## Literatur\n\n\n### Wikipedia\n\nEs gibt eine Reihe n√ºtzlicher (und recht informationsdichter) Wikipedia-Eintr√§ge zum Thema Informationstheorie.\n\n- [Wikipedia: Pointwise Mutual Information](https://en.wikipedia.org/wiki/Pointwise_mutual_information)\n- [Wikipedia: Mutual information](https://en.wikipedia.org/wiki/Mutual_information)\n- [Wikipedia: Information theory](https://en.wikipedia.org/wiki/Information_theory)\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}