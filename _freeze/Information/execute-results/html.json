{
  "hash": "6fb4c4ca54142ee1218d897d0989ee8b",
  "result": {
    "markdown": "# Informationstheorie\n\n\n\n### Lernziele\n\n\n- Die grundlegenden Konzepte der Informationstheorie erklären können\n\n\n\n### Vorbereitung\n\n- Lesen Sie [diesen Text](https://www.science4all.org/article/shannons-information-theory/) als Vorbereitung\n\n\n\n### Benötigte R-Pakete\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-1_ef9f77b3c46559acc4229417c02e5bf0'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(hcandersenr)   # Textdaten\nlibrary(easystats)  # Komfort für deskriptive Statistiken, wie `describe_distribution`\nlibrary(entropy)  # Entropie berechnen\n```\n:::\n\n\n\n\n\n\n## Grundlagen\n\n\nDie Informationstheorie ist eine der Sternstunden der Wissenschaft. [Manche sagen](http://www.science4all.org/article/shannons-information-theory/) dass Claude Shannon, der Autor der Theorie, auf einer Stufe mit Darwin und Einstein stehen sollte:\n\n>    In this single paper, Shannon introduced this new fundamental theory. He raised the right questions, which no one else even thought of asking. This would have been enough to make this contribution earthshaking. But amazingly enough, Shannon also provided most of the right answers with class and elegance. In comparison, it took decades for a dozen of top physicists to define the basics of quantum theory. Meanwhile, Shannon constructed something equivalent, all by himself, in a single paper.\nShannon’s theory has since transformed the world like no other ever had, from information technologies to telecommunications, from theoretical physics to economical globalization, from everyday life to philosophy. (...)\nI don’t think Shannon has had the credits he deserves. He should be right up there, near Darwin and Einstein, among the few greatest scientists mankind has ever had the chance to have. \n\n\nFür die Statistik ist die Informationstheorie von hoher Bedeutung.\nIm Folgenden schauen wir uns einige Grundlagen an.\n\n\n\n\n### Shannon-Information\n\nMit der *Shannon-Information* (Information, Selbstinformation) quantifizieren wir, wie viel \"Überraschung\" sich in einem Ereignis verbirgt [@shannon_1948].\n\nEin Ereignis mit ...\n\n- *geringer* Wahrscheinlichkeit: *Viel* Überraschung (Information)\n- *hoher* Wahrscheinlichkeit: *Wenig* Überraschung (Information)\n\n\n\nWenn wir also erfahren, dass ein unwahrscheinliches Ereignis eingetreten ist (Schnee im Sommer), sind wir überraschter als wenn wir höhen, dass ein wahrscheinliches Ereignis eingetreten ist (Schnee im Winter).\n\nDie Information eines Ereignis ist also eng verbunden mit seiner Wahrscheinlichkeit oder auch mit den Odds eines Ereignisses.\n\nDie Shannon-Information ist die einzige Größe, die einige wünschenswerte Anforderungen^[Desiderata, sagt man] erfüllt:\n\n1. Stetig\n2. Je mehr Ereignisse in einem Zufallsexperiment möglich sind, desto höher die Information, wenn ein bestimmtes Ereignis eintritt\n3. Additiv: Die Summe der Information zweier Teilereignisse ist gleich der Information des Gesamtereignis\n\n\n\n:::{#def-info}\n\n## Shannon-Information\n\n\nDie Information ist so definiert:\n\n\n$$I(x) = - \\log_2 \\left( Pr(x) \\right)$$\n\n:::\n\nAndere Logaritmusbasen sind möglich. Bei einem binären Logarithmus nennt man die Einheit *Bit*^[oder *shannon*].\n\n\nEin Münwzurf^[wie immer, als fair angenommen, wenn sonst nichts anderes angegeben ist] hat *1 Bit* Information:\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-2_e072da104ce091cb1dbcd379596c3b74'}\n\n```{.r .cell-code}\n-log(1/2, base = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n\n\n\n\nDamit gilt: $I = \\frac{1}{Pr(x)}$\n\nDie Information ist eng verwandt mit den Odds bzw. Log-Odds (Logits):\n\n\n$\\text{log-odds}(x)=\\log \\left({\\frac {p(x)}{p(\\lnot x)}}\\right)$\n\nLogits können als Differenz zweier Shannon-Infos ausgedrückt werden:\n\n\n$\\text{log-odds}(x)=I(\\lnot x)-I(x)$\n\n\n\nDie Information zweier unabhängiger Ereignisse ist additiv.\n\nDie gemeinsame Wahrscheinlichkeit zweier unabhängiger Ereignisse ist das Produkt der einzelnen Wahrscheinlichkeiten:\n\n$Pr(x,y) = Pr(x) \\cdot Pr(y)$\n\nDie *gemeinsame Information* ist dann\n\n$$\n{\\displaystyle {\\begin{aligned}\\operatorname {I}(x,y)&=-\\log _{2}\\left[p(x,y)\\right]=-\\log _{2}\\left[p(x)p(y)\\right]\\\\[5pt]&=-\\log _{2}\\left[p{(x)}\\right]-\\log _{2}\\left[p{(y)}\\right]\\\\[5pt]&=\\operatorname {I} (x)+\\operatorname {I} (y)\\end{aligned}}}\n$$\n\n\n\n\n\n:::{#exm-info1}\n\n\n## Information eines wahrscheinlichen Ereignisses\n\nDie Information eines fast sicheren Ereignisses ist gering.\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-3_e3bf136dd0052d4b7f4a47bc4125cbf0'}\n\n```{.r .cell-code}\n-log(99/100, base = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.01449957\n```\n:::\n:::\n\n\n\n:::\n\n\n\n\n\n:::{#exm-info2}\n\n\n## Information eines unwahrscheinlichen Ereignisses\n\nDie Information eines unwahrscheinlichen Ereignisses ist hoch.\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-4_52ebe5a8066f3720ea5ebff8223f4bb4'}\n\n```{.r .cell-code}\n-log(01/100, base = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6.643856\n```\n:::\n:::\n\n\n\n:::\n\n\n\n\n\n\n\n:::{#exm-info3}\n\n\n## Information eines Würfelwurfs\n\n\nDie Wahrscheinlichkeitsfunktion eines Würfel ist\n\n${\\displaystyle Pr(k)={\\begin{cases}{\\frac {1}{6}},&k\\in \\{1,2,3,4,5,6\\}\\\\0,&{\\text{ansonsten}}\\end{cases}}}$\n\n\n\nDie Wahrscheinlichkeit, eine 6 zu würfeln, ist $Pr(X=6) = \\frac{1}{6}$.\n\n\n\nDie Information von $X=6$ beträgt also\n\n\n$I(X=6) = -\\log_2 \\left( Pr(X=6) \\right) = -\\log_2(1/6) \\approx 2.585 \\, \\text{bits}$.\n\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-5_c916ec436f89446639eb597f39e7201e'}\n\n```{.r .cell-code}\n-log(1/6, base = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.584963\n```\n:::\n:::\n\n\n\n:::\n\n\n\n\n\n\n\n:::{#exm-info3}\n\n\n## Information zweier  Würfelwurfe\n\n\n\n\n\n\n\nDie Wahrscheinlichkeit, mit zwei Würfeln, $X$ und $Y$, jeweils *6* zu würfeln, \nbeträgt $Pr(X=6, Y=6) = \\frac{1}{36}$\n\n\n\nDie Information beträgt also\n\n\n$I(X=6, Y=6) = -\\log_2 \\left( Pr(6,6) \\right)$\n\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-6_d2e6db3315f4c4c9ff7446a8d8c53d0e'}\n\n```{.r .cell-code}\n-log(1/36, base = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.169925\n```\n:::\n:::\n\n\n\nAufgrund der Additivität der Information gilt\n\n\n$I(6,6) = I(6) + I(6)$\n\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-7_d0d18766698cb48b034e0c7cbe78797f'}\n\n```{.r .cell-code}\n-log(1/6, base = 2) + -log(1/6, base = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.169925\n```\n:::\n:::\n\n\n\n\n:::\n\n\n\n\n\n\n\n\n\n### Entropie\n\n\n(Informations)entropie ist die Summe der Informationen der Ereignisse einer Zufallsvariablen, $X$.\n\n\n\n:::{#def-entropie}\n\n\n## Informationsentropie\n\n\n*Informationsentropie*  ist so definiert:\n\n$$H(p) = - \\text{E log} (p_i) = - \\sum_{i = 1}^n p_i \\text{log} (p_i) = E\\left[I(X) \\right]$$\n\n:::\n\nDie Informationsentropie ist also die \"mittlere\" oder \"erwartete Information einer Zufallsvariablen.\n\n\nDie Entropie eines Münzwurf-Versuchs (Bernoulli-Experiment) ist dann maximal (1 bit), wenn die Trefferwahrscheinlichkeit 50% beträgt: $Pr(X=x) = 1/2$, s. Abb. @fig-bernoulli-entropy.\n\n\n![Der Zusammenhang von Wahrscheinlichkeit und Entropie bei einem Bernoulli-Versuch, Quelle: Wikipedia, CC-BY-SA, Brona, Rubber Duck](img/Binary_entropy_plot.svg.png){#fig-bernoulli-entropy width=\"30%\"}\n\n\n\n### Gemeinsame Information\n\nDie *gemeinsame Information* (mutual information, MI) zweier Zufallsvariablen $X$ und $Y$, $I(X,Y)$, quantifiziert die Informationsmenge, die man über $Y$ erhält, wenn man $X$ beobachtet. Mit anderen Worten: Die MI ist ein Maß des Zusammenhangs zweier (nominaler) Variablen. Im Gegensatz zur Korrelation ist die MI nicht auf lineare Abhängigkeiten beschränkt.\n\n\n\nDie MI quantifiziert den Unterschied zwischen der gemeinsamen Verteilung $Pr(X,Y)$ und dem Produkt einer einzelnen^[auch als *marginalen* Wahrscheinlichkeiten oder *Randwahrscheinlichkeiten* bezeichnet] Wahrscheinlichkeitsverteilungen, d.h. $Pr(X)$ und $Pr(Y)$.\n\nWenn die beiden Variablen (stochastisch) unabhängig^[Für stochastische Unabhängigkeit kann das Zeichen $\\bot$ verwendet werden] sind, ist ihre gemeinsame Information Null:\n\n$I(X,Y) = 0 \\quad \\text{gdw} \\quad \\bot(X,Y)$.\n\nDann gilt nämlich:\n\n$\\log \\left( \\frac{Pr(X,Y)} {Pr(X) \\cdot Pr(Y)} \\right) =\\log(1) = 0$.\n\n\nDas macht intuitiv Sinn: Sind zwei Variablen unabhängig, so erfährt man nichts über die zweite, wenn man die erste kennt. So ist Kenntnis der Sternzeichens einer Person und ihrer Körpergröße unabhängig.\n\nDas Gegenteil ist auch wahr: Sind zwei Variablen voneinander komplett abhängig, so weiß man alles über die zweite, wenn man die erste kennt.\n\n\nDie gemeinsame Information kann man sich als *Summe der einzelnen gemeinsamen Informationen* von $XY$ sehen (s. @tbl-mi1):\n\n\n::: {#tbl-mi1 .cell tbl-cap='Summe der punktweisen gemeinsamen Informationen' hash='Information_cache/html/tbl-mi1_22e5d6640ea6bab46c23e74f4d0a5f62'}\n\n```{.r .cell-code}\nd <- tibble::tribble(\n     ~x1,    ~x2,    ~x3,\n  \"x1y2\", \"x2y1\", \"x3y1\",\n  \"x2y1\", \"x2y2\", \"x3y2\",\n  \"x1y3\", \"x2y3\", \"x3y3\"\n  )\n```\n:::\n\n\n\n\n$I(X,Y) = \\Sigma_Y \\Sigma_y Pr(x,y) \\underbrace{\\log \\left( \\frac{Pr(X,Y)}{Pr(X) Pr(Y)} \\right)}_\\text{punktweise MI}$ \n\nDie Summanden der gemeinsamen Information bezeichnet man auch als *punktweise gemeinsame Information* (pointwise mutual information, PMI), entsprechend, s. @eq-pmi. MI ist also der Erwartungswert der PMI.\n\n\n$${\\displaystyle \\operatorname {PMI} (x,y)\\equiv \\log_{2}{\\frac {p(x,y)}{p(x)p(y)}}=\\log _{2}{\\frac {p(x|y)}{p(x)}}=\\log _{2}{\\frac {p(y|x)}{p(y)}}}\n$${#eq-pmi}\n\n\nAndere Basen als `log2` sind gebräuchlich, vor allem der natürliche Logarithmus.\n\n\n\n\n::: {.remark}\n\nDie zwei rechten Umformungen in @eq-pmi basieren auf der Umformung der gemeinsamen Wahrscheinlichkeit. \n\nZur Erinnerung: $p(x,y) = p(y)p(x|y) = p(x)p(y|x)$\n:::\n\n\n\n\n::: {#exm-pmi}\n\n## Interpretation der PMI\n\nSei $p(x) = p(y) = 1/10$ und $p(x,y) = 1/10$. Wären $x$ und $y$ unabhängig, dann wäre $p^{\\prime}(x,y) = p(x)p(y) = 1/100$.\nDas Verhältnis der Produkte der einzelnen Wahrscheinlichkeit zur gemeinsamen Wahrscheinlichkeit wäre dann 1 und der Logarithmus von 1 ist 0. Das Verhältnis von 1 entspricht also der Unabhängigkeit. Ist das Verhältnis z.B. 5, so zeigt das eine gewisse Abhängigkeit an.\nIm obigen Beispiel gilt: $\\frac{1/20}{1/100}=5$.\n\n:::\n\n\nDie MI wird auch über die sog. *Kullback-Leibler-Divergenz* definiert,\ndie die Differenz zwischen gemeinsamer Wahrscheinlichkeitsfunktion bzw. -dichte und dem Produkt der jeweiligen Randwahrscheinlichkeiten bzw. -dichte angibt.\n\n\n\n### Maximumentropie\n\n\n:::{#def-maxent}\n\n\n## Maximumentropie\n\nDie Verteilungsform, für die es die meisten Möglichkeiten (Pfade im Baumdiagramm) gibt,\nhat die höchste Informationsentropie.\n\n:::\n\n@fig-muenz3 zeigt ein Baumdiagramm für einen 3-fachen Münzwurf.\nIn den \"Blättern\" (Endknoten) sind die Ergebnisse des Experiments dargestellt\nsowie die Zufallsvariable $X$, die die Anzahl der \"Treffer\" (Kopf) fasst.\nWie man sieht, vereinen einige Werte der Zufallsvariable mehr Pfade auf sich als andere:\nDer Wert $X=1$ vereinigt 3 Pfade (von 8) auf sich; der Wert $X=3$ nur 1 Pfad.\n\n![Pfade im Baumdiagramm: 3-facher Münzwurf](img/muenz3.png){#fig-muenz3}\n\n\n\n#### Ilustration \n\n\nSagen wir, Sie stehen vor 5 Eimern und haben 10 Kieselsteine bei sich, die mit den Nummern 1 bis 10 beschriftet sind [@mcelreath_statistical_2020].\nWeil Sie nichts besseres zu tun haben, werfen Sie die Kiesel in die Eimer und zwar so, \ndass die Wahrscheinlichkeit für einen Kiesel in einen bestimmten Eimer zu landen für alle Eimer gleich ist.\nSie werfen also Ihre 10 Kiesel und betrachten das Ergebnis; die Kiesel sind jetzt in einem bestimmten (zufälligen)\nArrangement auf die Eimer verteilt.\nJede Aufteilung (der 10 Kiesel in den 5 Eimern) ist gleich wahrscheinlich^[so ähnlich wie mit den Lottozahlen] -- \ndie Wahrscheinlichkeit, dass alle 10 Kiesel in Eimer 1 landen ist also gleich hoch wie die Wahrscheinlichkeit,\ndass jeder Eimer einen Kiesel abkriegt.\nJetzt kommt's: Manche Arrangements können auf mehrere Arten erzielt werden als andere. \nSo gibt es nur *eine* Aufteilung für alle 10 Kiesel in einem Eimer (Teildiagramm a, in @fig-kiesel).\nAber es gibt 90 Möglichkeiten, die Kiesel so aufzuteilen, dass 2 in Eimer 2 landen, 8 in Eimer 4 und 2 in Eimer 4,\ns. Teildiagramm b in @fig-kiesel [@kurz_statistical_2021]. \nTeildiagramme c bis e zeigen, dass die Anzahl der Aufteilungen schnell astronomisch hoch wird,\nwenn sich die Kiesel \"gleichmäßiger\" auf die Eimer verteilen.\nDie gleichmäßigste Aufteilung (Diagramm e) hat die größte Zahl an möglichen Anordnungen.\nEine Aufteilung der Kiesel auch als Pfad durch ein Baumdiagramm beschrieben werden.\n\nHier sind ein paar verschiedene Arrangements, deren Anzahl von Aufteilungen wir hier anschauen:\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-9_2f43b8f7651c14664feee0ba4a60bee5'}\n\n```{.r .cell-code}\nd <-\n  tibble(a = c(0, 0, 10, 0, 0),\n         b = c(0, 1, 8, 1, 0),\n         c = c(0, 2, 6, 2, 0),\n         d = c(1, 2, 4, 2, 1),\n         e = 2) \nd\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|  a|  b|  c|  d|  e|\n|--:|--:|--:|--:|--:|\n|  0|  0|  0|  1|  2|\n|  0|  1|  2|  2|  2|\n| 10|  8|  6|  4|  2|\n|  0|  1|  2|  2|  2|\n|  0|  0|  0|  1|  2|\n\n</div>\n:::\n:::\n\n::: {.cell hash='Information_cache/html/fig-kiesel_25dd73ea0e0816034ae929e95c0c7443'}\n::: {.cell-output-display}\n![Anzahl von Aufteilungen der 10 Kiesel in 5 Eimer](Information_files/figure-html/fig-kiesel-1.png){#fig-kiesel width=672}\n:::\n:::\n\n\nHier sind die Wahrscheinlichkeitsverteilungen der 5 Arrangements^[Ist das nicht eine elegante Syntax?! Solomon Kurz, der Autor der Syntax, hielt sich nicht an die aktuelle Version von `dplyr`; `mutate_all` ist eigentlich überholt zugunsten von `mutate` mit `across`, aber die Prägnanz der Syntax hier ist schon beeindruckend, wie ich finde.]:\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-11_088ae5ff207a594fc40206aac7e357df'}\n\n```{.r .cell-code}\nd %>% \n  mutate_all(~. / sum(.))\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|  a|   b|   c|   d|   e|\n|--:|---:|---:|---:|---:|\n|  0| 0.0| 0.0| 0.1| 0.2|\n|  0| 0.1| 0.2| 0.2| 0.2|\n|  1| 0.8| 0.6| 0.4| 0.2|\n|  0| 0.1| 0.2| 0.2| 0.2|\n|  0| 0.0| 0.0| 0.1| 0.2|\n\n</div>\n:::\n:::\n\n\n\nDann kann man die Wahrscheinlichkeit einfach in Entropie umrechnen^[Syntax aus @kurz_statistical_2021]:\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-12_941c1de2fa7fa9ac91fafd3ad2afd1c5'}\n\n```{.r .cell-code}\nd %>% \n  mutate_all(~ . / sum(.)) %>% \n  gather() %>% \n  group_by(key) %>% \n  summarise(h = -sum(ifelse(value == 0, 0, value * log(value))))\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|key |         h|\n|:---|---------:|\n|a   | 0.0000000|\n|b   | 0.6390319|\n|c   | 0.9502705|\n|d   | 1.4708085|\n|e   | 1.6094379|\n\n</div>\n:::\n:::\n\n\n\nDas `ifelse` dient nur dazu, eine Wahrscheinlichkeit von 0 eine Entropie von 0 zu verpassen^[Regel von L'Hopital], denn sonst würden wir ein Problem rennen, wenn wir $log(0)$ ausrechnen.\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-13_4542385575c1e35ab26589681fbfea05'}\n\n```{.r .cell-code}\nlog(0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -Inf\n```\n:::\n:::\n\n\n\n\n## Zufallstext erkennen\n\n\n### Entropie von Zufallstext\n\n\nKann man wohl Zufallstext maschinell erkennen? Sicher gibt es viele Ansätze,\num das Problem anzugehen.\nLassen Sie uns einen Ansatz erforschen. \nErforschen heißt, *wir* erforschen für *uns*, es handelt sich um eine didaktische Übung, \ndas Ziel ist nicht, Neuland für die Menschheit zu betreten.\n\nAber zuerst müssen wir überlegen, was \"Zufallstext\" bedeuten soll.\n\n\nNehmen wir uns dazu zuerst einen richtigen Text, ein Märchen von H.C. Andersen zum Beispiel.\nNehmen wir das Erste aus der Liste in dem Tibble `hcandersen_de`, \"das Feuerzeug\".\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-14_4a86dad54c7fc181f2fd18083866086f'}\n\n```{.r .cell-code}\ndas_feuerzeug <-\n  hcandersen_de  %>% \n  filter(book == \"Das Feuerzeug\") %>% \n  unnest_tokens(input = text, output = word) %>% \n  pull(word) \n\nhead(das_feuerzeug)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"es\"         \"kam\"        \"ein\"        \"soldat\"     \"die\"       \n[6] \"landstraße\"\n```\n:::\n:::\n\n\n\nDas Märchen ist 2688 Wörter lang.\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-15_1eb3fdf036147b220915073f89a2d396'}\n\n```{.r .cell-code}\nwortliste <- \nhcandersen_de  %>% \n  filter(book == \"Das Feuerzeug\") %>% \n  unnest_tokens(output = word, input = text) %>% \n  pull(word) %>% \n  unique()\n\nhead(wortliste)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"es\"         \"kam\"        \"ein\"        \"soldat\"     \"die\"       \n[6] \"landstraße\"\n```\n:::\n:::\n\n\n\nJetzt ziehen wir Stichproben (mit Zurücklegen) aus dieser Liste und erstellen damit eine Art Zufallstext.\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-16_a047c57bf1a6ae2e5f01e5d18f5cdfa7'}\n\n```{.r .cell-code}\nzufallstext <- \n  sample(x = wortliste, size = length(das_feuerzeug)*10^2, replace = TRUE)\nhead(zufallstext)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"oben\"      \"offiziere\" \"morgen\"    \"leg\"       \"soldaten\"  \"aber\"     \n```\n:::\n:::\n\n\n\nZählen wir, wie häufig jedes Wort vorkommt:\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-17_1878fccdf4123b4723c5d20552c35565'}\n\n```{.r .cell-code}\nzufallstext_count <-\ntibble(zufallstext = zufallstext) %>% \n  count(zufallstext)\n\nhead(zufallstext_count)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|zufallstext |   n|\n|:-----------|---:|\n|ab          | 397|\n|abend       | 357|\n|aber        | 402|\n|abflog      | 382|\n|abschlagen  | 376|\n|acht        | 390|\n\n</div>\n:::\n:::\n\n\n\nDer Häufigkeitsvektor von `wortliste` besteht nur aus Einsen, \nso haben wir ja gerade die Wortliste definiert:\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-18_2f057cb0f26d44905767121c36d35dea'}\n\n```{.r .cell-code}\nwortliste_count <-\ntibble(wortliste = wortliste) %>% \n  count(wortliste)\n\nhead(wortliste_count)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|wortliste  |  n|\n|:----------|--:|\n|ab         |  1|\n|abend      |  1|\n|aber       |  1|\n|abflog     |  1|\n|abschlagen |  1|\n|acht       |  1|\n\n</div>\n:::\n:::\n\n\n\nDaher ist ihre Informationsentropy maximal.\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-19_51f6c5cd5528e4c2ca69d4d9de6e5722'}\n\n```{.r .cell-code}\nentropy(wortliste_count$n, unit = \"log2\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 9.47978\n```\n:::\n:::\n\n\n\n\nDie Häufigkeiten der Wörter in `zufallstext` hat eine hohe Entropie.\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-20_cfe7379fae9c1da504e2d35e820fd809'}\n\n```{.r .cell-code}\nentropy(zufallstext_count$n, unit = \"log2\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 9.477938\n```\n:::\n:::\n\n\n\nZählen wir die Häufigkeiten in der Geschichte \"Das Feuerzeug\".\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-21_d6a7b6ca5e8214dd36dc72274551bcec'}\n\n```{.r .cell-code}\ndas_feuerzeug_count <-\n  tibble(text = das_feuerzeug) %>% \n  count(text)\n\nhead(das_feuerzeug_count)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|text       |  n|\n|:----------|--:|\n|ab         |  2|\n|abend      |  3|\n|aber       | 21|\n|abflog     |  1|\n|abschlagen |  1|\n|acht       |  1|\n\n</div>\n:::\n:::\n\nUnd berechnen dann die Entropie:\n\n\n::: {.cell hash='Information_cache/html/unnamed-chunk-22_a02e25cc2c768b2b1a5c51ba08c8aab2'}\n\n```{.r .cell-code}\nentropy(das_feuerzeug_count$n, unit = \"log2\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8.075194\n```\n:::\n:::\n\n\n\nDer Zufallstext hat also eine höhere Entropie als der echte Märchentext.\nDer Zufallstext ist also gleichverteilter in den Worthäufigkeiten.\n\n\nPro Bit weniger Entropie halbiert sich die Anzahl der Möglichkeiten einer Häufigkeitsverteilung.\n\n\n\n### MI von Zufallstext\n\n\n\n\n\n\n## Literatur\n\n\n### Wikipedia\n\nEs gibt eine Reihe nützlicher (und recht informationsdichter) Wikipedia-Einträge zum Thema Informationstheorie.\n\n- [Wikipedia: Pointwise Mutual Information](https://en.wikipedia.org/wiki/Pointwise_mutual_information)\n- [Wikipedia: Mutual information](https://en.wikipedia.org/wiki/Mutual_information)\n- [Wikipedia: Information theory](https://en.wikipedia.org/wiki/Information_theory)\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}