{
  "hash": "5be51149a63613e80fc207e6601fb44b",
  "result": {
    "markdown": "\n\n\n![Text als Datenbasis prädiktiver Modelle](img/text-mining-1476780_640.png){width=10%}\n\nBild von <a href=\"https://pixabay.com/de/users/mcmurryjulie-2375405/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1476780\">mcmurryjulie</a> auf <a href=\"https://pixabay.com/de//?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1476780\">Pixabay</a>\n\n# Twitter Mining\n\n\n\n## Vorab\n\n\n### Lernziele\n\n\n- Twitterdaten via API von Twitter auslesen\n\n\n\n### Vorbereitung\n\n- Lesen Sie in @smltar Kap. 1.\n- Legen Sie sich ein Konto bei [Github](https://github.com/) an.\n- Legen Sie sich ein Konto bei [Twitter](twitter.com) an.\n- Lesen Sie [diesen Artikel zur Anmeldung bei der Twitter API](https://docs.ropensci.org/rtweet/articles/auth.html)^[Sie können [hier](https://www.howtogeek.com/343877/what-is-an-api/) nachlesen, was eine API ist.]\n\n\n\n\n### Benötigte R-Pakete\n\n\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-1_493145a2abd6f91e44c0568ce5387a43'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(rtweet)\n```\n:::\n\n\n![R-Paket {rtweet}](https://docs.ropensci.org/rtweet/logo.png){width=10%}\n\n\nEinen Überblick über die Funktionen des Pakets (function reference) findet sich [hier](https://docs.ropensci.org/rtweet/reference/index.html).\n\n\n\n## Anmelden bei Twitter\n\n\n### Welche Accounts interessieren uns?\n\n\nHier ist eine (subjektive) Auswahl von deutschen Politikern,\ndie einen Startpunkt gibt zur Analyse von Art und Ausmaß von Hate Speech gerichtet an deutsche Politiker:innen.\n\n\n::: {.cell hash='twittermining_cache/html/politicians-df-load_6be8a5376d3d9c6db892597e3d6a2da7'}\n\n```{.r .cell-code}\nd_path <- \"data/twitter-german-politicians.csv\"\n\nd <- read_csv(d_path)\nd\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|name                                             |party  |screenname      |comment                                |\n|:------------------------------------------------|:------|:---------------|:--------------------------------------|\n|Karl Lauterbach                                  |SPD    |Karl_Lauterbach |NA                                     |\n|Olaf Scholz                                      |SPD    |OlafScholz      |NA                                     |\n|Annalena Baerback                                |Gruene |ABaerbock       |NA                                     |\n|Bundesministerium für Wirtschaft und Klimaschutz |Gruene |BMWK            |Robert Habeck ist der Minister im BMWK |\n|Friedrich Merz                                   |CDU    |_FriedrichMerz  |CDU-Chef                               |\n|Markus Söder                                     |CSU    |Markus_Soeder   |CSU-Chef                               |\n|Cem Özdemir                                      |Gruene |cem_oezdemir    |BMEL                                   |\n|Janine Wissler                                   |Linke  |Janine_Wissler  |Linke-Chefin                           |\n|Martin Schirdewan                                |Linke  |schirdewan      |Linke-Chef                             |\n|Christian Lindner                                |FDP    |c_lindner       |FDP-Chef                               |\n|Marie-Agnes Strack-Zimmermann                    |FDP    |MAStrackZi      |Vorsitzende Verteidigungsausschuss     |\n|Tino Chrupalla                                   |AFD    |Tino_Chrupalla  |AFD-Bundessprecher                     |\n|Alice Weidel                                     |AFD    |Alice_Weidel    |AFD-Bundessprecherin                   |\n\n</div>\n:::\n:::\n\n\n\n\n### Twitter App erstellen\n\n[Tutorial](https://cran.r-project.org/web/packages/rtweet/vignettes/auth.html)\n\n\n### Intro\n\nDie Seite von [rtweet](https://docs.ropensci.org/rtweet/) gibt eine gute Starthilfe in die Benutzung des Pakets.\n\n\n### Zugangsdaten\n\n\nZugangsdaten sollte man geschützt speichern, also z.B. nicht in einem geteilten Ordner.\n\n\n\n::: {.cell hash='twittermining_cache/html/source-credentials-twitter_a449d902848402bd59a4cbe6b93bb15a'}\n\n```{.r .cell-code}\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\n```\n:::\n\n\n\nAnmelden:\n\n\n::: {.cell hash='twittermining_cache/html/oauth-twitter_7b2da3bb994227e10d083e04a85f6c39'}\n\n```{.r .cell-code}\n#auth_setup_default() \n\nauth <- rtweet_bot(api_key = api_key,\n                   api_secret = api_secret,\n                   access_token = access_token,\n                   access_secret = access_secret)\n```\n:::\n\n\n\n\n## Tweets einlesen\n\n\nZu beachten ist, dass es Limits gibt, wie viele Informationen (pro Zeiteinheit) man über die Twitter-API auslesen darf.\nInformationen dazu findet man z.B. [hier](https://developer.twitter.com/en/docs/twitter-api/rate-limits) oder auch mit `rate_limit()`.\n\n\n\n\n\nEin gängiges Limit der Twitter-API sind 900 Anfragen (z.B. Tweets auslesen) pro 15 Minuten.\n\n### Timeline einlesen einzelner Accounts\n\nMal ein paar Tweets zur Probe:\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-2_c52ac9bb57d67a2f903b057dea98e3c8'}\n\n```{.r .cell-code}\nsesa_test <- get_timeline(user = \"sauer_sebastian\", n = 3) %>% \n  select(full_text)\n```\n:::\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-3_7ec4e11011bf0c09248746e2ff8dba83'}\n::: {.cell-output .cell-output-stdout}\n```\nRT @pia_lamberty: Ein Ansatz, der sich beim Debunking wissenschaftlich als erfolgreich herausgestellt hat, ist das sog. Faktensandwich: htt…\nRT @ianbremmer: sure, it’s the hottest summer europe has ever had in history \n\nbut look at the upside\n\nit’s one of the coolest summers euro…\nRT @twisteddoodles: Balanced news reporting https://t.co/O1iiItEQrs\n```\n:::\n:::\n\n::: {.cell hash='twittermining_cache/html/get-timeline1_a4896e9f706429e53db24f5a60a93ca9'}\n\n```{.r .cell-code}\ntweets <- get_timeline(user = d$screenname)\nsaveRDS(tweets, file = \"tweets/tweets01.rds\")\n```\n:::\n\n\n\n[Michael Kearney](https://rtweet-workshop.mikewk.com/#25) rät uns:\n\n>   PRO TIP #4: (for developer accounts only) Use `bearer_token()` to increase rate limit to 45,000 per fifteen minutes.\n\n### Retweets einlesen\n\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-4_c9680ff5fe0a99eb9ae876055073aa68'}\n\n```{.r .cell-code}\noptions(rtweet.retryonratelimit = TRUE)\n```\n:::\n\n::: {.cell hash='twittermining_cache/html/get-retweets1_23bc2370081e4cbbe020f74dad5ae145'}\n\n```{.r .cell-code}\ntweets01_retweets <- \n  tweets$id_str %>% \n  head(3) %>% \n  map_dfr( ~ get_retweets(status_id = .x, retryonratelimit = TRUE))\n```\n:::\n\n\n\n\nDa die meisten Retweets aber nix sagen, sondern nur auf das einen Tweet wiederholen, ist das Sammeln der Retweets ziemlich langweilig.\n\n\n### EPINetz Twitter Politicians 2021\n\n\n@konig_epinetz_2022 [Volltext hier](https://link.springer.com/article/10.1007/s11615-022-00405-7) haben einen Datensatz mit knapp 2500 Twitter Accounts deutscher Politiker zusammengestellt, zum Stand 2021.\n\n\nDer Datensatz kann über [Gesis](https://search.gesis.org/research_data/SDN-10.7802-2415?doi=10.7802/2415) bezogen werden.\n\nAuf der gleichen Seite findet sich auch eine [Dokumentation des Vorgehens](https://access.gesis.org/sharing/2415/3675).\n\nNachdem wir den Datensatz heruntergeladen haben, können wir ihn einlesen:\n\n\n::: {.cell hash='twittermining_cache/html/read-epinetz_17727ab1e3931b3e8a86d6d606bac8e3'}\n\n```{.r .cell-code}\npoliticians_path <- \"data/EPINetz_TwitterPoliticians_2021.RDs\"\npoliticians_twitter <- read_rds(politicians_path)\n\nhead(politicians_twitter)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|   ID|official_name   |party     |region             |institution        |office          |user_id            |twitter_name       |twitter_handle |from       |until | year_of_birth|abgeordnetenwatch_id |gender |wikidata_id |\n|----:|:---------------|:---------|:------------------|:------------------|:---------------|:------------------|:------------------|:--------------|:----------|:-----|-------------:|:--------------------|:------|:-----------|\n|  535|Manja Schüle    |SPD       |Brandenburg        |State Parliament   |Parliamentarian |827090742162100224 |Manja Schüle       |ManjaSchuele   |2019-09-25 |NA    |          1976|146790               |female |Q40974942   |\n|  962|Petra Pau       |DIE LINKE |Federal            |Federal Parliament |Parliamentarian |1683845126         |Team PetraPau      |TeamPetraPau   |2017-10-24 |NA    |          1963|79091                |female |Q77195      |\n|  864|Dagmar Schmidt  |SPD       |Federal            |Federal Parliament |Parliamentarian |1377117206         |Team #dieschmidt   |TeamDieSchmidt |2017-10-24 |NA    |          1973|79036                |female |Q15433815   |\n| 2517|Bernd Buchholz  |FDP       |Schleswig-Holstein |State Parliament   |Parliamentarian |1073605033         |Bernd Buchholz     |BerndBuchholz  |2017-06-06 |NA    |          1961|121092               |male   |Q823715     |\n| 1378|Ingrid Remmers  |DIE LINKE |Federal            |Federal Parliament |Parliamentarian |551802475          |Ingrid Remmers MdB |ingrid_remmers |2017-10-24 |NA    |          1965|120775               |female |Q1652660    |\n| 1116|Reinhard Brandl |CSU       |Federal            |Federal Parliament |Parliamentarian |262730721          |Reinhard Brandl    |reinhardbrandl |2017-10-24 |NA    |          1977|79427                |male   |Q111160     |\n\n</div>\n:::\n:::\n\n\nDann lesen wir die Timelines (die Tweets) dieser Konten aus;\nin diesem Beispiel nur 10 Tweets pro Account:\n\n\n\n::: {.cell hash='twittermining_cache/html/get-timeline2_c19317f6e6c84f31a7371435606990ef'}\n\n```{.r .cell-code}\nepi_tweets <- get_timeline(user = head(politicians_twitter$twitter_name), n = 10)\nhead(epi_tweets)\n```\n:::\n\n\n\nNatürlich könnte man auch mehr als 10 Tweets pro Konto einsammeln, braucht nur seine Zeit.\n\n### Followers suchen\n\n\n\n\n\n::: {.cell hash='twittermining_cache/html/save-followers1_067e380a642610a5148c5316a30d7434'}\n\n```{.r .cell-code}\nfollowers01 <-\n  d$screenname %>% \n map_dfr( ~ get_followers(user = .x, retryonratelimit = TRUE))\n\nsaveRDS(followers01, file = \"tweets/followers01.rds\")\n```\n:::\n\n\n\nDamit haben wir eine Liste an Followers, deren Tweets wir einlesen und analysieren können,\nz.B. nach Hate Speech.\n\nIm Gegensatz zu Followers heißen bei Twitter die Accounts, denen ei Nutzi folgt \"Friends\".\n\n\n\n### Follower Tweets einlesen\n\n\n\n::: {.cell hash='twittermining_cache/html/get-timeline3_5b1fd4d4d5b0eb19275c292a011ae4b8'}\n\n```{.r .cell-code}\nfollowers_tweets <- get_timeline(user = head(followers01$from_id), n = 10)\n```\n:::\n\n\n\n\n## Cron Jobs\n\n\n\n### Was ist ein Cron Job?\n\n[Cron](https://en.wikipedia.org/wiki/Cron) ist ein Programm auf Unix-artigen Betriebssystemen, das Skripte zu eingestellten Zeiten (wiederholt) ausführt, das sind dann \"Cron Jobs\".\nAuf Windows gibt es aber analoge Funktionen.\nCron Jobs sind praktisch, da man nicht jedes Mal selber z.B. Tweets, die heute zu einem Thema getweetet wurden, herunterladen muss.\nDas wird dann vom Cron Job übernommen.\n\nIn R gibt es eine API zum Programm Cron mit dem Paket `{cronR}`, s. [Anleitung hier](https://github.com/bnosac/cronR).\n\nDas analoge R-Paket für Windows heißt [`{taskscheduleR}`](https://github.com/bnosac/taskscheduleR).\n\n\n### Beispiel für einen Cron Job\n\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-5_f246dd1b0e2d85731fc5f99e0889c28a'}\n\n```{.r .cell-code}\nlibrary(cronR)\n\nscrape_script <- cron_rscript(\"scrape_tweets.R\")\n\n# Cron Job hinzufügen:\ncron_add(command = scrape_script, \n         frequency = 'daily', \n         at = \"10AM\",\n         id = 'Hate Speech')  # Name des Cron Jobs\n\ncron_clear(ask = FALSE)  # Alle Cron Jobs löschen\ncron_ls()  # Liste aller Cron Jobs\n```\n:::\n\n\n\nIm obigen Beispiel wird das R-Skript `scrape_tweets.R` täglich um 10h ausgeführt.\n\n\n\n\nDer Inhalt von `scrape_tweets.R` könnte dann, in Grundzügen, so aussehen:\n\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-6_e16e0ada64ce94bae528f7a079576fad'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(rtweet)\nfollowers_lauterbach <-\n  followers01 %>% \n  filter(to_id == \"Karl_Lauterbach\")\n\nfollowers_lauterbach_tweets <- \n  get_timeline(user = followers_lauterbach$from_id[1:10], n = 10, retryonratelimit = TRUE, verbose = FALSE)\n\n\npath_output <- \"/Users/sebastiansaueruser/Google Drive/RRRing/Scrape-Tweets/tweets/\"\n\nwrite_csv(x = followers_lauterbach_tweets,\n          file = paste0(path_output, \"followers_lauterbach_tweets.csv\"),\n          append = TRUE)\n```\n:::\n\n\n\nWir schreiben nicht jedes Mal (jeden Tag) eine neue CSV-Datei, sondern wir hängen hier die neu ausgelesenen Daten an die Datei an.\n\nLeider ist es mit `rtweet` nicht möglich, ein Datum anzugeben, ab dem man Tweets auslesen möchte^[Mit dem R-Paket `twitteR`, das mittlerweile zugunsten von `rtweet` aufgegeben wurde, war das möglich. Allerdings zeigt ein [Blick in die Dokumentation der Twitter-API](https://developer.twitter.com/en/docs/twitter-api/v1/tweets/timelines/api-reference/get-statuses-home_timeline), das Datumsangaben offenbar gar nicht unterstützt werden.]\n\n\n## Workshop zu rtweet\n\nErarbeiten Sie die Folien zu diesem [rtweet-Workshop](https://rtweet-workshop.mikewk.com/#1). Eine Menge guter Tipps!\n\n\n\n## Aufgaben\n\n1. Überlegen Sie, wie Sie das Ausmaß an Hate Speech, dem deutsche Politikerinnen und Politiker konfrontiert sind, messen können.\n2. Argumentieren Sie die Vorteile und Nachteile Ihres Ansatzes. Außerdem, auf welches Ergebnis dieser Analyse sie gespannt sind bzw. wären.\n3. Überlegen Sie Korrelate, oder besser noch: (mögliche) Ursachen, des Hasses in den Tweets, gerichtet auf Polikter:innen. Sie können auch Gruppen von Ursachen bilden, etwas personengebundene Variablen der Politiker:innen (z.B. Alter? Geschlecht? Migrationshintergrund?).\n1. Erstellen Sie sich eine Liste an Personen, deren Tweets sich lohnen (könnten), auf Hate Speech hin analysiert zu werden. Laden Sie deren Tweets (ggf. in Auszügen) herunter.\n6. Das Skript zu `scrape_tweets.R` könnte man noch verbessern, in dem man jeden Tag nur die neuesten Tweets herunterlädt. Dazu kann man bei [get_timeline()](https://docs.ropensci.org/rtweet/reference/get_timeline.html) mit dem Argument `since_id` eine Untergrenze der ID festlegen, so dass nur neuere Tweets (d.h. mit größerem Wert bei ID) ausgelesen werden. Ändern Sie das Skript entsprechend, so dass nur neuerer Tweets gelesen werden.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}