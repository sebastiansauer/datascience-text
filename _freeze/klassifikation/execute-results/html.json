{
  "hash": "5f420272fb5f1686d3b44c4dd187aed4",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n\n# Klassifikation von Hatespeech\n\n\n\n\n\n## Vorab\n\n\n### Lernziele\n\n\n- Sie können grundlegende Verfahren zur Klassifikation von Hatespeech einsetzen und erklären\n\n\n\n\n\n\n\n\n### Benötigte R-Pakete\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-1_5011f77326c4e3c6b6cdebf87a44c70a'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(tidymodels)\nlibrary(tidytext)\nlibrary(textrecipes)\nlibrary(lsa)  # stopwords\nlibrary(discrim)  # naive bayes classification\nlibrary(naivebayes)\nlibrary(tictoc)  # Zeitmessung\nlibrary(fastrtext)  # Worteinbettungen\nlibrary(remoji)  # Emojis\nlibrary(tokenizers)  # Vektoren tokenisieren\n```\n:::\n\n\n\n## Daten\n\n\nFür Maschinenlernen brauchen wir Trainingsdaten,\nDaten also, bei denen wir pro Beobachtung der Wert der Zielvariablen kennen.\nMan spricht auch von \"gelabelten\" Daten.\n\nWir nutzen die Daten von @wiegand_germeval bzw. @wiegand-data.\nDie Daten sind unter CC-By-4.0 Int. lizensiert.\n\n\n::: {.cell hash='klassifikation_cache/html/import-heidelberg-data_d15f702434e18cf1a6c930f35108cbc2'}\n\n```{.r .cell-code}\nd_raw <- \n  import(\"data/germeval2018.training.txt\",\n         header = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in (function (input = \"\", file = NULL, text = NULL, cmd = NULL, : Found\nand resolved improper quoting out-of-sample. First healed line 111: <<\"Edel sei\nder Mensch, hilfreich und gut\" - Nicht eine dieser Charaktereigenschaften kann\nMerkel für sich beanspruchen. OTHER OTHER>>. If the fields are not quoted (e.g.\nfield separator does not appear within any field), try quote=\"\" to avoid this\nwarning.\n```\n:::\n:::\n\n\n\nDa die Daten keine Spaltenköpfe haben, informieren wir die Funktion dazu mit `header = FALSE`.\n\nBenennen wir die die Spalten um:\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-2_ffce71786af673ec0286d362e54fac1e'}\n\n```{.r .cell-code}\nnames(d_raw) <- c(\"text\", \"c1\", \"c2\")\n```\n:::\n\n\nDabei soll `c1` und `c2` für die 1. bzw. 2. Klassifikation stehen.\n\n\nIn `c1` finden sich diese Werte:\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-3_ec8a5462101a2e904367eab06bb7623b'}\n\n```{.r .cell-code}\nd_raw %>% \n  count(c1)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|c1      |    n|\n|:-------|----:|\n|OFFENSE | 1688|\n|OTHER   | 3321|\n\n</div>\n:::\n:::\n\n\nHier wurde klassifiziert,\nob beleidigende Sprache (offensive language) vorlag oder nicht [@isch-etal-2021-overview, S. 2]:\n\n\n>   Task 1 was to decide whether a tweet includes some form of offensive language or not. The tweets had to be classiﬁed into the two classes OFFENSE and OTHER. The OFFENSE category covered abusive language, insults, as well as merely profane statements.\n\n\nUnd in `c2` finden sich folgende Ausprägungen:\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-4_d84ef45b2c95f0fb28f404187739ae23'}\n\n```{.r .cell-code}\nd_raw %>% \n  count(c2)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|c2        |    n|\n|:---------|----:|\n|ABUSE     | 1022|\n|INSULT    |  595|\n|OTHER     | 3321|\n|PROFANITY |   71|\n\n</div>\n:::\n:::\n\n\n\nIn `c2` ging es um eine feinere Klassifikation beleidigender Sprache [@isch-etal-2021-overview, S. 2]:\n\n>   The second task involved four categories, a nonoffensive OTHER class and three sub-categories of what is OFFENSE in Task 1. In the case of PROFANITY, profane words are used, however, the tweet does not want to insult anyone. This typically concerns the usage of swearwords (Scheiße, Fuck etc.) and cursing (Zur Hölle! Verdammt! etc.). This can be often found in youth language. Swearwords and cursing may, but need not, co-occur with insults or abusive speech. Profane language may in fact be used in tweets with positive sentiment to express emphasis. Whenever profane words are not directed towards a speciﬁc person or group of persons and there are no separate cues of INSULT or ABUSE, then tweets are labeled as simple cases of PROFANITY.\n\n\n\n\nSind Texte, die als `OFFENSE` klassifiziert sind,\nauch (fast) immer als `ABUSE`, `INSULT` oder `PROFANITY` klassifiziert?\n\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-5_ef75f7566b7d35a79c95ff27f839d505'}\n\n```{.r .cell-code}\nd_raw %>% \n  filter(c1 == \"OTHER\", c2 == \"OTHER\") %>% \n  nrow() / nrow(d_raw)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6630066\n```\n:::\n:::\n\n\nIn ca. 2/3 der Fälle wurden in beiden Klassifikation `OTHER` klassifiziert.\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-6_729200e672190fbda2cf3addbe896963'}\n\n```{.r .cell-code}\nd_raw %>% \n  filter(c1 != \"OTHER\", c2 != \"OTHER\") %>% \n  nrow() / nrow(d_raw)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3369934\n```\n:::\n:::\n\n\nEntsprechend in ca. 1/3 der Fälle wurde jeweils nicht mit `OTHER` klassifiziert.\n\n\nWir begnügen uns hier mit der ersten, gröberen Klassifikation.\n\n\n## Feature Engineering\n\n\nReichern wir die Daten mit weiteren Features an,\nin der Hoffnung, damit eine bessere Klassifikation erzielen zu können.\n\n\n### Textlänge\n\n\n\n\n\n::: {.cell hash='klassifikation_cache/html/d2_34d67ac0e2789dfd4d53bd64705b0862'}\n\n```{.r .cell-code}\nd2 <-\n  d_raw %>% \n  mutate(text_length = str_length(text)) %>% \n  mutate(id = 1:nrow(.))\n\nhead(d2)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|text                                                                                                                                                                                                                                                                                         |c1      |c2     | text_length| id|\n|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------|:------|-----------:|--:|\n|@corinnamilborn Liebe Corinna, wir würden dich gerne als Moderatorin für uns gewinnen! Wärst du begeisterbar?                                                                                                                                                                                |OTHER   |OTHER  |         109|  1|\n|@Martin28a Sie haben ja auch Recht. Unser Tweet war etwas missverständlich. Dass das BVerfG Sachleistungen nicht ausschließt, kritisieren wir.                                                                                                                                               |OTHER   |OTHER  |         142|  2|\n|@ahrens_theo fröhlicher gruß aus der schönsten stadt der welt theo ⚓️                                                                                                                                                                                                                        |OTHER   |OTHER  |          69|  3|\n|@dushanwegner Amis hätten alles und jeden gewählt...nur Hillary wollten sie nicht und eine Fortsetzung von Obama-Politik erst recht nicht..!                                                                                                                                                 |OTHER   |OTHER  |         140|  4|\n|@spdde kein verläßlicher Verhandlungspartner. Nachkarteln nach den Sondierzngsgesprächen - schickt diese Stümper #SPD in die Versenkung.                                                                                                                                                     |OFFENSE |INSULT |         136|  5|\n|@Dirki_M Ja, aber wo widersprechen die Zahlen denn denen, die im von uns verlinkten Artikel stehen? In unserem Tweet geht es rein um subs. Geschützte. 2017 ist der gesamte Familiennachzug im Vergleich zu 2016 - die Zahlen, die Hr. Brandner bemüht - übrigens leicht rückläufig gewesen. |OTHER   |OTHER  |         284|  6|\n\n</div>\n:::\n:::\n\n\n\n\n### Sentimentanalyse\n\nWir nutzen dazu `SentiWS` [@Remus2010].\n\n\n::: {.cell hash='klassifikation_cache/html/read-sentiws_2f767942281bb38d29abade5a1aa0999'}\n\n```{.r .cell-code}\nsentiws <- read_csv(\"https://osf.io/x89wq/?action=download\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 3468 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): neg_pos, word, inflections\ndbl (1): value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/d2-long_77a2f69e65e911b3658137b948ada80a'}\n\n```{.r .cell-code}\nd2_long <-\n  d2 %>% \n  unnest_tokens(input = text, output = token)\n\nhead(d2_long)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|c1    |c2    | text_length| id|token          |\n|:-----|:-----|-----------:|--:|:--------------|\n|OTHER |OTHER |         109|  1|corinnamilborn |\n|OTHER |OTHER |         109|  1|liebe          |\n|OTHER |OTHER |         109|  1|corinna        |\n|OTHER |OTHER |         109|  1|wir            |\n|OTHER |OTHER |         109|  1|würden         |\n|OTHER |OTHER |         109|  1|dich           |\n\n</div>\n:::\n:::\n\n\nJetzt filtern wir unsere Textdaten so,\ndass nur Wörter mit Sentimentwert übrig bleiben:\n\n\n::: {.cell hash='klassifikation_cache/html/d2-senti-long_2b9caa6e08a624466310b69af250e5b9'}\n\n```{.r .cell-code}\nd2_long_senti <- \n  d2_long %>%  \n  inner_join(sentiws %>% select(-inflections), by = c(\"token\" = \"word\"))\n\nhead(d2_long)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|c1    |c2    | text_length| id|token          |\n|:-----|:-----|-----------:|--:|:--------------|\n|OTHER |OTHER |         109|  1|corinnamilborn |\n|OTHER |OTHER |         109|  1|liebe          |\n|OTHER |OTHER |         109|  1|corinna        |\n|OTHER |OTHER |         109|  1|wir            |\n|OTHER |OTHER |         109|  1|würden         |\n|OTHER |OTHER |         109|  1|dich           |\n\n</div>\n:::\n:::\n\n\n\nSchließlich berechnen wir die Sentimentwert pro Polarität und pro Tweet:\n\n\n::: {.cell hash='klassifikation_cache/html/d2-sentis_5c4829559233ee9cbd1e3ec1127865e4'}\n\n```{.r .cell-code}\nd2_sentis <-\n  d2_long_senti %>% \n  group_by(id, neg_pos) %>% \n  summarise(senti_avg = mean(value))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'id'. You can override using the `.groups`\nargument.\n```\n:::\n\n```{.r .cell-code}\nhead(d2_sentis)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| id|neg_pos | senti_avg|\n|--:|:-------|---------:|\n|  1|pos     |    0.0040|\n|  2|neg     |   -0.3466|\n|  6|neg     |   -0.2042|\n|  6|pos     |    0.0040|\n|  8|neg     |   -0.5023|\n|  9|pos     |    0.5161|\n\n</div>\n:::\n:::\n\n\n\nDiese Tabelle bringen wir wieder eine breitere Form,\num sie dann wieder mit den Hauptdaten zu vereinigen.\n\n\n\n::: {.cell hash='klassifikation_cache/html/d2-sentis-wide_9de5e00b949578004a81dcb9a79e8311'}\n\n```{.r .cell-code}\nd2_sentis_wide <-\n  d2_sentis %>% \n  pivot_wider(names_from = \"neg_pos\", values_from = \"senti_avg\")\n\nd2_sentis_wide %>% head()\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| id|    pos|     neg|\n|--:|------:|-------:|\n|  1| 0.0040|      NA|\n|  2|     NA| -0.3466|\n|  6| 0.0040| -0.2042|\n|  8|     NA| -0.5023|\n|  9| 0.5161|      NA|\n| 11| 0.0040|      NA|\n\n</div>\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/d3_d83097a4828f8e33a43bba65b437d2df'}\n\n```{.r .cell-code}\nd3 <-\n  d2 %>% \n  full_join(d2_sentis_wide)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"id\"\n```\n:::\n\n```{.r .cell-code}\nhead(d3)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|text                                                                                                                                                                                                                                                                                         |c1      |c2     | text_length| id|   pos|     neg|\n|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------|:------|-----------:|--:|-----:|-------:|\n|@corinnamilborn Liebe Corinna, wir würden dich gerne als Moderatorin für uns gewinnen! Wärst du begeisterbar?                                                                                                                                                                                |OTHER   |OTHER  |         109|  1| 0.004|      NA|\n|@Martin28a Sie haben ja auch Recht. Unser Tweet war etwas missverständlich. Dass das BVerfG Sachleistungen nicht ausschließt, kritisieren wir.                                                                                                                                               |OTHER   |OTHER  |         142|  2|    NA| -0.3466|\n|@ahrens_theo fröhlicher gruß aus der schönsten stadt der welt theo ⚓️                                                                                                                                                                                                                        |OTHER   |OTHER  |          69|  3|    NA|      NA|\n|@dushanwegner Amis hätten alles und jeden gewählt...nur Hillary wollten sie nicht und eine Fortsetzung von Obama-Politik erst recht nicht..!                                                                                                                                                 |OTHER   |OTHER  |         140|  4|    NA|      NA|\n|@spdde kein verläßlicher Verhandlungspartner. Nachkarteln nach den Sondierzngsgesprächen - schickt diese Stümper #SPD in die Versenkung.                                                                                                                                                     |OFFENSE |INSULT |         136|  5|    NA|      NA|\n|@Dirki_M Ja, aber wo widersprechen die Zahlen denn denen, die im von uns verlinkten Artikel stehen? In unserem Tweet geht es rein um subs. Geschützte. 2017 ist der gesamte Familiennachzug im Vergleich zu 2016 - die Zahlen, die Hr. Brandner bemüht - übrigens leicht rückläufig gewesen. |OTHER   |OTHER  |         284|  6| 0.004| -0.2042|\n\n</div>\n:::\n:::\n\n\n\n:::callout-note\nDie Sentimentanalyse hier vernachlässigt Flexionen der Wörter. \nDer  Autor fühlt den Drang zu schreiben: \"Left as an exercise for the reader\" :-)\n:::\n\n\n### Schimpfwörter\n\n\nZählen wir die Schimpfwörter pro Text.\nDazu nutzen wir die Daten von [LDNOOBW](https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/blob/master/LICENSE), lizensiert nach CC-BY-4.0-Int.\n\n\n\n\n\n::: {.cell hash='klassifikation_cache/html/schimpf1_d1299213ad9b6a15bf862917328d302b'}\n\n```{.r .cell-code}\nschimpf1 <- import(\"https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/de\", format = \",\", header = FALSE)\n```\n:::\n\n\n\nLänger aber noch ist die Liste aus dem [InsultWiki](https://www.insult.wiki/schimpfwort-liste), lizensiert CC0.\n\n\n\n::: {.cell hash='klassifikation_cache/html/schimpf2_2042032e8abae3146d357f5614a67231'}\n\n```{.r .cell-code}\nschimpf2 <- \n  import(\"data/insult-de.txt\", header = FALSE) %>% \n  mutate_all(str_to_lower)\n```\n:::\n\n\n\nBinden wir die Listen zusammen:\n\n\n::: {.cell hash='klassifikation_cache/html/schimpf_ecce05ce4ac3c086638e936ea1a06201'}\n\n```{.r .cell-code}\nschimpf <-\n  schimpf1 %>% \n  bind_rows(schimpf2) %>% \n  distinct() %>% \n  rename(word = \"V1\")\n\nnrow(schimpf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6208\n```\n:::\n:::\n\n\n\nUm die Lesis vor (unnötiger?) Kopfverschmutzung zu bewahren,\nsind diese Schimpfwörter hier nicht abgedruckt.\n\nJetzt zählen wir, ob unsere Tweets/Texte solcherlei Wörter enthalten.\n\n\n\n::: {.cell hash='klassifikation_cache/html/d_schimpf_57a07b8f735ebff8cd47cc45337655a0'}\n\n```{.r .cell-code}\nd_schimpf <- \nd2_long %>% \n  select(id, token) %>% \n  mutate(schimpf = token %in% schimpf$word)\n```\n:::\n\n\n\nWie viele Schimpfwörter haben wir gefunden?\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-7_5c705ef651ce148fdbbe428ebd0b76e9'}\n\n```{.r .cell-code}\nd_schimpf %>% \n  count(schimpf)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|schimpf |     n|\n|:-------|-----:|\n|FALSE   | 99081|\n|TRUE    |  1136|\n\n</div>\n:::\n:::\n\n\n\nEtwa ein Prozent der Wörter sind Schimpfwörter in unserem Corpus.\n\n\n\n::: {.cell hash='klassifikation_cache/html/d-schimpf2_984b0cf907f12cf5b82a79ea4a40201d'}\n\n```{.r .cell-code}\nd_schimpf2 <-\n  d_schimpf %>% \n  group_by(id) %>% \n  summarise(schimpf_n = sum(schimpf))\n\nhead(d_schimpf2)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| id| schimpf_n|\n|--:|---------:|\n|  1|         0|\n|  2|         0|\n|  3|         0|\n|  4|         0|\n|  5|         1|\n|  6|         0|\n\n</div>\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/d_main_e41e00cfd20f68b086b0ff8103652b8c'}\n\n```{.r .cell-code}\nd_main <-\n  d3 %>% \n  full_join(d_schimpf2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"id\"\n```\n:::\n:::\n\n\n\n:::callout-important\nNamen wie `final`, `main` oder `result` sind gefährlich,\nda es unter Garantie ein \"final-final geben wird, oder der \"Haupt-Datensat\" plötzlich nicht mehr so wichtig erscheint und so weiter.\n:::\n\n\n\n### Emojis\n\n\n\n::: {.cell hash='klassifikation_cache/html/get-emoji-list_ca798f5f68205b7edbb1f7a73d40dfff'}\n\n```{.r .cell-code}\nemj <- emoji(list_emoji(), pad = FALSE)\n\nhead(emj)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"😄\" \"😃\" \"😀\" \"😊\" \"☺️\"  \"😉\"\n```\n:::\n:::\n\n\nDiese Liste umfasst knapp 900 Emojis, \ndas sind allerdings noch nicht alle, die es gibt.\n[Diese Liste](https://unicode.org/emoji/charts/full-emoji-list.html) umfasst mit gut 1800 Emojis\ngut das Doppelte.\n\n\nSelbstkuratierte Liste an \"wilden\" Emoji;\ndiese Liste ist inspiriert von [emojicombos.com](https://emojicombos.com/disgust).\n\n\n::: {.cell hash='klassifikation_cache/html/wild-emojis_e4ae2bdc1b3de20b3ed6c3cf4216a934'}\n\n```{.r .cell-code}\nwild_emojis <- \n  c(\n    emoji(find_emoji(\"gun\")),\n    emoji(find_emoji(\"bomb\")),\n    emoji(find_emoji(\"fist\")),\n    emoji(find_emoji(\"knife\"))[1],\n     emoji(find_emoji(\"ambulance\")),\n    \"😠\",\n    \"👹\",\n    \"💩\",\n    \"☠\",\n    \"🖕\",\n    emoji(find_emoji(\"middle finger\")),\n    \"😡\",\n    \"🤢\",\n    \"🤮\",\n    \"😖\",\n    \"😣\",\n    \"😩\",\n    \"😨\",\n    \"😝\",\n    \"😳\",\n    \"😬\",\n    \"😱\",\n    \"😵\",\n    \"😤\",\n    \"🤦‍♀️\",\n    \"🤦‍♂️\"\n  )\n```\n:::\n\n\n\n\n\nAuf dieser Basis können wir einen Prädiktor erstellen,\nder zählt, ob ein Tweet einen oder mehrere der \"wilden\" Emojis enthält.\n\n\n## Workflow 1: Rezept 1 + Naive-Bayes\n\n\n### Dummy-Rezept\n\n\nHier ist ein einfaches Beispiel,\num die Textvorbereitung mit `{textrecipes}` zu verdeutlichen.\n\nWir erstellen uns einen Dummy-Text:\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-8_3655bce3cec3126f80da286b0993920d'}\n\n```{.r .cell-code}\ndummy <- \n  tibble(text = c(\"Ich gehe heim und der die das nicht in ein and the\"))\n```\n:::\n\n\n\nDann tokenisieren wir den Text:\n\n\n::: {.cell hash='klassifikation_cache/html/rec-dummy1_9643d039e34554853ae829e08551ed05'}\n\n```{.r .cell-code}\nrec_dummy <-\n  recipe(text ~ 1, data = dummy) %>% \n  step_tokenize(text)\n  \nrec_dummy\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n    role #variables\n outcome          1\n\nOperations:\n\nTokenization for text\n```\n:::\n:::\n\n\n\nDie Tokens kann man sich so zeigen lassen:\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-9_72e327b3c9f9f1614ef88e707f02008c'}\n\n```{.r .cell-code}\nshow_tokens(rec_dummy, text)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n [1] \"ich\"   \"gehe\"  \"heim\"  \"und\"   \"der\"   \"die\"   \"das\"   \"nicht\" \"in\"   \n[10] \"ein\"   \"and\"   \"the\"  \n```\n:::\n:::\n\n\n\nJetzt entfernen wir die Stopwörter deutscher Sprache;\ndafür nutzen wir die Stopwort-Quelle `snowball`:\n\n\n\n::: {.cell hash='klassifikation_cache/html/rec-dummy2_17b8002eaabf808eb5898d5d9c9adc77'}\n\n```{.r .cell-code}\nrec_dummy <-\n  recipe(text ~ 1, data = dummy) %>% \n  step_tokenize(text) %>% \n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\")\n\nrec_dummy\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n    role #variables\n outcome          1\n\nOperations:\n\nTokenization for text\nStop word removal for text\n```\n:::\n:::\n\n\n\nPrüfen wir die Tokens; \nsind die Stopwörter wirklich entfernt?\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-10_6dd7e56fba4945514cdbec27d8b01103'}\n\n```{.r .cell-code}\nshow_tokens(rec_dummy, text)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[1] \"gehe\" \"heim\" \"and\"  \"the\" \n```\n:::\n:::\n\n\n\nJa, die deutschen Stopwörter sind entfernt. Die englischen nicht;\ndas macht Sinn!\n\n\n### Datenaufteilung\n\n\n\n::: {.cell hash='klassifikation_cache/html/d-split2_5cd853b5f88eb053f31d264cfb811947'}\n\n```{.r .cell-code}\nd_split <- initial_split(d_main, strata = c1)\n\nd_train <- training(d_split)\nd_test <- testing(d_split)\n```\n:::\n\n\n\n\n### Rezept 1\n\n\nRezept definieren:\n\n\n::: {.cell hash='klassifikation_cache/html/rec1_862c91870170ce220bfcb73a09c9c2b7'}\n\n```{.r .cell-code}\nrec1 <- \n  recipe(c1 ~ ., data = select(d_train, text, c1, id)) %>% \n  update_role(id, new_role = \"id\") %>% \n  step_tokenize(text) %>% \n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %>% \n  step_stem(text) %>% \n  step_tokenfilter(text, max_tokens = 1e2) %>% \n  step_tfidf(text) %>% \n  step_normalize(all_numeric_predictors())\n\nrec1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n        id          1\n   outcome          1\n predictor          1\n\nOperations:\n\nTokenization for text\nStop word removal for text\nStemming for text\nText filtering for text\nTerm frequency-inverse document frequency with text\nCentering and scaling for all_numeric_predictors()\n```\n:::\n:::\n\n\n\nPreppen:\n\n\n::: {.cell hash='klassifikation_cache/html/rec1-prepped_4ab9fbfb57c04c4b61cfeff52c0a27d3'}\n\n```{.r .cell-code}\nrec1_prepped <- prep(rec1)\n```\n:::\n\n\nUnd backen:\n\n\n::: {.cell hash='klassifikation_cache/html/rec1-baked_77d9d0bd3a5afd6d50de7a41ade43a57'}\n\n```{.r .cell-code}\nd_rec1 <- bake(rec1_prepped, new_data = NULL)\n\nhead(d_rec1)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| id|c1      | tfidf_text__macmik| tfidf_text_2| tfidf_text_ab| tfidf_text_afd| tfidf_text_amp| tfidf_text_anna_iina| tfidf_text_antisemitismu| tfidf_text_athinamala| tfidf_text_besser| tfidf_text_bild| tfidf_text_cdu| tfidf_text_charlie_silv| tfidf_text_csu| tfidf_text_d| tfidf_text_dafür| tfidf_text_dank| tfidf_text_dass| tfidf_text_deutsch| tfidf_text_deutschen| tfidf_text_deutschland| tfidf_text_dumm| tfidf_text_einfach| tfidf_text_ellibisathid| tfidf_text_endlich| tfidf_text_ennof_| tfidf_text_erst| tfidf_text_eu| tfidf_text_europa| tfidf_text_fdp| tfidf_text_feldenfrizz| tfidf_text_focusonlin| tfidf_text_frage| tfidf_text_frau| tfidf_text_ganz| tfidf_text_geht| tfidf_text_gerad| tfidf_text_gibt| tfidf_text_grünen| tfidf_text_gt| tfidf_text_gut| tfidf_text_hätte| tfidf_text_heut| tfidf_text_immer| tfidf_text_info2099| tfidf_text_islam| tfidf_text_israel| tfidf_text_ja| tfidf_text_jahr| tfidf_text_juden| tfidf_text_kommt| tfidf_text_krippmari| tfidf_text_land| tfidf_text_lassen| tfidf_text_lbr| tfidf_text_lifetrend| tfidf_text_link| tfidf_text_macht| tfidf_text_machtjanix23| tfidf_text_mal| tfidf_text_md_franz| tfidf_text_mehr| tfidf_text_menschen| tfidf_text_merkel| tfidf_text_miriamozen| tfidf_text_moslem| tfidf_text_müssen| tfidf_text_nancypeggymandi| tfidf_text_nasanas| tfidf_text_noherrman| tfidf_text_norbinator2403| tfidf_text_partei| tfidf_text_petpanther0| tfidf_text_politik| tfidf_text_recht| tfidf_text_richtig| tfidf_text_sagt| tfidf_text_schmiddiemaik| tfidf_text_schon| tfidf_text_schulz| tfidf_text_seit| tfidf_text_sicher| tfidf_text_spd| tfidf_text_tagesschau| tfidf_text_thomasgbau| tfidf_text_troll_putin| tfidf_text_trump| tfidf_text_tun| tfidf_text_türken| tfidf_text_u| tfidf_text_unser| tfidf_text_viel| tfidf_text_volk| tfidf_text_wäre| tfidf_text_warum| tfidf_text_welt| tfidf_text_wer| tfidf_text_willjrosenblatt| tfidf_text_wohl| tfidf_text_wurd| tfidf_text_zeit|\n|--:|:-------|------------------:|------------:|-------------:|--------------:|--------------:|--------------------:|------------------------:|---------------------:|-----------------:|---------------:|--------------:|-----------------------:|--------------:|------------:|----------------:|---------------:|---------------:|------------------:|--------------------:|----------------------:|---------------:|------------------:|-----------------------:|------------------:|-----------------:|---------------:|-------------:|-----------------:|--------------:|----------------------:|---------------------:|----------------:|---------------:|---------------:|---------------:|----------------:|---------------:|-----------------:|-------------:|--------------:|----------------:|---------------:|----------------:|-------------------:|----------------:|-----------------:|-------------:|---------------:|----------------:|----------------:|--------------------:|---------------:|-----------------:|--------------:|--------------------:|---------------:|----------------:|-----------------------:|--------------:|-------------------:|---------------:|-------------------:|-----------------:|---------------------:|-----------------:|-----------------:|--------------------------:|------------------:|--------------------:|-------------------------:|-----------------:|----------------------:|------------------:|----------------:|------------------:|---------------:|------------------------:|----------------:|-----------------:|---------------:|-----------------:|--------------:|---------------------:|---------------------:|----------------------:|----------------:|--------------:|-----------------:|------------:|----------------:|---------------:|---------------:|---------------:|----------------:|---------------:|--------------:|--------------------------:|---------------:|---------------:|---------------:|\n|  9|OFFENSE |         -0.1472962|   -0.1025123|    -0.1045303|      6.1558148|     -0.1196412|           -0.1016498|               -0.0827688|            -0.1412023|        -0.0994077|      -0.1000636|     -0.1061901|              -0.1450561|     -0.0881935|   -0.1365693|       -0.0938978|      -0.1176754|      -0.1911377|         -0.1543987|           -0.1519219|             -0.1996601|      -0.1040022|         -0.1195335|              -0.1473094|         -0.0995996|        -0.1182767|      -0.1107598|     -0.111993|        -0.1066217|     -0.0974439|             -0.1472962|            -0.0985444|       -0.0967565|      -0.0949119|      -0.1145095|      -0.1429535|        -0.108501|      -0.1648097|        -0.1039788|    -0.0926214|      -0.126624|       -0.0955942|      -0.1650169|       -0.1623571|          -0.0804698|       -0.1036414|        -0.0836286|     -0.175643|      -0.0937953|       -0.0974094|       -0.0961735|           -0.1455853|      -0.1441996|        -0.1116953|     -0.4023143|           -0.1445296|      -0.1052941|       -0.1245217|              -0.1088789|     -0.1709422|          -0.1082684|      -0.1829629|          -0.1167916|        -0.2184447|            -0.0966546|        -0.0942649|        -0.1315249|                 -0.1182767|         -0.1096851|            -0.090148|                -0.0631584|        -0.0922659|             -0.0922453|         -0.1476062|       -0.1162279|         -0.0976874|      -0.0964543|               -0.1435878|       -0.2044771|        -0.1013316|      -0.1270926|        -0.0937495|     -0.1342917|             9.5543591|            -0.1473094|             -0.1211889|       -0.1009074|     -0.1080669|        -0.1045075|    -0.150687|       -0.1585372|      -0.0998685|      -0.1090701|      -0.1112479|        -0.103608|      -0.1466292|     -0.1567758|                 -0.1435878|      -0.1091472|      -0.1136727|      -0.1001727|\n| 12|OFFENSE |         -0.1472962|   -0.1025123|    -0.1045303|     -0.1545404|     -0.1196412|           -0.1016498|               -0.0827688|            -0.1412023|        -0.0994077|      -0.1000636|     -0.1061901|              -0.1450561|     -0.0881935|   -0.1365693|       -0.0938978|      -0.1176754|      -0.1911377|         -0.1543987|           -0.1519219|             -0.1996601|      -0.1040022|         -0.1195335|              -0.1473094|         -0.0995996|        -0.1182767|      -0.1107598|     -0.111993|        -0.1066217|     -0.0974439|             -0.1472962|            -0.0985444|       -0.0967565|      -0.0949119|      -0.1145095|      -0.1429535|        -0.108501|      -0.1648097|        -0.1039788|    -0.0926214|      -0.126624|       -0.0955942|      -0.1650169|        3.6921593|          -0.0804698|       -0.1036414|        -0.0836286|      3.385033|      -0.0937953|       -0.0974094|       -0.0961735|           -0.1455853|      -0.1441996|        -0.1116953|      1.2297516|           -0.1445296|      -0.1052941|       -0.1245217|              -0.1088789|     -0.1709422|          -0.1082684|      -0.1829629|          -0.1167916|        -0.2184447|            -0.0966546|        -0.0942649|        -0.1315249|                 -0.1182767|         -0.1096851|            -0.090148|                -0.0631584|        -0.0922659|             -0.0922453|         -0.1476062|       -0.1162279|         -0.0976874|      -0.0964543|               -0.1435878|       -0.2044771|        -0.1013316|      -0.1270926|        -0.0937495|     -0.1342917|            -0.1043247|            -0.1473094|             -0.1211889|       -0.1009074|     -0.1080669|        -0.1045075|    -0.150687|       -0.1585372|      -0.0998685|      -0.1090701|      -0.1112479|        -0.103608|      -0.1466292|     -0.1567758|                 -0.1435878|      -0.1091472|      -0.1136727|      -0.1001727|\n| 17|OFFENSE |         -0.1472962|   -0.1025123|    -0.1045303|     -0.1545404|     -0.1196412|           -0.1016498|               -0.0827688|            -0.1412023|        -0.0994077|      -0.1000636|     -0.1061901|              -0.1450561|     -0.0881935|   -0.1365693|       -0.0938978|      -0.1176754|      -0.1911377|         -0.1543987|            3.7258962|             -0.1996601|      -0.1040022|         -0.1195335|              -0.1473094|         -0.0995996|        -0.1182767|      -0.1107598|     -0.111993|        -0.1066217|     -0.0974439|             -0.1472962|            -0.0985444|       -0.0967565|      -0.0949119|      -0.1145095|      -0.1429535|        -0.108501|      -0.1648097|        -0.1039788|    -0.0926214|      -0.126624|       -0.0955942|      -0.1650169|       -0.1623571|          -0.0804698|       -0.1036414|        -0.0836286|     -0.175643|      -0.0937953|       -0.0974094|       -0.0961735|           -0.1455853|      -0.1441996|        -0.1116953|     -0.4023143|           -0.1445296|      -0.1052941|       -0.1245217|              -0.1088789|     -0.1709422|          -0.1082684|      -0.1829629|          -0.1167916|        -0.2184447|            -0.0966546|        -0.0942649|        -0.1315249|                 -0.1182767|         -0.1096851|            -0.090148|                -0.0631584|        -0.0922659|             -0.0922453|          4.1966394|       -0.1162279|         -0.0976874|      -0.0964543|               -0.1435878|       -0.2044771|        -0.1013316|       4.4285094|        -0.0937495|     -0.1342917|            -0.1043247|            -0.1473094|             -0.1211889|       -0.1009074|     -0.1080669|        -0.1045075|    -0.150687|       -0.1585372|      -0.0998685|      -0.1090701|      -0.1112479|        -0.103608|      -0.1466292|     -0.1567758|                 -0.1435878|      -0.1091472|      -0.1136727|      -0.1001727|\n| 33|OFFENSE |         -0.1472962|   -0.1025123|    -0.1045303|     -0.1545404|     -0.1196412|           -0.1016498|               -0.0827688|            -0.1412023|        -0.0994077|      -0.1000636|     -0.1061901|              -0.1450561|     -0.0881935|   -0.1365693|       -0.0938978|      -0.1176754|      -0.1911377|          3.6445592|           -0.1519219|             -0.1996601|      -0.1040022|         -0.1195335|              -0.1473094|         -0.0995996|        -0.1182767|      -0.1107598|     -0.111993|        -0.1066217|     -0.0974439|             -0.1472962|            -0.0985444|       -0.0967565|      -0.0949119|      -0.1145095|      -0.1429535|        -0.108501|      -0.1648097|        -0.1039788|    -0.0926214|      -0.126624|       -0.0955942|      -0.1650169|       -0.1623571|          -0.0804698|       -0.1036414|        -0.0836286|     -0.175643|      -0.0937953|       -0.0974094|       -0.0961735|           -0.1455853|      -0.1441996|        -0.1116953|     -0.4023143|           -0.1445296|      -0.1052941|       -0.1245217|              -0.1088789|     -0.1709422|          -0.1082684|       3.1601002|          -0.1167916|        -0.2184447|            -0.0966546|         5.4434700|        -0.1315249|                 -0.1182767|         -0.1096851|            -0.090148|                -0.0631584|        -0.0922659|             -0.0922453|         -0.1476062|       -0.1162279|         -0.0976874|      -0.0964543|               -0.1435878|       -0.2044771|        -0.1013316|      -0.1270926|        -0.0937495|     -0.1342917|            -0.1043247|            -0.1473094|             -0.1211889|       -0.1009074|     -0.1080669|        -0.1045075|    -0.150687|       -0.1585372|      -0.0998685|      -0.1090701|      -0.1112479|        -0.103608|      -0.1466292|     -0.1567758|                 -0.1435878|      -0.1091472|      -0.1136727|      -0.1001727|\n| 42|OFFENSE |         -0.1472962|   -0.1025123|    -0.1045303|     -0.1545404|     -0.1196412|           -0.1016498|               -0.0827688|            -0.1412023|        -0.0994077|      -0.1000636|     -0.1061901|              -0.1450561|     -0.0881935|   -0.1365693|       -0.0938978|      -0.1176754|      -0.1911377|         -0.1543987|           -0.1519219|             -0.1996601|      -0.1040022|         -0.1195335|              -0.1473094|         -0.0995996|        -0.1182767|      -0.1107598|     -0.111993|        -0.1066217|      6.6612452|             -0.1472962|            -0.0985444|       -0.0967565|      -0.0949119|      -0.1145095|      -0.1429535|        -0.108501|      -0.1648097|        -0.1039788|    -0.0926214|      -0.126624|       -0.0955942|      -0.1650169|       -0.1623571|          -0.0804698|       -0.1036414|        -0.0836286|     -0.175643|      -0.0937953|       -0.0974094|       -0.0961735|           -0.1455853|      -0.1441996|        -0.1116953|      1.2297516|           -0.1445296|      -0.1052941|       -0.1245217|              -0.1088789|     -0.1709422|          -0.1082684|      -0.1829629|          -0.1167916|        -0.2184447|            -0.0966546|        -0.0942649|        -0.1315249|                 -0.1182767|         -0.1096851|            -0.090148|                -0.0631584|        -0.0922659|             -0.0922453|         -0.1476062|       -0.1162279|         -0.0976874|      -0.0964543|               -0.1435878|       -0.2044771|        -0.1013316|      -0.1270926|        -0.0937495|     -0.1342917|            -0.1043247|            -0.1473094|             -0.1211889|       -0.1009074|      5.9134349|        -0.1045075|    -0.150687|       -0.1585372|      -0.0998685|      -0.1090701|      -0.1112479|        -0.103608|      -0.1466292|     -0.1567758|                 -0.1435878|      -0.1091472|      -0.1136727|      -0.1001727|\n| 44|OFFENSE |         -0.1472962|   -0.1025123|    -0.1045303|     -0.1545404|     -0.1196412|           -0.1016498|               -0.0827688|            -0.1412023|        -0.0994077|      -0.1000636|     -0.1061901|              -0.1450561|     -0.0881935|   -0.1365693|       -0.0938978|      -0.1176754|      -0.1911377|         -0.1543987|           -0.1519219|             -0.1996601|      -0.1040022|         -0.1195335|              -0.1473094|         -0.0995996|        -0.1182767|      -0.1107598|     -0.111993|        -0.1066217|     -0.0974439|             -0.1472962|            -0.0985444|       -0.0967565|      -0.0949119|      -0.1145095|      -0.1429535|        -0.108501|      -0.1648097|        -0.1039788|    -0.0926214|      -0.126624|       17.7948786|      -0.1650169|       -0.1623571|          -0.0804698|       -0.1036414|        -0.0836286|     -0.175643|      -0.0937953|       -0.0974094|       -0.0961735|           -0.1455853|      -0.1441996|        -0.1116953|     -0.4023143|           -0.1445296|      -0.1052941|       -0.1245217|              -0.1088789|     -0.1709422|          -0.1082684|      -0.1829629|          -0.1167916|        -0.2184447|            -0.0966546|        -0.0942649|        -0.1315249|                 -0.1182767|         -0.1096851|            -0.090148|                -0.0631584|        -0.0922659|             -0.0922453|         -0.1476062|       -0.1162279|         -0.0976874|      -0.0964543|               -0.1435878|       -0.2044771|        -0.1013316|      -0.1270926|        -0.0937495|     -0.1342917|            -0.1043247|            -0.1473094|             -0.1211889|       -0.1009074|     -0.1080669|        -0.1045075|    -0.150687|       -0.1585372|      -0.0998685|      -0.1090701|      -0.1112479|        -0.103608|      -0.1466292|     -0.1567758|                 -0.1435878|      -0.1091472|      -0.1136727|      -0.1001727|\n\n</div>\n:::\n:::\n\n\n\n\n### Modellspezifikation 1\n\nWir definiere einen Naive-Bayes-Algorithmus:\n\n\n::: {.cell hash='klassifikation_cache/html/nb-spec_a7791b89b490498e0e4012c733f7328e'}\n\n```{.r .cell-code}\nnb_spec <- naive_Bayes() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"naivebayes\")\n\nnb_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNaive Bayes Model Specification (classification)\n\nComputational engine: naivebayes \n```\n:::\n:::\n\n\n\n\nUnd setzen auf die klassische zehnfache Kreuzvalidierung.\n\n\n\n::: {.cell hash='klassifikation_cache/html/folds1_b3892f13a41b2929c825b89d84c66d8c'}\n\n```{.r .cell-code}\nset.seed(42)\nfolds1 <- vfold_cv(d_train)\n```\n:::\n\n\n\n\n### Workflow 1\n\n\n\n::: {.cell hash='klassifikation_cache/html/wf1_e72c4ba077f687919fd19f44ec6270be'}\n\n```{.r .cell-code}\nwf1 <-\n  workflow() %>% \n  add_recipe(rec1) %>% \n  add_model(nb_spec)\n\nwf1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: naive_Bayes()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_tokenize()\n• step_stopwords()\n• step_stem()\n• step_tokenfilter()\n• step_tfidf()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nNaive Bayes Model Specification (classification)\n\nComputational engine: naivebayes \n```\n:::\n:::\n\n\n\n### Fitting 1\n\n\n\n::: {.cell hash='klassifikation_cache/html/fit1_328969f3ab8f347fdb510934be5cc1ba'}\n\n```{.r .cell-code}\nfit1 <-\n  fit_resamples(\n    wf1,\n    folds1,\n    control = control_resamples(save_pred = TRUE)\n  )\n```\n:::\n\n\nDie Vorhersagen speichern wir ab,\num die Performanz in den Faltungen des Hold-out-Samples zu berechnen.\n\n\nMöchte man sich die Zeit sparen, die Syntax wieder durchlaufen zu lassen,\nkann man das Objekt speichern. \nAber Vorsicht: Dabei kann es passieren, dass man mit veralteten Objekten arbeitet.\n\n\n\n\n::: {.cell hash='klassifikation_cache/html/write-fit1_6ed4f8078985f5247760098e7c0de1fd'}\n\n```{.r .cell-code}\nwrite_rds(fit1, \"objects/chap_classific_fit1.rds\")\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/read-fit1_db8cda8f55020b1d50431c737904ef67'}\n\n:::\n\n\n\n\n### Performanz 1\n\n\n::: {.cell hash='klassifikation_cache/html/wf1-perf_3811d11fb3139d008e377f589ac8ba58'}\n\n```{.r .cell-code}\nwf1_performance <-\n  collect_metrics(fit1)\n\nwf1_performance\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/wf1-preds_9e5a96ca972fd7ae2a9a70142a6d0d43'}\n\n```{.r .cell-code}\nwf_preds <-\n  collect_predictions(fit1)\n\nwf_preds %>% \n  group_by(id) %>% \n  roc_curve(truth = c1, .pred_OFFENSE) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](klassifikation_files/figure-html/wf1-preds-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-11_ea236098c2ea6b3269affd24767a2c5b'}\n\n```{.r .cell-code}\nconf_mat_resampled(fit1, tidy = FALSE) %>% \n  autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](klassifikation_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Nullmodell\n\n\n\n::: {.cell hash='klassifikation_cache/html/nullmodel_b690571da86498d71a6bb535a17f8ebd'}\n\n```{.r .cell-code}\nnull_classification <- \n  parsnip::null_model() %>%\n  set_engine(\"parsnip\") %>%\n  set_mode(\"classification\")\n\nnull_rs <- workflow() %>%\n  add_recipe(rec1) %>%\n  add_model(null_classification) %>%\n  fit_resamples(\n    folds1\n  )\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-12_dd57384be2732b328c9339c968505ff9'}\n\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-13_65bbd96a7cf51f378286db10dad252dc'}\n\n:::\n\n\n\n\nHier ist die Performanz des Nullmodells.\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-14_95c2277fea20fb5429f418c5883f7cfd'}\n\n```{.r .cell-code}\nnull_rs %>%\n  collect_metrics()\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-15_d30fc6459327830e6ca46f2af82e77a8'}\n\n```{.r .cell-code}\nshow_best(null_rs)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n```\n:::\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|   penalty|.metric |.estimator |      mean|  n|   std_err|.config               |\n|---------:|:-------|:----------|---------:|--:|---------:|:---------------------|\n| 0.0017433|roc_auc |binary     | 0.8116297| 10| 0.0087106|Preprocessor1_Model22 |\n| 0.0007880|roc_auc |binary     | 0.8111630| 10| 0.0089414|Preprocessor1_Model21 |\n| 0.0003562|roc_auc |binary     | 0.8103737| 10| 0.0092381|Preprocessor1_Model20 |\n| 0.0001610|roc_auc |binary     | 0.8099159| 10| 0.0094448|Preprocessor1_Model19 |\n| 0.0000000|roc_auc |binary     | 0.8095907| 10| 0.0096029|Preprocessor1_Model01 |\n\n</div>\n:::\n:::\n\n\n\n\n\n## Workflow 2: Rezept 1 + Lasso\n\n\n\n\n::: {.cell hash='klassifikation_cache/html/lasso-spec_a3aec9ef905b014eab24150902bf3f0c'}\n\n```{.r .cell-code}\nlasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"glmnet\")\n\nlasso_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n\n\nWir definieren die Ausprägungen von `penalty`, \ndie wir ausprobieren wollen:\n\n\n\n::: {.cell hash='klassifikation_cache/html/lambda-grid_535a5f4c7d20cfe55216cdbe231211d1'}\n\n```{.r .cell-code}\nlambda_grid <- grid_regular(penalty(), levels = 30)\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/wf2_d7cd23d180dba6309eb8d24a4268d4e6'}\n\n```{.r .cell-code}\nwf2 <-\n  workflow() %>% \n  add_recipe(rec1) %>% \n  add_model(lasso_spec)\n\nwf2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_tokenize()\n• step_stopwords()\n• step_stem()\n• step_tokenfilter()\n• step_tfidf()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n\nTunen und Fitten:\n\n\n::: {.cell hash='klassifikation_cache/html/fit2_72d0763e5fb111902bb4886c7c4a1b28'}\n\n```{.r .cell-code}\nset.seed(42)\n\nfit2 <-\n  tune_grid(\n    wf2,\n    folds1,\n    grid = lambda_grid,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nfit2\n```\n:::\n\n\n\nVorsicht beim Abspeichern.\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-16_3ad15af70ec18ef05156adac9f60d8b6'}\n\n```{.r .cell-code}\nwrite_rds(fit2, \"objects/chap_classific_fit2.rds\")\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/read-fit2_2588060668926c80c2d7bc67ff0a6553'}\n\n:::\n\n\n\n\nHier ist die Performanz:\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-17_2d69a2dc3119ef7d2a9ac6b76c6b0de3'}\n\n```{.r .cell-code}\ncollect_metrics(fit2) %>% \n  filter(.metric == \"roc_auc\") %>% \n  slice_max(mean, n = 3)\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-18_c6e265628955703678e0fe0e0a98c04a'}\n\n```{.r .cell-code}\nautoplot(fit2)\n```\n\n::: {.cell-output-display}\n![](klassifikation_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-19_9058c61de81de8826803a9cecb572ff2'}\n\n```{.r .cell-code}\nfit2 %>% \n  show_best(\"roc_auc\")\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| penalty|.metric |.estimator |      mean|  n|   std_err|.config               |\n|-------:|:-------|:----------|---------:|--:|---------:|:---------------------|\n|       0|roc_auc |binary     | 0.5997476| 10| 0.0090575|Preprocessor1_Model01 |\n|       0|roc_auc |binary     | 0.5997476| 10| 0.0090575|Preprocessor1_Model02 |\n|       0|roc_auc |binary     | 0.5997476| 10| 0.0090575|Preprocessor1_Model03 |\n|       0|roc_auc |binary     | 0.5997476| 10| 0.0090575|Preprocessor1_Model04 |\n|       0|roc_auc |binary     | 0.5997476| 10| 0.0090575|Preprocessor1_Model05 |\n\n</div>\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/chosen-auc-fit2_f25fa5cfdd99153c2a62003420ec98f3'}\n\n```{.r .cell-code}\nchosen_auc <- \n  fit2 %>%\n  select_by_one_std_err(metric = \"roc_auc\", -penalty)\n```\n:::\n\n\n\n\nFinalisieren:\n\n\n\n::: {.cell hash='klassifikation_cache/html/wf2-final_216ce62197475b9a9cd6027de21d075c'}\n\n```{.r .cell-code}\nwf2_final <-\n  finalize_workflow(wf2, chosen_auc)\n\nwf2_final\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_tokenize()\n• step_stopwords()\n• step_stem()\n• step_tokenfilter()\n• step_tfidf()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = 0.00853167852417281\n  mixture = 1\n\nComputational engine: glmnet \n```\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/fit2-final-train_d2071737f7c1acf60b93ee6664042cab'}\n\n```{.r .cell-code}\nfit2_final_train <-\n  fit(wf2_final, d_train)\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-20_79f35bc123785f1fead2307b2e2e45eb'}\n\n```{.r .cell-code}\nfit2_final_train %>% \n  extract_fit_parsnip() %>% \n  tidy() %>% \n  arrange(-abs(estimate)) %>% \n  head()\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|term              |   estimate|   penalty|\n|:-----------------|----------:|---------:|\n|(Intercept)       |  0.6992837| 0.0085317|\n|tfidf_text_dumm   | -0.2201192| 0.0085317|\n|tfidf_text_merkel | -0.2139564| 0.0085317|\n|tfidf_text_moslem | -0.1772228| 0.0085317|\n|tfidf_text_lbr    | -0.1459591| 0.0085317|\n|tfidf_text_islam  | -0.1168911| 0.0085317|\n\n</div>\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/fit2-final-test_c575137863264e8190e03ee8e269f361'}\n\n```{.r .cell-code}\nfit2_final_test <-\n  last_fit(wf2_final, d_split)\n\ncollect_metrics(fit2_final_test)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|.metric  |.estimator | .estimate|.config              |\n|:--------|:----------|---------:|:--------------------|\n|accuracy |binary     | 0.6831604|Preprocessor1_Model1 |\n|roc_auc  |binary     | 0.6561186|Preprocessor1_Model1 |\n\n</div>\n:::\n:::\n\n\n\n\n### Vorhersage\n\n\n### Vohersagedaten\n\nPfad zu den Daten:\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-21_74325ebfa3d01bb8da7b25cd99d51e87'}\n\n```{.r .cell-code}\ntweet_data_path <- \"/Users/sebastiansaueruser/github-repos/hate-speech/data/\"\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-22_b14f0bf8ebf05e553da6ae612fde9567'}\n\n```{.r .cell-code}\ntweet_data_files_names <- list.files(path = tweet_data_path,\n                                     pattern  = \"tweets-to-.*\\\\.rds$\")\nhead(tweet_data_files_names)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"tweets-to-_FriedrichMerz_2021.rds\" \"tweets-to-_FriedrichMerz_2022.rds\"\n[3] \"tweets-to-ABaerbock_2021.rds\"      \"tweets-to-ABaerbock_2022.rds\"     \n[5] \"tweets-to-Alice_Weidel_2021.rds\"   \"tweets-to-Alice_Weidel_2022.rds\"  \n```\n:::\n:::\n\n\n\nWie viele Dateien sind es?\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-23_9f5e48689aadaea8bd875c97a08da53b'}\n\n```{.r .cell-code}\nlength(tweet_data_files_names)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 26\n```\n:::\n:::\n\n\n\nWir geben den Elementen des Vektors gängige Namen,\ndas hilft uns gleich bei `map`:\n\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-24_53c7e929847ba9fd741ba88333d22b30'}\n\n```{.r .cell-code}\nnames(tweet_data_files_names) <- str_remove(tweet_data_files_names, \"\\\\.rds\")\n```\n:::\n\n\n\n\n\n\nOK, weiter: So können wir *eine* der Datendateien einlesen:\n\n\n::: {.cell hash='klassifikation_cache/html/read-tweets_1deee70c232f4351aeee6ab604f706c9'}\n\n```{.r .cell-code}\nd_raw <-\n  read_rds(file = paste0(tweet_data_path, tweet_data_files_names[1])) \n\nd <- \n  d_raw %>% \n  select(id, author_id, created_at, public_metrics) %>% \n  unnest_wider(public_metrics)\n\nhead(d)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|id                  |author_id           |created_at               | retweet_count| reply_count| like_count| quote_count|\n|:-------------------|:-------------------|:------------------------|-------------:|-----------:|----------:|-----------:|\n|1476992850944475136 |1270540287786565632 |2021-12-31T19:05:11.000Z |             0|           0|          0|           0|\n|1476982994556665862 |1471100575337140229 |2021-12-31T18:26:01.000Z |             0|           0|          0|           0|\n|1476958785977597958 |1438467230157602821 |2021-12-31T16:49:49.000Z |             0|           0|          0|           0|\n|1476637742884925447 |589112870           |2021-12-30T19:34:07.000Z |             0|           0|          0|           0|\n|1476587037046226949 |1041038433064562688 |2021-12-30T16:12:37.000Z |             0|           0|          0|           0|\n|1476534413802549249 |1425085042800406536 |2021-12-30T12:43:31.000Z |            10|           2|         44|           2|\n\n</div>\n:::\n:::\n\n\n\nUnd so lesen wir alle ein:\n\n\nZunächst erstellen wir uns eine Helper-Funktion:\n\n\n\n::: {.cell hash='klassifikation_cache/html/fun-read-and-select_a29abe2945bdbc6943603487c9d1c089'}\n\n```{.r .cell-code}\nread_and_select <- function(file_name, path_to_tweet_data = tweet_data_path) {\n  \n  out <- \n    read_rds(file = paste0(path_to_tweet_data, file_name)) %>% \n    select(id, author_id, created_at, text, public_metrics) %>% \n    unnest_wider(public_metrics)\n  \n  cat(\"Data file was read.\\n\")\n  \n  return(out)\n}\n```\n:::\n\n\nTesten:\n\n\n::: {.cell hash='klassifikation_cache/html/read-and-seelct-test_ce59829c5f29f605c8c90f3d9190dc91'}\n\n```{.r .cell-code}\nd1 <- read_and_select(tweet_data_files_names[1])\n\nhead(d1)\n```\n:::\n\n\n\n\nDie Funktion `read_and_select`  mappen wir auf alle Datendateien:\n\n\n\n::: {.cell hash='klassifikation_cache/html/map-read-and-select_aeaddc7e47ba8c59fc580347ac6204c3'}\n\n```{.r .cell-code}\ntic()\nds <-\n  tweet_data_files_names %>% \n  map_dfr(read_and_select, .id = \"dataset\")\ntoc()\n```\n:::\n\n\n\n`214.531 sec elapsed`\n\nDa wir den Elementen von `tweet_data_files_names` Namen gegeben haben, \nfinden wir diese Namen praktischerweise wieder in `ds`:\n\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-25_ddce0549d30777d79942408828357840'}\n\n:::\n\n::: {.cell hash='klassifikation_cache/html/read-ds_b1d9b039db86373a9b9bb32c166b9078'}\n\n:::\n\n\n\n\n\nVielleicht ist es zum Entwickeln besser,\nmit einem kleineren Datensatz einstweilen zu arbeiten:\n\n\n::: {.cell hash='klassifikation_cache/html/ds-short_f52ce1b406f293a50c02abdc84a18f92'}\n\n```{.r .cell-code}\nds_short <- slice_sample(ds, prop = .05)\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-26_94b086dd8d90cca604f34ea03360dcf3'}\n\n:::\n\n\n\n\n\n### Vokabular erstellen\n\n\n\n::: {.cell hash='klassifikation_cache/html/ds-long_9a59a699b11568c41e8923bf03bdefa8'}\n\n```{.r .cell-code}\nds_long <-\n  ds %>% \n  select(text) %>% \n  unnest_tweets(input = text, output = word)\n```\n:::\n\n\nPuh, das hat gedauert!\n\nSpeichern wir uns diese Daten daher auf die Festplatte:\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-27_a3824a47ca28b7764264de35be7a1014'}\n\n```{.r .cell-code}\nwrite_rds(ds_long, file = paste0(tweet_data_path, \"ds_long.rds\"))\n```\n:::\n\n\nEntfernen wir daraus die Duplikate,\num uns ein Vokabular zu erstellen:\n\n\n::: {.cell hash='klassifikation_cache/html/ds-voc_384707de03df4016afa6b58050525bfa'}\n\n```{.r .cell-code}\nds_voc <-\n  ds_long %>% \n  #slice_head(n = 10) %>% \n  distinct(word)\n```\n:::\n\n\nUnd das resultierende Objekt speichern wir wieder ab:\n\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-28_6a3ec426e922aa8c94e91b2e19b7e209'}\n\n```{.r .cell-code}\nwrite_rds(ds_voc, file = paste0(tweet_data_path, \"ds_voc.rds\"))\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-29_41aac4adc5af23cb98a30466d2dc5ffa'}\n\n:::\n\n\n\n\n\n\n## Worteinbettungen erstellen\n\n\n### FastText-Modell\n\nDefiniere die Konstanten für das fastText-Modell:\n\n\n::: {.cell hash='klassifikation_cache/html/fastText-constants_26b917cd79da8c4033508ecd2cd5aaf3'}\n\n```{.r .cell-code}\ntexts <- ds %>% pull(text)\ntexts <- tolower(texts)\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/fastText-filenames_53ad86c71e321424be9d1735e4b34191'}\n\n```{.r .cell-code}\nout_file_txt <- \"/Users/sebastiansaueruser/datasets/Twitter/twitter-polit-model.vec\"\nout_file_model <- \"/Users/sebastiansaueruser/datasets/Twitter/twitter-polit-model.bin\"\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/fasttext-modell_a2a076c45f3a35d3257966457052bb7e'}\n\n```{.r .cell-code}\nwriteLines(text = texts, con = out_file_txt)\nexecute(commands = c(\"skipgram\", \"-input\", tmp_file_txt, \"-output\", out_file_model, \"-verbose\", 1))\n```\n:::\n\n\n\n```\nRead 22M words\nNumber of words:  130328\nNumber of labels: 0\nProgress: 100.0% words/sec/thread:   49218 lr:  0.000000 avg.loss:  1.720812 ETA:   0h 0m 0s\n```\n\nJetzt laden wir das Modell von der Festplatte:\n\n\n::: {.cell hash='klassifikation_cache/html/twitter-fasttext-model_2f97c32425b459022dae51cf7b682554'}\n\n```{.r .cell-code}\ntwitter_fasttext_model <- load_model(out_file_model)\ndict <- get_dictionary(twitter_fasttext_model)\n```\n:::\n\n\n\nSchauen wir uns einige Begriffe aus dem Vokabular an:\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-30_09c8f6acffd73243951230a4631d4786'}\n\n```{.r .cell-code}\nprint(head(dict, 10))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"</s>\"            \"die\"             \"und\"             \"der\"            \n [5] \"sie\"             \"das\"             \"nicht\"           \"in\"             \n [9] \"ist\"             \"@_friedrichmerz\"\n```\n:::\n:::\n\n\nHier sind die ersten paar Elemente des Vektors für `menschen`:\n\n\n\n::: {.cell hash='klassifikation_cache/html/vector-menschen_2f9a9505c385f3dbeade4b7e1ddf3fac'}\n\n```{.r .cell-code}\nget_word_vectors(twitter_fasttext_model, c(\"menschen\")) %>% `[`(1:10)\n```\n:::\n\n\n\n```\n [1]  0.14156282  0.44875699  0.23911817 -0.02580349  0.29811972  0.03870077\n [7]  0.06518744  0.22527063  0.28198120  0.39931887\n ```\n\n\nErstellen wir uns einen Tibble, der \nals erste Spalte das Vokabular und in den übrigen 100 Spalten die Dimensionen enthält:\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-31_c6f2cee4437ec694117dbfb55a5c74ea'}\n\n```{.r .cell-code}\nword_embedding_twitter <-\n  tibble(\n    word = dict\n  )\n```\n:::\n\n\n\n::: {.cell hash='klassifikation_cache/html/words-vec-twitter_68a582ec380b02004c2b50cd750e9340'}\n\n```{.r .cell-code}\nwords_vecs_twitter <-\n  get_word_vectors(twitter_fasttext_model)\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/df-word-embedding-twitter_8dda266018d3a61d4321f7b610d52ebc'}\n\n```{.r .cell-code}\nword_embedding_twitter <-\n  word_embedding_twitter %>% \n  bind_cols(words_vecs_twitter)\n\nnames(word_embedding_twitter) <- c(\"word\", paste0(\"v\", sprintf(\"%03d\", 1:100)))  # Namen verschönern\n```\n:::\n\n\n\nUnd als Worteinbettungs-Datei abspeichern:\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-32_de3738944047088ea3a2a8def3bdcd2d'}\n\n```{.r .cell-code}\nwrite_rds(word_embedding_twitter, file = paste0(tweet_data_path, \"word_embedding_twitter.rds\"))\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-33_6cb814ceddf5d6df46a05c075063abcf'}\n\n:::\n\n\n\n\n### Aufbereiten\n\nAm besten nur die Spalten behalten,\ndie wir zum Modellieren nutzen:\n\n\n::: {.cell hash='klassifikation_cache/html/ds-short2_af311d5a1144cffc658e173130588d21'}\n\n```{.r .cell-code}\nds_short2 <-\n  ds_short %>% \n  select(text, id)\n```\n:::\n\n\n\nDann backen wir die Daten mit dem vorhandenen Rezept:\n\n\n\n\n::: {.cell hash='klassifikation_cache/html/ds-baked_e60c03b90fbf7466f351b1dcff8da245'}\n\n```{.r .cell-code}\nds_baked <- bake(rec1_prepped, new_data = ds_short2)\n```\n:::\n\n\n\nIst das nicht komfortabel?\nDas Textrezept übernimmt die Arbeit für uns,\nmit den richtigen Features zu arbeiten,\ndie tf-idfs für die richtigen Tokens zu berechnen.\n\nWer dem Frieden nicht traut,\ndem sei geraten, nachzuprüfen :-)\n\n\n\n## Workflow 3: Rezept 2 + Lasso\n\n### Daten aufteilen\n\n\n\n\n::: {.cell hash='klassifikation_cache/html/split-data3_284178b7689c029dae98e7eff9cb245c'}\n\n```{.r .cell-code}\nd_split <- initial_split(d2, strata = c1)\n\nd_train <- training(d_split)\nd_test <- testing(d_split)\n```\n:::\n\n\n\n\n### Hilfsfunktionen\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-34_0ccb1f85aac13fddd97c7912b935d82d'}\n\n```{.r .cell-code}\ndummy <- c(\"hallo\", \"baby\", \"fatal\")\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-35_2190eaebbbd2ca3de4c3701f0d9a484b'}\n\n```{.r .cell-code}\ncount_profane <- function(text) {\n  sum((tokenize_tweets(text, simplify = TRUE) %>% simplify()) %in% schimpf$word)\n}\n\ncount_profane(dummy) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-36_964cf1ae9c17d20834ff0671d67a644a'}\n\n```{.r .cell-code}\ncount_emo_words <- function(text) {\n  sum((tokenize_tweets(text, simplify = TRUE) %>% simplify()) %in% sentiws$word)\n}\n\ncount_emo_words(dummy)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-37_1417e6a3aad94f63a310e7e752188e0e'}\n\n```{.r .cell-code}\ncount_emojis <- function(text){\n  sum((tokenize_tweets(text, simplify = TRUE) %>% simplify()) %in% trimws(emj))\n}\n\ndummy <- c(\"baby\", \"und\", \"🆗\", \"🖕\")\n\ncount_emojis(dummy)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-38_eae3e32961aba79e54608a706717461b'}\n\n```{.r .cell-code}\ncount_wild_emojis <- function(text){\n  sum((tokenize_tweets(text, simplify = TRUE) %>% simplify()) %in% wild_emojis)\n}\n\ncount_wild_emojis(dummy) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n\n\n\n### Rezept mit Worteinbettungen\n\n\n::: {.cell hash='klassifikation_cache/html/rec2_13e90d8dd5346b790b20de9311324250'}\n\n```{.r .cell-code}\nrec2 <- \n  recipe(c1 ~ ., data = select(d_train, text, c1, id)) %>% \n  update_role(id, new_role = \"id\") %>% \n  step_text_normalization(text) %>% \n  step_mutate(text_copy = text,\n              profane_n = map_int(text, count_profane),\n              emo_words_n = map_int(text, count_emo_words),\n              emojis_n = map_int(text, count_emojis),\n              wild_emojis_n = map_int(text, count_wild_emojis)\n  ) %>% \n  step_textfeature(text_copy) %>% \n  step_tokenize(text, token = \"tweets\") %>% \n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %>% \n  step_word_embeddings(text, embeddings = word_embedding_twitter)\n \nrec2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n        id          1\n   outcome          1\n predictor          1\n\nOperations:\n\nText Normalization for text\nVariable mutation for text, map_int(text, count_profane), map_in...\nText feature extraction for text_copy\nTokenization for text\nStop word removal for text\nWord embeddings aggregated from text\n```\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/rec2-prepped-baked_f4a1463a13a2f2ae42f3eccda9ee3748'}\n\n```{.r .cell-code}\nrec2_prepped <- prep(rec2)\nrec2_baked <- bake(rec2_prepped, new_data = NULL)\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-39_9595b770422f1c066aa41aab84771620'}\n\n```{.r .cell-code}\nrec2_baked %>% \n  select(1:15) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 3,756\nColumns: 15\n$ id                                  <int> 5, 7, 9, 10, 17, 42, 44, 48, 53, 5…\n$ c1                                  <fct> OFFENSE, OFFENSE, OFFENSE, OFFENSE…\n$ profane_n                           <int> 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1…\n$ emo_words_n                         <int> 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0…\n$ emojis_n                            <int> 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 2…\n$ wild_emojis_n                       <int> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n$ textfeature_text_copy_n_words       <int> 16, 32, 12, 15, 19, 30, 31, 6, 24,…\n$ textfeature_text_copy_n_uq_words    <int> 16, 28, 12, 15, 17, 29, 29, 6, 23,…\n$ textfeature_text_copy_n_charS       <int> 121, 145, 66, 119, 112, 171, 170, …\n$ textfeature_text_copy_n_uq_charS    <int> 31, 29, 29, 30, 36, 42, 35, 23, 30…\n$ textfeature_text_copy_n_digits      <int> 0, 4, 0, 0, 4, 0, 1, 0, 2, 0, 2, 0…\n$ textfeature_text_copy_n_hashtags    <int> 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0…\n$ textfeature_text_copy_n_uq_hashtags <int> 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0…\n$ textfeature_text_copy_n_mentions    <int> 1, 1, 1, 0, 0, 5, 1, 0, 1, 1, 0, 2…\n$ textfeature_text_copy_n_uq_mentions <int> 1, 1, 1, 0, 0, 5, 1, 0, 1, 1, 0, 2…\n```\n:::\n:::\n\n\n\n\n### Fitting 3\n\n\n\n\n\n::: {.cell hash='klassifikation_cache/html/wf3_1ea87c63019fb4bc1ba67c3156c57ddd'}\n\n```{.r .cell-code}\nwf3 <-\n  workflow() %>% \n  add_recipe(rec2) %>% \n  add_model(lasso_spec)\n\nwf3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_text_normalization()\n• step_mutate()\n• step_textfeature()\n• step_tokenize()\n• step_stopwords()\n• step_word_embeddings()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n\n\n\n\nTunen und Fitten:\n\n\n::: {.cell hash='klassifikation_cache/html/wf3-fit_758cef2da1566f33e6bb3939df4e28ce'}\n\n```{.r .cell-code}\nset.seed(42)\n\ntic()\nfit3 <-\n  tune_grid(\n    wf3,\n    folds1,\n    grid = lambda_grid,\n    control = control_resamples(save_pred = TRUE)\n  )\n(toc)\nfit3\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-40_8fca145aae38a3b2d05b4b00aa4069e8'}\n\n```{.r .cell-code}\nwrite_rds(fit3, \"objects/chap_classific_fit3.rds\")\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-41_b0ee09c91f3c23c87b36d3c843b640df'}\n\n:::\n\n\n\n\nHier ist die Performanz:\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-42_1f725b8d0fab0ab3fce87fc7e1d33c11'}\n\n```{.r .cell-code}\ncollect_metrics(fit3)\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-43_556b6ecf0f4558cdd29e5c19fbeba380'}\n\n```{.r .cell-code}\nautoplot(fit3)\n```\n\n::: {.cell-output-display}\n![](klassifikation_files/figure-html/unnamed-chunk-43-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-44_32f6b39e64d34ef5005fabd1c4c3a640'}\n\n```{.r .cell-code}\nfit3 %>% \n  show_best(\"roc_auc\")\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|   penalty|.metric |.estimator |      mean|  n|   std_err|.config               |\n|---------:|:-------|:----------|---------:|--:|---------:|:---------------------|\n| 0.0017433|roc_auc |binary     | 0.8116297| 10| 0.0087106|Preprocessor1_Model22 |\n| 0.0007880|roc_auc |binary     | 0.8111630| 10| 0.0089414|Preprocessor1_Model21 |\n| 0.0003562|roc_auc |binary     | 0.8103737| 10| 0.0092381|Preprocessor1_Model20 |\n| 0.0001610|roc_auc |binary     | 0.8099159| 10| 0.0094448|Preprocessor1_Model19 |\n| 0.0000000|roc_auc |binary     | 0.8095907| 10| 0.0096029|Preprocessor1_Model01 |\n\n</div>\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/chosenauc-fit3_c25e5e9f32430a0b8159dac8698641ba'}\n\n```{.r .cell-code}\nchosen_auc_fit3 <- \n  fit3 %>%\n  select_by_one_std_err(metric = \"roc_auc\", -penalty)\n```\n:::\n\n\n\n\nFinalisieren:\n\n\n\n::: {.cell hash='klassifikation_cache/html/wf3-final_bd074d0e893b8ee46db45869b23c9096'}\n\n```{.r .cell-code}\nwf3_final <-\n  finalize_workflow(wf3, chosen_auc_fit3)\n\nwf3_final\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_text_normalization()\n• step_mutate()\n• step_textfeature()\n• step_tokenize()\n• step_stopwords()\n• step_word_embeddings()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = 0.00853167852417281\n  mixture = 1\n\nComputational engine: glmnet \n```\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/fit3-final-train_ed41ad3fe81b6ebfb62d253e91f68d19'}\n\n```{.r .cell-code}\nfit3_final_train <-\n  fit(wf3_final, d_train)\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/fit3-final-train2_a77e4ab9d71f839a5e9ca4eac2f9dfb0'}\n\n```{.r .cell-code}\nfit3_final_train %>% \n  extract_fit_parsnip() %>% \n  tidy() %>% \n  arrange(-abs(estimate)) %>% \n  head()\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|term                             |   estimate|   penalty|\n|:--------------------------------|----------:|---------:|\n|(Intercept)                      |  1.2325023| 0.0085317|\n|profane_n                        | -0.5899329| 0.0085317|\n|textfeature_text_copy_n_exclaims | -0.1987554| 0.0085317|\n|wordembed_text_v055              |  0.1799862| 0.0085317|\n|wordembed_text_v059              | -0.1465378| 0.0085317|\n|wordembed_text_v054              |  0.1455451| 0.0085317|\n\n</div>\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/fit3-final-test_fad2bad470e89c93845e7b252cf55b56'}\n\n```{.r .cell-code}\nfit3_final_test <-\n  last_fit(wf3_final, d_split)\n\ncollect_metrics(fit3_final_test)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|.metric  |.estimator | .estimate|.config              |\n|:--------|:----------|---------:|:--------------------|\n|accuracy |binary     | 0.7533919|Preprocessor1_Model1 |\n|roc_auc  |binary     | 0.7871291|Preprocessor1_Model1 |\n\n</div>\n:::\n:::\n\n\n\nAm Ende so eines Arbeitsganges,\nbei dem man wieder (und wieder) die gleichen Funktionen kopiert,\nund nur aufpassen muss, aus `fit2` an der richtigen Stelle `fit3` zu machen:\nDa blickt man jedem Umbau dieses Codes zu einer Funktion freudig ins Gesicht.\n\nEin anderes Problem,\nfür das hier keine elegante Lösung vorliegt,\nsind die langen Berechnungszeiten, die, wenn man Pecht hat, auch noch\nmehrfach wiederholt werden müssen.\n\nZu diesen Punkten später mehr.\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}