{
  "hash": "5f420272fb5f1686d3b44c4dd187aed4",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n\n# Klassifikation von Hatespeech\n\n\n\n\n\n## Vorab\n\n\n### Lernziele\n\n\n- Sie k√∂nnen grundlegende Verfahren zur Klassifikation von Hatespeech einsetzen und erkl√§ren\n\n\n\n\n\n\n\n\n### Ben√∂tigte R-Pakete\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-1_5011f77326c4e3c6b6cdebf87a44c70a'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(tidymodels)\nlibrary(tidytext)\nlibrary(textrecipes)\nlibrary(lsa)  # stopwords\nlibrary(discrim)  # naive bayes classification\nlibrary(naivebayes)\nlibrary(tictoc)  # Zeitmessung\nlibrary(fastrtext)  # Worteinbettungen\nlibrary(remoji)  # Emojis\nlibrary(tokenizers)  # Vektoren tokenisieren\n```\n:::\n\n\n\n## Daten\n\n\nF√ºr Maschinenlernen brauchen wir Trainingsdaten,\nDaten also, bei denen wir pro Beobachtung der Wert der Zielvariablen kennen.\nMan spricht auch von \"gelabelten\" Daten.\n\nWir nutzen die Daten von @wiegand_germeval bzw. @wiegand-data.\nDie Daten sind unter CC-By-4.0 Int. lizensiert.\n\n\n::: {.cell hash='klassifikation_cache/html/import-heidelberg-data_d15f702434e18cf1a6c930f35108cbc2'}\n\n```{.r .cell-code}\nd_raw <- \n  import(\"data/germeval2018.training.txt\",\n         header = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in (function (input = \"\", file = NULL, text = NULL, cmd = NULL, : Found\nand resolved improper quoting out-of-sample. First healed line 111: <<\"Edel sei\nder Mensch, hilfreich und gut\" - Nicht eine dieser Charaktereigenschaften kann\nMerkel f√ºr sich beanspruchen. OTHER OTHER>>. If the fields are not quoted (e.g.\nfield separator does not appear within any field), try quote=\"\" to avoid this\nwarning.\n```\n:::\n:::\n\n\n\nDa die Daten keine Spaltenk√∂pfe haben, informieren wir die Funktion dazu mit `header = FALSE`.\n\nBenennen wir die die Spalten um:\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-2_ffce71786af673ec0286d362e54fac1e'}\n\n```{.r .cell-code}\nnames(d_raw) <- c(\"text\", \"c1\", \"c2\")\n```\n:::\n\n\nDabei soll `c1` und `c2` f√ºr die 1. bzw. 2. Klassifikation stehen.\n\n\nIn `c1` finden sich diese Werte:\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-3_ec8a5462101a2e904367eab06bb7623b'}\n\n```{.r .cell-code}\nd_raw %>% \n  count(c1)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|c1      |    n|\n|:-------|----:|\n|OFFENSE | 1688|\n|OTHER   | 3321|\n\n</div>\n:::\n:::\n\n\nHier wurde klassifiziert,\nob beleidigende Sprache (offensive language) vorlag oder nicht [@isch-etal-2021-overview, S. 2]:\n\n\n>   Task 1 was to decide whether a tweet includes some form of offensive language or not. The tweets had to be classiÔ¨Åed into the two classes OFFENSE and OTHER. The OFFENSE category covered abusive language, insults, as well as merely profane statements.\n\n\nUnd in `c2` finden sich folgende Auspr√§gungen:\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-4_d84ef45b2c95f0fb28f404187739ae23'}\n\n```{.r .cell-code}\nd_raw %>% \n  count(c2)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|c2        |    n|\n|:---------|----:|\n|ABUSE     | 1022|\n|INSULT    |  595|\n|OTHER     | 3321|\n|PROFANITY |   71|\n\n</div>\n:::\n:::\n\n\n\nIn `c2` ging es um eine feinere Klassifikation beleidigender Sprache [@isch-etal-2021-overview, S. 2]:\n\n>   The second task involved four categories, a nonoffensive OTHER class and three sub-categories of what is OFFENSE in Task 1. In the case of PROFANITY, profane words are used, however, the tweet does not want to insult anyone. This typically concerns the usage of swearwords (Schei√üe, Fuck etc.) and cursing (Zur H√∂lle! Verdammt! etc.). This can be often found in youth language. Swearwords and cursing may, but need not, co-occur with insults or abusive speech. Profane language may in fact be used in tweets with positive sentiment to express emphasis. Whenever profane words are not directed towards a speciÔ¨Åc person or group of persons and there are no separate cues of INSULT or ABUSE, then tweets are labeled as simple cases of PROFANITY.\n\n\n\n\nSind Texte, die als `OFFENSE` klassifiziert sind,\nauch (fast) immer als `ABUSE`, `INSULT` oder `PROFANITY` klassifiziert?\n\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-5_ef75f7566b7d35a79c95ff27f839d505'}\n\n```{.r .cell-code}\nd_raw %>% \n  filter(c1 == \"OTHER\", c2 == \"OTHER\") %>% \n  nrow() / nrow(d_raw)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6630066\n```\n:::\n:::\n\n\nIn ca. 2/3 der F√§lle wurden in beiden Klassifikation `OTHER` klassifiziert.\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-6_729200e672190fbda2cf3addbe896963'}\n\n```{.r .cell-code}\nd_raw %>% \n  filter(c1 != \"OTHER\", c2 != \"OTHER\") %>% \n  nrow() / nrow(d_raw)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3369934\n```\n:::\n:::\n\n\nEntsprechend in ca. 1/3 der F√§lle wurde jeweils nicht mit `OTHER` klassifiziert.\n\n\nWir begn√ºgen uns hier mit der ersten, gr√∂beren Klassifikation.\n\n\n## Feature Engineering\n\n\nReichern wir die Daten mit weiteren Features an,\nin der Hoffnung, damit eine bessere Klassifikation erzielen zu k√∂nnen.\n\n\n### Textl√§nge\n\n\n\n\n\n::: {.cell hash='klassifikation_cache/html/d2_34d67ac0e2789dfd4d53bd64705b0862'}\n\n```{.r .cell-code}\nd2 <-\n  d_raw %>% \n  mutate(text_length = str_length(text)) %>% \n  mutate(id = 1:nrow(.))\n\nhead(d2)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|text                                                                                                                                                                                                                                                                                         |c1      |c2     | text_length| id|\n|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------|:------|-----------:|--:|\n|@corinnamilborn Liebe Corinna, wir w√ºrden dich gerne als Moderatorin f√ºr uns gewinnen! W√§rst du begeisterbar?                                                                                                                                                                                |OTHER   |OTHER  |         109|  1|\n|@Martin28a Sie haben ja auch Recht. Unser Tweet war etwas missverst√§ndlich. Dass das BVerfG Sachleistungen nicht ausschlie√üt, kritisieren wir.                                                                                                                                               |OTHER   |OTHER  |         142|  2|\n|@ahrens_theo fr√∂hlicher gru√ü aus der sch√∂nsten stadt der welt theo ‚öìÔ∏è                                                                                                                                                                                                                        |OTHER   |OTHER  |          69|  3|\n|@dushanwegner Amis h√§tten alles und jeden gew√§hlt...nur Hillary wollten sie nicht und eine Fortsetzung von Obama-Politik erst recht nicht..!                                                                                                                                                 |OTHER   |OTHER  |         140|  4|\n|@spdde kein verl√§√ülicher Verhandlungspartner. Nachkarteln nach den Sondierzngsgespr√§chen - schickt diese St√ºmper #SPD in die Versenkung.                                                                                                                                                     |OFFENSE |INSULT |         136|  5|\n|@Dirki_M Ja, aber wo widersprechen die Zahlen denn denen, die im von uns verlinkten Artikel stehen? In unserem Tweet geht es rein um subs. Gesch√ºtzte. 2017 ist der gesamte Familiennachzug im Vergleich zu 2016 - die Zahlen, die Hr. Brandner bem√ºht - √ºbrigens leicht r√ºckl√§ufig gewesen. |OTHER   |OTHER  |         284|  6|\n\n</div>\n:::\n:::\n\n\n\n\n### Sentimentanalyse\n\nWir nutzen dazu `SentiWS` [@Remus2010].\n\n\n::: {.cell hash='klassifikation_cache/html/read-sentiws_2f767942281bb38d29abade5a1aa0999'}\n\n```{.r .cell-code}\nsentiws <- read_csv(\"https://osf.io/x89wq/?action=download\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 3468 Columns: 4\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (3): neg_pos, word, inflections\ndbl (1): value\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/d2-long_77a2f69e65e911b3658137b948ada80a'}\n\n```{.r .cell-code}\nd2_long <-\n  d2 %>% \n  unnest_tokens(input = text, output = token)\n\nhead(d2_long)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|c1    |c2    | text_length| id|token          |\n|:-----|:-----|-----------:|--:|:--------------|\n|OTHER |OTHER |         109|  1|corinnamilborn |\n|OTHER |OTHER |         109|  1|liebe          |\n|OTHER |OTHER |         109|  1|corinna        |\n|OTHER |OTHER |         109|  1|wir            |\n|OTHER |OTHER |         109|  1|w√ºrden         |\n|OTHER |OTHER |         109|  1|dich           |\n\n</div>\n:::\n:::\n\n\nJetzt filtern wir unsere Textdaten so,\ndass nur W√∂rter mit Sentimentwert √ºbrig bleiben:\n\n\n::: {.cell hash='klassifikation_cache/html/d2-senti-long_2b9caa6e08a624466310b69af250e5b9'}\n\n```{.r .cell-code}\nd2_long_senti <- \n  d2_long %>%  \n  inner_join(sentiws %>% select(-inflections), by = c(\"token\" = \"word\"))\n\nhead(d2_long)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|c1    |c2    | text_length| id|token          |\n|:-----|:-----|-----------:|--:|:--------------|\n|OTHER |OTHER |         109|  1|corinnamilborn |\n|OTHER |OTHER |         109|  1|liebe          |\n|OTHER |OTHER |         109|  1|corinna        |\n|OTHER |OTHER |         109|  1|wir            |\n|OTHER |OTHER |         109|  1|w√ºrden         |\n|OTHER |OTHER |         109|  1|dich           |\n\n</div>\n:::\n:::\n\n\n\nSchlie√ülich berechnen wir die Sentimentwert pro Polarit√§t und pro Tweet:\n\n\n::: {.cell hash='klassifikation_cache/html/d2-sentis_5c4829559233ee9cbd1e3ec1127865e4'}\n\n```{.r .cell-code}\nd2_sentis <-\n  d2_long_senti %>% \n  group_by(id, neg_pos) %>% \n  summarise(senti_avg = mean(value))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'id'. You can override using the `.groups`\nargument.\n```\n:::\n\n```{.r .cell-code}\nhead(d2_sentis)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| id|neg_pos | senti_avg|\n|--:|:-------|---------:|\n|  1|pos     |    0.0040|\n|  2|neg     |   -0.3466|\n|  6|neg     |   -0.2042|\n|  6|pos     |    0.0040|\n|  8|neg     |   -0.5023|\n|  9|pos     |    0.5161|\n\n</div>\n:::\n:::\n\n\n\nDiese Tabelle bringen wir wieder eine breitere Form,\num sie dann wieder mit den Hauptdaten zu vereinigen.\n\n\n\n::: {.cell hash='klassifikation_cache/html/d2-sentis-wide_9de5e00b949578004a81dcb9a79e8311'}\n\n```{.r .cell-code}\nd2_sentis_wide <-\n  d2_sentis %>% \n  pivot_wider(names_from = \"neg_pos\", values_from = \"senti_avg\")\n\nd2_sentis_wide %>% head()\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| id|    pos|     neg|\n|--:|------:|-------:|\n|  1| 0.0040|      NA|\n|  2|     NA| -0.3466|\n|  6| 0.0040| -0.2042|\n|  8|     NA| -0.5023|\n|  9| 0.5161|      NA|\n| 11| 0.0040|      NA|\n\n</div>\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/d3_d83097a4828f8e33a43bba65b437d2df'}\n\n```{.r .cell-code}\nd3 <-\n  d2 %>% \n  full_join(d2_sentis_wide)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"id\"\n```\n:::\n\n```{.r .cell-code}\nhead(d3)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|text                                                                                                                                                                                                                                                                                         |c1      |c2     | text_length| id|   pos|     neg|\n|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------|:------|-----------:|--:|-----:|-------:|\n|@corinnamilborn Liebe Corinna, wir w√ºrden dich gerne als Moderatorin f√ºr uns gewinnen! W√§rst du begeisterbar?                                                                                                                                                                                |OTHER   |OTHER  |         109|  1| 0.004|      NA|\n|@Martin28a Sie haben ja auch Recht. Unser Tweet war etwas missverst√§ndlich. Dass das BVerfG Sachleistungen nicht ausschlie√üt, kritisieren wir.                                                                                                                                               |OTHER   |OTHER  |         142|  2|    NA| -0.3466|\n|@ahrens_theo fr√∂hlicher gru√ü aus der sch√∂nsten stadt der welt theo ‚öìÔ∏è                                                                                                                                                                                                                        |OTHER   |OTHER  |          69|  3|    NA|      NA|\n|@dushanwegner Amis h√§tten alles und jeden gew√§hlt...nur Hillary wollten sie nicht und eine Fortsetzung von Obama-Politik erst recht nicht..!                                                                                                                                                 |OTHER   |OTHER  |         140|  4|    NA|      NA|\n|@spdde kein verl√§√ülicher Verhandlungspartner. Nachkarteln nach den Sondierzngsgespr√§chen - schickt diese St√ºmper #SPD in die Versenkung.                                                                                                                                                     |OFFENSE |INSULT |         136|  5|    NA|      NA|\n|@Dirki_M Ja, aber wo widersprechen die Zahlen denn denen, die im von uns verlinkten Artikel stehen? In unserem Tweet geht es rein um subs. Gesch√ºtzte. 2017 ist der gesamte Familiennachzug im Vergleich zu 2016 - die Zahlen, die Hr. Brandner bem√ºht - √ºbrigens leicht r√ºckl√§ufig gewesen. |OTHER   |OTHER  |         284|  6| 0.004| -0.2042|\n\n</div>\n:::\n:::\n\n\n\n:::callout-note\nDie Sentimentanalyse hier vernachl√§ssigt Flexionen der W√∂rter. \nDer  Autor f√ºhlt den Drang zu schreiben: \"Left as an exercise for the reader\" :-)\n:::\n\n\n### Schimpfw√∂rter\n\n\nZ√§hlen wir die Schimpfw√∂rter pro Text.\nDazu nutzen wir die Daten von [LDNOOBW](https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/blob/master/LICENSE), lizensiert nach CC-BY-4.0-Int.\n\n\n\n\n\n::: {.cell hash='klassifikation_cache/html/schimpf1_d1299213ad9b6a15bf862917328d302b'}\n\n```{.r .cell-code}\nschimpf1 <- import(\"https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/de\", format = \",\", header = FALSE)\n```\n:::\n\n\n\nL√§nger aber noch ist die Liste aus dem [InsultWiki](https://www.insult.wiki/schimpfwort-liste), lizensiert CC0.\n\n\n\n::: {.cell hash='klassifikation_cache/html/schimpf2_2042032e8abae3146d357f5614a67231'}\n\n```{.r .cell-code}\nschimpf2 <- \n  import(\"data/insult-de.txt\", header = FALSE) %>% \n  mutate_all(str_to_lower)\n```\n:::\n\n\n\nBinden wir die Listen zusammen:\n\n\n::: {.cell hash='klassifikation_cache/html/schimpf_ecce05ce4ac3c086638e936ea1a06201'}\n\n```{.r .cell-code}\nschimpf <-\n  schimpf1 %>% \n  bind_rows(schimpf2) %>% \n  distinct() %>% \n  rename(word = \"V1\")\n\nnrow(schimpf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6208\n```\n:::\n:::\n\n\n\nUm die Lesis vor (unn√∂tiger?) Kopfverschmutzung zu bewahren,\nsind diese Schimpfw√∂rter hier nicht abgedruckt.\n\nJetzt z√§hlen wir, ob unsere Tweets/Texte solcherlei W√∂rter enthalten.\n\n\n\n::: {.cell hash='klassifikation_cache/html/d_schimpf_57a07b8f735ebff8cd47cc45337655a0'}\n\n```{.r .cell-code}\nd_schimpf <- \nd2_long %>% \n  select(id, token) %>% \n  mutate(schimpf = token %in% schimpf$word)\n```\n:::\n\n\n\nWie viele Schimpfw√∂rter haben wir gefunden?\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-7_5c705ef651ce148fdbbe428ebd0b76e9'}\n\n```{.r .cell-code}\nd_schimpf %>% \n  count(schimpf)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|schimpf |     n|\n|:-------|-----:|\n|FALSE   | 99081|\n|TRUE    |  1136|\n\n</div>\n:::\n:::\n\n\n\nEtwa ein Prozent der W√∂rter sind Schimpfw√∂rter in unserem Corpus.\n\n\n\n::: {.cell hash='klassifikation_cache/html/d-schimpf2_984b0cf907f12cf5b82a79ea4a40201d'}\n\n```{.r .cell-code}\nd_schimpf2 <-\n  d_schimpf %>% \n  group_by(id) %>% \n  summarise(schimpf_n = sum(schimpf))\n\nhead(d_schimpf2)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| id| schimpf_n|\n|--:|---------:|\n|  1|         0|\n|  2|         0|\n|  3|         0|\n|  4|         0|\n|  5|         1|\n|  6|         0|\n\n</div>\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/d_main_e41e00cfd20f68b086b0ff8103652b8c'}\n\n```{.r .cell-code}\nd_main <-\n  d3 %>% \n  full_join(d_schimpf2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"id\"\n```\n:::\n:::\n\n\n\n:::callout-important\nNamen wie `final`, `main` oder `result` sind gef√§hrlich,\nda es unter Garantie ein \"final-final geben wird, oder der \"Haupt-Datensat\" pl√∂tzlich nicht mehr so wichtig erscheint und so weiter.\n:::\n\n\n\n### Emojis\n\n\n\n::: {.cell hash='klassifikation_cache/html/get-emoji-list_ca798f5f68205b7edbb1f7a73d40dfff'}\n\n```{.r .cell-code}\nemj <- emoji(list_emoji(), pad = FALSE)\n\nhead(emj)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"üòÑ\" \"üòÉ\" \"üòÄ\" \"üòä\" \"‚ò∫Ô∏è\"  \"üòâ\"\n```\n:::\n:::\n\n\nDiese Liste umfasst knapp 900 Emojis, \ndas sind allerdings noch nicht alle, die es gibt.\n[Diese Liste](https://unicode.org/emoji/charts/full-emoji-list.html) umfasst mit gut 1800 Emojis\ngut das Doppelte.\n\n\nSelbstkuratierte Liste an \"wilden\" Emoji;\ndiese Liste ist inspiriert von [emojicombos.com](https://emojicombos.com/disgust).\n\n\n::: {.cell hash='klassifikation_cache/html/wild-emojis_e4ae2bdc1b3de20b3ed6c3cf4216a934'}\n\n```{.r .cell-code}\nwild_emojis <- \n  c(\n    emoji(find_emoji(\"gun\")),\n    emoji(find_emoji(\"bomb\")),\n    emoji(find_emoji(\"fist\")),\n    emoji(find_emoji(\"knife\"))[1],\n     emoji(find_emoji(\"ambulance\")),\n    \"üò†\",\n    \"üëπ\",\n    \"üí©\",\n    \"‚ò†\",\n    \"üñï\",\n    emoji(find_emoji(\"middle finger\")),\n    \"üò°\",\n    \"ü§¢\",\n    \"ü§Æ\",\n    \"üòñ\",\n    \"üò£\",\n    \"üò©\",\n    \"üò®\",\n    \"üòù\",\n    \"üò≥\",\n    \"üò¨\",\n    \"üò±\",\n    \"üòµ\",\n    \"üò§\",\n    \"ü§¶‚Äç‚ôÄÔ∏è\",\n    \"ü§¶‚Äç‚ôÇÔ∏è\"\n  )\n```\n:::\n\n\n\n\n\nAuf dieser Basis k√∂nnen wir einen Pr√§diktor erstellen,\nder z√§hlt, ob ein Tweet einen oder mehrere der \"wilden\" Emojis enth√§lt.\n\n\n## Workflow 1: Rezept 1 + Naive-Bayes\n\n\n### Dummy-Rezept\n\n\nHier ist ein einfaches Beispiel,\num die Textvorbereitung mit `{textrecipes}` zu verdeutlichen.\n\nWir erstellen uns einen Dummy-Text:\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-8_3655bce3cec3126f80da286b0993920d'}\n\n```{.r .cell-code}\ndummy <- \n  tibble(text = c(\"Ich gehe heim und der die das nicht in ein and the\"))\n```\n:::\n\n\n\nDann tokenisieren wir den Text:\n\n\n::: {.cell hash='klassifikation_cache/html/rec-dummy1_9643d039e34554853ae829e08551ed05'}\n\n```{.r .cell-code}\nrec_dummy <-\n  recipe(text ~ 1, data = dummy) %>% \n  step_tokenize(text)\n  \nrec_dummy\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n    role #variables\n outcome          1\n\nOperations:\n\nTokenization for text\n```\n:::\n:::\n\n\n\nDie Tokens kann man sich so zeigen lassen:\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-9_72e327b3c9f9f1614ef88e707f02008c'}\n\n```{.r .cell-code}\nshow_tokens(rec_dummy, text)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n [1] \"ich\"   \"gehe\"  \"heim\"  \"und\"   \"der\"   \"die\"   \"das\"   \"nicht\" \"in\"   \n[10] \"ein\"   \"and\"   \"the\"  \n```\n:::\n:::\n\n\n\nJetzt entfernen wir die Stopw√∂rter deutscher Sprache;\ndaf√ºr nutzen wir die Stopwort-Quelle `snowball`:\n\n\n\n::: {.cell hash='klassifikation_cache/html/rec-dummy2_17b8002eaabf808eb5898d5d9c9adc77'}\n\n```{.r .cell-code}\nrec_dummy <-\n  recipe(text ~ 1, data = dummy) %>% \n  step_tokenize(text) %>% \n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\")\n\nrec_dummy\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n    role #variables\n outcome          1\n\nOperations:\n\nTokenization for text\nStop word removal for text\n```\n:::\n:::\n\n\n\nPr√ºfen wir die Tokens; \nsind die Stopw√∂rter wirklich entfernt?\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-10_6dd7e56fba4945514cdbec27d8b01103'}\n\n```{.r .cell-code}\nshow_tokens(rec_dummy, text)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[1] \"gehe\" \"heim\" \"and\"  \"the\" \n```\n:::\n:::\n\n\n\nJa, die deutschen Stopw√∂rter sind entfernt. Die englischen nicht;\ndas macht Sinn!\n\n\n### Datenaufteilung\n\n\n\n::: {.cell hash='klassifikation_cache/html/d-split2_5cd853b5f88eb053f31d264cfb811947'}\n\n```{.r .cell-code}\nd_split <- initial_split(d_main, strata = c1)\n\nd_train <- training(d_split)\nd_test <- testing(d_split)\n```\n:::\n\n\n\n\n### Rezept 1\n\n\nRezept definieren:\n\n\n::: {.cell hash='klassifikation_cache/html/rec1_862c91870170ce220bfcb73a09c9c2b7'}\n\n```{.r .cell-code}\nrec1 <- \n  recipe(c1 ~ ., data = select(d_train, text, c1, id)) %>% \n  update_role(id, new_role = \"id\") %>% \n  step_tokenize(text) %>% \n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %>% \n  step_stem(text) %>% \n  step_tokenfilter(text, max_tokens = 1e2) %>% \n  step_tfidf(text) %>% \n  step_normalize(all_numeric_predictors())\n\nrec1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n        id          1\n   outcome          1\n predictor          1\n\nOperations:\n\nTokenization for text\nStop word removal for text\nStemming for text\nText filtering for text\nTerm frequency-inverse document frequency with text\nCentering and scaling for all_numeric_predictors()\n```\n:::\n:::\n\n\n\nPreppen:\n\n\n::: {.cell hash='klassifikation_cache/html/rec1-prepped_4ab9fbfb57c04c4b61cfeff52c0a27d3'}\n\n```{.r .cell-code}\nrec1_prepped <- prep(rec1)\n```\n:::\n\n\nUnd backen:\n\n\n::: {.cell hash='klassifikation_cache/html/rec1-baked_77d9d0bd3a5afd6d50de7a41ade43a57'}\n\n```{.r .cell-code}\nd_rec1 <- bake(rec1_prepped, new_data = NULL)\n\nhead(d_rec1)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| id|c1      | tfidf_text__macmik| tfidf_text_2| tfidf_text_ab| tfidf_text_afd| tfidf_text_amp| tfidf_text_anna_iina| tfidf_text_antisemitismu| tfidf_text_athinamala| tfidf_text_besser| tfidf_text_bild| tfidf_text_cdu| tfidf_text_charlie_silv| tfidf_text_csu| tfidf_text_d| tfidf_text_daf√ºr| tfidf_text_dank| tfidf_text_dass| tfidf_text_deutsch| tfidf_text_deutschen| tfidf_text_deutschland| tfidf_text_dumm| tfidf_text_einfach| tfidf_text_ellibisathid| tfidf_text_endlich| tfidf_text_ennof_| tfidf_text_erst| tfidf_text_eu| tfidf_text_europa| tfidf_text_fdp| tfidf_text_feldenfrizz| tfidf_text_focusonlin| tfidf_text_frage| tfidf_text_frau| tfidf_text_ganz| tfidf_text_geht| tfidf_text_gerad| tfidf_text_gibt| tfidf_text_gr√ºnen| tfidf_text_gt| tfidf_text_gut| tfidf_text_h√§tte| tfidf_text_heut| tfidf_text_immer| tfidf_text_info2099| tfidf_text_islam| tfidf_text_israel| tfidf_text_ja| tfidf_text_jahr| tfidf_text_juden| tfidf_text_kommt| tfidf_text_krippmari| tfidf_text_land| tfidf_text_lassen| tfidf_text_lbr| tfidf_text_lifetrend| tfidf_text_link| tfidf_text_macht| tfidf_text_machtjanix23| tfidf_text_mal| tfidf_text_md_franz| tfidf_text_mehr| tfidf_text_menschen| tfidf_text_merkel| tfidf_text_miriamozen| tfidf_text_moslem| tfidf_text_m√ºssen| tfidf_text_nancypeggymandi| tfidf_text_nasanas| tfidf_text_noherrman| tfidf_text_norbinator2403| tfidf_text_partei| tfidf_text_petpanther0| tfidf_text_politik| tfidf_text_recht| tfidf_text_richtig| tfidf_text_sagt| tfidf_text_schmiddiemaik| tfidf_text_schon| tfidf_text_schulz| tfidf_text_seit| tfidf_text_sicher| tfidf_text_spd| tfidf_text_tagesschau| tfidf_text_thomasgbau| tfidf_text_troll_putin| tfidf_text_trump| tfidf_text_tun| tfidf_text_t√ºrken| tfidf_text_u| tfidf_text_unser| tfidf_text_viel| tfidf_text_volk| tfidf_text_w√§re| tfidf_text_warum| tfidf_text_welt| tfidf_text_wer| tfidf_text_willjrosenblatt| tfidf_text_wohl| tfidf_text_wurd| tfidf_text_zeit|\n|--:|:-------|------------------:|------------:|-------------:|--------------:|--------------:|--------------------:|------------------------:|---------------------:|-----------------:|---------------:|--------------:|-----------------------:|--------------:|------------:|----------------:|---------------:|---------------:|------------------:|--------------------:|----------------------:|---------------:|------------------:|-----------------------:|------------------:|-----------------:|---------------:|-------------:|-----------------:|--------------:|----------------------:|---------------------:|----------------:|---------------:|---------------:|---------------:|----------------:|---------------:|-----------------:|-------------:|--------------:|----------------:|---------------:|----------------:|-------------------:|----------------:|-----------------:|-------------:|---------------:|----------------:|----------------:|--------------------:|---------------:|-----------------:|--------------:|--------------------:|---------------:|----------------:|-----------------------:|--------------:|-------------------:|---------------:|-------------------:|-----------------:|---------------------:|-----------------:|-----------------:|--------------------------:|------------------:|--------------------:|-------------------------:|-----------------:|----------------------:|------------------:|----------------:|------------------:|---------------:|------------------------:|----------------:|-----------------:|---------------:|-----------------:|--------------:|---------------------:|---------------------:|----------------------:|----------------:|--------------:|-----------------:|------------:|----------------:|---------------:|---------------:|---------------:|----------------:|---------------:|--------------:|--------------------------:|---------------:|---------------:|---------------:|\n|  9|OFFENSE |         -0.1472962|   -0.1025123|    -0.1045303|      6.1558148|     -0.1196412|           -0.1016498|               -0.0827688|            -0.1412023|        -0.0994077|      -0.1000636|     -0.1061901|              -0.1450561|     -0.0881935|   -0.1365693|       -0.0938978|      -0.1176754|      -0.1911377|         -0.1543987|           -0.1519219|             -0.1996601|      -0.1040022|         -0.1195335|              -0.1473094|         -0.0995996|        -0.1182767|      -0.1107598|     -0.111993|        -0.1066217|     -0.0974439|             -0.1472962|            -0.0985444|       -0.0967565|      -0.0949119|      -0.1145095|      -0.1429535|        -0.108501|      -0.1648097|        -0.1039788|    -0.0926214|      -0.126624|       -0.0955942|      -0.1650169|       -0.1623571|          -0.0804698|       -0.1036414|        -0.0836286|     -0.175643|      -0.0937953|       -0.0974094|       -0.0961735|           -0.1455853|      -0.1441996|        -0.1116953|     -0.4023143|           -0.1445296|      -0.1052941|       -0.1245217|              -0.1088789|     -0.1709422|          -0.1082684|      -0.1829629|          -0.1167916|        -0.2184447|            -0.0966546|        -0.0942649|        -0.1315249|                 -0.1182767|         -0.1096851|            -0.090148|                -0.0631584|        -0.0922659|             -0.0922453|         -0.1476062|       -0.1162279|         -0.0976874|      -0.0964543|               -0.1435878|       -0.2044771|        -0.1013316|      -0.1270926|        -0.0937495|     -0.1342917|             9.5543591|            -0.1473094|             -0.1211889|       -0.1009074|     -0.1080669|        -0.1045075|    -0.150687|       -0.1585372|      -0.0998685|      -0.1090701|      -0.1112479|        -0.103608|      -0.1466292|     -0.1567758|                 -0.1435878|      -0.1091472|      -0.1136727|      -0.1001727|\n| 12|OFFENSE |         -0.1472962|   -0.1025123|    -0.1045303|     -0.1545404|     -0.1196412|           -0.1016498|               -0.0827688|            -0.1412023|        -0.0994077|      -0.1000636|     -0.1061901|              -0.1450561|     -0.0881935|   -0.1365693|       -0.0938978|      -0.1176754|      -0.1911377|         -0.1543987|           -0.1519219|             -0.1996601|      -0.1040022|         -0.1195335|              -0.1473094|         -0.0995996|        -0.1182767|      -0.1107598|     -0.111993|        -0.1066217|     -0.0974439|             -0.1472962|            -0.0985444|       -0.0967565|      -0.0949119|      -0.1145095|      -0.1429535|        -0.108501|      -0.1648097|        -0.1039788|    -0.0926214|      -0.126624|       -0.0955942|      -0.1650169|        3.6921593|          -0.0804698|       -0.1036414|        -0.0836286|      3.385033|      -0.0937953|       -0.0974094|       -0.0961735|           -0.1455853|      -0.1441996|        -0.1116953|      1.2297516|           -0.1445296|      -0.1052941|       -0.1245217|              -0.1088789|     -0.1709422|          -0.1082684|      -0.1829629|          -0.1167916|        -0.2184447|            -0.0966546|        -0.0942649|        -0.1315249|                 -0.1182767|         -0.1096851|            -0.090148|                -0.0631584|        -0.0922659|             -0.0922453|         -0.1476062|       -0.1162279|         -0.0976874|      -0.0964543|               -0.1435878|       -0.2044771|        -0.1013316|      -0.1270926|        -0.0937495|     -0.1342917|            -0.1043247|            -0.1473094|             -0.1211889|       -0.1009074|     -0.1080669|        -0.1045075|    -0.150687|       -0.1585372|      -0.0998685|      -0.1090701|      -0.1112479|        -0.103608|      -0.1466292|     -0.1567758|                 -0.1435878|      -0.1091472|      -0.1136727|      -0.1001727|\n| 17|OFFENSE |         -0.1472962|   -0.1025123|    -0.1045303|     -0.1545404|     -0.1196412|           -0.1016498|               -0.0827688|            -0.1412023|        -0.0994077|      -0.1000636|     -0.1061901|              -0.1450561|     -0.0881935|   -0.1365693|       -0.0938978|      -0.1176754|      -0.1911377|         -0.1543987|            3.7258962|             -0.1996601|      -0.1040022|         -0.1195335|              -0.1473094|         -0.0995996|        -0.1182767|      -0.1107598|     -0.111993|        -0.1066217|     -0.0974439|             -0.1472962|            -0.0985444|       -0.0967565|      -0.0949119|      -0.1145095|      -0.1429535|        -0.108501|      -0.1648097|        -0.1039788|    -0.0926214|      -0.126624|       -0.0955942|      -0.1650169|       -0.1623571|          -0.0804698|       -0.1036414|        -0.0836286|     -0.175643|      -0.0937953|       -0.0974094|       -0.0961735|           -0.1455853|      -0.1441996|        -0.1116953|     -0.4023143|           -0.1445296|      -0.1052941|       -0.1245217|              -0.1088789|     -0.1709422|          -0.1082684|      -0.1829629|          -0.1167916|        -0.2184447|            -0.0966546|        -0.0942649|        -0.1315249|                 -0.1182767|         -0.1096851|            -0.090148|                -0.0631584|        -0.0922659|             -0.0922453|          4.1966394|       -0.1162279|         -0.0976874|      -0.0964543|               -0.1435878|       -0.2044771|        -0.1013316|       4.4285094|        -0.0937495|     -0.1342917|            -0.1043247|            -0.1473094|             -0.1211889|       -0.1009074|     -0.1080669|        -0.1045075|    -0.150687|       -0.1585372|      -0.0998685|      -0.1090701|      -0.1112479|        -0.103608|      -0.1466292|     -0.1567758|                 -0.1435878|      -0.1091472|      -0.1136727|      -0.1001727|\n| 33|OFFENSE |         -0.1472962|   -0.1025123|    -0.1045303|     -0.1545404|     -0.1196412|           -0.1016498|               -0.0827688|            -0.1412023|        -0.0994077|      -0.1000636|     -0.1061901|              -0.1450561|     -0.0881935|   -0.1365693|       -0.0938978|      -0.1176754|      -0.1911377|          3.6445592|           -0.1519219|             -0.1996601|      -0.1040022|         -0.1195335|              -0.1473094|         -0.0995996|        -0.1182767|      -0.1107598|     -0.111993|        -0.1066217|     -0.0974439|             -0.1472962|            -0.0985444|       -0.0967565|      -0.0949119|      -0.1145095|      -0.1429535|        -0.108501|      -0.1648097|        -0.1039788|    -0.0926214|      -0.126624|       -0.0955942|      -0.1650169|       -0.1623571|          -0.0804698|       -0.1036414|        -0.0836286|     -0.175643|      -0.0937953|       -0.0974094|       -0.0961735|           -0.1455853|      -0.1441996|        -0.1116953|     -0.4023143|           -0.1445296|      -0.1052941|       -0.1245217|              -0.1088789|     -0.1709422|          -0.1082684|       3.1601002|          -0.1167916|        -0.2184447|            -0.0966546|         5.4434700|        -0.1315249|                 -0.1182767|         -0.1096851|            -0.090148|                -0.0631584|        -0.0922659|             -0.0922453|         -0.1476062|       -0.1162279|         -0.0976874|      -0.0964543|               -0.1435878|       -0.2044771|        -0.1013316|      -0.1270926|        -0.0937495|     -0.1342917|            -0.1043247|            -0.1473094|             -0.1211889|       -0.1009074|     -0.1080669|        -0.1045075|    -0.150687|       -0.1585372|      -0.0998685|      -0.1090701|      -0.1112479|        -0.103608|      -0.1466292|     -0.1567758|                 -0.1435878|      -0.1091472|      -0.1136727|      -0.1001727|\n| 42|OFFENSE |         -0.1472962|   -0.1025123|    -0.1045303|     -0.1545404|     -0.1196412|           -0.1016498|               -0.0827688|            -0.1412023|        -0.0994077|      -0.1000636|     -0.1061901|              -0.1450561|     -0.0881935|   -0.1365693|       -0.0938978|      -0.1176754|      -0.1911377|         -0.1543987|           -0.1519219|             -0.1996601|      -0.1040022|         -0.1195335|              -0.1473094|         -0.0995996|        -0.1182767|      -0.1107598|     -0.111993|        -0.1066217|      6.6612452|             -0.1472962|            -0.0985444|       -0.0967565|      -0.0949119|      -0.1145095|      -0.1429535|        -0.108501|      -0.1648097|        -0.1039788|    -0.0926214|      -0.126624|       -0.0955942|      -0.1650169|       -0.1623571|          -0.0804698|       -0.1036414|        -0.0836286|     -0.175643|      -0.0937953|       -0.0974094|       -0.0961735|           -0.1455853|      -0.1441996|        -0.1116953|      1.2297516|           -0.1445296|      -0.1052941|       -0.1245217|              -0.1088789|     -0.1709422|          -0.1082684|      -0.1829629|          -0.1167916|        -0.2184447|            -0.0966546|        -0.0942649|        -0.1315249|                 -0.1182767|         -0.1096851|            -0.090148|                -0.0631584|        -0.0922659|             -0.0922453|         -0.1476062|       -0.1162279|         -0.0976874|      -0.0964543|               -0.1435878|       -0.2044771|        -0.1013316|      -0.1270926|        -0.0937495|     -0.1342917|            -0.1043247|            -0.1473094|             -0.1211889|       -0.1009074|      5.9134349|        -0.1045075|    -0.150687|       -0.1585372|      -0.0998685|      -0.1090701|      -0.1112479|        -0.103608|      -0.1466292|     -0.1567758|                 -0.1435878|      -0.1091472|      -0.1136727|      -0.1001727|\n| 44|OFFENSE |         -0.1472962|   -0.1025123|    -0.1045303|     -0.1545404|     -0.1196412|           -0.1016498|               -0.0827688|            -0.1412023|        -0.0994077|      -0.1000636|     -0.1061901|              -0.1450561|     -0.0881935|   -0.1365693|       -0.0938978|      -0.1176754|      -0.1911377|         -0.1543987|           -0.1519219|             -0.1996601|      -0.1040022|         -0.1195335|              -0.1473094|         -0.0995996|        -0.1182767|      -0.1107598|     -0.111993|        -0.1066217|     -0.0974439|             -0.1472962|            -0.0985444|       -0.0967565|      -0.0949119|      -0.1145095|      -0.1429535|        -0.108501|      -0.1648097|        -0.1039788|    -0.0926214|      -0.126624|       17.7948786|      -0.1650169|       -0.1623571|          -0.0804698|       -0.1036414|        -0.0836286|     -0.175643|      -0.0937953|       -0.0974094|       -0.0961735|           -0.1455853|      -0.1441996|        -0.1116953|     -0.4023143|           -0.1445296|      -0.1052941|       -0.1245217|              -0.1088789|     -0.1709422|          -0.1082684|      -0.1829629|          -0.1167916|        -0.2184447|            -0.0966546|        -0.0942649|        -0.1315249|                 -0.1182767|         -0.1096851|            -0.090148|                -0.0631584|        -0.0922659|             -0.0922453|         -0.1476062|       -0.1162279|         -0.0976874|      -0.0964543|               -0.1435878|       -0.2044771|        -0.1013316|      -0.1270926|        -0.0937495|     -0.1342917|            -0.1043247|            -0.1473094|             -0.1211889|       -0.1009074|     -0.1080669|        -0.1045075|    -0.150687|       -0.1585372|      -0.0998685|      -0.1090701|      -0.1112479|        -0.103608|      -0.1466292|     -0.1567758|                 -0.1435878|      -0.1091472|      -0.1136727|      -0.1001727|\n\n</div>\n:::\n:::\n\n\n\n\n### Modellspezifikation 1\n\nWir definiere einen Naive-Bayes-Algorithmus:\n\n\n::: {.cell hash='klassifikation_cache/html/nb-spec_a7791b89b490498e0e4012c733f7328e'}\n\n```{.r .cell-code}\nnb_spec <- naive_Bayes() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"naivebayes\")\n\nnb_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNaive Bayes Model Specification (classification)\n\nComputational engine: naivebayes \n```\n:::\n:::\n\n\n\n\nUnd setzen auf die klassische zehnfache Kreuzvalidierung.\n\n\n\n::: {.cell hash='klassifikation_cache/html/folds1_b3892f13a41b2929c825b89d84c66d8c'}\n\n```{.r .cell-code}\nset.seed(42)\nfolds1 <- vfold_cv(d_train)\n```\n:::\n\n\n\n\n### Workflow 1\n\n\n\n::: {.cell hash='klassifikation_cache/html/wf1_e72c4ba077f687919fd19f44ec6270be'}\n\n```{.r .cell-code}\nwf1 <-\n  workflow() %>% \n  add_recipe(rec1) %>% \n  add_model(nb_spec)\n\nwf1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: naive_Bayes()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n6 Recipe Steps\n\n‚Ä¢ step_tokenize()\n‚Ä¢ step_stopwords()\n‚Ä¢ step_stem()\n‚Ä¢ step_tokenfilter()\n‚Ä¢ step_tfidf()\n‚Ä¢ step_normalize()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nNaive Bayes Model Specification (classification)\n\nComputational engine: naivebayes \n```\n:::\n:::\n\n\n\n### Fitting 1\n\n\n\n::: {.cell hash='klassifikation_cache/html/fit1_328969f3ab8f347fdb510934be5cc1ba'}\n\n```{.r .cell-code}\nfit1 <-\n  fit_resamples(\n    wf1,\n    folds1,\n    control = control_resamples(save_pred = TRUE)\n  )\n```\n:::\n\n\nDie Vorhersagen speichern wir ab,\num die Performanz in den Faltungen des Hold-out-Samples zu berechnen.\n\n\nM√∂chte man sich die Zeit sparen, die Syntax wieder durchlaufen zu lassen,\nkann man das Objekt speichern. \nAber Vorsicht: Dabei kann es passieren, dass man mit veralteten Objekten arbeitet.\n\n\n\n\n::: {.cell hash='klassifikation_cache/html/write-fit1_6ed4f8078985f5247760098e7c0de1fd'}\n\n```{.r .cell-code}\nwrite_rds(fit1, \"objects/chap_classific_fit1.rds\")\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/read-fit1_db8cda8f55020b1d50431c737904ef67'}\n\n:::\n\n\n\n\n### Performanz 1\n\n\n::: {.cell hash='klassifikation_cache/html/wf1-perf_3811d11fb3139d008e377f589ac8ba58'}\n\n```{.r .cell-code}\nwf1_performance <-\n  collect_metrics(fit1)\n\nwf1_performance\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/wf1-preds_9e5a96ca972fd7ae2a9a70142a6d0d43'}\n\n```{.r .cell-code}\nwf_preds <-\n  collect_predictions(fit1)\n\nwf_preds %>% \n  group_by(id) %>% \n  roc_curve(truth = c1, .pred_OFFENSE) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](klassifikation_files/figure-html/wf1-preds-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-11_ea236098c2ea6b3269affd24767a2c5b'}\n\n```{.r .cell-code}\nconf_mat_resampled(fit1, tidy = FALSE) %>% \n  autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](klassifikation_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Nullmodell\n\n\n\n::: {.cell hash='klassifikation_cache/html/nullmodel_b690571da86498d71a6bb535a17f8ebd'}\n\n```{.r .cell-code}\nnull_classification <- \n  parsnip::null_model() %>%\n  set_engine(\"parsnip\") %>%\n  set_mode(\"classification\")\n\nnull_rs <- workflow() %>%\n  add_recipe(rec1) %>%\n  add_model(null_classification) %>%\n  fit_resamples(\n    folds1\n  )\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-12_dd57384be2732b328c9339c968505ff9'}\n\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-13_65bbd96a7cf51f378286db10dad252dc'}\n\n:::\n\n\n\n\nHier ist die Performanz des Nullmodells.\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-14_95c2277fea20fb5429f418c5883f7cfd'}\n\n```{.r .cell-code}\nnull_rs %>%\n  collect_metrics()\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-15_d30fc6459327830e6ca46f2af82e77a8'}\n\n```{.r .cell-code}\nshow_best(null_rs)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n```\n:::\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|   penalty|.metric |.estimator |      mean|  n|   std_err|.config               |\n|---------:|:-------|:----------|---------:|--:|---------:|:---------------------|\n| 0.0017433|roc_auc |binary     | 0.8116297| 10| 0.0087106|Preprocessor1_Model22 |\n| 0.0007880|roc_auc |binary     | 0.8111630| 10| 0.0089414|Preprocessor1_Model21 |\n| 0.0003562|roc_auc |binary     | 0.8103737| 10| 0.0092381|Preprocessor1_Model20 |\n| 0.0001610|roc_auc |binary     | 0.8099159| 10| 0.0094448|Preprocessor1_Model19 |\n| 0.0000000|roc_auc |binary     | 0.8095907| 10| 0.0096029|Preprocessor1_Model01 |\n\n</div>\n:::\n:::\n\n\n\n\n\n## Workflow 2: Rezept 1 + Lasso\n\n\n\n\n::: {.cell hash='klassifikation_cache/html/lasso-spec_a3aec9ef905b014eab24150902bf3f0c'}\n\n```{.r .cell-code}\nlasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"glmnet\")\n\nlasso_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n\n\nWir definieren die Auspr√§gungen von `penalty`, \ndie wir ausprobieren wollen:\n\n\n\n::: {.cell hash='klassifikation_cache/html/lambda-grid_535a5f4c7d20cfe55216cdbe231211d1'}\n\n```{.r .cell-code}\nlambda_grid <- grid_regular(penalty(), levels = 30)\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/wf2_d7cd23d180dba6309eb8d24a4268d4e6'}\n\n```{.r .cell-code}\nwf2 <-\n  workflow() %>% \n  add_recipe(rec1) %>% \n  add_model(lasso_spec)\n\nwf2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: logistic_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n6 Recipe Steps\n\n‚Ä¢ step_tokenize()\n‚Ä¢ step_stopwords()\n‚Ä¢ step_stem()\n‚Ä¢ step_tokenfilter()\n‚Ä¢ step_tfidf()\n‚Ä¢ step_normalize()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n\nTunen und Fitten:\n\n\n::: {.cell hash='klassifikation_cache/html/fit2_72d0763e5fb111902bb4886c7c4a1b28'}\n\n```{.r .cell-code}\nset.seed(42)\n\nfit2 <-\n  tune_grid(\n    wf2,\n    folds1,\n    grid = lambda_grid,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nfit2\n```\n:::\n\n\n\nVorsicht beim Abspeichern.\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-16_3ad15af70ec18ef05156adac9f60d8b6'}\n\n```{.r .cell-code}\nwrite_rds(fit2, \"objects/chap_classific_fit2.rds\")\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/read-fit2_2588060668926c80c2d7bc67ff0a6553'}\n\n:::\n\n\n\n\nHier ist die Performanz:\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-17_2d69a2dc3119ef7d2a9ac6b76c6b0de3'}\n\n```{.r .cell-code}\ncollect_metrics(fit2) %>% \n  filter(.metric == \"roc_auc\") %>% \n  slice_max(mean, n = 3)\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-18_c6e265628955703678e0fe0e0a98c04a'}\n\n```{.r .cell-code}\nautoplot(fit2)\n```\n\n::: {.cell-output-display}\n![](klassifikation_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-19_9058c61de81de8826803a9cecb572ff2'}\n\n```{.r .cell-code}\nfit2 %>% \n  show_best(\"roc_auc\")\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| penalty|.metric |.estimator |      mean|  n|   std_err|.config               |\n|-------:|:-------|:----------|---------:|--:|---------:|:---------------------|\n|       0|roc_auc |binary     | 0.5997476| 10| 0.0090575|Preprocessor1_Model01 |\n|       0|roc_auc |binary     | 0.5997476| 10| 0.0090575|Preprocessor1_Model02 |\n|       0|roc_auc |binary     | 0.5997476| 10| 0.0090575|Preprocessor1_Model03 |\n|       0|roc_auc |binary     | 0.5997476| 10| 0.0090575|Preprocessor1_Model04 |\n|       0|roc_auc |binary     | 0.5997476| 10| 0.0090575|Preprocessor1_Model05 |\n\n</div>\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/chosen-auc-fit2_f25fa5cfdd99153c2a62003420ec98f3'}\n\n```{.r .cell-code}\nchosen_auc <- \n  fit2 %>%\n  select_by_one_std_err(metric = \"roc_auc\", -penalty)\n```\n:::\n\n\n\n\nFinalisieren:\n\n\n\n::: {.cell hash='klassifikation_cache/html/wf2-final_216ce62197475b9a9cd6027de21d075c'}\n\n```{.r .cell-code}\nwf2_final <-\n  finalize_workflow(wf2, chosen_auc)\n\nwf2_final\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: logistic_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n6 Recipe Steps\n\n‚Ä¢ step_tokenize()\n‚Ä¢ step_stopwords()\n‚Ä¢ step_stem()\n‚Ä¢ step_tokenfilter()\n‚Ä¢ step_tfidf()\n‚Ä¢ step_normalize()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = 0.00853167852417281\n  mixture = 1\n\nComputational engine: glmnet \n```\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/fit2-final-train_d2071737f7c1acf60b93ee6664042cab'}\n\n```{.r .cell-code}\nfit2_final_train <-\n  fit(wf2_final, d_train)\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-20_79f35bc123785f1fead2307b2e2e45eb'}\n\n```{.r .cell-code}\nfit2_final_train %>% \n  extract_fit_parsnip() %>% \n  tidy() %>% \n  arrange(-abs(estimate)) %>% \n  head()\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|term              |   estimate|   penalty|\n|:-----------------|----------:|---------:|\n|(Intercept)       |  0.6992837| 0.0085317|\n|tfidf_text_dumm   | -0.2201192| 0.0085317|\n|tfidf_text_merkel | -0.2139564| 0.0085317|\n|tfidf_text_moslem | -0.1772228| 0.0085317|\n|tfidf_text_lbr    | -0.1459591| 0.0085317|\n|tfidf_text_islam  | -0.1168911| 0.0085317|\n\n</div>\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/fit2-final-test_c575137863264e8190e03ee8e269f361'}\n\n```{.r .cell-code}\nfit2_final_test <-\n  last_fit(wf2_final, d_split)\n\ncollect_metrics(fit2_final_test)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|.metric  |.estimator | .estimate|.config              |\n|:--------|:----------|---------:|:--------------------|\n|accuracy |binary     | 0.6831604|Preprocessor1_Model1 |\n|roc_auc  |binary     | 0.6561186|Preprocessor1_Model1 |\n\n</div>\n:::\n:::\n\n\n\n\n### Vorhersage\n\n\n### Vohersagedaten\n\nPfad zu den Daten:\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-21_74325ebfa3d01bb8da7b25cd99d51e87'}\n\n```{.r .cell-code}\ntweet_data_path <- \"/Users/sebastiansaueruser/github-repos/hate-speech/data/\"\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-22_b14f0bf8ebf05e553da6ae612fde9567'}\n\n```{.r .cell-code}\ntweet_data_files_names <- list.files(path = tweet_data_path,\n                                     pattern  = \"tweets-to-.*\\\\.rds$\")\nhead(tweet_data_files_names)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"tweets-to-_FriedrichMerz_2021.rds\" \"tweets-to-_FriedrichMerz_2022.rds\"\n[3] \"tweets-to-ABaerbock_2021.rds\"      \"tweets-to-ABaerbock_2022.rds\"     \n[5] \"tweets-to-Alice_Weidel_2021.rds\"   \"tweets-to-Alice_Weidel_2022.rds\"  \n```\n:::\n:::\n\n\n\nWie viele Dateien sind es?\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-23_9f5e48689aadaea8bd875c97a08da53b'}\n\n```{.r .cell-code}\nlength(tweet_data_files_names)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 26\n```\n:::\n:::\n\n\n\nWir geben den Elementen des Vektors g√§ngige Namen,\ndas hilft uns gleich bei `map`:\n\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-24_53c7e929847ba9fd741ba88333d22b30'}\n\n```{.r .cell-code}\nnames(tweet_data_files_names) <- str_remove(tweet_data_files_names, \"\\\\.rds\")\n```\n:::\n\n\n\n\n\n\nOK, weiter: So k√∂nnen wir *eine* der Datendateien einlesen:\n\n\n::: {.cell hash='klassifikation_cache/html/read-tweets_1deee70c232f4351aeee6ab604f706c9'}\n\n```{.r .cell-code}\nd_raw <-\n  read_rds(file = paste0(tweet_data_path, tweet_data_files_names[1])) \n\nd <- \n  d_raw %>% \n  select(id, author_id, created_at, public_metrics) %>% \n  unnest_wider(public_metrics)\n\nhead(d)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|id                  |author_id           |created_at               | retweet_count| reply_count| like_count| quote_count|\n|:-------------------|:-------------------|:------------------------|-------------:|-----------:|----------:|-----------:|\n|1476992850944475136 |1270540287786565632 |2021-12-31T19:05:11.000Z |             0|           0|          0|           0|\n|1476982994556665862 |1471100575337140229 |2021-12-31T18:26:01.000Z |             0|           0|          0|           0|\n|1476958785977597958 |1438467230157602821 |2021-12-31T16:49:49.000Z |             0|           0|          0|           0|\n|1476637742884925447 |589112870           |2021-12-30T19:34:07.000Z |             0|           0|          0|           0|\n|1476587037046226949 |1041038433064562688 |2021-12-30T16:12:37.000Z |             0|           0|          0|           0|\n|1476534413802549249 |1425085042800406536 |2021-12-30T12:43:31.000Z |            10|           2|         44|           2|\n\n</div>\n:::\n:::\n\n\n\nUnd so lesen wir alle ein:\n\n\nZun√§chst erstellen wir uns eine Helper-Funktion:\n\n\n\n::: {.cell hash='klassifikation_cache/html/fun-read-and-select_a29abe2945bdbc6943603487c9d1c089'}\n\n```{.r .cell-code}\nread_and_select <- function(file_name, path_to_tweet_data = tweet_data_path) {\n  \n  out <- \n    read_rds(file = paste0(path_to_tweet_data, file_name)) %>% \n    select(id, author_id, created_at, text, public_metrics) %>% \n    unnest_wider(public_metrics)\n  \n  cat(\"Data file was read.\\n\")\n  \n  return(out)\n}\n```\n:::\n\n\nTesten:\n\n\n::: {.cell hash='klassifikation_cache/html/read-and-seelct-test_ce59829c5f29f605c8c90f3d9190dc91'}\n\n```{.r .cell-code}\nd1 <- read_and_select(tweet_data_files_names[1])\n\nhead(d1)\n```\n:::\n\n\n\n\nDie Funktion `read_and_select`  mappen wir auf alle Datendateien:\n\n\n\n::: {.cell hash='klassifikation_cache/html/map-read-and-select_aeaddc7e47ba8c59fc580347ac6204c3'}\n\n```{.r .cell-code}\ntic()\nds <-\n  tweet_data_files_names %>% \n  map_dfr(read_and_select, .id = \"dataset\")\ntoc()\n```\n:::\n\n\n\n`214.531 sec elapsed`\n\nDa wir den Elementen von `tweet_data_files_names` Namen gegeben haben, \nfinden wir diese Namen praktischerweise wieder in `ds`:\n\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-25_ddce0549d30777d79942408828357840'}\n\n:::\n\n::: {.cell hash='klassifikation_cache/html/read-ds_b1d9b039db86373a9b9bb32c166b9078'}\n\n:::\n\n\n\n\n\nVielleicht ist es zum Entwickeln besser,\nmit einem kleineren Datensatz einstweilen zu arbeiten:\n\n\n::: {.cell hash='klassifikation_cache/html/ds-short_f52ce1b406f293a50c02abdc84a18f92'}\n\n```{.r .cell-code}\nds_short <- slice_sample(ds, prop = .05)\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-26_94b086dd8d90cca604f34ea03360dcf3'}\n\n:::\n\n\n\n\n\n### Vokabular erstellen\n\n\n\n::: {.cell hash='klassifikation_cache/html/ds-long_9a59a699b11568c41e8923bf03bdefa8'}\n\n```{.r .cell-code}\nds_long <-\n  ds %>% \n  select(text) %>% \n  unnest_tweets(input = text, output = word)\n```\n:::\n\n\nPuh, das hat gedauert!\n\nSpeichern wir uns diese Daten daher auf die Festplatte:\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-27_a3824a47ca28b7764264de35be7a1014'}\n\n```{.r .cell-code}\nwrite_rds(ds_long, file = paste0(tweet_data_path, \"ds_long.rds\"))\n```\n:::\n\n\nEntfernen wir daraus die Duplikate,\num uns ein Vokabular zu erstellen:\n\n\n::: {.cell hash='klassifikation_cache/html/ds-voc_384707de03df4016afa6b58050525bfa'}\n\n```{.r .cell-code}\nds_voc <-\n  ds_long %>% \n  #slice_head(n = 10) %>% \n  distinct(word)\n```\n:::\n\n\nUnd das resultierende Objekt speichern wir wieder ab:\n\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-28_6a3ec426e922aa8c94e91b2e19b7e209'}\n\n```{.r .cell-code}\nwrite_rds(ds_voc, file = paste0(tweet_data_path, \"ds_voc.rds\"))\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-29_41aac4adc5af23cb98a30466d2dc5ffa'}\n\n:::\n\n\n\n\n\n\n## Worteinbettungen erstellen\n\n\n### FastText-Modell\n\nDefiniere die Konstanten f√ºr das fastText-Modell:\n\n\n::: {.cell hash='klassifikation_cache/html/fastText-constants_26b917cd79da8c4033508ecd2cd5aaf3'}\n\n```{.r .cell-code}\ntexts <- ds %>% pull(text)\ntexts <- tolower(texts)\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/fastText-filenames_53ad86c71e321424be9d1735e4b34191'}\n\n```{.r .cell-code}\nout_file_txt <- \"/Users/sebastiansaueruser/datasets/Twitter/twitter-polit-model.vec\"\nout_file_model <- \"/Users/sebastiansaueruser/datasets/Twitter/twitter-polit-model.bin\"\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/fasttext-modell_a2a076c45f3a35d3257966457052bb7e'}\n\n```{.r .cell-code}\nwriteLines(text = texts, con = out_file_txt)\nexecute(commands = c(\"skipgram\", \"-input\", tmp_file_txt, \"-output\", out_file_model, \"-verbose\", 1))\n```\n:::\n\n\n\n```\nRead 22M words\nNumber of words:  130328\nNumber of labels: 0\nProgress: 100.0% words/sec/thread:   49218 lr:  0.000000 avg.loss:  1.720812 ETA:   0h 0m 0s\n```\n\nJetzt laden wir das Modell von der Festplatte:\n\n\n::: {.cell hash='klassifikation_cache/html/twitter-fasttext-model_2f97c32425b459022dae51cf7b682554'}\n\n```{.r .cell-code}\ntwitter_fasttext_model <- load_model(out_file_model)\ndict <- get_dictionary(twitter_fasttext_model)\n```\n:::\n\n\n\nSchauen wir uns einige Begriffe aus dem Vokabular an:\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-30_09c8f6acffd73243951230a4631d4786'}\n\n```{.r .cell-code}\nprint(head(dict, 10))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"</s>\"            \"die\"             \"und\"             \"der\"            \n [5] \"sie\"             \"das\"             \"nicht\"           \"in\"             \n [9] \"ist\"             \"@_friedrichmerz\"\n```\n:::\n:::\n\n\nHier sind die ersten paar Elemente des Vektors f√ºr `menschen`:\n\n\n\n::: {.cell hash='klassifikation_cache/html/vector-menschen_2f9a9505c385f3dbeade4b7e1ddf3fac'}\n\n```{.r .cell-code}\nget_word_vectors(twitter_fasttext_model, c(\"menschen\")) %>% `[`(1:10)\n```\n:::\n\n\n\n```\n [1]  0.14156282  0.44875699  0.23911817 -0.02580349  0.29811972  0.03870077\n [7]  0.06518744  0.22527063  0.28198120  0.39931887\n ```\n\n\nErstellen wir uns einen Tibble, der \nals erste Spalte das Vokabular und in den √ºbrigen 100 Spalten die Dimensionen enth√§lt:\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-31_c6f2cee4437ec694117dbfb55a5c74ea'}\n\n```{.r .cell-code}\nword_embedding_twitter <-\n  tibble(\n    word = dict\n  )\n```\n:::\n\n\n\n::: {.cell hash='klassifikation_cache/html/words-vec-twitter_68a582ec380b02004c2b50cd750e9340'}\n\n```{.r .cell-code}\nwords_vecs_twitter <-\n  get_word_vectors(twitter_fasttext_model)\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/df-word-embedding-twitter_8dda266018d3a61d4321f7b610d52ebc'}\n\n```{.r .cell-code}\nword_embedding_twitter <-\n  word_embedding_twitter %>% \n  bind_cols(words_vecs_twitter)\n\nnames(word_embedding_twitter) <- c(\"word\", paste0(\"v\", sprintf(\"%03d\", 1:100)))  # Namen versch√∂nern\n```\n:::\n\n\n\nUnd als Worteinbettungs-Datei abspeichern:\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-32_de3738944047088ea3a2a8def3bdcd2d'}\n\n```{.r .cell-code}\nwrite_rds(word_embedding_twitter, file = paste0(tweet_data_path, \"word_embedding_twitter.rds\"))\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-33_6cb814ceddf5d6df46a05c075063abcf'}\n\n:::\n\n\n\n\n### Aufbereiten\n\nAm besten nur die Spalten behalten,\ndie wir zum Modellieren nutzen:\n\n\n::: {.cell hash='klassifikation_cache/html/ds-short2_af311d5a1144cffc658e173130588d21'}\n\n```{.r .cell-code}\nds_short2 <-\n  ds_short %>% \n  select(text, id)\n```\n:::\n\n\n\nDann backen wir die Daten mit dem vorhandenen Rezept:\n\n\n\n\n::: {.cell hash='klassifikation_cache/html/ds-baked_e60c03b90fbf7466f351b1dcff8da245'}\n\n```{.r .cell-code}\nds_baked <- bake(rec1_prepped, new_data = ds_short2)\n```\n:::\n\n\n\nIst das nicht komfortabel?\nDas Textrezept √ºbernimmt die Arbeit f√ºr uns,\nmit den richtigen Features zu arbeiten,\ndie tf-idfs f√ºr die richtigen Tokens zu berechnen.\n\nWer dem Frieden nicht traut,\ndem sei geraten, nachzupr√ºfen :-)\n\n\n\n## Workflow 3: Rezept 2 + Lasso\n\n### Daten aufteilen\n\n\n\n\n::: {.cell hash='klassifikation_cache/html/split-data3_284178b7689c029dae98e7eff9cb245c'}\n\n```{.r .cell-code}\nd_split <- initial_split(d2, strata = c1)\n\nd_train <- training(d_split)\nd_test <- testing(d_split)\n```\n:::\n\n\n\n\n### Hilfsfunktionen\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-34_0ccb1f85aac13fddd97c7912b935d82d'}\n\n```{.r .cell-code}\ndummy <- c(\"hallo\", \"baby\", \"fatal\")\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-35_2190eaebbbd2ca3de4c3701f0d9a484b'}\n\n```{.r .cell-code}\ncount_profane <- function(text) {\n  sum((tokenize_tweets(text, simplify = TRUE) %>% simplify()) %in% schimpf$word)\n}\n\ncount_profane(dummy) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-36_964cf1ae9c17d20834ff0671d67a644a'}\n\n```{.r .cell-code}\ncount_emo_words <- function(text) {\n  sum((tokenize_tweets(text, simplify = TRUE) %>% simplify()) %in% sentiws$word)\n}\n\ncount_emo_words(dummy)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-37_1417e6a3aad94f63a310e7e752188e0e'}\n\n```{.r .cell-code}\ncount_emojis <- function(text){\n  sum((tokenize_tweets(text, simplify = TRUE) %>% simplify()) %in% trimws(emj))\n}\n\ndummy <- c(\"baby\", \"und\", \"üÜó\", \"üñï\")\n\ncount_emojis(dummy)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-38_eae3e32961aba79e54608a706717461b'}\n\n```{.r .cell-code}\ncount_wild_emojis <- function(text){\n  sum((tokenize_tweets(text, simplify = TRUE) %>% simplify()) %in% wild_emojis)\n}\n\ncount_wild_emojis(dummy) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n\n\n\n### Rezept mit Worteinbettungen\n\n\n::: {.cell hash='klassifikation_cache/html/rec2_13e90d8dd5346b790b20de9311324250'}\n\n```{.r .cell-code}\nrec2 <- \n  recipe(c1 ~ ., data = select(d_train, text, c1, id)) %>% \n  update_role(id, new_role = \"id\") %>% \n  step_text_normalization(text) %>% \n  step_mutate(text_copy = text,\n              profane_n = map_int(text, count_profane),\n              emo_words_n = map_int(text, count_emo_words),\n              emojis_n = map_int(text, count_emojis),\n              wild_emojis_n = map_int(text, count_wild_emojis)\n  ) %>% \n  step_textfeature(text_copy) %>% \n  step_tokenize(text, token = \"tweets\") %>% \n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %>% \n  step_word_embeddings(text, embeddings = word_embedding_twitter)\n \nrec2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n        id          1\n   outcome          1\n predictor          1\n\nOperations:\n\nText Normalization for text\nVariable mutation for text, map_int(text, count_profane), map_in...\nText feature extraction for text_copy\nTokenization for text\nStop word removal for text\nWord embeddings aggregated from text\n```\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/rec2-prepped-baked_f4a1463a13a2f2ae42f3eccda9ee3748'}\n\n```{.r .cell-code}\nrec2_prepped <- prep(rec2)\nrec2_baked <- bake(rec2_prepped, new_data = NULL)\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-39_9595b770422f1c066aa41aab84771620'}\n\n```{.r .cell-code}\nrec2_baked %>% \n  select(1:15) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 3,756\nColumns: 15\n$ id                                  <int> 5, 7, 9, 10, 17, 42, 44, 48, 53, 5‚Ä¶\n$ c1                                  <fct> OFFENSE, OFFENSE, OFFENSE, OFFENSE‚Ä¶\n$ profane_n                           <int> 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1‚Ä¶\n$ emo_words_n                         <int> 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0‚Ä¶\n$ emojis_n                            <int> 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 2‚Ä¶\n$ wild_emojis_n                       <int> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ textfeature_text_copy_n_words       <int> 16, 32, 12, 15, 19, 30, 31, 6, 24,‚Ä¶\n$ textfeature_text_copy_n_uq_words    <int> 16, 28, 12, 15, 17, 29, 29, 6, 23,‚Ä¶\n$ textfeature_text_copy_n_charS       <int> 121, 145, 66, 119, 112, 171, 170, ‚Ä¶\n$ textfeature_text_copy_n_uq_charS    <int> 31, 29, 29, 30, 36, 42, 35, 23, 30‚Ä¶\n$ textfeature_text_copy_n_digits      <int> 0, 4, 0, 0, 4, 0, 1, 0, 2, 0, 2, 0‚Ä¶\n$ textfeature_text_copy_n_hashtags    <int> 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0‚Ä¶\n$ textfeature_text_copy_n_uq_hashtags <int> 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0‚Ä¶\n$ textfeature_text_copy_n_mentions    <int> 1, 1, 1, 0, 0, 5, 1, 0, 1, 1, 0, 2‚Ä¶\n$ textfeature_text_copy_n_uq_mentions <int> 1, 1, 1, 0, 0, 5, 1, 0, 1, 1, 0, 2‚Ä¶\n```\n:::\n:::\n\n\n\n\n### Fitting 3\n\n\n\n\n\n::: {.cell hash='klassifikation_cache/html/wf3_1ea87c63019fb4bc1ba67c3156c57ddd'}\n\n```{.r .cell-code}\nwf3 <-\n  workflow() %>% \n  add_recipe(rec2) %>% \n  add_model(lasso_spec)\n\nwf3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: logistic_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n6 Recipe Steps\n\n‚Ä¢ step_text_normalization()\n‚Ä¢ step_mutate()\n‚Ä¢ step_textfeature()\n‚Ä¢ step_tokenize()\n‚Ä¢ step_stopwords()\n‚Ä¢ step_word_embeddings()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n\n\n\n\nTunen und Fitten:\n\n\n::: {.cell hash='klassifikation_cache/html/wf3-fit_758cef2da1566f33e6bb3939df4e28ce'}\n\n```{.r .cell-code}\nset.seed(42)\n\ntic()\nfit3 <-\n  tune_grid(\n    wf3,\n    folds1,\n    grid = lambda_grid,\n    control = control_resamples(save_pred = TRUE)\n  )\n(toc)\nfit3\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-40_8fca145aae38a3b2d05b4b00aa4069e8'}\n\n```{.r .cell-code}\nwrite_rds(fit3, \"objects/chap_classific_fit3.rds\")\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-41_b0ee09c91f3c23c87b36d3c843b640df'}\n\n:::\n\n\n\n\nHier ist die Performanz:\n\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-42_1f725b8d0fab0ab3fce87fc7e1d33c11'}\n\n```{.r .cell-code}\ncollect_metrics(fit3)\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-43_556b6ecf0f4558cdd29e5c19fbeba380'}\n\n```{.r .cell-code}\nautoplot(fit3)\n```\n\n::: {.cell-output-display}\n![](klassifikation_files/figure-html/unnamed-chunk-43-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/unnamed-chunk-44_32f6b39e64d34ef5005fabd1c4c3a640'}\n\n```{.r .cell-code}\nfit3 %>% \n  show_best(\"roc_auc\")\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|   penalty|.metric |.estimator |      mean|  n|   std_err|.config               |\n|---------:|:-------|:----------|---------:|--:|---------:|:---------------------|\n| 0.0017433|roc_auc |binary     | 0.8116297| 10| 0.0087106|Preprocessor1_Model22 |\n| 0.0007880|roc_auc |binary     | 0.8111630| 10| 0.0089414|Preprocessor1_Model21 |\n| 0.0003562|roc_auc |binary     | 0.8103737| 10| 0.0092381|Preprocessor1_Model20 |\n| 0.0001610|roc_auc |binary     | 0.8099159| 10| 0.0094448|Preprocessor1_Model19 |\n| 0.0000000|roc_auc |binary     | 0.8095907| 10| 0.0096029|Preprocessor1_Model01 |\n\n</div>\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/chosenauc-fit3_c25e5e9f32430a0b8159dac8698641ba'}\n\n```{.r .cell-code}\nchosen_auc_fit3 <- \n  fit3 %>%\n  select_by_one_std_err(metric = \"roc_auc\", -penalty)\n```\n:::\n\n\n\n\nFinalisieren:\n\n\n\n::: {.cell hash='klassifikation_cache/html/wf3-final_bd074d0e893b8ee46db45869b23c9096'}\n\n```{.r .cell-code}\nwf3_final <-\n  finalize_workflow(wf3, chosen_auc_fit3)\n\nwf3_final\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: logistic_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n6 Recipe Steps\n\n‚Ä¢ step_text_normalization()\n‚Ä¢ step_mutate()\n‚Ä¢ step_textfeature()\n‚Ä¢ step_tokenize()\n‚Ä¢ step_stopwords()\n‚Ä¢ step_word_embeddings()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = 0.00853167852417281\n  mixture = 1\n\nComputational engine: glmnet \n```\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/fit3-final-train_ed41ad3fe81b6ebfb62d253e91f68d19'}\n\n```{.r .cell-code}\nfit3_final_train <-\n  fit(wf3_final, d_train)\n```\n:::\n\n::: {.cell hash='klassifikation_cache/html/fit3-final-train2_a77e4ab9d71f839a5e9ca4eac2f9dfb0'}\n\n```{.r .cell-code}\nfit3_final_train %>% \n  extract_fit_parsnip() %>% \n  tidy() %>% \n  arrange(-abs(estimate)) %>% \n  head()\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|term                             |   estimate|   penalty|\n|:--------------------------------|----------:|---------:|\n|(Intercept)                      |  1.2325023| 0.0085317|\n|profane_n                        | -0.5899329| 0.0085317|\n|textfeature_text_copy_n_exclaims | -0.1987554| 0.0085317|\n|wordembed_text_v055              |  0.1799862| 0.0085317|\n|wordembed_text_v059              | -0.1465378| 0.0085317|\n|wordembed_text_v054              |  0.1455451| 0.0085317|\n\n</div>\n:::\n:::\n\n::: {.cell hash='klassifikation_cache/html/fit3-final-test_fad2bad470e89c93845e7b252cf55b56'}\n\n```{.r .cell-code}\nfit3_final_test <-\n  last_fit(wf3_final, d_split)\n\ncollect_metrics(fit3_final_test)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|.metric  |.estimator | .estimate|.config              |\n|:--------|:----------|---------:|:--------------------|\n|accuracy |binary     | 0.7533919|Preprocessor1_Model1 |\n|roc_auc  |binary     | 0.7871291|Preprocessor1_Model1 |\n\n</div>\n:::\n:::\n\n\n\nAm Ende so eines Arbeitsganges,\nbei dem man wieder (und wieder) die gleichen Funktionen kopiert,\nund nur aufpassen muss, aus `fit2` an der richtigen Stelle `fit3` zu machen:\nDa blickt man jedem Umbau dieses Codes zu einer Funktion freudig ins Gesicht.\n\nEin anderes Problem,\nf√ºr das hier keine elegante L√∂sung vorliegt,\nsind die langen Berechnungszeiten, die, wenn man Pecht hat, auch noch\nmehrfach wiederholt werden m√ºssen.\n\nZu diesen Punkten sp√§ter mehr.\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}