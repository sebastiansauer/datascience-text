{
  "hash": "413900ea1aa2ebfae25e1503024291e1",
  "result": {
    "markdown": "\n\n\n![Text als Datenbasis pr√§diktiver Modelle](img/text-mining-1476780_640.png){width=10%}\n\nBild von <a href=\"https://pixabay.com/de/users/mcmurryjulie-2375405/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1476780\">mcmurryjulie</a> auf <a href=\"https://pixabay.com/de//?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1476780\">Pixabay</a>\n\n# Twitter Mining\n\n\n\n## Vorab\n\n\n### Lernziele\n\n\n- Twitterdaten via API von Twitter auslesen\n\n\n\n### Vorbereitung\n\n- Lesen Sie in @smltar Kap. 1.\n- Legen Sie sich ein Konto bei [Github](https://github.com/) an.\n- Legen Sie sich ein Konto bei [Twitter](twitter.com) an.\n- Lesen Sie [diesen Artikel zur Anmeldung bei der Twitter API](https://docs.ropensci.org/rtweet/articles/auth.html)^[Sie k√∂nnen [hier](https://www.howtogeek.com/343877/what-is-an-api/) nachlesen, was eine API ist.]\n\n\n\n\n### Ben√∂tigte R-Pakete\n\n\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-1_77e2c6fec2084696228a8e4d0f97d8de'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(rtweet)\nlibrary(tweetbotornot)  # optional\n```\n:::\n\n\n![R-Paket {rtweet}](https://docs.ropensci.org/rtweet/logo.png){width=10%}\n\n\nEinen √úberblick √ºber die Funktionen des Pakets (function reference) findet sich [hier](https://docs.ropensci.org/rtweet/reference/index.html).\n\n\n\n## Anmelden bei Twitter\n\n\n### Welche Accounts interessieren uns?\n\n\nHier ist eine (subjektive) Auswahl von deutschen Politikern^[Stand November 2022],\ndie einen Startpunkt gibt zur Analyse von Art und Ausma√ü von Hate Speech gerichtet an deutsche Politiker:innen.\n\n\n::: {.cell hash='twittermining_cache/html/politicians-df-load_6be8a5376d3d9c6db892597e3d6a2da7'}\n\n```{.r .cell-code}\nd_path <- \"data/twitter-german-politicians.csv\"\n\nd <- read_csv(d_path)\nd\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|name                                             |party  |screenname      |comment                                |\n|:------------------------------------------------|:------|:---------------|:--------------------------------------|\n|Karl Lauterbach                                  |SPD    |Karl_Lauterbach |NA                                     |\n|Olaf Scholz                                      |SPD    |OlafScholz      |NA                                     |\n|Annalena Baerback                                |Gruene |ABaerbock       |NA                                     |\n|Bundesministerium f√ºr Wirtschaft und Klimaschutz |Gruene |BMWK            |Robert Habeck ist der Minister im BMWK |\n|Friedrich Merz                                   |CDU    |_FriedrichMerz  |CDU-Chef                               |\n|Markus S√∂der                                     |CSU    |Markus_Soeder   |CSU-Chef                               |\n|Cem √ñzdemir                                      |Gruene |cem_oezdemir    |BMEL                                   |\n|Janine Wissler                                   |Linke  |Janine_Wissler  |Linke-Chefin                           |\n|Martin Schirdewan                                |Linke  |schirdewan      |Linke-Chef                             |\n|Christian Lindner                                |FDP    |c_lindner       |FDP-Chef                               |\n|Marie-Agnes Strack-Zimmermann                    |FDP    |MAStrackZi      |Vorsitzende Verteidigungsausschuss     |\n|Tino Chrupalla                                   |AFD    |Tino_Chrupalla  |AFD-Bundessprecher                     |\n|Alice Weidel                                     |AFD    |Alice_Weidel    |AFD-Bundessprecherin                   |\n\n</div>\n:::\n:::\n\n\n\n\n### Twitter App erstellen\n\n[Tutorial](https://cran.r-project.org/web/packages/rtweet/vignettes/auth.html)\n\n[Auf der Twitter Developer Seite](https://developer.twitter.com/en/portal/dashboard) k√∂nnen Sie sich ein Konto erstellen und dann anmelden.\n\n\n### Intro\n\nDie Seite von [rtweet](https://docs.ropensci.org/rtweet/) gibt eine gute Starthilfe in die Benutzung des Pakets.\n\n\n### Zugangsdaten\n\n\nZugangsdaten sollte man gesch√ºtzt speichern, also z.B. *nicht* in einem geteilten Ordner.\n\n\n\n::: {.cell hash='twittermining_cache/html/source-credentials-twitter_a449d902848402bd59a4cbe6b93bb15a'}\n\n```{.r .cell-code}\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\n```\n:::\n\n\n\nAnmelden:\n\n\n::: {.cell hash='twittermining_cache/html/oauth-twitter_8c104f82682d64d493c76a63b9b5f0a7'}\n\n```{.r .cell-code}\nauth <- rtweet_bot(api_key = API_Key,\n                   api_secret = API_Key_Secret,\n                   access_token = Access_Token,\n                   access_secret = Access_Token_Secret)\n```\n:::\n\n\n\n\n\n\n\nAlternativ kann man sich auch als `App` anmelden,\ndamit kann man z.B. nicht posten, aber daf√ºr mehr herunterladen [Quelle](https://docs.ropensci.org/rtweet/articles/auth.html).\n\n\n\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-2_2de34121fee6cffa6cb15ed5e1cad6c7'}\n\n```{.r .cell-code}\nauth <- rtweet_app(bearer_token = Bearer_Token)\n```\n:::\n\n\n\nJetzt haben wir ein Anmeldeobjekt, \ndas wir f√ºr die weiteren Anfragen dieser Session nutzen k√∂nnen.\nDas sagen wir jetzt der Twitter-API:\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-3_c119b10a9c41b85dbc7f3dd079cfb3ec'}\n\n```{.r .cell-code}\nauth_as(auth)\n```\n:::\n\n\n\n### Sch√ºtzen Sie Ihre Zugangsdaten\n\n\nAchtung, Sicherheitshinweis ...\nPassw√∂rter und andere sensitive (Anmelde-)Informationen muss man sch√ºtzen,\ndas wei√ü jeder.\nKonkret bedeutet es, dass Sie diese Daten *nicht* in einem √∂ffentlichen oder\ngeteilten Repo herumliegen lassen.\nAchten Sie auch darauf, dass, wenn Sie diese Information `source`en,\nso wie ich gerade, diese dann ungesch√ºtzt in Ihrem RStudio  `Environment` Fenster\nzu sehen sind. Falls Sie also den Bildschirm teilen,\noder Ihnen jemand √ºber die Schulter schaut,\nsind Ihre Zugangsdaten nicht gesch√ºtzt.\n\n\nEin √§hnlicher Fehler w√§re, die History-Dateien von R in ein √∂ffentliches Repo einzustellen (z.B. via Git).\nIn der Datei `.gitignore` sollten  daher folgende Dateien aufgef√ºhrt sein:\n\n```\n.Rhistory\n.Rapp.history\n````\n\n[Ein Rat von `rtweet` dazu](https://docs.ropensci.org/rtweet/articles/auth.html):\n\n>   It‚Äôs good practice to only provide secrets interactively, because that makes it harder to accidentally share them in either your .Rhistory or an .R file.\n\n\nEinen alternativen, sichereren Zugang bietet z.B. das [Paket `keyring`](https://r-lib.github.io/keyring/index.html).\nDieses Paket bietet eine Anbindung zur Schl√ºsselbundverwaltung Ihres Betriebssystems:\n\n\n>   Platform independent API to access the operating systems credential store. \n\n\nIm MacOS wird die zentrale Schl√ºsselbundverwaltung genutzt, in Windows und Linux die analoge Vorrichtungen.\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-4_5903a971b3abd6272c4238d20922c466'}\n\n```{.r .cell-code}\nlibrary(keyring)\n```\n:::\n\n\nWir erstellen uns einen Schl√ºsselbund:\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-5_ece0b5cc947221ec596456486c8e8aa5'}\n\n```{.r .cell-code}\nkeyring_create(keyring = \"hate-speech-twitter\")\n```\n:::\n\n\n\n\nDann k√∂nnen wir einen Eintrag im Schl√ºsselbund erstellen.\nGgf. werden Sie zuerst nach dem Password des Schl√ºsselbunds gefragt.\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-6_c557d94705b4ffd731b6d080392a72bc'}\n\n```{.r .cell-code}\nkey_set(service = \"client_id\",\n        keyring = \"hate-speech-twitter\")\n```\n:::\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-7_c6825e3cde4864ecdb6e3c72aa43c54a'}\n\n```{.r .cell-code}\nkey_set(service = \"client_secret\",\n        keyring = \"hate-speech-twitter\")\n```\n:::\n\n\n\nK√ºnftig k√∂nnen wir dann die Passw√∂rter aus dem Schl√ºsselbund abrufen:\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-8_65358a2f0e09e4a1ac962e1ae69e4d22'}\n\n```{.r .cell-code}\nkey_get(service = \"client_id\",\n        keyring = \"hate-speech-twitter\")\n```\n:::\n\n\n\n\n## Tweets einlesen\n\n\nZu beachten ist, dass es Limits gibt, wie viele Informationen (pro Zeiteinheit) man √ºber die Twitter-API auslesen darf.\nInformationen dazu findet man z.B. [hier](https://developer.twitter.com/en/docs/twitter-api/rate-limits) oder auch mit `rate_limit()`.\n\n\n\n\n\nEin g√§ngiges Limit der Twitter-API sind 900 Anfragen (z.B. Tweets auslesen) pro 15 Minuten.\n\n### Timeline einlesen einzelner Accounts\n\nMal ein paar Tweets zur Probe:\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-9_aa9a5244f816ab3f432dd7db3fbfe0bd'}\n\n```{.r .cell-code}\nsesa_test <- get_timeline(user = \"sauer_sebastian\", n = 3) %>% \n  select(full_text)\n```\n:::\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-10_a4dfedb0edd346e570725d87eea0d4f4'}\n::: {.cell-output .cell-output-stdout}\n```\nRT @pia_lamberty: Ein Ansatz, der sich beim Debunking wissenschaftlich als erfolgreich herausgestellt hat, ist das sog. Faktensandwich: htt‚Ä¶\nRT @ianbremmer: sure, it‚Äôs the hottest summer europe has ever had in history \n\nbut look at the upside\n\nit‚Äôs one of the coolest summers euro‚Ä¶\nRT @twisteddoodles: Balanced news reporting https://t.co/O1iiItEQrs\n```\n:::\n:::\n\n::: {.cell hash='twittermining_cache/html/get-timeline1_a4896e9f706429e53db24f5a60a93ca9'}\n\n```{.r .cell-code}\ntweets <- get_timeline(user = d$screenname)\nsaveRDS(tweets, file = \"tweets/tweets01.rds\")\n```\n:::\n\n\n\n[Michael Kearney](https://rtweet-workshop.mikewk.com/#25) r√§t uns:\n\n>   PRO TIP #4: (for developer accounts only) Use `bearer_token()` to increase rate limit to 45,000 per fifteen minutes.\n\n### Retweets einlesen\n\n\n\n\n::: {.cell hash='twittermining_cache/html/get-retweets1_23bc2370081e4cbbe020f74dad5ae145'}\n\n```{.r .cell-code}\ntweets01_retweets <- \n  tweets$id_str %>% \n  head(3) %>% \n  map_dfr( ~ get_retweets(status_id = .x, retryonratelimit = TRUE))\n```\n:::\n\n\n\n\nDa die meisten Retweets aber nix sagen, sondern nur auf das einen Tweet wiederholen, ist das Sammeln der Retweets ziemlich langweilig.\n\n\nM√∂chte man `retry on rate limit` im Standard auf `TRUE` setzen, \nso kann man das √ºber die Optionen von R tun.\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-11_08bb5624b603cefabb6db20575cb41d6'}\n\n```{.r .cell-code}\noptions(rtweet.retryonratelimit = TRUE)\n```\n:::\n\n\n\n\n### EPINetz Twitter Politicians 2021\n\n\n@konig_epinetz_2022 [Volltext hier](https://link.springer.com/article/10.1007/s11615-022-00405-7) haben einen Datensatz mit knapp 2500 Twitter Accounts deutscher Politiker zusammengestellt, zum Stand 2021.\n\n\nDer Datensatz kann √ºber [Gesis](https://search.gesis.org/research_data/SDN-10.7802-2415?doi=10.7802/2415) bezogen werden.\n\nAuf der gleichen Seite findet sich auch eine [Dokumentation des Vorgehens](https://access.gesis.org/sharing/2415/3675).\n\nNachdem wir den Datensatz heruntergeladen haben, k√∂nnen wir ihn einlesen:\n\n\n::: {.cell hash='twittermining_cache/html/read-epinetz_17727ab1e3931b3e8a86d6d606bac8e3'}\n\n```{.r .cell-code}\npoliticians_path <- \"data/EPINetz_TwitterPoliticians_2021.RDs\"\npoliticians_twitter <- read_rds(politicians_path)\n\nhead(politicians_twitter)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|   ID|official_name   |party     |region             |institution        |office          |user_id            |twitter_name       |twitter_handle |from       |until | year_of_birth|abgeordnetenwatch_id |gender |wikidata_id |\n|----:|:---------------|:---------|:------------------|:------------------|:---------------|:------------------|:------------------|:--------------|:----------|:-----|-------------:|:--------------------|:------|:-----------|\n|  535|Manja Sch√ºle    |SPD       |Brandenburg        |State Parliament   |Parliamentarian |827090742162100224 |Manja Sch√ºle       |ManjaSchuele   |2019-09-25 |NA    |          1976|146790               |female |Q40974942   |\n|  962|Petra Pau       |DIE LINKE |Federal            |Federal Parliament |Parliamentarian |1683845126         |Team PetraPau      |TeamPetraPau   |2017-10-24 |NA    |          1963|79091                |female |Q77195      |\n|  864|Dagmar Schmidt  |SPD       |Federal            |Federal Parliament |Parliamentarian |1377117206         |Team #dieschmidt   |TeamDieSchmidt |2017-10-24 |NA    |          1973|79036                |female |Q15433815   |\n| 2517|Bernd Buchholz  |FDP       |Schleswig-Holstein |State Parliament   |Parliamentarian |1073605033         |Bernd Buchholz     |BerndBuchholz  |2017-06-06 |NA    |          1961|121092               |male   |Q823715     |\n| 1378|Ingrid Remmers  |DIE LINKE |Federal            |Federal Parliament |Parliamentarian |551802475          |Ingrid Remmers MdB |ingrid_remmers |2017-10-24 |NA    |          1965|120775               |female |Q1652660    |\n| 1116|Reinhard Brandl |CSU       |Federal            |Federal Parliament |Parliamentarian |262730721          |Reinhard Brandl    |reinhardbrandl |2017-10-24 |NA    |          1977|79427                |male   |Q111160     |\n\n</div>\n:::\n:::\n\n\nDann lesen wir die Timelines (die Tweets) dieser Konten aus;\nin diesem Beispiel nur 10 Tweets pro Account:\n\n\n\n::: {.cell hash='twittermining_cache/html/get-timeline2_c19317f6e6c84f31a7371435606990ef'}\n\n```{.r .cell-code}\nepi_tweets <- get_timeline(user = head(politicians_twitter$twitter_name), n = 10)\nhead(epi_tweets)\n```\n:::\n\n\n\nNat√ºrlich k√∂nnte man auch mehr als 10 Tweets pro Konto einsammeln, braucht nur seine Zeit.\n\n### Followers suchen\n\n\n\n\n\n::: {.cell hash='twittermining_cache/html/save-followers1_8440cf0ff100e8b6c4ba0f3a5288f018'}\n\n```{.r .cell-code}\nfollowers01 <-\n  d$screenname %>% \n map_dfr( ~ get_followers(user = .x, retryonratelimit = TRUE))\n```\n:::\n\n\n\nDa es dauern kann, Daten auszulesen (wir d√ºrfen pro 15 Min. nur eine begrenzte Zahl an Information abrufen), kann es Sinn machen, die Daten lokal zu speichern.\n\n\n\n\n::: {.cell hash='twittermining_cache/html/save-flllowers01_c621552557c12fd9c15d6236d1c38dd9'}\n\n```{.r .cell-code}\nsaveRDS(followers01, file = \"tweets/followers01.rds\")\n```\n:::\n\n\n\nUnd ggf. wieder importieren:\n\n\n::: {.cell hash='twittermining_cache/html/read-flllowers0_e434ac6e5c413bef0b9cd61b31edc6e3'}\n\n```{.r .cell-code}\nfollowers01 <- read_rds(file = \"tweets/followers01.rds\")\n```\n:::\n\n\n\nWie viele unique Followers haben wir identifiziert?\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-12_30c4b1369eff75e370ed26c9677291b9'}\n\n```{.r .cell-code}\nfollowers02 <- \n  followers01 %>% \n  distinct(from_id)\n```\n:::\n\n\n\nDie Screennames w√§ren noch n√ºtzlich:\n\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-13_9e67179735ce6015b86d50dbc416495f'}\n\n```{.r .cell-code}\nlookup_users(users = \"1690868335\")\n```\n:::\n\n\n\nDie Anzahl der Users, die man nachschauen kann, ist begrenzt auf 180 pro 15 Minuten.\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-14_9ef071a87ad98975d50a04c7019cf806'}\n\n```{.r .cell-code}\nfollowers03 <-\n  followers02 %>% \n  mutate(screenname = \n           list(lookup_users(users = from_id, retryonratelimit = TRUE,verbose = TRUE)))\n```\n:::\n\n\n\n\n\nEntsprechend kann man wieder einlesen:\n\n\n\n\nDamit haben wir eine Liste an Followers, deren Tweets wir einlesen und analysieren k√∂nnen,\nz.B. nach Hate Speech.\n\nIm Gegensatz zu Followers hei√üen bei Twitter die Accounts, denen ei Nutzi folgt \"Friends\".\n\n\nLesen wir mal die Followers von `karl_lauterbach` ein:\n\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-15_75c6086862c92717dbd896ad7eb12653'}\n\n```{.r .cell-code}\nkarl_followers <- get_followers(user = \"karl_lauterbach\", verbose = TRUE)\n```\n:::\n\n\n\nUm nicht jedes Mal aufs Neue die Daten herunterzuladen, \nbietet es sich an, die Daten lokal zu speichern:\n\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-16_8c69ecff79ad24e274855d9d572b1950'}\n\n```{.r .cell-code}\nwrite_rds(karl_followers, file = \"tweets/karl_followers.rds\",\n          compress = \"gz\")\n```\n:::\n\n\nEntsprechend kann man die Daten dann auch wieder einlesen:\n\n\n\n::: {.cell hash='twittermining_cache/html/read-karl-followers_9cd432fd30ce21acb26982f8db52bc72'}\n\n```{.r .cell-code}\nkarl_followers <- read_rds(file = \"tweets/karl_followers.rds\")\n```\n:::\n\n\n\n\n\n### Follower Tweets einlesen\n\n\n\n::: {.cell hash='twittermining_cache/html/get-timeline3_5b1fd4d4d5b0eb19275c292a011ae4b8'}\n\n```{.r .cell-code}\nfollowers_tweets <- get_timeline(user = head(followers01$from_id), n = 10)\n```\n:::\n\n\n\n\n### Tweets nach Stichwort suchen\n\n\nUm nach einem Stichwort, allgemeiner nach einem bestimmten Text,\nin einem Tweet zu suchen,\nkann man die Funktion `search_tweets` nutzen:\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-17_a6b0e057386d6da5d7a7a92018639851'}\n\n```{.r .cell-code}\nmy_tweet <- search_tweets(\"Sebastian Sauer\", n = 1)\n```\n:::\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-18_d1ff634ae2d44e6aa8e3487d9fac5977'}\n\n:::\n\n\n\n\nSchaut man sich das zur√ºckgelieferte Objekt (einen Tibble) n√§her an,\nentdeckt man eine F√ºlle an Informationen.\nSatte 43 Spalten (teilweise Listenspalten) finden sich dort:\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-19_d2f5107cc6a8a37adb17f38c1e4820a7'}\n\n```{.r .cell-code}\nnames(my_tweet)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"created_at\"                    \"id\"                           \n [3] \"id_str\"                        \"full_text\"                    \n [5] \"truncated\"                     \"display_text_range\"           \n [7] \"entities\"                      \"metadata\"                     \n [9] \"source\"                        \"in_reply_to_status_id\"        \n[11] \"in_reply_to_status_id_str\"     \"in_reply_to_user_id\"          \n[13] \"in_reply_to_user_id_str\"       \"in_reply_to_screen_name\"      \n[15] \"geo\"                           \"coordinates\"                  \n[17] \"place\"                         \"contributors\"                 \n[19] \"retweeted_status\"              \"is_quote_status\"              \n[21] \"quoted_status_id\"              \"quoted_status_id_str\"         \n[23] \"retweet_count\"                 \"favorite_count\"               \n[25] \"favorited\"                     \"retweeted\"                    \n[27] \"lang\"                          \"possibly_sensitive\"           \n[29] \"quoted_status\"                 \"text\"                         \n[31] \"favorited_by\"                  \"scopes\"                       \n[33] \"display_text_width\"            \"quoted_status_permalink\"      \n[35] \"quote_count\"                   \"timestamp_ms\"                 \n[37] \"reply_count\"                   \"filter_level\"                 \n[39] \"query\"                         \"withheld_scope\"               \n[41] \"withheld_copyright\"            \"withheld_in_countries\"        \n[43] \"possibly_sensitive_appealable\"\n```\n:::\n:::\n\n\n\nDie Tweet-ID dieses Tweets bekommen Sie,\nwenn Sie die Variable `id_str` auslesen:\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-20_f37dd7d2601ac0a68ddd527a5cd3872f'}\n\n```{.r .cell-code}\nmy_tweet$id_str\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"1593598440675500032\"\n```\n:::\n:::\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-21_92f6b3b4817f309a2f46b2e209a3b364'}\n\n```{.r .cell-code}\nmy_tweet$source\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"<a href=\\\"https://mobile.twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web App</a>\"\n```\n:::\n:::\n\n\n\n\n\nDabei ist `source` nicht etwa die Person, die tweetet,\nwie man vielleicht meinen k√∂nnte,\nsondern das Frontend, das dabei verwendet wurde,\nalso z.B. die iphone-App oder die Twitter-Webseite.\n\nLeider sucht man den `screenname` zu einen Tweet vergeblich in `my_tweet`.\n\nGegeben eines Dataframes mit Tweets kann man sich aber wie folgt den Nutzernamen (`screen_name`) ausgeben lassen.\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-22_96fba01a50aaca282e1cce7baf4d6285'}\n\n```{.r .cell-code}\nusers_data(my_tweet)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|         id|id_str     |name               |screen_name     |location              |description  |url                     |protected | followers_count| friends_count| listed_count|created_at                     | favourites_count|verified | statuses_count|profile_image_url_https                                                     |profile_banner_url                                          |default_profile |default_profile_image |withheld_in_countries |derived |withheld_scope |entities                                                                                                                                                                                                                                                                                                       |\n|----------:|:----------|:------------------|:---------------|:---------------------|:------------|:-----------------------|:---------|---------------:|-------------:|------------:|:------------------------------|----------------:|:--------|--------------:|:---------------------------------------------------------------------------|:-----------------------------------------------------------|:---------------|:---------------------|:---------------------|:-------|:--------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 1690868335|1690868335 |Sebastian Sauer üá∫üá¶ |sauer_sebastian |N√ºrnberg, Deutschland |Data Science |https://t.co/zwh3dCw3Dc |FALSE     |             485|           346|           34|Thu Aug 22 11:49:09 +0000 2013 |             7517|FALSE    |           1241|https://pbs.twimg.com/profile_images/853024804676521984/VQ5YBKVu_normal.jpg |https://pbs.twimg.com/profile_banners/1690868335/1453724621 |TRUE            |FALSE                 |NULL                  |NA      |NA             |NA                          , NA                          , NA                          , NA                          , NA                          , https://t.co/zwh3dCw3Dc     , https://data-se.netlify.com/, data-se.netlify.com         , 0                           , 23                          , NA |\n\n</div>\n:::\n:::\n\n\n\n\nAu√üerdem gibt es einen \"Trick\" [laut dieser Quelle](https://www.bram.us/2017/11/22/accessing-a-tweet-using-only-its-id-and-without-the-twitter-api/), vgl. auch [diesen SO-Post](https://stackoverflow.com/questions/897107/can-i-fetch-the-tweet-from-twitter-if-i-know-the-tweets-id): \nGibt man in die URL eines Tweets einen beliebigen Nutzernamen - \ndas kann ein Fantasiename sein - \nso wird man automatisch zum richtigen Nutzer geleitet.\n\n\nDie Rohform der URL sieht also so aus:\n\n`https://twitter.com/irgendeinnutzer/status/<id_str>`\n\nGeben Sie also z.B. Folgende URL in Ihren Browser sein:\n\n`https://twitter.com/irgendeinnutzer/status/1593598440675500032`\n\nUnd Sie werden zum Nutzer `sauer_sebastian` weitergeleitet bzw.\nzu seinem Tweet mit obiger ID.\n\n\n### Der Volltext ist manchmal abgeschniten\n\nManchmal ist der Volltext abgeschnitten\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-23_25e310d46e5d0872f1050664a688aa73'}\n\n```{.r .cell-code}\nmy_tweet$full_text\n```\n:::\n\n\n```\nHier steht der Beginn des Tweet-Textes, aber dann endet der Text abrup...\"\n```\n\n\nGl√ºcklicherweise - sofern man bei einer so umst√§ndlichen Darstellung  von Gl√ºck reden kann - findet man den kompletten Text andernorts [Quelle](https://stackoverflow.com/questions/47211501/twitter-streaming-api-not-return-full-tweets?noredirect=1&lq=1).\n\n\nDazu schreibt [in diesem SO-Post](https://stackoverflow.com/questions/47211501/twitter-streaming-api-not-return-full-tweets?noredirect=1&lq=1) der Nutzer `Jonas`: \n\n\n>   You will need to check if the tweet is a retweet. If it is, use the retweet's full_text. If it is not, use the tweet's full_text. ‚Äì Jonas,  Nov 13, 2017 at 15:00\n\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-24_bca1d48f23fbfcd81b9191c9f0237349'}\n\n```{.r .cell-code}\nmy_tweet$retweeted_status[[1]][[\"full_text\"]]\n```\n:::\n\n\n\n```\nHier steht der Beginn des Tweet-Textes, aber dann endet der Text abrupt? \nNein,er geht weiter und irgendwann ist der dann wirklich aus.\"\n```\n\n\n\n\n\n\n### Tweets nach ID suchen\n\nMit `lookup_tweets(id_des_tweets)` k√∂nnen Sie sich die Informationen zu einen Tweet ausgeben lassen.\nDas ist nat√ºrlich prim√§r der Volltext:\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-25_b800941c1d0532da39cc835860a35f32'}\n\n```{.r .cell-code}\ntweet_example <- lookup_tweets(\"1593598440675500032\")\ntweet_example$full_text\n```\n:::\n\n\nAber auch die √ºbrigen Informationen k√∂nnen interessant sein.\n\n\n\n## Tweets verarbeiten\n\n\n### Grundlegende Verarbeitung\n\nSind die Tweets eingelesen, kann man z.B. eine Sentimentanalyse, s. @sec-sentimentanalyse, durchf√ºhren, oder schlicht vergleichen, welche Personen welche W√∂rter h√§ufig verwenden, s. @sec-woerterzaehlen.\n\n\n\n\n\n\n### Bot or not?\n\nEine interessante Methode, Tweets zu verarbeiten, bietet das R-Paket `tweetbotornot` von [M. Kearney](https://github.com/mkearney/tweetbotornot).\n\n\nAus der `Readme`: \n\n\n>   Due to Twitter‚Äôs REST API rate limits, users are limited to only 180 estimates per every 15 minutes. To maximize the number of estimates per 15 minutes (at the cost of being less accurate), use the fast = TRUE argument. This method uses only users-level data, which increases the maximum number of estimates per 15 minutes to 90,000! Due to losses in accuracy, this method should be used with caution!\n\n\n\n\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-26_01302d47365ba6f7a3c93c4c5dc7405c'}\n\n```{.r .cell-code}\nusers <- c(\"sauer_sebastian\")\nbot01 <-\n  tweetbotornot(users)\n```\n:::\n\n\n\n\n:::callout-important\nIch habe ein Fehlermeldung bekommen bei `tweetbotornot`.\nDa k√∂nnte ein technisches Problem in der Funktion vorliegen.\n:::\n\n\n\n\n\n\n## Cron Jobs\n\n\n\n### Was ist ein Cron Job?\n\n[Cron](https://en.wikipedia.org/wiki/Cron) ist ein Programm auf Unix-artigen Betriebssystemen, das Skripte zu eingestellten Zeiten (wiederholt) ausf√ºhrt, das sind dann \"Cron Jobs\".\nAuf Windows gibt es aber analoge Funktionen.\nCron Jobs sind praktisch, da man nicht jedes Mal selber z.B. Tweets, die heute zu einem Thema getweetet wurden, herunterladen muss.\nDas wird dann vom Cron Job √ºbernommen.\n\nIn R gibt es eine API zum Programm Cron mit dem Paket `{cronR}`, s. [Anleitung hier](https://github.com/bnosac/cronR).\n\nDas analoge R-Paket f√ºr Windows hei√üt [`{taskscheduleR}`](https://github.com/bnosac/taskscheduleR).\n\n\n\n### Beispiel f√ºr einen Cron Job\n\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-27_37b43de2497a38c40906f6d3f60b637f'}\n\n```{.r .cell-code}\nlibrary(cronR)\n\nscrape_script <- cron_rscript(\"scrape_tweets.R\")\n\n# Cron Job hinzuf√ºgen:\ncron_add(command = scrape_script, \n         frequency = 'daily', \n         at = \"10AM\",\n         id = 'Hate Speech')  # Name des Cron Jobs\n\ncron_clear(ask = FALSE)  # Alle Cron Jobs l√∂schen\ncron_ls()  # Liste aller Cron Jobs\n```\n:::\n\n\n\nIm obigen Beispiel wird das R-Skript `scrape_tweets.R` t√§glich um 10h ausgef√ºhrt.\n\n\n\n\nDer Inhalt von `scrape_tweets.R` k√∂nnte dann, in Grundz√ºgen, so aussehen:\n\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-28_8749303764887df4719ef8d8c5ebe4c7'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(rtweet)\nfollowers_lauterbach <-\n  followers01 %>% \n  filter(to_id == \"Karl_Lauterbach\")\n\nfollowers_lauterbach_tweets <- \n  get_timeline(user = followers_lauterbach$from_id[1:10], n = 10, retryonratelimit = TRUE, verbose = FALSE)\n\n\npath_output <- \"/Users/sebastiansaueruser/Google Drive/RRRing/Scrape-Tweets/tweets/\"\n\nwrite_csv(x = followers_lauterbach_tweets,\n          file = paste0(path_output, \"followers_lauterbach_tweets.csv\"),\n          append = TRUE)\n```\n:::\n\n\n\nWir schreiben nicht jedes Mal (jeden Tag) eine neue CSV-Datei, sondern wir h√§ngen hier die neu ausgelesenen Daten an die Datei an.\n\nLeider ist es mit `rtweet` nicht m√∂glich, ein Datum anzugeben, ab dem man Tweets auslesen m√∂chte^[Mit dem R-Paket `twitteR`, das mittlerweile zugunsten von `rtweet` aufgegeben wurde, war das m√∂glich. Allerdings zeigt ein [Blick in die Dokumentation der Twitter-API](https://developer.twitter.com/en/docs/twitter-api/v1/tweets/timelines/api-reference/get-statuses-home_timeline), das Datumsangaben offenbar gar nicht unterst√ºtzt werden.]\n\n\n## Datenbank an Tweets aufbauen\n\n\n### Stamm an bisherigen Tweets\n\nIn diesem Abschnitt k√ºmmern wir uns in gr√∂√üerem Detail um das Aufbauen einer Tweets-Datenbank.\n\n\nDiese Pakete ben√∂tigen wir:\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-29_0525351a482f28a0a0c61b5c7f538c23'}\n\n```{.r .cell-code}\nlibrary(rtweet)\nlibrary(tidyverse)\nlibrary(rio)  # R Data import/export\n```\n:::\n\n\n\nDann melden wir uns an:\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-30_fb48d3c8c653eddc75270844de9e147b'}\n\n```{.r .cell-code}\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\nauth <- rtweet_app(bearer_token = Bearer_Token)\n```\n:::\n\n\n\nDann brauchen wir eine Liste an Twitterkonten,\ndie uns interessieren.\nIm Kontext von Hate Speech soll uns hier interessieren,\nwelche Tweets *an* deutsche Spitzenpolitikis^[zur Zeit, als diese Zeilen geschrieben wurden] gesendet werden.\nWir suchen also nach Tweets mit dem Text `@karl_lauterbach`,\num ein Beispiel f√ºr einen Spitzenpolitiker zu nennen, der vermutlich von Hate Speech in h√∂herem Ma√üe betroffen ist.\n\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-31_bf68bc496e9ed9e8aa6f9f751073460d'}\n\n```{.r .cell-code}\npoliticians_twitter_path <- \"/Users/sebastiansaueruser/github-repos/datascience-text/data/twitter-german-politicians.csv\"\n\npoliticians_twitter <- rio::import(file = politicians_twitter_path)\n```\n:::\n\n\n\n\nIn der Liste befinden sich 13 Politiker.\nEs macht die Sache vielleicht einfacher,\nwenn wir die Rate nicht √ºberziehen.\nBleiben wir daher bei 1000 Tweets pro Politiki:\n\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-32_75379a7d06451e40bd55a0f7dfe43279'}\n\n```{.r .cell-code}\nn_tweets_per_politician <- 1e3\n```\n:::\n\n\n\nDie R-Syntax, die die Arbeit leistet,\nist in Funktionen ausgelagert,\nder √úbersichtlichkeit halber.\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-33_513cc47da250e2b79cb0ce7c0174420c'}\n\n```{.r .cell-code}\nsource(\"funs/filter_recent_tweets.R\")\nsource(\"funs/download_recent_tweets.R\")\nsource(\"funs/add_tweets_to_tweet_db.R\")\n```\n:::\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-34_aa2984e630c03a03fe7cc8657bdaf3da'}\n\n:::\n\n\nJetzt laden wir einfach die aktuellsten 1000 Tweets\npro Konto herunter,\ndaher brauchen wir keine Tweet-ID angeben,\ndie ein Mindest- oder Maximum-Datum (bzw. ID) f√ºr einen \nTweet angibt:\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-35_1d71239e773d43f76d7364f6c80354e3'}\n\n```{.r .cell-code}\ntweets_older <-\n  download_recent_tweets(screenname = politicians_twitter$screenname,\n                         max_or_since_id_str = NULL,\n                         n = n_tweets_per_politician,\n                         strip_columns = TRUE,\n                         reverse = TRUE)\n```\n:::\n\n\n\nWie weit in die Vergangenheit reicht unsere Tweet-Sammlung?\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-36_2444e6fa3b1ec61efe1cf4cf0ab0f1f7'}\n\n```{.r .cell-code}\noldest_tweets <- filter_recent_tweets(tweets_older, max_or_min_id_str = is_min_id_str)\noldest_tweets\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|id_str              |screenname      |created_at          |is_min_id_str |is_max_id_str |\n|:-------------------|:---------------|:-------------------|:-------------|:-------------|\n|1590754620649115648 |Karl_Lauterbach |2022-11-10 18:13:51 |TRUE          |FALSE         |\n|1589837879693352960 |OlafScholz      |2022-11-08 05:31:02 |TRUE          |FALSE         |\n|1590485330742083585 |ABaerbock       |2022-11-10 00:23:47 |TRUE          |FALSE         |\n|1589983737621942272 |BMWK            |2022-11-08 15:10:38 |TRUE          |FALSE         |\n|1590646264433373184 |_FriedrichMerz  |2022-11-10 11:03:16 |TRUE          |FALSE         |\n|1588595577360875520 |Markus_Soeder   |2022-11-04 19:14:34 |TRUE          |FALSE         |\n|1590097264613425152 |cem_oezdemir    |2022-11-08 22:41:45 |TRUE          |FALSE         |\n|1588082031656898560 |Janine_Wissler  |2022-11-03 09:13:56 |TRUE          |FALSE         |\n|1588082031656898560 |schirdewan      |2022-11-03 09:13:56 |TRUE          |FALSE         |\n|1590628128791007233 |c_lindner       |2022-11-10 09:51:13 |TRUE          |FALSE         |\n|1589277825152208898 |MAStrackZi      |2022-11-06 16:25:35 |TRUE          |FALSE         |\n|1587964349993422848 |Tino_Chrupalla  |2022-11-03 01:26:18 |TRUE          |FALSE         |\n|1589696708447186945 |Alice_Weidel    |2022-11-07 20:10:05 |TRUE          |FALSE         |\n\n</div>\n:::\n:::\n\n\n\nWas sind die neuesten Tweets, die wir habven?\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-37_8ce6349f0bf5a39a33a5b6aad1f2037a'}\n\n```{.r .cell-code}\nmost_recent_tweets <- filter_recent_tweets(oldest_tweets)\nmost_recent_tweets\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|id_str              |screenname      |created_at          |is_min_id_str |is_max_id_str |\n|:-------------------|:---------------|:-------------------|:-------------|:-------------|\n|1590754620649115648 |Karl_Lauterbach |2022-11-10 18:13:51 |TRUE          |TRUE          |\n|1589837879693352960 |OlafScholz      |2022-11-08 05:31:02 |TRUE          |TRUE          |\n|1590485330742083585 |ABaerbock       |2022-11-10 00:23:47 |TRUE          |TRUE          |\n|1589983737621942272 |BMWK            |2022-11-08 15:10:38 |TRUE          |TRUE          |\n|1590646264433373184 |_FriedrichMerz  |2022-11-10 11:03:16 |TRUE          |TRUE          |\n|1588595577360875520 |Markus_Soeder   |2022-11-04 19:14:34 |TRUE          |TRUE          |\n|1590097264613425152 |cem_oezdemir    |2022-11-08 22:41:45 |TRUE          |TRUE          |\n|1588082031656898560 |Janine_Wissler  |2022-11-03 09:13:56 |TRUE          |TRUE          |\n|1588082031656898560 |schirdewan      |2022-11-03 09:13:56 |TRUE          |TRUE          |\n|1590628128791007233 |c_lindner       |2022-11-10 09:51:13 |TRUE          |TRUE          |\n|1589277825152208898 |MAStrackZi      |2022-11-06 16:25:35 |TRUE          |TRUE          |\n|1587964349993422848 |Tino_Chrupalla  |2022-11-03 01:26:18 |TRUE          |TRUE          |\n|1589696708447186945 |Alice_Weidel    |2022-11-07 20:10:05 |TRUE          |TRUE          |\n\n</div>\n:::\n:::\n\n\n\nJetzt laden wir die *neueren* Tweets herunter,\nalso mit einer ID *gr√∂√üer* als die gr√∂√üte in unserer Sammlung:\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-38_3b270a0dde8517ca15fbd565c4429404'}\n\n:::\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-39_6ffd618d0f26bd27f259e68ae4027e3a'}\n\n```{.r .cell-code}\ntweets_new <- \n  download_recent_tweets(screenname = most_recent_tweets$screenname,\n                         max_or_since_id_str = most_recent_tweets$id_str)\n\ntweets_new %>% \n  select(screenname, created_at, id_str) %>% \n  head()\n```\n:::\n\n\n\n\nJetzt - und jedes Mal, wenn wir Tweets herunterladen - \nf√ºgen wir diese einer Datenbank (oder zumindest einer \"Gesamt-Tabelle\") hinzu:\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-40_89b29bfc7e1dfb78838e6a4c2e5eb9cd'}\n\n```{.r .cell-code}\ntweets_db <- add_tweets_to_tweets_db(tweets_new, tweets_older)\n\nnrow(tweets_db)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 10969\n```\n:::\n:::\n\n\n\nSchlie√ülich sollten wir nicht vergessen\ndiese in einer Datei zu speichern:\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-41_447868f472efdbdf4ffad94d844ad2c8'}\n\n```{.r .cell-code}\nwrite_rds(tweets_db, file = \"~/datasets/Twitter/tweets-db-2022-11-11.rds\")\n```\n:::\n\n\n\n\n... ... So, einige Zeit ist vergangen.\nLaden wir noch √§ltere Tweets herunter und f√ºgen Sie unserer Datenbank hinzu:\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-42_232d694d203ba40cc574d4e97e19bda7'}\n\n```{.r .cell-code}\ntweets_older2 <-\n  download_recent_tweets(screenname = politicians_twitter$screenname,\n                         max_or_since_id_str = oldest_tweets$id_str,\n                         n = 1e3,\n                         strip_columns = TRUE,\n                         reverse = TRUE)\n```\n:::\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-43_f4b14a2e193c05fdad5a1dfc23b55e3d'}\n\n:::\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-44_78757c295caf31d38679b7000e8d2d01'}\n\n```{.r .cell-code}\ntweets_db <- add_tweets_to_tweets_db(tweets_new, tweets_older2)\n\nnrow(tweets_db)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 10011\n```\n:::\n:::\n\n\n\nUnd wieder speichern wir die vergr√∂√üerte Datenbasis auf der Festplatte:\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-45_adc9da93d44daa23311c4787596c7649'}\n\n```{.r .cell-code}\nwrite_rds(tweets_db, file = \"~/datasets/Twitter/hate-speech-twitter.rds\")\n```\n:::\n\n\nLeider ist die Datenbasis nicht mehr deutlich gewachsen.\nEine plausible Ursache ist, dass Twitter den Zugriff auf alte Tweets einschr√§nkt.\n\nAus der Hilfe von `search_tweets`:\n\n>   Returns Twitter statuses matching a user provided search query. ONLY RETURNS DATA FROM THE PAST 6-9 DAYS.\n\n\nMit Hilfe des [Academic Research Access](https://developer.twitter.com/en/products/twitter-api/academic-research) sind deutlich h√∂here Raten m√∂glich.\n\n\n\n\n\n### Neue Tweets per Cron Job\n\n\nWie oben schon ausprobiert,\nlegen wir uns einen Cron Job an.\n\nDas ist √ºbrigens auch eine komfortable L√∂sung.\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-46_79ffd7ca58721672abd6dea47cbf262f'}\n\n```{.r .cell-code}\nlibrary(cronR)\n\nscrape_script <- cron_rscript(\"/Users/sebastiansaueruser/github-repos/datascience-text/funs/get_tweets_politicians.R\")\n\n# Cron Job hinzuf√ºgen:\ncron_add(command = scrape_script, \n         frequency = 'daily', \n         at = \"10AM\",\n         id = 'Hate Speech')  # Name des Cron Jobs\n```\n:::\n\n\n\nDas Skript `get_tweets_politicians.R` birgt die Schritte,\ndie wir in diesem Abschnitt ausprobiert haben, [hier](https://github.com/sebastiansauer/datascience-text/blob/main/specifics/get_tweets_politicians.R) liegt es.\nKurz gesagt sucht es nach neuen Tweets, die \nalso noch nicht in Ihrer \"Datenbank\" vorhanden sind,\nund l√§dt diese herunter.\nDabei werden maximal 1000 Tweets pro Konto (derer sind es 13)\nheruntergeladen.\n\nBei einem Cronjob sollten *absolute* Pfade angegeben werden, da der Cronjob *nicht* aus dem aktuellen Projekt-Repo startet.\n\nDie Ergebnisse eines Cronjob-Durchlaufs werden in einer Log-Datei abgelegt, und \nzwar in dem Ordner, in dem auch das Skript liegt, das \nim Rahmen des Cronjobs durchgef√ºhrt wird.\n\n\n\n\n:::callout-note\nSchauen Sie sich die Funktionen im Ordner `/funs` einmal in Ruhe an.\n[Hier](https://github.com/sebastiansauer/datascience-text/tree/main/funs) geht es zu dem Ordner im Github-Repo.\nEs ist alles keine Zauberei,\naber im Detail gibt es immer wieder Schwierigkeiten.\nAm meisten lernt man,\nwenn man selber Hand anlegt.\n:::\n\n\n\n\n\nM√∂chte man den Cron Job wieder l√∂schen, so kann man das so tun:\n\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-47_f05a36051b95389b2d98b34f2519e27f'}\n\n```{.r .cell-code}\ncron_clear(ask = FALSE)  # Alle Cron Jobs l√∂schen\ncron_ls()  # Liste aller Cron Jobs\n```\n:::\n\n\n\nUm die Tweets \"h√§ndisch\" herunterzuladen,\nkann man `get_tweets_politicians()` aufrufen:\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-48_7f4c087a94c8d536f53e23606dc22758'}\n\n```{.r .cell-code}\nsource(\"funs/get_tweets_politicians.R\")\nget_tweets_politicians()\n```\n:::\n\n\n### Tweets in Excel exportieren\n\nUm pr√§diktive Modelle zu erstellen, braucht man ein\nTrainingsset, Tweets also, die schon vorklassifiziert sind, \nz.B. im Hinblick auf Hassrede mit `ja` oder `nein`.\nTechnisch bietet sich ein `1` vs. `0` an.\n\nDazu laden wir einen Datensatz mit Tweets,\nz.B. diesen hier:\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-49_7c6a96f2f280ed95cba027a6bab4b533'}\n\n```{.r .cell-code}\ntweets_to_kl <- import(\"/Users/sebastiansaueruser/datasets/Twitter/tweets_to_karl_lauterbach.rds\")\n```\n:::\n\n\n\nDa es viele Spalten gibt, die teilweise Listenspalten sind,\nalso komplex, begrenzen wir uns auf das Wesentliche,\nden Tweet-Text und die ID des Tweets.\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-50_3b838e6dd8e61d986f7df9d9d67dedf4'}\n\n```{.r .cell-code}\ntweets_to_kl2 <-\n  tweets_to_kl %>% \n  select(id_str, full_text) \n```\n:::\n\n\n\n:::callout-note\nDie Tweet-ID wird einmal als String und einmal als Integer gespeichert.\nAllerdings √ºbersteigt die Anzahl der Ziffern die Speichergr√∂√üe von (normalen) Integer-Formaten in R. \nDaher ist die Twitter-ID als Integer *nicht* zuverl√§ssig;\nals Text hingegen schon.\n:::\n\nUnd schlie√ülich k√∂nnen wir die Excel-Datei importieren.\n\n\n::: {.cell hash='twittermining_cache/html/unnamed-chunk-51_09615a7753db77e11a1c39a760488775'}\n\n```{.r .cell-code}\nexport(tweets_to_kl2, file = \"~/datasets/Twitter/tweets_to_kl.xlsx\")\n```\n:::\n\n\nDie Excel-Tabelle k√∂nnen wir dann bequem hernehmen,\num Tweets manuell zu klassifizieren.\n\n\n\n\n## Aufgaben\n\n1. √úberlegen Sie, wie Sie das Ausma√ü an Hate Speech, dem deutsche Politikerinnen und Politiker konfrontiert sind, messen k√∂nnen.\n2. Argumentieren Sie die Vorteile und Nachteile Ihres Ansatzes. Au√üerdem, auf welches Ergebnis dieser Analyse sie gespannt sind bzw. w√§ren.\n3. √úberlegen Sie Korrelate, oder besser noch: (m√∂gliche) Ursachen, des Hasses in den Tweets, gerichtet auf Polikter:innen. Sie k√∂nnen auch Gruppen von Ursachen bilden, etwas personengebundene Variablen der Politiker:innen (z.B. Alter? Geschlecht? Migrationshintergrund?).\n1. Erstellen Sie sich eine Liste an Personen, deren Tweets sich lohnen (k√∂nnten), auf Hate Speech hin analysiert zu werden. Laden Sie deren Tweets (ggf. in Ausz√ºgen) herunter.\n6. Das Skript zu `scrape_tweets.R` k√∂nnte man noch verbessern, in dem man jeden Tag nur die neuesten Tweets herunterl√§dt. Dazu kann man bei [get_timeline()](https://docs.ropensci.org/rtweet/reference/get_timeline.html) mit dem Argument `since_id` eine Untergrenze der ID festlegen, so dass nur neuere Tweets (d.h. mit gr√∂√üerem Wert bei ID) ausgelesen werden. √Ñndern Sie das Skript entsprechend, so dass nur neuerer Tweets gelesen werden.\n7. Erarbeiten Sie die Folien zu diesem [rtweet-Workshop](https://rtweet-workshop.mikewk.com/#1). Eine Menge guter Tipps!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}