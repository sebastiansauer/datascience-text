---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Transformer



## Vorab


### Lernziele



- Sie k√∂nnen die grundlegende Architektur eines Transformer-Modells beschreiben.
- Sie k√∂nnen Transformer-Modelle mit der API von Hugging-Face berechnen.



### Begleitliteratur 


Der [Blogpost von Jay Alammar](https://jalammar.github.io/illustrated-transformer/) gibt einen illustrierten √úberblick √ºber Transformer.






### Ben√∂tigte Software (Python+R)

Wir ben√∂tigen Python, R sowie einige im Folgenden aufgef√ºhrte Python-Module.

```{python}
import pandas as pd  # Data Frames
import os  # zB f√ºr Environment-Variablen
from transformers import pipeline  # Hugging Face Modelle
import time  # Rechenzeit
from sklearn.metrics import accuracy_score  # Modellg√ºte
```


F√ºr den Sch√ºleraustausch von R nach Python (und retour) nutzen wir das R-Paket `reticulate`:

```{r}
library(reticulate)
```


Au√üerdem starte ich die "richtige" Python-Version, wo die ben√∂tigten Pakete (in der richtigen Version) installiert sind:

```{r use-virtualenv}
#use_virtualenv("r-tensorflow")
```

Check:

```{r}
py_available()
```

Welche Python-Version nutzt `reticulate` gerade?

```{r}
py_config()
```



## √úberblick

Transformer sind eine Architekturvariante neuronaler Netze. Sie stellen die Grundlage vieler aktueller gro√üer Sprachmodelle^[Man spricht von *Large Language Models*, LLM]; da sie einige Vorz√ºge gegen√ºber Vorg√§ngermodellen aufweisen, haben sie einen zentralen Platz f√ºr verschiedenen Aufgaben des NLP eingenommen.

Im Jahr 2017 erschien ein Paper auf Arxive mit dem Titel "Attention is all you need", @vaswani_attention_2023^[Da die Autoren immer wieder Updates bei Arxive eingestellt haben, ist hier die aktuellste Version, V7 aus 2023, zitiert.].
Transformer basieren auf einer bestimmten Art von "Aufmerksamkeit", genannt Selbst-Aufmerksamkeit (self-attention).
Nat√ºrlich ist damit eine bestimmte Architektur im neuronalen Netzwerk gemeint, kein kognitivpsychologiches Konstruktr; allerdings lehnt sich die Methode an Konzepte der Kognitionspsychologie vage an.

Self-Attention weist zwei gro√üe Verteile auf: Erstens erlaubt es parallele Verarbeitung, was viele Vorg√§ngermodelle nicht erlaubten. Zweitens kann es den Kontext eines Tokens, also den Text um ein bestimmtes Wort herum, deutlich besser "im Blick" (oder in der Aufmerksamkeit) behalten als viele Vorg√§ngermodelle.

Gerade f√ºr Daten mit sequenziellem Charakter, wie Text oder Sprache, sind Transformer-Modelle gut geeignet^[relativ zu anderen, bisherigen Modellen].


## Grundkonzepte

{{< video https://youtu.be/4Bdc55j80l8?si=t3ku0MxhWDD7z2TG >}}



## Einf√ºhrung in Hugging Face ü§ó

Dieser Abschnitt orientiert sich an @tunstall_natural_2022.
Die Syntax zu allen Kapiteln des [Buchs](https://transformersbook.com/) findet sich praktischerweise [in diesem Github-Repo](https://github.com/nlp-with-transformers/notebooks).



Bei ü§ó liegt der Schwerpunkt klar bei Python, nicht bei R.
Allerdings erlaubt RStudio ein einfaches Wechseln zwischen R und Python:
Funktionen und Daten aus Python k√∂nnen einfach mit dem `$`-Operator angesprochen werden.
[In diesem Post](https://rpubs.com/eR_ic/transfoRmers) wirds das demonstriert.


Schauen wir uns das einf√ºhrende Beispiel aus @tunstall_natural_2022. an.


### Hugging Face mit R

Hier ein ein Text-Schnipsel,
dessen Sentiment wir detektieren wollen:

```{r}
#| fenced: true
text <- ("Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee.")
```

Und hier in der Python-Version:

```{python}
#| fenced: true
text_py = r.text
```


Dann importieren wir die n√∂tigen Module:


:::{.panel-tabset}

### Python

```{python}
#import tensorflow
from transformers import pipeline
```


Nat√ºrlich m√ºssen Python-Module installiert sein, bevor man sie nutzen kann, genau so wie R-Pakete.


### R

Man kann die die Python-Module auch √ºber R starten:



```{r}
transformers <- reticulate::import("transformers")
```

:::



### Einfache Pipeline




### Python

Wir bereiten das Modell vor; im Default wird 
`distilbert-base-uncased-finetuned-sst-2-english` verwendet.

```{python}
classifier = pipeline("text-classification")
```




## Germeval Out-of-the-Box mit Hugging Face

Zuerst importieren wir die Daten.

:::{.panel-tabset}


### Ein bisschen R

```{r}
data(germeval_train, package = "pradadata")
text <- germeval_train$text[1:2]
text[1:2]
```


### Ein bisschen Python Python

```{python}
germeval_train_py = r.text
```

:::


### Standard-Pipeline


Hugging Face bietet eine sehr einfache Oberfl√§che: Im einfachsten Fall kann man mit `pipeline()` das Ziel der Analyse (wie Textklassifikation) oder das zu verwendende Modell angegeben. Dann wird das entsprechende Modell heruntergeladen und vorbereitet.
Mit `classifier()` wird ein Datensatz dann entsprechend klassifiziert.

```{python}
classifier = pipeline("text-classification")  
outputs2 = classifier(germeval_train_py)
outputs2
```


Tja, vielleicht sollten wir ein Modell verwenden, das die deutsche Sprache versteht?


### Man spricht Deutsh

Auf Hugging Face gibt es eine Menge von Modellen. Welches nehm ich nur? [DISTILBERT](https://huggingface.co/distilbert-base-german-cased) oder [BERT](https://huggingface.co/bert-base-uncased)-Varianten d√ºrfte kein schlechter Start sein.



```{python}
#| eval: false
#classifier = pipeline("text-classification", model="distilbert-base-german-cased")
```


```{python german-sentiment-bert}
classifier = pipeline(
  "text-classification", model="oliverguhr/german-sentiment-bert")
```



```{python class-preds}
outputs3 = classifier(germeval_train_py)
df = pd.DataFrame(outputs3)    
df.head()
```

```{r head-df}
#| eval: false
df_r <- py$pd
head(df_r)
```


## Fine-Tuning

### Grundlagen

:::{#def-finetuning}
### Fine-Tuning
Unter Fine-Tuning^["Weitertraininieren" ist ein Versuch, den Term "Fine-Tuning" auf Deutsch zu √ºbersetzen.] versteht man das Anpassen der Gewichte eines gro√üen (neuronalen) Sprachmodells (Large Language Models (LLM); Foundational Model) an einen spezifischen Datensatz. $\square$
:::


Fine-Tuning ist eine Art "Trick", wie man die Power eines gro√üen Sprachmodells an die Spezifika eines bestimmten (Ihres!) Datensatzes anzupassen. Insofern k√∂nnte man sagen, dass man mit Fine-Tuning die Vorteile eines LLM nutzen kann, auch wenn man einen kleinen Datensatz hat.


:::callout-tip
Nutzen Sie Fine-Tuning, wo immer m√∂glich. Sie sparen nicht nur Energie und Rechenzeit und verbessern damit Ihren √∂kologischen Fu√üabdruck (als Nutzer von LLM haben Sie (wir!) ganz sch√∂n viel Energie verbraucht). Sie verbessern mit etwas Gl√ºck auch die pr√§diktive Leistung Ihres Modells. 
:::


:::{#def-zeroshotlearning}
### Zero-Shot-Learning
Nutzt man ein LLM ohne Fine-Tuning, etwa indem man das Modell mittels eines Prompts zu einer Sentiment-Klassifikation auffordert, so spricht man von Zero-Shot-Learning. In diesem Fall lernt das Modell ohne (spezifisches) Train-Sample. $\square$
:::


### Fine-Tuning vorgekocht

Nat√ºrlich kann man ein Modell selber an einen spezifischen Datensatz fitten. In dem Fall werden anstelle von Zufallsgewichten im neuronalen Netz die Gewichte des Modells als Ausgangspunkt genommen.
Allerdings kann es auch sein, dass es auf einem Hub wie Hugging Face schon vortrainierte ("gefinetunte"?) Modelle gibt, so dass man sich die Arbeit des selber Fine-Tunings sparen kann.


:::{#exm-hatespeech}
In [dieser Sammlung](https://huggingface.co/collections/sebastiansauer/hate-speech-detection-655e66e27b44c113b821423d) finden sich LLMs, die an deutschen Hate-Speech-Datasets weitertrainiert wurden. $\square$
:::



Wir holen uns ein an deutschem Hate-Speech-Daten vortrainiertes Modell von Hugging Face:


```{python pipeline-deepset-bert-german}
pipe_bert_germeval = pipeline("text-classification", model="deepset/bert-base-german-cased-hatespeech-GermEval18Coarse")
```

:::callout-note
Wenn man ein Modell zum ersten Mal anfragt, wird das Modell heruntergeladen; das kann ggf. etwas dauern (und braucht ewtas Speicherplatz). $\square$
:::


Hier ist ein Beispiel-Satz:

```{python text-sample1}
text = '@Martin28a Sie haben ja auch Recht. Unser Tweet war etwas missverst√§ndlich. Dass das BVerfG Sachleistungen nicht ausschlie√üt, kritisieren wir.'
```


Und dann lassen wir uns die Vorhersage des Modells ausgeben:

```{python output-test}
outputs = pipe_bert_germeval(text)
pd.DataFrame(outputs)
```


:::{.callout-attention}
Dieses Modell wurde explizit am Datensatz [germeval2018](https://github.com/uds-lsv/GermEval-2018-Data) (Coarse Classification) trainiert. Eine hohe Klassifikationsg√ºte ist daher vorprogrammiert.
Bliebe noch zu pr√ºfen, ob auch das Test-Sample zum Training verwendet wurde. $\square$
:::


### Fallbeispiel 



Hier ist unser Germeval-Datensatz:

```{python data-test}
csv_file_path_test = 'https://github.com/sebastiansauer/pradadata/raw/master/data-raw/germeval_test.csv'

germeval_test = pd.read_csv(csv_file_path_test)
```


Nachdem der Datensatz als DataFrame vorliegt, konvertieren wir ihn noch zu einer Liste:

```{python}
tweets = germeval_test["text"].tolist()
```

Zu Testzwecken ist es oft sinnvoll, sich einen "Toy-Datensatz" zu erstellen:

```{python}
tweets_head = germeval_test["text"].head(2).tolist()
```



Und dann kommt das Vorhersagen.
Zuerst, zum Testen, mit dem kleinen Spielzeug-Datensatz:

```{python output-tweets-head}
outputs = pipe_bert_germeval(tweets_head)
pd.DataFrame(outputs)
```


Schein zu klappen. 
Dann wagen wir uns also an den ganzen GermEval-(Test-)Datensatz:

```{python bert-preds}
#| eval: false
start_time = time.time()

outputs = pipe_bert_germeval(tweets)
preds = pd.DataFrame(outputs)

end_time = time.time()
end_time - start_time
```

`1250.577404975891`

Da es einige Zeit gedauert hat, speichern wir uns die Predictions als CSV-Datei:

```{python}
#| eval: false
preds.to_csv("data/pipe_bert_germeval_preds.csv")
```

Und wenn man sie gespeichert hat, kann man sie wieder importieren:

```{python}
preds = pd.read_csv("data/pipe_bert_germeval_preds.csv")
```


Z√§hlen wir, wie oft jede Klasse vorhergesagt wurde:

```{python}
preds["label"].value_counts()
```


Wir konvertieren die Label-Spalte der Vorhersagen in eine Python-Liste, da die Accuracy-Funktion dies verlangt:


```{python}
preds_list = preds["label"].to_list()
```



Als N√§chstes bewerten wir die Modellg√ºte^[Performance, Scoring] im Test-Set.




Hier ist die *Liste* der wahren Werte (die sich in der Spalte `c1` finden lassen):

```{python def-y-list}
y = germeval_test["c1"].values.tolist()
```




```{python py-score}
accuracy = accuracy_score(y, preds_list)
print("Accuracy:", accuracy)
```



:::{:::callout-attention}
### Overfitting?
Es ist nicht klar, ob unser Modell den GermEval-Test-Datensatz als Trainingsinput gesehen hat. 
In dem Fall w√§re nat√ºrlich die Modellg√ºte von massivem Overfitting betroffen, also "too good to be true";
k√ºnftige Vorhersagen m√ºssten mal also mit deutlich geringer G√ºte erwarten. $\square$
:::


## Vertiefung

Der Originalartikel von @vaswani_attention_2023 gibt einen guten Einblick in die Konzepte; der Anspruch ist auf mittlerem Niveau.
Von den Hugging-Face-Machern gibt es ein Buch, das - ebenfalls auf Einstiegs- bis mittlerem Niveau - einen Einblick in Transformer-Modelle im Hugging-Face-√ñkosystem gew√§hrt [@tunstall_natural_2022].
@rothman_transformers_2022 scheint gute Freunde bei Google zu haben, wenn man sein Buch √ºber Transformer liest, jedenfalls sind die Modelle jener Firma in dem Buch gut gefeatured. 
@geron_hands-machine_2023 Standardwerk zu Scikit-Learn bietet auch einen Einblick in Attention-Konzepte (Kap. 16).
√úbrigens ist das Buch (3. Auflage) jetzt auch in deutscher Sprache erh√§ltlich [@geron_praxiseinstieg_2023-1].

