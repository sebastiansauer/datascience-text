# Fallstudie Hatespeech




Wir sagen vorher, welche Tweets an führende deutsche Politikis Hassrede bzw. hasserfüllte Rede enthalten.


## Vorab




### Lernziele


- Sie können grundlegende Verfahren zur Klassifikation von Hatespeech einsetzen und erklären








### Benötigte R-Pakete

```{r}
#| message: false
library(tidyverse)
library(tidymodels)
library(easystats)
library(tidytext)
library(textrecipes)
library(tictoc)  # Zeitmessung
library(beepr)  # piebt, wenn fertig
library(remoji)  # Emojis
library(feather)  # Daten speichern
library(pradadata)  # Hilfsdaten wie Schimpfwoerter
library(lubridate)  # Datum und Zeit
library(tokenizers)
library(feather)  # feather data
library(pradadata)  # helper data
library(remoji)  # processing emojis
```


```{r}
#| echo: false
theme_set(theme_minimal())
```


## Daten


### Traindaten


```{r}
d1 <- read_rds("objects/d1.rds")  # Traindaten einlesen
```


In Train- und Test-Datensatz aufsplitten:

```{r d-split-d1}
d_split <- initial_split(d1, strata = c1)

d_train <- training(d_split)
d_test <- testing(d_split)
```






### Testdaten

Wir importieren die Tweets führender deutscher Politikis.



Pfad zu den Daten:

```{r}
tweet_data_path <- "/Users/sebastiansaueruser/github-repos/hate-speech/data/"
```






So lesen wir alle ein:


Zunächst erstellen wir uns eine Helper-Funktion:


```{r source-fun-read-and-select}
source("funs/read-and-select.R")
```



Die Funktion `read_and_select`  mappen wir auf alle Datendateien:


```{r map-read-and-select}
#| eval: false
tic()
ds <-
  tweet_data_files_names %>% 
  map_dfr(read_and_select, .id = "dataset")
toc()
```


`214.531 sec elapsed`

Da wir den Elementen von `tweet_data_files_names` Namen gegeben haben, 
finden wir diese Namen praktischerweise wieder in `ds`.


```{r}
#| echo: false
#| eval: false
#write_rds(ds, file = paste0(tweet_data_path, "ds.rds"))
#write_feather(ds, path = paste0(tweet_data_path, "ds.feather"))
```



Recht groß, das Objekt `ds`, knapp ein halbes Gigabyte.


Eine Alternative besteht darin, 
den vereinigten Datensatz (Summe aller Datendateien) in einer Datei zu speichern.
Ein interessantes neues Format ist [Feather](https://github.com/wesm/feather):

>   Feather: fast, interoperable data frame storage
Feather provides binary columnar serialization for data frames. 
It is designed to make reading and writing data frames efficient, and to make sharing data across data analysis languages easy. 





```{r read-ds}
#| echo: false
#| results: hide
tic()
ds <- read_feather(path = "/Users/sebastiansaueruser/datasets/Twitter/hate-classific/ds.feather")
toc()
```

Weil der Datensatz so groß ist, nämlich `r nrow(ds)` Zeilen,
teilen wir ihn in zwei Hälften auf:




```{r}
ds_short_part1 <- 
  ds %>% 
  slice_head(n = 5e5)

ds_short_part2 <-
  ds %>% 
  slice(n = 5e5+1:n())
```


Weil wir im Folgenden schwieriges Gelände betreten,
also nicht im ersten Versuch zum Ziel gelangen - vielleicht - ,
speichern wir diese Daten auf der Festplatte ab:

```{r}
#| eval: false
#write_rds(ds_short_part1, file = "/Users/sebastiansaueruser/datasets/Twitter/ds_short_part1.rds")
#write_rds(ds_short_part1, file = "/Users/sebastiansaueruser/datasets/Twitter/ds_short_part2.rds")
```



Ein Blick in die Daten:

```{r}
glimpse(ds)
```


### Worteinbettungen

Wie in @sec-fasttext dargestellt, importieren wir unser FastText-Modell.

```{r}
#| eval: false
word_embedding_twitter <- read_rds(file = "/Users/sebastiansaueruser/datasets/Twitter/word_embedding_twitter.rds")
```


Wie viel Speicher benötigt das Worteinbettungsobjekt?

```{r}
format(object.size(word_embedding_twitter), units = "Mb")
```



### Hilfsdaten

```{r}
data("schimpwoerter")
data("sentiws")
data("wild_emojis")
```



## Aufbereiten der Vorhersagedaten

### Hilfsfunktionen



```{r}
source("funs/helper-funs-recipes.R")
```


## Rezept


Da wir schon ein Rezept "trainiert" haben,
können wir die Test-Daten einfach mit dem Rezept "backen".

Streng genommen müssten wir nicht mal das tun,
denn `tidymodels` würde das beim Vorhersagen für uns übernehmen.
Aber es ist nützlich, die Daten in aufbereiteter Form zu sehen,
bzw. sie direkt zugänglich zu haben.



```{r rec2}
#| eval: false
rec2 <- 
  recipe(c1 ~ ., data = select(d_train, text, c1, id)) %>% 
  update_role(id, new_role = "id") %>% 
  step_text_normalization(text) %>% 
  step_mutate(text_copy = text,
              profane_n = map_int(text_copy, count_profane, profane_list = schimpfwoerter$word),
              emo_words_n = map_int(text_copy, count_emo_words, emo_list = sentiws$word),
              emojis_n = map_int(text_copy, count_emojis, emoji_list = emoji(list_emoji(), pad = FALSE)),
              wild_emojis_n = map_int(text_copy, count_wild_emojis, wild_emoji_list = wild_emojis$emojis)
  ) %>% 
  step_textfeature(text_copy) %>% 
  step_tokenize(text, token = "tweets") %>% 
  step_stopwords(text, language = "de", stopword_source = "snowball") %>% 
  step_word_embeddings(text, embeddings = word_embedding_twitter)
 
rec2
```


### Preppen und Backen

Preppen:

```{r rec2-prepped-baked}
#| eval: false
tic()
rec2_prepped <- prep(rec2)
toc()
```


```
29.377 sec elapsed
````

Zur Sicherheit speichern wir auch dieses Objekt ab.

```{r}
#| echo: true
#| eval: false
# write_rds(rec2_prepped, "objects/rec2_prepped.rds")
rec2_prepped <- read_rds("/Users/sebastiansaueruser/github-repos/datascience-text/objects/rec2_prepped.rds")
```



Als nächstes kommt das Backen der Vorhersagedaten.
Das ist die Stelle, an der zum ersten Mal die neuen Daten (die Vorhersagedaten) ins Spiel kommen.


```{r}
#| eval: false
tic()
d_predict <-
  bake(rec2_prepped, new_data = head(ds, 1e3))
toc()
beepr:beep()
```

Puh, das Backen dauert ewig!
Daher ist das `beep`en praktisch:
Es klingelt, wenn die Berechnung fertig ist.
Außerdem kann es helfen, die Arbeit in zwei Hälften aufzuteilen:


Teil 1:

```{r}
ds_short_part1 <- ds %>% 
  slice_head(n = 5e5)

tic()
d_predict_part1 <-
  bake(rec2_prepped, new_data = ds_short_part1)
toc()
beep()
```




Teil 2:

```{r}
ds_short_part2 <- ds %>% 
  slice(n = 5e5+1:n())


tic()
d_predict_part2 <-
  bake(rec2_prepped, new_data = ds_short_part2)
toc()
beep()
```



```{r}
#| echo: false
d_predict_part1 <- read_feather("/Users/sebastiansaueruser/datasets/Twitter/hate-classific/ds_predict_part1.feather")

d_predict_part2 <- read_feather("/Users/sebastiansaueruser/datasets/Twitter/hate-classific/ds_predict_part2.feather")
```


Auch solche Objekte sind groß:

```{r}
format(object.size(d_predict_part1), unit = "Mb")
```




### Vereinigen

Wir vereinigen die beiden Teile von `d_predict` wieder:

```{r}
d_predict <-
  d_predict_part1 %>% 
  bind_rows(d_predict_part2)
```



```{r}
#| echo :false
#| eval: false
#write_rds(d_predict, "/Users/sebastiansaueruser/datasets/Twitter/hate-classific/d_predict.feather")
d_predict_baken <- read_rds("/Users/sebastiansaueruser/datasets/Twitter/hate-classific/d_predict.feather")
```


Zur Erinnerung: `d_predict` ist der "gebackene" Testdatensatz.
Der Testdatensatz also,
auf dem die ganzen Operationen der Vorverarbeitung angewandt wurden.




### Git Large File System


Wenn Sie Ihre Arbeit mit einem Versionierungssystem schützen - und Sie sollten es tun - 
dann verwenden Sie vermutlich Git.
Git ist für Textdateien ausgelegt - was bei Quellcode ja auch Sinn macht,
und für Quellcode ist Git gemacht.
Allerdings will man manchmal auch binäre Dateien sichern,
etwa Daten im RDS-Format.
Solche binären Formante funktionieren nicht wirklich aus der Sicht von Git,
sie lassen sich nicht zeilenweise nachverfolgen.
Kurz gesagt sollte man sie aus diesem Grund nicht in Git nachverfolgen.
Eine bequeme Lösung ist das *Large File System* von Github (git lfs),
das diese großen Dateien außerhalb des Git-Index verwaltet.
Trotzdem sieht es für Nutzis aus wie immer,
ist also sehr komfortabel.
Dazu ist es nötig, [git lfs](https://www.veit-schiele.de/dienstleistungen/technische-dokumentation/git/git-lfs) zu installieren.




### Metadaten


Metadaten wieder hinzufügen:


```{r}
d_predict2 <-
  d_predict_baken %>% 
  left_join(ds, by = "id") %>% 
  relocate(dataset, id, author_id, created_at, text, retweet_count, reply_count, quote_count, .after = id)
```

<!-- Wir müssen die `id` als Integer setzen, -->
<!-- das dies im Rezept bzw. im Train-Datensatz so definiert ist. -->
<!-- Was im Nachhinein betrachtet nicht optimal ist, -->
<!-- da wir hier gerade die Information zerstört haben. -->
<!-- Aber kein Drama, -->
<!-- wir fügen die ID-Spalte nachher einfach wieder dazu. -->


Spaltennamen mal anschauen:

```{r}
names(d_predict2)[1:33]
```



## Vorhersagen

Wir beziehen uns auf das Modell von @sec-klassifik-fit3.

```{r}
fit3 <- read_rds("/Users/sebastiansaueruser/github-repos/datascience-text/objects/chap_classific_fit3.rds")
```


```{r}
fit3_final_train <- read_rds("/Users/sebastiansaueruser/datasets/Twitter/hate-classific/fit3_final_train.rds")
```




Und nutzen dann die [predict](https://parsnip.tidymodels.org/reference/predict.model_fit.html)-Methode von `{tidymodels}`:

```{r}
#| eval: false
tic()
#d_predicted_values <- predict(fit3_final_train, d_predict2)
toc()
beep()
```



Puh, hier ist mein Rechner abgestürzt!

Besser, wir probieren erstmal mit einem winzigen Teil der Daten,
ob unsere Funktion "im Prinzip" oder "grundsätzlich" funktioniert:


```{r error = TRUE}
d_predicted_values_tiny <- predict(fit3_final_train, head(d_predict2))
```


Moment! Wir hatten im letzten Kapitel `id` als Integer definiert...
Das müssen wir hier noch nachholen:

```{r}
d_predict2 <-
  d_predict2 %>% 
  mutate(id = as.integer(id))
```


Dabei haben wir uns die Information zerstört,
da die Zahl (als String) zu groß war,
um als Integer konvertiert zu werden.
Nicht so schlimm, das machen wir später wieder gut.


```{r}
d_predicted_values_tiny <- predict(fit3_final_train, head(d_predict2))

d_predicted_values_tiny
```


Funktioniert! Gut! Also weiter.


:::callout-note
Mit dieser Datenstruktur ist es möglich,
die Daten in mehreren "Happen" zu analysieren.
Das macht die Analyse dann zugänglicher und robuster.
:::


Wir teilen den Datensatz in, sagen wir, $k=10$ Teile.
Dazu können wir folgende Überlegung nutzen:

Teilt man die Folge der natürlichen Zahlen durch $k$ und betrachtet nur den Rest,
so kann man so bequem in $k$ Gruppen aufteilen.

Kurz gesagt:

```{r}
1:20 %>% 
  `%%`(3)
```


Oder im Beispiel von `mtcars`, mit $k=3$ Gruppen:

```{r}
mtcars <-
  mtcars %>% 
  mutate(group_id = row_number() %% 3)


mtcars %>% 
  group_by(group_id) %>% 
  summarise(n())
```



Das wenden wir an unseren großen Datensatz an:

```{r}
d_predict2 <-
  d_predict2 %>% 
  mutate(group_id = row_number() %% 10)
```



Check:

```{r}
d_predict2 %>% 
  head(30) %>% 
  group_by(group_id) %>% 
  summarise(n())
```

Noch ein Check:

```{r}
d_predict2 %>% 
  head(30) %>% 
  filter(group_id == 1) %>% 
  select(id, dataset, group_id)
```




Passt!


```{r}
#| echo :false
#| eval: false
#write_rds(d_predict2, "/Users/sebastiansaueruser/datasets/Twitter/hate-classific/d_predict.feather")
#d_predict2 <- read_rds("/Users/sebastiansaueruser/datasets/Twitter/hate-classific/d_predict2.feather")
```





So, das probieren wir jetzt für den ersten Teil aus


```{r}
#| eval: false
tic()
d_predicted_values_part1 <- predict(fit3_final_train, d_predict2 %>% filter(group_id == 1))
toc()
beep()
```






IN ENTWICKLUNG


Vorhersagen mit den übrigen Spalten der Vorhersage-Daten vereinigen:

```{r}
#| eval: false
# d_predict4 <-
#   d_predict %>% 
#   select(id) %>% 
#   bind_cols(d_predict3) %>% 
#   bind_cols(d_predict2 %>% select(-id, -text))
```

```{r}
#| eval: false
#feather::write_feather(d_predict4, "objects/d_predict4.feather")
```


```{r}
#| echo: false
d_predict4 <- feather::read_feather("objects/d_predict4.feather")
```


Schauen wir uns zur Sicherheit ein paar Namen des Datensatzes an:

```{r}
names(d_predict4)[1:37]
```


Sieht soweit gut aus: ID-Spalten (`id`, `dataset`), Vorhersage-Saplte (`.pred_class`),
Zeitinformation (`created_at)`, Hassrede-Proxies (`profane_n` bis `wid_emojis_n`),
Text-Features und dann die Wordeinbettungen.


## Ergebnisse

### Hass-Proxis pro Politiki insgesamt


```{r}
res_summary1 <- 
d_predict4 %>% 
  group_by(dataset) %>% 
  summarise(emo_words_n_mean = mean(emo_words_n),
            profane_words_count_mean = mean(profane_n),
            wild_emojis_n_mean = mean(wild_emojis_n),
            exclaims_n_mean = mean(textfeature_text_copy_n_exclaims))


res_summary1_long <-
  res_summary1 %>% 
    pivot_longer(-dataset, names_to = "hate_proxy", values_to = "prop")
```


```{r}
res_summary1_long %>% 
  ggplot(aes(x = prop, y = hate_proxy)) +
  geom_col() +
  facet_wrap(~ dataset)
```




### Hass-Proxis pro Politiki im Zeitverlauf


```{r}
res_summary2 <- 
d_predict4 %>%
  select(created_at, profane_n, dataset, emo_words_n, wild_emojis_n, textfeature_text_copy_n_exclaims) %>% 
  mutate(month = ymd_hms(created_at) %>% round_date(unit = "month")) %>% 
  group_by(month, dataset) %>% 
  summarise(emo_words_n_mean = mean(emo_words_n),
            profane_words_count_mean = mean(profane_n),
            wild_emojis_n_mean = mean(wild_emojis_n),
            exclaims_n_mean = mean(textfeature_text_copy_n_exclaims)) %>% 
  rowwise() %>% 
  mutate(hate_proxy = mean(c_across(emo_words_n_mean:exclaims_n_mean))) %>% 
  ungroup()
  
res_summary2 %>% 
  head()
```

Langifizieren fürs Plotten:

```{r}
res_summary2_long <- 
  res_summary2 %>% 
  pivot_longer(emo_words_n_mean:hate_proxy)

res_summary2_long %>% 
  head()
```

```{r}
res_summary2_long %>% 
  count(month)
```


```{r}
res_summary2_long %>% 
  ggplot() +
  aes(x = month, y = value) +
  facet_grid(dataset  ~ name) +
  geom_point() +
  geom_line(group=1, alpha = .7)
```



