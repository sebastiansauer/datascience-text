{
  "hash": "882e1e7e7cdf5b1c6bf3529b632f68aa",
  "result": {
    "markdown": "# Word Embedding\n\n\n![Text als Datenbasis pr√§diktiver Modelle](img/text-mining-1476780_640.png){width=10%}\nBild von <a href=\"https://pixabay.com/de/users/mcmurryjulie-2375405/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1476780\">mcmurryjulie</a> auf <a href=\"https://pixabay.com/de//?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1476780\">Pixabay</a>\n\n\n\n## Vorab\n\n\n### Lernziele\n\n\n\n- Die grundlegenden Konzepte der Informationstheorie erkl√§ren k√∂nnen\n- Die vorgestellten Techniken des Textminings mit R anwenden k√∂nnen\n\n\n\n### Vorbereitung\n\n- Lesen Sie [diesen Text](https://www.science4all.org/article/shannons-information-theory/) als Vorbereitung\n- Arbeiten Sie @smltar, [Kap. 5](https://smltar.com/embeddings.html) durch.\n\n\n\n\n\n\n### Ben√∂tigte R-Pakete\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-1_157e3b30c7f523a0bd620dd4ac0c1285'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(easystats)  # Komfort f√ºr deskriptive Statistiken, wie `describe_distribution`\nlibrary(tidytext)\nlibrary(hcandersenr)   # Textdaten\nlibrary(slider)  # slide\nlibrary(widyr)  # pairwise_pmi\nlibrary(furrr)  # mehrere Kerne gleichzeitig nutzen\nlibrary(textdata)  # Worteinbettungen, vorgekocht\nlibrary(entropy)  # Entropie berechnen\nlibrary(widyr)  # √Ñhnlichkeit berechnen mit widyr_svd\nlibrary(furrr)  # Mehrere Kerne gleichzeitig\n```\n:::\n\n\n\n\n\n\n## Informationstheorie\n\n\n\n\n\n\n### Einf√ºhrung\n\n\nDie Informationstheorie ist eine der Sternstunden der Wissenschaft. [Manche sagen](http://www.science4all.org/article/shannons-information-theory/) dass Claude Shannon, der Autor der Theorie, auf einer Stufe mit Darwin und Einstein stehen sollte:\n\n>    In this single paper, Shannon introduced this new fundamental theory. He raised the right questions, which no one else even thought of asking. This would have been enough to make this contribution earthshaking. But amazingly enough, Shannon also provided most of the right answers with class and elegance. In comparison, it took decades for a dozen of top physicists to define the basics of quantum theory. Meanwhile, Shannon constructed something equivalent, all by himself, in a single paper.\nShannon‚Äôs theory has since transformed the world like no other ever had, from information technologies to telecommunications, from theoretical physics to economical globalization, from everyday life to philosophy. (...)\nI don‚Äôt think Shannon has had the credits he deserves. He should be right up there, near Darwin and Einstein, among the few greatest scientists mankind has ever had the chance to have. \n\n\nF√ºr die Statistik ist die Informationstheorie von hoher Bedeutung.\nIm Folgenden schauen wir uns einige Grundlagen an.\n\n\n### Wozu ist das gut?\n\n\nBevor man sich mit einem Thema wie der Informationtheorie (Informationsentropie mit verwandten Konstrukten oder kurz Entropie) besch√§ftigt, sollte die Frage\ngekl√§rt sein, wozu das Thema gut ist.\nHier sind drei Antworten dazu:\n\n1. Im Maschinenlernen wird die Informationtheorie verwendet, um die G√ºte von Klassifikationsmodellen zu berechnen.\n2. Speziell im Textmining wird die Entropie verwendet, um den Zusammenhang von W√∂rtern zu quantifizieren.\n3. Wenige Theorien haben so viel neue Forschung initiert, wie  @shannon_1948 ber√ºhmtes Paper. Es ist also auf jeden Fall eine Perle der Geistesgeschichte.\n\n\n\n### Shannon-Information\n\nMit der *Shannon-Information* (Information, Selbstinformation) quantifizieren wir, wie viel \"√úberraschung\" sich in einem Ereignis verbirgt [@shannon_1948].\n\nEin Ereignis mit ...\n\n- *geringer* Wahrscheinlichkeit: *Viel* √úberraschung (Information)\n- *hoher* Wahrscheinlichkeit: *Wenig* √úberraschung (Information)\n\n\n\nWenn wir also erfahren, dass ein unwahrscheinliches Ereignis eingetreten ist (Schnee im Sommer), sind wir √ºberraschter als wenn wir h√∂hen, dass ein wahrscheinliches Ereignis eingetreten ist (Schnee im Winter).\n\nDie Information eines Ereignis ist also eng verbunden mit seiner Wahrscheinlichkeit oder auch mit den Odds eines Ereignisses.\n\nDie Shannon-Information ist die einzige Gr√∂√üe, die einige w√ºnschenswerte Anforderungen^[Desiderata, sagt man] erf√ºllt:\n\n1. Stetig\n2. Je mehr Ereignisse in einem Zufallsexperiment m√∂glich sind, desto h√∂her die Information, wenn ein bestimmtes Ereignis eintritt\n3. Additiv: Die Summe der Information zweier Teilereignisse ist gleich der Information des Gesamtereignis\n\n\n\n:::{#def-info}\n\n## Shannon-Information\n\n\nDie Information ist so definiert:\n\n\n$$I(x) = - \\log_2 \\left( Pr(x) \\right)$$\n\n:::\n\nAndere Logaritmusbasen sind m√∂glich. Bei einem bin√§ren Logarithmus nennt man die Einheit *Bit*^[oder *shannon*].\n\n\nEin M√ºnwzurf^[wie immer, als fair angenommen, wenn sonst nichts anderes angegeben ist] hat *1 Bit* Information:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-2_fa4b77689da6dbda83ff67ed105b5871'}\n\n```{.r .cell-code}\n-log(1/2, base = 2)\n## [1] 1\n```\n:::\n\n\n\n\n\nDamit gilt: $I = \\frac{1}{Pr(x)}$\n\nDie Information ist eng verwandt mit den Odds bzw. Log-Odds (Logits):\n\n\n$\\text{log-odds}(x)=\\log \\left({\\frac {p(x)}{p(\\lnot x)}}\\right)$\n\nLogits k√∂nnen als Differenz zweier Shannon-Infos ausgedr√ºckt werden:\n\n\n$\\text{log-odds}(x)=I(\\lnot x)-I(x)$\n\n\n\nDie Information zweier unabh√§ngiger Ereignisse ist additiv.\n\nDie gemeinsame Wahrscheinlichkeit zweier unabh√§ngiger Ereignisse ist das Produkt der einzelnen Wahrscheinlichkeiten:\n\n$Pr(x,y) = Pr(x) \\cdot Pr(y)$\n\nDie *gemeinsame Information* ist dann\n\n$$\n{\\displaystyle {\\begin{aligned}\\operatorname {I}(x,y)&=-\\log _{2}\\left[p(x,y)\\right]=-\\log _{2}\\left[p(x)p(y)\\right]\\\\[5pt]&=-\\log _{2}\\left[p{(x)}\\right]-\\log _{2}\\left[p{(y)}\\right]\\\\[5pt]&=\\operatorname {I} (x)+\\operatorname {I} (y)\\end{aligned}}}\n$$\n\n\n\n\n\n:::{#exm-info1}\n\n\n## Information eines wahrscheinlichen Ereignisses\n\nDie Information eines fast sicheren Ereignisses ist gering.\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-3_0bb109c55a876ac7930ed3f5770abb67'}\n\n```{.r .cell-code}\n-log(99/100, base = 2)\n## [1] 0.01449957\n```\n:::\n\n\n\n:::\n\n\n\n\n\n:::{#exm-info2}\n\n\n## Information eines unwahrscheinlichen Ereignisses\n\nDie Information eines unwahrscheinlichen Ereignisses ist hoch.\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-4_e064ea9b1d1c319d1754a23e3c3ba34d'}\n\n```{.r .cell-code}\n-log(01/100, base = 2)\n## [1] 6.643856\n```\n:::\n\n\n\n:::\n\n\n\n\n\n\n\n:::{#exm-info3}\n\n\n## Information eines W√ºrfelwurfs\n\n\nDie Wahrscheinlichkeitsfunktion eines W√ºrfel ist\n\n${\\displaystyle Pr(k)={\\begin{cases}{\\frac {1}{6}},&k\\in \\{1,2,3,4,5,6\\}\\\\0,&{\\text{ansonsten}}\\end{cases}}}$\n\n\n\nDie Wahrscheinlichkeit, eine 6 zu w√ºrfeln, ist $Pr(X=6) = \\frac{1}{6}$.\n\n\n\nDie Information von $X=6$ betr√§gt also\n\n\n$I(X=6) = -\\log_2 \\left( Pr(X=6) \\right) = -\\log_2(1/6) \\approx 2.585 \\, \\text{bits}$.\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-5_5c073ff2e38742ba08478feced3c762e'}\n\n```{.r .cell-code}\n-log(1/6, base = 2)\n## [1] 2.584963\n```\n:::\n\n\n\n:::\n\n\n\n\n\n\n\n:::{#exm-info3}\n\n\n## Information zweier  W√ºrfelwurfe\n\n\n\n\n\n\n\nDie Wahrscheinlichkeit, mit zwei W√ºrfeln, $X$ und $Y$, jeweils *6* zu w√ºrfeln, \nbetr√§gt $Pr(X=6, Y=6) = \\frac{1}{36}$\n\n\n\nDie Information betr√§gt also\n\n\n$I(X=6, Y=6) = -\\log_2 \\left( Pr(6,6) \\right)$\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-6_c62d91ebee0d95985974eb2ad26971b4'}\n\n```{.r .cell-code}\n-log(1/36, base = 2)\n## [1] 5.169925\n```\n:::\n\n\n\nAufgrund der Additivit√§t der Information gilt\n\n\n$I(6,6) = I(6) + I(6)$\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-7_1beba0cdc805413a7ea8d31f55f309b8'}\n\n```{.r .cell-code}\n-log(1/6, base = 2) + -log(1/6, base = 2)\n## [1] 5.169925\n```\n:::\n\n\n\n\n:::\n\n\n\n\n\n\n\n\n\n### Entropie\n\n\n(Informations)entropie ist die Summe der Informationen der Ereignisse einer Zufallsvariablen, $X$.\n\n\n\n:::{#def-entropie}\n\n\n## Informationsentropie\n\n\n*Informationsentropie*  ist so definiert:\n\n$$H(p) = - \\text{E log} (p_i) = - \\sum_{i = 1}^n p_i \\text{log} (p_i) = E\\left[I(X) \\right]$$\n\n:::\n\nDie Informationsentropie ist also die \"mittlere\" oder \"erwartete Information einer Zufallsvariablen.\n\n\nDie Entropie eines M√ºnzwurf-Versuchs (Bernoulli-Experiment) ist dann maximal (1 bit), wenn die Trefferwahrscheinlichkeit 50% betr√§gt: $Pr(X=x) = 1/2$, s. Abb. @fig-bernoulli-entropy.\n\n\n![Der Zusammenhang von Wahrscheinlichkeit und Entropie bei einem Bernoulli-Versuch, Quelle: Wikipedia, CC-BY-SA, Brona, Rubber Duck](img/Binary_entropy_plot.svg.png){#fig-bernoulli-entropy width=\"30%\"}\n\n\n\n### Gemeinsame Information\n\nDie *gemeinsame Information* (mutual information, MI) zweier Zufallsvariablen $X$ und $Y$, $I(X,Y)$, quantifiziert die Informationsmenge, die man √ºber $Y$ erh√§lt, wenn man $X$ beobachtet. Mit anderen Worten: Die MI ist ein Ma√ü des Zusammenhangs zweier (nominaler) Variablen. Im Gegensatz zur Korrelation ist die MI nicht auf lineare Abh√§ngigkeiten beschr√§nkt.\n\n\n\nDie MI quantifiziert den Unterschied zwischen der gemeinsamen Verteilung $Pr(X,Y)$ und dem Produkt einer einzelnen^[auch als *marginalen* Wahrscheinlichkeiten oder *Randwahrscheinlichkeiten* bezeichnet] Wahrscheinlichkeitsverteilungen, d.h. $Pr(X)$ und $Pr(Y)$.\n\nWenn die beiden Variablen (stochastisch) unabh√§ngig^[F√ºr stochastische Unabh√§ngigkeit kann das Zeichen $\\bot$ verwendet werden] sind, ist ihre gemeinsame Information Null:\n\n$I(X,Y) = 0 \\quad \\text{gdw} \\quad \\bot(X,Y)$.\n\nDann gilt n√§mlich:\n\n$\\log \\left( \\frac{Pr(X,Y)} {Pr(X) \\cdot Pr(Y)} \\right) =\\log(1) = 0$.\n\n\nDas macht intuitiv Sinn: Sind zwei Variablen unabh√§ngig, so erf√§hrt man nichts √ºber die zweite, wenn man die erste kennt. So ist Kenntnis der Sternzeichens einer Person und ihrer K√∂rpergr√∂√üe unabh√§ngig.\n\nDas Gegenteil ist auch wahr: Sind zwei Variablen voneinander komplett abh√§ngig, so wei√ü man alles √ºber die zweite, wenn man die erste kennt.\n\n\nDie gemeinsame Information kann man sich als *Summe der einzelnen gemeinsamen Informationen* von $XY$ sehen (s. @tbl-mi1):\n\n\n::: {#tbl-mi1 .cell tbl-cap='Summe der punktweisen gemeinsamen Informationen' hash='050-word-embedding_cache/html/tbl-mi1_9ca099bb0d038b9e0736d0e701bb1547'}\n\n```{.r .cell-code}\nd <- tibble::tribble(\n     ~x1,    ~x2,    ~x3,\n  \"x1y2\", \"x2y1\", \"x3y1\",\n  \"x2y1\", \"x2y2\", \"x3y2\",\n  \"x1y3\", \"x2y3\", \"x3y3\"\n  )\nd\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"x1\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"x2\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"x3\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"x1y2\",\"2\":\"x2y1\",\"3\":\"x3y1\"},{\"1\":\"x2y1\",\"2\":\"x2y2\",\"3\":\"x3y2\"},{\"1\":\"x1y3\",\"2\":\"x2y3\",\"3\":\"x3y3\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n\n$I(X,Y) = \\Sigma_Y \\Sigma_y Pr(x,y) \\underbrace{\\log \\left( \\frac{Pr(X,Y)}{Pr(X) Pr(Y)} \\right)}_\\text{punktweise MI}$ \n\nDie Summanden der gemeinsamen Information bezeichnet man auch als *punktweise gemeinsame Information* (pointwise mutual information, PMI), entsprechend, s. @eq-pmi. MI ist also der Erwartungswert der PMI.\n\n\n$${\\displaystyle \\operatorname {PMI} (x,y)\\equiv \\log_{2}{\\frac {p(x,y)}{p(x)p(y)}}=\\log _{2}{\\frac {p(x|y)}{p(x)}}=\\log _{2}{\\frac {p(y|x)}{p(y)}}}\n$${#eq-pmi}\n\n\nAndere Basen als `log2` sind gebr√§uchlich, vor allem der nat√ºrliche Logarithmus.\n\n\n\n\n::: {.remark}\n\nDie zwei rechten Umformungen in @eq-pmi basieren auf der Umformung der gemeinsamen Wahrscheinlichkeit. \n\nZur Erinnerung: $p(x,y) = p(y)p(x|y) = p(x)p(y|x)$\n:::\n\n\n\n\n::: {#exm-pmi}\n\n## Interpretation der PMI\n\nSei $p(x) = p(y) = 1/10$ und $p(x,y) = 1/10$. W√§ren $x$ und $y$ unabh√§ngig, dann w√§re $p^{\\prime}(x,y) = p(x)p(y) = 1/100$.\nDas Verh√§ltnis der Produkte der einzelnen Wahrscheinlichkeit zur gemeinsamen Wahrscheinlichkeit w√§re dann 1 und der Logarithmus von 1 ist 0. Das Verh√§ltnis von 1 entspricht also der Unabh√§ngigkeit. Ist das Verh√§ltnis z.B. 5, so zeigt das eine gewisse Abh√§ngigkeit an.\nIm obigen Beispiel gilt: $\\frac{1/20}{1/100}=5$.\n\n:::\n\n\nDie MI wird auch √ºber die sog. *Kullback-Leibler-Divergenz* definiert,\ndie die Differenz zwischen gemeinsamer Wahrscheinlichkeitsfunktion bzw. -dichte und dem Produkt der jeweiligen Randwahrscheinlichkeiten bzw. -dichte angibt.\n\n\n\n### Maximumentropie\n\n\n:::{#def-maxent}\n\n\n## Maximumentropie\n\nDie Verteilungsform, f√ºr die es die meisten M√∂glichkeiten (Pfade im Baumdiagramm) gibt,\nhat die h√∂chste Informationsentropie.\n\n:::\n\n@fig-muenz3 zeigt ein Baumdiagramm f√ºr einen 3-fachen M√ºnzwurf.\nIn den \"Bl√§ttern\" (Endknoten) sind die Ergebnisse des Experiments dargestellt\nsowie die Zufallsvariable $X$, die die Anzahl der \"Treffer\" (Kopf) fasst.\nWie man sieht, vereinen einige Werte der Zufallsvariable mehr Pfade auf sich als andere:\nDer Wert $X=1$ vereinigt 3 Pfade (von 8) auf sich; der Wert $X=3$ nur 1 Pfad.\n\n![Pfade im Baumdiagramm: 3-facher M√ºnzwurf](img/muenz3.png){#fig-muenz3}\n\n\n\n### Ilustration \n\n\nSagen wir, Sie stehen vor 5 Eimern und haben 10 Kieselsteine bei sich, die mit den Nummern 1 bis 10 beschriftet sind [@mcelreath_statistical_2020].\nWeil Sie nichts besseres zu tun haben, werfen Sie die Kiesel in die Eimer und zwar so, \ndass die Wahrscheinlichkeit f√ºr einen Kiesel in einen bestimmten Eimer zu landen f√ºr alle Eimer gleich ist.\nSie werfen also Ihre 10 Kiesel und betrachten das Ergebnis; die Kiesel sind jetzt in einem bestimmten (zuf√§lligen)\nArrangement auf die Eimer verteilt.\nJede Aufteilung (der 10 Kiesel in den 5 Eimern) ist gleich wahrscheinlich^[so √§hnlich wie mit den Lottozahlen] -- \ndie Wahrscheinlichkeit, dass alle 10 Kiesel in Eimer 1 landen ist also gleich hoch wie die Wahrscheinlichkeit,\ndass jeder Eimer einen Kiesel abkriegt.\nJetzt kommt's: Manche Arrangements k√∂nnen auf mehrere Arten erzielt werden als andere. \nSo gibt es nur *eine* Aufteilung f√ºr alle 10 Kiesel in einem Eimer (Teildiagramm a, in @fig-kiesel).\nAber es gibt 90 M√∂glichkeiten, die Kiesel so aufzuteilen, dass 2 in Eimer 2 landen, 8 in Eimer 4 und 2 in Eimer 4,\ns. Teildiagramm b in @fig-kiesel [@kurz_statistical_2021]. \nTeildiagramme c bis e zeigen, dass die Anzahl der Aufteilungen schnell astronomisch hoch wird,\nwenn sich die Kiesel \"gleichm√§√üiger\" auf die Eimer verteilen.\nDie gleichm√§√üigste Aufteilung (Diagramm e) hat die gr√∂√üte Zahl an m√∂glichen Anordnungen.\nEine Aufteilung der Kiesel auch als Pfad durch ein Baumdiagramm beschrieben werden.\n\nHier sind ein paar verschiedene Arrangements, deren Anzahl von Aufteilungen wir hier anschauen, s. @tbl-arr:\n\n\n::: {#tbl-arr .cell tbl-cap='Ein paar verschiedene Arrangements (a-e) der Kiesel in den f√ºnf Eimern' hash='050-word-embedding_cache/html/tbl-arr_ec9ae173fa1aeab0a8d8b22796e91982'}\n\n```{.r .cell-code}\nd <-\n  tibble(a = c(0, 0, 10, 0, 0),\n         b = c(0, 1, 8, 1, 0),\n         c = c(0, 2, 6, 2, 0),\n         d = c(1, 2, 4, 2, 1),\n         e = 2) \nd\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"a\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"b\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"c\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"d\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"e\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"0\",\"2\":\"0\",\"3\":\"0\",\"4\":\"1\",\"5\":\"2\"},{\"1\":\"0\",\"2\":\"1\",\"3\":\"2\",\"4\":\"2\",\"5\":\"2\"},{\"1\":\"10\",\"2\":\"8\",\"3\":\"6\",\"4\":\"4\",\"5\":\"2\"},{\"1\":\"0\",\"2\":\"1\",\"3\":\"2\",\"4\":\"2\",\"5\":\"2\"},{\"1\":\"0\",\"2\":\"0\",\"3\":\"0\",\"4\":\"1\",\"5\":\"2\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n::: {.cell hash='050-word-embedding_cache/html/fig-kiesel_c2f4e0a60f955e67ea30ac957d39d679'}\n::: {.cell-output-display}\n![Anzahl von Aufteilungen der 10 Kiesel in 5 Eimer](050-word-embedding_files/figure-html/fig-kiesel-1.png){#fig-kiesel width=672}\n:::\n:::\n\n\nHier sind die Wahrscheinlichkeitsverteilungen der 5 Arrangements^[Ist das nicht eine elegante Syntax?! Solomon Kurz, der Autor der Syntax, hielt sich nicht an die aktuelle Version von `dplyr`; `mutate_all` ist eigentlich √ºberholt zugunsten von `mutate` mit `across`, aber die Pr√§gnanz der Syntax hier ist schon beeindruckend, wie ich finde.]:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-10_b97b759d0b9a59f961998d236ce49049'}\n\n```{.r .cell-code}\nd %>% \n  mutate_all(~. / sum(.))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"a\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"b\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"c\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"d\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"e\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"0\",\"2\":\"0.0\",\"3\":\"0.0\",\"4\":\"0.1\",\"5\":\"0.2\"},{\"1\":\"0\",\"2\":\"0.1\",\"3\":\"0.2\",\"4\":\"0.2\",\"5\":\"0.2\"},{\"1\":\"1\",\"2\":\"0.8\",\"3\":\"0.6\",\"4\":\"0.4\",\"5\":\"0.2\"},{\"1\":\"0\",\"2\":\"0.1\",\"3\":\"0.2\",\"4\":\"0.2\",\"5\":\"0.2\"},{\"1\":\"0\",\"2\":\"0.0\",\"3\":\"0.0\",\"4\":\"0.1\",\"5\":\"0.2\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\nDann kann man die Wahrscheinlichkeit einfach in Entropie umrechnen^[Syntax aus @kurz_statistical_2021]:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-11_55fd65a0c455cc22af8951b1be6bc690'}\n\n```{.r .cell-code}\nd %>% \n  mutate_all(~ . / sum(.)) %>% \n  gather() %>% \n  group_by(key) %>% \n  summarise(h = -sum(ifelse(value == 0, 0, value * log(value))))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"key\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"h\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"a\",\"2\":\"0.0000000\"},{\"1\":\"b\",\"2\":\"0.6390319\"},{\"1\":\"c\",\"2\":\"0.9502705\"},{\"1\":\"d\",\"2\":\"1.4708085\"},{\"1\":\"e\",\"2\":\"1.6094379\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\nDas `ifelse` dient nur dazu, eine Wahrscheinlichkeit von 0 eine Entropie von 0 zu verpassen^[[Regel von L'Hospital](https://de.wikipedia.org/wiki/Regel_von_de_L%E2%80%99Hospital); [s. auch hier](https://www.mathebibel.de/regel-von-lhospital)], denn sonst w√ºrden wir ein Problem rennen, wenn wir $log(0)$ ausrechnen.\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-12_9550ef1335a8992e91b0bb9c447aa0a0'}\n\n```{.r .cell-code}\nlog(0)\n## [1] -Inf\n```\n:::\n\n\n\n\n## Zufallstext erkennen\n\n\n### Entropie von Zufallstext\n\n\nKann man wohl Zufallstext maschinell erkennen? Sicher gibt es viele Ans√§tze,\num das Problem anzugehen.\nLassen Sie uns einen Ansatz erforschen. \nErforschen hei√üt, *wir* erforschen f√ºr *uns*, es handelt sich um eine didaktische √úbung, \ndas Ziel ist nicht, Neuland f√ºr die Menschheit zu betreten.\n\nAber zuerst m√ºssen wir √ºberlegen, was \"Zufallstext\" bedeuten soll.\n\n\nNehmen wir uns dazu zuerst einen richtigen Text, ein M√§rchen von H.C. Andersen zum Beispiel.\nNehmen wir das Erste aus der Liste in dem Tibble `hcandersen_de`, \"das Feuerzeug\".\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-13_5ca75105311c67c699539314eaf14dd7'}\n\n```{.r .cell-code}\ndas_feuerzeug <-\n  hcandersen_de  %>% \n  filter(book == \"Das Feuerzeug\") %>% \n  unnest_tokens(input = text, output = word) %>% \n  pull(word) \n\nhead(das_feuerzeug)\n## [1] \"es\"         \"kam\"        \"ein\"        \"soldat\"     \"die\"       \n## [6] \"landstra√üe\"\n```\n:::\n\n\n\nDas M√§rchen ist 2688 W√∂rter lang.\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-14_3474d37203f3084cf56966ce2c46cd02'}\n\n```{.r .cell-code}\nwortliste <- \nhcandersen_de  %>% \n  filter(book == \"Das Feuerzeug\") %>% \n  unnest_tokens(output = word, input = text) %>% \n  pull(word) %>% \n  unique()\n\nhead(wortliste)\n## [1] \"es\"         \"kam\"        \"ein\"        \"soldat\"     \"die\"       \n## [6] \"landstra√üe\"\n```\n:::\n\n\n\nJetzt ziehen wir Stichproben (mit Zur√ºcklegen) aus dieser Liste und erstellen damit eine Art Zufallstext.\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-15_09248bfde89a13966a83ef4005b7a7e7'}\n\n```{.r .cell-code}\nzufallstext <- \n  sample(x = wortliste, size = length(das_feuerzeug)*10^2, replace = TRUE)\nhead(zufallstext)\n## [1] \"geldkiste\"      \"silber\"         \"hunderttausend\" \"beutel\"        \n## [5] \"haben\"          \"kann\"\n```\n:::\n\n\n\nZ√§hlen wir, wie h√§ufig jedes Wort vorkommt:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-16_c2e0308b86b54b46190ac9c9d00c3a41'}\n\n```{.r .cell-code}\nzufallstext_count <-\ntibble(zufallstext = zufallstext) %>% \n  count(zufallstext)\n\nhead(zufallstext_count)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"zufallstext\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"ab\",\"2\":\"367\"},{\"1\":\"abend\",\"2\":\"346\"},{\"1\":\"aber\",\"2\":\"332\"},{\"1\":\"abflog\",\"2\":\"366\"},{\"1\":\"abschlagen\",\"2\":\"358\"},{\"1\":\"acht\",\"2\":\"355\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\nDer H√§ufigkeitsvektor von `wortliste` besteht nur aus Einsen, \nso haben wir ja gerade die Wortliste definiert:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-17_915bbc04466edf31efbf987165823076'}\n\n```{.r .cell-code}\nwortliste_count <-\ntibble(wortliste = wortliste) %>% \n  count(wortliste)\n\nhead(wortliste_count)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"wortliste\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"ab\",\"2\":\"1\"},{\"1\":\"abend\",\"2\":\"1\"},{\"1\":\"aber\",\"2\":\"1\"},{\"1\":\"abflog\",\"2\":\"1\"},{\"1\":\"abschlagen\",\"2\":\"1\"},{\"1\":\"acht\",\"2\":\"1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\nDaher ist ihre Informationsentropy maximal.\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-18_2e748bd99705102a6c47c5facf6d0a20'}\n\n```{.r .cell-code}\nentropy(wortliste_count$n, unit = \"log2\")\n## [1] 9.47978\n```\n:::\n\n\n\n\nDie H√§ufigkeiten der W√∂rter in `zufallstext` hat eine hohe Entropie.\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-19_2afbfa0c40bc4b2ae9f39297280f1fd2'}\n\n```{.r .cell-code}\nentropy(zufallstext_count$n, unit = \"log2\")\n## [1] 9.477836\n```\n:::\n\n\n\nZ√§hlen wir die H√§ufigkeiten in der Geschichte \"Das Feuerzeug\".\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-20_ab10c497abbcacc0aa272345d8d547fb'}\n\n```{.r .cell-code}\ndas_feuerzeug_count <-\n  tibble(text = das_feuerzeug) %>% \n  count(text)\n\nhead(das_feuerzeug_count)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"text\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"ab\",\"2\":\"2\"},{\"1\":\"abend\",\"2\":\"3\"},{\"1\":\"aber\",\"2\":\"21\"},{\"1\":\"abflog\",\"2\":\"1\"},{\"1\":\"abschlagen\",\"2\":\"1\"},{\"1\":\"acht\",\"2\":\"1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nUnd berechnen dann die Entropie:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-21_e8f7f33c284cdb392c4559e52c14a7c7'}\n\n```{.r .cell-code}\nentropy(das_feuerzeug_count$n, unit = \"log2\")\n## [1] 8.075194\n```\n:::\n\n\n\nDer Zufallstext hat also eine h√∂here Entropie als der echte M√§rchentext.\nDer Zufallstext ist also gleichverteilter in den Worth√§ufigkeiten.\n\n\nPro Bit weniger Entropie halbiert sich die Anzahl der M√∂glichkeiten einer H√§ufigkeitsverteilung.\n\n\n\n### MI von Zufallstext\n\n\n\nLeft as an exercises for the reader^[[Vgl. hier](https://academia.stackexchange.com/questions/20084/is-using-the-phrase-is-left-as-an-exercise-for-the-reader-considered-good)] ü•≥.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Daten\n\n\n### Complaints-Datensatz\n\nDer Datensatz `complaints` stammt aus [dieser Quelle](https://www.consumerfinance.gov/data-research/consumer-complaints/).\n\nDen Datensatz `complaints` kann man [hier](https://github.com/EmilHvitfeldt/smltar/raw/master/data/complaints.csv.gz) herunterladen. Im Buch ist die Webseite nicht (direkt?) angegeben. Die Datei ist mit `gz` gepackt; `read_csv` sollte das automatisch entpacken. Achtung: Die Datei ist recht gro√ü.\n\n\n::: {.cell hash='050-word-embedding_cache/html/read-complaints-data_09bcfb92c97b03243f50b57cfc394d0b'}\n\n```{.r .cell-code}\nd_path <- \"https://github.com/EmilHvitfeldt/smltar/raw/master/data/complaints.csv.gz\"\ncomplaints <- read_csv(d_path)\n```\n:::\n\n\nGeschickter als jedes Mal die Datei herunterzuladen, ist es, sie einmal herunterzuladen, und dann lokal zu speichern,\netwa im Unterordner `data` des RStudio-Projektordners.\n\n\nNach dem Importieren wird der Datensatz in eine Tidy-Form gebracht (mit `unnest_tokens`) und dann verschachtelt, mit `nest`.\n\n\n### Complaints verk√ºrzt und geschachtelt\n\nUm ein Herumprobieren zu erleichtern, ist hier der Datensatz `complaints` in zwei verk√ºrzten Formen bereitgestellt:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-22_b7306b0becaff4668fe084367de2bc0d'}\n\n```{.r .cell-code}\nnested_words2_path <- \"https://github.com/sebastiansauer/datascience-text/raw/main/data/nested_words2.rds\"\nnested_words3_path <- \"https://github.com/sebastiansauer/datascience-text/raw/main/data/nested_words3.rds\"\n```\n:::\n\n\n\n`nested_words2` enth√§lt die ersten 10% des Datensatz `nested_words`und ist gut 4 MB gro√ü (mit `gz` gezippt); er besteht aus ca. 11 Tausend Beschwerden.\n`nested_words3` enth√§lt nur die ersten 10 Beschwerden, was ihn gut zum Handhaben macht.\n\nBeide sind verschachtelt und aus `tidy_complaints` (s. [Kap. 5.1](https://smltar.com/embeddings.html#motivatingsparse)) hervorgegangen.\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/read-rds-nested-data_e20a2e031b46ddce829b2bab8afc4e73'}\n\n```{.r .cell-code}\nnested_words3 <- read_rds(nested_words3_path)\n```\n:::\n\n\n\nDas sieht dann so aus:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-23_3d98c453b913c3e0b875119955c68054'}\n\n```{.r .cell-code}\nnested_words3 %>% \n  head(3)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"complaint_id\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"words\"],\"name\":[2],\"type\":[\"list\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"3384392\",\"2\":\"<tibble[,1]>\"},{\"1\":\"3417821\",\"2\":\"<tibble[,1]>\"},{\"1\":\"3433198\",\"2\":\"<tibble[,1]>\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\nWerfen wir einen Blick in den TExt der ersten Beschwerde des Datensatzes mit der ID `nested_words3_path$complaint_id[1]`.\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-24_67b17e62053486073e55929170d395c8'}\n\n```{.r .cell-code}\nbeschwerde1_text <- nested_words3$words[[1]]\n```\n:::\n\n\nDas ist ein Tibble mit einer Spalte und 17 W√∂rtern; \nda wir schon auf Unigramme aufgeteilt haben, ist jedes Wort ein Element des Vektors `word`: \n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-25_6859fceca726c64a81d2e9d74029e7d0'}\n\n```{.r .cell-code}\nbeschwerde1_text %>% \n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"word\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"systems\"},{\"1\":\"inc\"},{\"1\":\"is\"},{\"1\":\"trying\"},{\"1\":\"to\"},{\"1\":\"collect\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-26_04ce0ef3ca2920f28496e567eca8c503'}\n\n```{.r .cell-code}\nbeschwerde1_text$word\n##  [1] \"systems\"    \"inc\"        \"is\"         \"trying\"     \"to\"        \n##  [6] \"collect\"    \"a\"          \"debt\"       \"that\"       \"is\"        \n## [11] \"not\"        \"mine\"       \"not\"        \"owed\"       \"and\"       \n## [16] \"is\"         \"inaccurate\"\n```\n:::\n\n\n\n\n## Wordembeddings selber erstellen\n\n\n\n\n\n\n### PMI berechnen\n\n\nRufen Sie sich die Definition der PMI ins Ged√§chtnis, s. @eq-pmi.\n\nMit R kann man die PMI z.B. so berechnen, s. `? pairwise_pmi` aus dem Paket `{widyr}`.\n\n\nZum Paket `widyr` von Robinson und Silge:\n\n>   This package wraps the pattern of un-tidying data into a wide matrix, performing some processing, then turning it back into a tidy form. This is useful for several mathematical operations such as co-occurrence counts, correlations, or clustering that are best done on a wide matrix.\n\n\n[Quelle](https://juliasilge.github.io/widyr/)\n\nErzeugen wir uns Dummy-Daten:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-27_99c1a487b5f1cba29651a32a5c8ffeb9'}\n\n```{.r .cell-code}\ndat <- tibble(feature = rep(1:5, each = 2),\n              item = c(\"a\", \"b\",\n                       \"a\", \"c\",\n                       \"a\", \"c\",\n                       \"b\", \"e\",\n                       \"b\", \"f\"))\n\ndat\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"feature\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"item\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"1\",\"2\":\"a\"},{\"1\":\"1\",\"2\":\"b\"},{\"1\":\"2\",\"2\":\"a\"},{\"1\":\"2\",\"2\":\"c\"},{\"1\":\"3\",\"2\":\"a\"},{\"1\":\"3\",\"2\":\"c\"},{\"1\":\"4\",\"2\":\"b\"},{\"1\":\"4\",\"2\":\"e\"},{\"1\":\"5\",\"2\":\"b\"},{\"1\":\"5\",\"2\":\"f\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nAus der Hilfe der Funktion:\n\n>   Find pointwise mutual information of pairs of items in a column, based on a \"feature\" column that links them together. This is an example of the spread-operate-retidy pattern.\n\nDie Argumente der Funktion sind:\n\n*item*\n\nItem to compare; will end up in item1 and item2 columns\n\n*feature*\t\n\nColumn describing the feature that links one item to others\n\n\nManche Berechnungen (Operationen) lassen sich vielleicht leichter nicht in der Tidy-Form (Langform), sondern in der \"breiten\" oder Matrixform ausf√ºhren.\nWandeln wir mal `dat` von der Langform in die Breitform um:\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-28_251bbdc09f80968c3cf4bce8e25f11f9'}\n\n```{.r .cell-code}\ntable(dat$item, dat$feature)\n##    \n##     1 2 3 4 5\n##   a 1 1 1 0 0\n##   b 1 0 0 1 1\n##   c 0 1 1 0 0\n##   e 0 0 0 1 0\n##   f 0 0 0 0 1\n```\n:::\n\n\nSilge und Robinson verdeutlichen das Prinzip von `widyr` so, s. @fig-widyr.\n\n\n![Die Funktionsweise von widyr, Quelle: Silge und Robinson](img/widyr.jpeg){#fig-widyr}\n\n(Vgl. auch die [Erkl√§rung hier](https://bookdown.org/Maxine/tidy-text-mining/counting-and-correlating-pairs-of-words-with-widyr.html).)\n\nBauen wir das mal von Hand nach.\n\n\n\nRandwahrscheinlichkeiten von `a` und `c` sowie deren Produkt, `p_a_p_c`:\n\n\n::: {.cell hash='050-word-embedding_cache/html/p_a_und_p_c_9eb579c13ce103dc6ac7ef85f326e8c2'}\n\n```{.r .cell-code}\np_a <- 3/5\np_c <- 2/5\n\np_a_p_c <- p_a * p_c\np_a_p_c\n## [1] 0.24\n```\n:::\n\n\n\nGemeinsame Wahrscheinlichkeit von `a` und `c`:\n\n\n::: {.cell hash='050-word-embedding_cache/html/p_ac_7266fe7bae269f19f02389cac656f652'}\n\n```{.r .cell-code}\np_ac <- 2/5\n```\n:::\n\n\n\nPMI von Hand berechnet:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-29_c9b594587306bd54d0f32e4a8a56d9c2'}\n\n```{.r .cell-code}\nlog(p_ac/p_a_p_c)\n## [1] 0.5108256\n```\n:::\n\n\nMan beachte, dass hier als Basis $e$, der nat√ºrliche Logarithmus, verwendet wurde (nicht 2).\n\nJetzt berechnen wir die PMI mit `pairwise_pmi`.\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-30_dcaf526281cef759a45e2021ca066b60'}\n\n```{.r .cell-code}\npairwise_pmi(dat, item = item, feature = feature)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"item1\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"item2\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"pmi\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"b\",\"2\":\"a\",\"3\":\"-0.5877867\"},{\"1\":\"c\",\"2\":\"a\",\"3\":\"0.5108256\"},{\"1\":\"a\",\"2\":\"b\",\"3\":\"-0.5877867\"},{\"1\":\"e\",\"2\":\"b\",\"3\":\"0.5108256\"},{\"1\":\"f\",\"2\":\"b\",\"3\":\"0.5108256\"},{\"1\":\"a\",\"2\":\"c\",\"3\":\"0.5108256\"},{\"1\":\"b\",\"2\":\"e\",\"3\":\"0.5108256\"},{\"1\":\"b\",\"2\":\"f\",\"3\":\"0.5108256\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nWie man sieht, entspricht unserer Berechnung von Hand der Berechnung mit `pairwise_pmi`. \n\n\n\n\n\n### Sliding\n\nSliding ist ein interessantes Konzept, aber man braucht vielleicht etwas Zeit, \num sein Hirn um das Konzept zu wickeln...\n\nHier eine Illustration:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-31_333bb120d5b632be2eb35b18f6a75ce6'}\n\n```{.r .cell-code}\ntxt_vec <- \"Das ist ein Test, von dem nicht viel zu erwarten ist\"\n\nslider::slide(txt_vec, ~ .x, .before = 2)\n## [[1]]\n## [1] \"Das ist ein Test, von dem nicht viel zu erwarten ist\"\n```\n:::\n\n\n\nOh, da passiert nichts?! Kaputt? Nein, wir m√ºssen jedes Wort als *ein Element* des Vektors auffassen.\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-32_3ad7f403a99cc1672d55581e9a4b8f1e'}\n\n```{.r .cell-code}\ntxt_df <-\n  tibble(txt = txt_vec) %>% \n  unnest_tokens(input = txt, output = word)\n\nhead(txt_df)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"word\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"das\"},{\"1\":\"ist\"},{\"1\":\"ein\"},{\"1\":\"test\"},{\"1\":\"von\"},{\"1\":\"dem\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-33_0c68b7b70a9aeab3b517d3f9ad628a51'}\n\n```{.r .cell-code}\nslider::slide(txt_df$word, ~ .x, .before = 2)\n## [[1]]\n## [1] \"das\"\n## \n## [[2]]\n## [1] \"das\" \"ist\"\n## \n## [[3]]\n## [1] \"das\" \"ist\" \"ein\"\n## \n## [[4]]\n## [1] \"ist\"  \"ein\"  \"test\"\n## \n## [[5]]\n## [1] \"ein\"  \"test\" \"von\" \n## \n## [[6]]\n## [1] \"test\" \"von\"  \"dem\" \n## \n## [[7]]\n## [1] \"von\"   \"dem\"   \"nicht\"\n## \n## [[8]]\n## [1] \"dem\"   \"nicht\" \"viel\" \n## \n## [[9]]\n## [1] \"nicht\" \"viel\"  \"zu\"   \n## \n## [[10]]\n## [1] \"viel\"     \"zu\"       \"erwarten\"\n## \n## [[11]]\n## [1] \"zu\"       \"erwarten\" \"ist\"\n```\n:::\n\n\n\nAh!\n\n\nDas Aufteilen in einzelne W√∂rter pro Element des Vektors k√∂nnte man auch so erreichen:\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/slide2_644b76a7721296f1c15d7914263be1f3'}\n\n```{.r .cell-code}\ntxt_vec2 <- str_split(txt_vec, pattern = boundary(\"word\")) %>% \n  simplify() \n\nslide(txt_vec2, .f = ~.x, .before = 2)\n## [[1]]\n## [1] \"Das\"\n## \n## [[2]]\n## [1] \"Das\" \"ist\"\n## \n## [[3]]\n## [1] \"Das\" \"ist\" \"ein\"\n## \n## [[4]]\n## [1] \"ist\"  \"ein\"  \"Test\"\n## \n## [[5]]\n## [1] \"ein\"  \"Test\" \"von\" \n## \n## [[6]]\n## [1] \"Test\" \"von\"  \"dem\" \n## \n## [[7]]\n## [1] \"von\"   \"dem\"   \"nicht\"\n## \n## [[8]]\n## [1] \"dem\"   \"nicht\" \"viel\" \n## \n## [[9]]\n## [1] \"nicht\" \"viel\"  \"zu\"   \n## \n## [[10]]\n## [1] \"viel\"     \"zu\"       \"erwarten\"\n## \n## [[11]]\n## [1] \"zu\"       \"erwarten\" \"ist\"\n```\n:::\n\n\n\n\n\nIn unserem Beispiel mit den Beschwerden:\n\n\n::: {.cell hash='050-word-embedding_cache/html/slide3_8d359533439d20adf867d847aa726ea0'}\n\n```{.r .cell-code}\nslide(beschwerde1_text$word,  ~.x, .before = 2)\n## [[1]]\n## [1] \"systems\"\n## \n## [[2]]\n## [1] \"systems\" \"inc\"    \n## \n## [[3]]\n## [1] \"systems\" \"inc\"     \"is\"     \n## \n## [[4]]\n## [1] \"inc\"    \"is\"     \"trying\"\n## \n## [[5]]\n## [1] \"is\"     \"trying\" \"to\"    \n## \n## [[6]]\n## [1] \"trying\"  \"to\"      \"collect\"\n## \n## [[7]]\n## [1] \"to\"      \"collect\" \"a\"      \n## \n## [[8]]\n## [1] \"collect\" \"a\"       \"debt\"   \n## \n## [[9]]\n## [1] \"a\"    \"debt\" \"that\"\n## \n## [[10]]\n## [1] \"debt\" \"that\" \"is\"  \n## \n## [[11]]\n## [1] \"that\" \"is\"   \"not\" \n## \n## [[12]]\n## [1] \"is\"   \"not\"  \"mine\"\n## \n## [[13]]\n## [1] \"not\"  \"mine\" \"not\" \n## \n## [[14]]\n## [1] \"mine\" \"not\"  \"owed\"\n## \n## [[15]]\n## [1] \"not\"  \"owed\" \"and\" \n## \n## [[16]]\n## [1] \"owed\" \"and\"  \"is\"  \n## \n## [[17]]\n## [1] \"and\"        \"is\"         \"inaccurate\"\n```\n:::\n\n\n\n\n### Funktion `slide_windows`\n\n\nDie Funktion `slide_windows` im [Kapitel 5.2](https://smltar.com/embeddings.html#understand-word-embeddings-by-finding-them-yourself) ist recht kompliziert. \nIn solchen F√§llen ist es hilfreich, sich jeden Schritt einzeln ausf√ºhren zu lassen. \nDas machen wir jetzt mal.\n\nHier ist die Syntax der Funktion `slide_windows`: \n\n\n\n::: {.cell hash='050-word-embedding_cache/html/fun-slide-win_526741bd2e22f855be774bf7b07a7625'}\n\n```{.r .cell-code}\nslide_windows <- function(tbl, window_size) {\n  skipgrams <- slider::slide(\n    tbl, \n    ~.x,  # Syntax √§hnlich zu purrr::map()\n    .after = window_size - 1, \n    .step = 1, \n    .complete = TRUE\n  )\n  \n  safe_mutate <- safely(mutate)\n  \n  out <- map2(skipgrams,\n              1:length(skipgrams),\n              ~ safe_mutate(.x, window_id = .y))\n  \n  out %>%\n    transpose() %>%\n    pluck(\"result\") %>%\n    compact() %>%\n    bind_rows()\n}\n```\n:::\n\n\n\n\n\nErschwerend kommt eine gro√üe Datenmenge und eine lange Berechnungszeit dazu, was das Debuggen (Nachvollziehen und Durchdenken) der Schritte zus√§tzlich erschwert.\nIn solchen F√§llen hilft die goldene Regel: Mach es dir so einfach wie m√∂glich (aber nicht einfacher).\nWir nutzen also den stark verkleinerten Datensatz `nested_words3`, den wir oben importiert haben.\n\n\nZuerst erlauben wir mal, \ndasss unsere R-Session mehrere Kerne benutzen darf.\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-34_f25824ae0e61697ae12557114727f97d'}\n\n```{.r .cell-code}\nplan(multisession)  ## for parallel processing\n```\n:::\n\n\nDie Funktion `slide_windows` ist recht kompliziert.\nEs hilft oft, sich mit `debug(fun)` eine Funktion Schritt f√ºr Schritt anzuschauen.\n\n\n\nGehen wir Schritt f√ºr Schritt durch die Syntax von `slide_windows`.\n\n\n\nWerfen wir einen Blick in `words`, erstes Element (ein Tibble mit einer Spalte). \nDenn `die einzelnen Elemente von `words` werden an die Funktion `slide_windows` als \"Futter\" √ºbergeben.\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-35_d530aa6d9a99939621958d1a00a333f8'}\n\n```{.r .cell-code}\nfutter1 <- nested_words3[[\"words\"]][[1]]\nfutter1\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"word\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"systems\"},{\"1\":\"inc\"},{\"1\":\"is\"},{\"1\":\"trying\"},{\"1\":\"to\"},{\"1\":\"collect\"},{\"1\":\"a\"},{\"1\":\"debt\"},{\"1\":\"that\"},{\"1\":\"is\"},{\"1\":\"not\"},{\"1\":\"mine\"},{\"1\":\"not\"},{\"1\":\"owed\"},{\"1\":\"and\"},{\"1\":\"is\"},{\"1\":\"inaccurate\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nDas ist der Text der ersten Beschwerde.\n\nOkay, also dann geht's los durch die einzelnen Schritte der Funktion `slide_windows`.\n\n\nZun√§chst holen wir uns die \"Fenster\" oder \"Skipgrams\":\n\n\n::: {.cell hash='050-word-embedding_cache/html/skipgrams1_c32767556b2427be440754b8f8218ed1'}\n\n```{.r .cell-code}\nskipgrams1 <- slider::slide(\n   futter1, \n    ~.x, \n    .after = 3, \n    .step = 1, \n    .complete = TRUE\n  )\n```\n:::\n\n\n\nBei `slide(tbl, ~.x)` geben wir die Funktion an, die auf `tbl` angewendet werden soll. \nDaher auch die Tilde, die uns von `purrr::map()` her bekannt ist.\nIn unserem Fall wollen wir nur die Elemente auslesen;\nElemente auslesen erreicht man,\nin dem man sie mit Namen anspricht,\nin diesem Fall mit dem Platzhalter `.x`.\n\n\nJedes Element von `skipgrams1` ist ein 4*1-Tibble und ist ein Skripgram.\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-36_a48ae88e815b4d33c5fd2e09b0e55a1c'}\n\n```{.r .cell-code}\nskipgrams1 %>% str()\n## List of 17\n##  $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"systems\" \"inc\" \"is\" \"trying\"\n##  $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"inc\" \"is\" \"trying\" \"to\"\n##  $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"is\" \"trying\" \"to\" \"collect\"\n##  $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"trying\" \"to\" \"collect\" \"a\"\n##  $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"to\" \"collect\" \"a\" \"debt\"\n##  $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"collect\" \"a\" \"debt\" \"that\"\n##  $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"a\" \"debt\" \"that\" \"is\"\n##  $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"debt\" \"that\" \"is\" \"not\"\n##  $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"that\" \"is\" \"not\" \"mine\"\n##  $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"is\" \"not\" \"mine\" \"not\"\n##  $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"not\" \"mine\" \"not\" \"owed\"\n##  $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"mine\" \"not\" \"owed\" \"and\"\n##  $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"not\" \"owed\" \"and\" \"is\"\n##  $ : tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n##   ..$ word: chr [1:4] \"owed\" \"and\" \"is\" \"inaccurate\"\n##  $ : NULL\n##  $ : NULL\n##  $ : NULL\n```\n:::\n\n\n\nDas zweite Skipgram von `skipgrams1` enth√§lt, naja, das zweite Skipgram.\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-37_71e57e9e016118a98ddb886c3ba056ef'}\n\n```{.r .cell-code}\nskipgrams1[[2]] %>% str()\n## tibble [4 √ó 1] (S3: tbl_df/tbl/data.frame)\n##  $ word: chr [1:4] \"inc\" \"is\" \"trying\" \"to\"\n```\n:::\n\n\nUnd so weiter.\n\n\n\n\nOkay, weiter im Programm. Jetzt mappen wir das Erstellen der Skipgrams \n\n\n::: {.cell hash='050-word-embedding_cache/html/skipgrams2_cde8325514e58023b2649eb1d7662d82'}\n\n```{.r .cell-code}\nsafe_mutate <- safely(mutate)\n  \nout1 <- map2(skipgrams1,\n             1:length(skipgrams1),\n             ~ safe_mutate(.x, window_id = .y))\n  \nout1 %>% \n  head(2) %>% \n  str()\n## List of 2\n##  $ :List of 2\n##   ..$ result: tibble [4 √ó 2] (S3: tbl_df/tbl/data.frame)\n##   .. ..$ word     : chr [1:4] \"systems\" \"inc\" \"is\" \"trying\"\n##   .. ..$ window_id: int [1:4] 1 1 1 1\n##   ..$ error : NULL\n##  $ :List of 2\n##   ..$ result: tibble [4 √ó 2] (S3: tbl_df/tbl/data.frame)\n##   .. ..$ word     : chr [1:4] \"inc\" \"is\" \"trying\" \"to\"\n##   .. ..$ window_id: int [1:4] 2 2 2 2\n##   ..$ error : NULL\n```\n:::\n\n\n`out1` ist eine Liste mit 17 Elementen; jedes Element  mit jeweils zwei Elementen: Den Ergebnissen und ob es einen Fehler gab bei `safe_mutate`.\nDie 10 Elemente entsprechen den 10 Skipgrams.\nWir k√∂nnen aber `out1` auch \"drehen\", transponieren genauer gesagt.\nso dass wir eine Liste mit *zwei* Elementen bekommen: \ndas erste Element hat die (zehn) Ergebnisse (n√§mlich die Skipgrams) und das zweite Elemente, ob es Fehler gab.\n\nDas Prinzip des Transponierens ist in @fig-Matrix-transpose dargestellt.\n\n![Transponieren einer Matrix (\"Tabelle\")](img/Matrix_transpose.gif){#fig-Matrix-transpose}\n\n\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-38_6ad8c239105dbec98c91d040e2b24986'}\n\n```{.r .cell-code}\nout2 <-\nout1 %>%\n  transpose() \n```\n:::\n\n\n\nPuh, das ist schon anstrengendes Datenyoga...\n\n\nAber jetzt ist es einfach. \nWir ziehen das erste der beiden Elemente, die Ergebnisse heraus (`pluck`), \nentfernen leere Elemente (`compact`) und machen einen Tibble daraus (`bind_rows`):\n\n\n::: {.cell hash='050-word-embedding_cache/html/skipgrams3_167ccb5d2f086c6e7f943d95e539fb73'}\n\n```{.r .cell-code}\nout2 %>% \n  pluck(\"result\") %>%\n  compact() %>%\n  bind_rows() %>% \n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"word\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"window_id\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"systems\",\"2\":\"1\"},{\"1\":\"inc\",\"2\":\"1\"},{\"1\":\"is\",\"2\":\"1\"},{\"1\":\"trying\",\"2\":\"1\"},{\"1\":\"inc\",\"2\":\"2\"},{\"1\":\"is\",\"2\":\"2\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nGeschafft!\n\n\n### √Ñhnlichkeit berechnen\n\n\nNachdem wir jetzt `slide_windows` kennen, schauen wir uns die n√§chsten Schritte an:\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/tidy-pmi1_9430cfd274063d15a357a1749457bd6a'}\n\n```{.r .cell-code}\ntidy_pmi1 <- nested_words3 %>%  # <--- Kleiner Datensatz!\n  mutate(words = future_map(words, slide_windows, 4L))\n```\n:::\n\n\nWir werden `slide_windows` auf die Liste `words` an,\ndie die Beschwerden enth√§lt. \nF√ºr jede Beschwerde erstellen wir die Skipgrams;\ndiese Schleife wird realisiert √ºber `map` bzw. `future_map`,\ndie uns erlaubt, mehrere Kerne des Rechners gleichzeitig zu nutzen,\ndamit es schneller geht.\n\nHier sehen wir z.B. die Skipgram-IDs der ersten Beschwerde.\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-39_61c0d66061bd94639c10bcb6864884be'}\n\n```{.r .cell-code}\ntidy_pmi1[[\"words\"]][[1]] %>% \n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"word\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"window_id\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"systems\",\"2\":\"1\"},{\"1\":\"inc\",\"2\":\"1\"},{\"1\":\"is\",\"2\":\"1\"},{\"1\":\"trying\",\"2\":\"1\"},{\"1\":\"inc\",\"2\":\"2\"},{\"1\":\"is\",\"2\":\"2\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\nGenestet siehst es so aus:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-40_1ab8944dc44781ac2c928e439d30e8a5'}\n\n```{.r .cell-code}\ntidy_pmi1 %>% \n  head(1)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"complaint_id\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"words\"],\"name\":[2],\"type\":[\"list\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"3384392\",\"2\":\"<tibble[,2]>\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\nDie Listenspalte entschachteln wir mal:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-41_4a51138a41d3da9d8070d8d0966dd4f0'}\n\n```{.r .cell-code}\ntidy_pmi2 <- tidy_pmi1 %>% \n  unnest(words)  # entschachtele\n\ntidy_pmi2 %>% \n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"complaint_id\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"word\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"window_id\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"3384392\",\"2\":\"systems\",\"3\":\"1\"},{\"1\":\"3384392\",\"2\":\"inc\",\"3\":\"1\"},{\"1\":\"3384392\",\"2\":\"is\",\"3\":\"1\"},{\"1\":\"3384392\",\"2\":\"trying\",\"3\":\"1\"},{\"1\":\"3384392\",\"2\":\"inc\",\"3\":\"2\"},{\"1\":\"3384392\",\"2\":\"is\",\"3\":\"2\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nZum Berechnen der √Ñhnlichkeit brauchen wir eineindeutige IDs, \nnach dem Prinzip \"1. Skipgram der 1. Beschwerde\" etc:\n\n\n::: {.cell hash='050-word-embedding_cache/html/tidy-pmi3_b46af44adcc444e3b052fe3bfff9a96d'}\n\n```{.r .cell-code}\ntidy_pmi3 <- tidy_pmi2 %>% \n  unite(window_id, complaint_id, window_id)  # f√ºhre Spalten zusammen\n\ntidy_pmi3 %>% \n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"window_id\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"word\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"3384392_1\",\"2\":\"systems\"},{\"1\":\"3384392_1\",\"2\":\"inc\"},{\"1\":\"3384392_1\",\"2\":\"is\"},{\"1\":\"3384392_1\",\"2\":\"trying\"},{\"1\":\"3384392_2\",\"2\":\"inc\"},{\"1\":\"3384392_2\",\"2\":\"is\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nSchlie√ülich berechnen wir die √Ñhnlichkeit mit `pairwise_pmi`,\ndas hatten wir uns oben schon mal n√§her angeschaut:\n\n\n::: {.cell hash='050-word-embedding_cache/html/tidy-pmi4_db17830c9c68f1d437fa7b65d9371c23'}\n\n```{.r .cell-code}\ntidy_pmi4 <- tidy_pmi3 %>% \n  pairwise_pmi(word, window_id)  # berechne √Ñhnlichkeit\n\ntidy_pmi <- tidy_pmi4  # mit dem Objekt arbeiten wir dann weiter\n\ntidy_pmi %>% \n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"item1\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"item2\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"pmi\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"inc\",\"2\":\"systems\",\"3\":\"5.728498\"},{\"1\":\"is\",\"2\":\"systems\",\"3\":\"2.838126\"},{\"1\":\"trying\",\"2\":\"systems\",\"3\":\"5.035351\"},{\"1\":\"systems\",\"2\":\"inc\",\"3\":\"5.728498\"},{\"1\":\"is\",\"2\":\"inc\",\"3\":\"2.838126\"},{\"1\":\"trying\",\"2\":\"inc\",\"3\":\"5.035351\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n\n\n### SVD \n\n\nDie *Singul√§rwertzerlegung* (singular value decomposition, SVD) ist ein Verfahren zur Verringerung der Spaltenzahl (Dimension), vergleichbar zur Faktorenanalyse.\nZur Anschaulichkeit - und ohne substanziellen Hintergrund - sei folgendes Beispiel genannt:\nDie Verben \"gehen\", \"rennen\", \"laufen\", \"schwimmen\", \"fahren\", \"rutschen\" k√∂nnten zu einer gemeinsamen Dimension, etwa \"fortbewegen\" reduziert werden.\nJedes einzelne der eingehenden Verben erh√§lt eine Zahl von 0 bis 1, \ndas die konzeptionelle N√§he des Verbs\nzur \"dahinterliegenden\" Dimension (fortbewegen) quantifiziert; \ndie Zahl nennt man auch die \"Ladung\" des Items (Worts) auf die Dimension.\nSagen wir, wir identifizieren 10 Dimensionen.\nMan erh√§lt dann f√ºr jedes unique Wort im Corpus einen Vektor mit den Ladungen auf die Dimensionen.\nIm genannten Beispiel w√§re es ein 10-stelliger Vektor.\nSo wie ein 3-stelliger Vektor die Position einer Biene im Raum beschreibt^[Man k√∂nnte erg√§nzen: plus eine 4. Dimension f√ºr Zeit, plus noch ein paar Weitere f√ºr die Beschleunigung in verschiedene Richtungen...],\nbeschreibt hier unser 10-stelliger Vektor die \"Position\" eines Worts in unserem *Einbettungsvektor*.\n\n\nDie Syntax dazu ist dieses Mal einfach:\n\n\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/widely-svd_dec835564c5e3369408be1147add578d'}\n\n```{.r .cell-code}\ntidy_word_vectors <- \n  tidy_pmi %>%\n  widely_svd(\n    item1, item2, pmi,\n    nv = 100\n  )\n\ntidy_word_vectors %>% \n  (head)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"item1\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"dimension\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"value\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"inc\",\"2\":\"1\",\"3\":\"-0.03789628\"},{\"1\":\"is\",\"2\":\"1\",\"3\":\"-0.11320691\"},{\"1\":\"trying\",\"2\":\"1\",\"3\":\"-0.05127645\"},{\"1\":\"systems\",\"2\":\"1\",\"3\":\"-0.03333321\"},{\"1\":\"to\",\"2\":\"1\",\"3\":\"-0.12034339\"},{\"1\":\"collect\",\"2\":\"1\",\"3\":\"-0.05542110\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\nMit `nv = 100` haben wir die Anzahl (`n`) der Dimensionen (Variablen, `v`) auf 100 bestimmt.\n\n\n\n### Wort√§hnlichkeit\n\n\nJetzt, da wir mit der SVD jedes Wort in einem Koordinatensystem verortet haben, k√∂nnen wir die Abst√§nde der W√∂rter im Koordinatensystem bestimmen.\nDas geht mit Hilfe des alten Pythagoras, s. @fig-euklid-distance.\nDer Abstand, den man mit Hilfe des Satz des Pythagoras berechnet, nennt man auch *euklidische Distanz*.\n\n\n![Euklidische Distanz in 2D, Quelle: Wikipedia, CC BY4.0, Kmhkmh](img/euklid-distance.png){#fig-euklid-distance width=50%}\n\nOkay, wir sind in einem Raum mit vielen Dimensionen, was mein Hirn nicht mitmacht, [aber der Algebra ist das egal](https://mathworld.wolfram.com/Distance.html). \nPythagoras' Satz l√§sst sich genauso anwenden, wenn es mehr als Dimensionen sind.\n\n\n\n\n\nDie Autoren basteln sich selber eine Funktion in [Kap. 5.3](https://smltar.com/embeddings.html#exploring-cfpb-word-embeddings),\naber der Einfachheit halber nehme ich (erstmal) die entsprechende Funktion aus `widyr`:\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/pairwise-dist_bce751fb141b1afb62995bbc68170777'}\n\n```{.r .cell-code}\nword_neighbors <- \ntidy_word_vectors %>% \n  pairwise_dist(item1, dimension, value)\n\nword_neighbors %>% \n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"item1\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"item2\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"distance\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"is\",\"2\":\"inc\",\"3\":\"1.0220133\"},{\"1\":\"trying\",\"2\":\"inc\",\"3\":\"0.9332886\"},{\"1\":\"systems\",\"2\":\"inc\",\"3\":\"0.4161241\"},{\"1\":\"to\",\"2\":\"inc\",\"3\":\"1.0913872\"},{\"1\":\"collect\",\"2\":\"inc\",\"3\":\"0.5221771\"},{\"1\":\"a\",\"2\":\"inc\",\"3\":\"1.0309526\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n\n\n\n\nSchauen wir uns ein Beispiel an.\nWas sind die Nachbarn von \"inaccurate\"?\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-42_19a25af9244c09dcf3adb7980790fa38'}\n\n```{.r .cell-code}\nword_neighbors %>% \n  filter(item1 == \"inaccurate\") %>% \n  arrange(distance) %>% \n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"item1\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"item2\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"distance\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"inaccurate\",\"2\":\"mine\",\"3\":\"0.5248874\"},{\"1\":\"inaccurate\",\"2\":\"score\",\"3\":\"0.5310125\"},{\"1\":\"inaccurate\",\"2\":\"oh\",\"3\":\"0.5400912\"},{\"1\":\"inaccurate\",\"2\":\"ny\",\"3\":\"0.5400912\"},{\"1\":\"inaccurate\",\"2\":\"dob\",\"3\":\"0.5801280\"},{\"1\":\"inaccurate\",\"2\":\"cell\",\"3\":\"0.6093669\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nHier ist die Datenmenge zu klein, um vern√ºnftige Schl√ºsse zu ziehen.\nAber \"incorrectly\", \"correct\", \"balance\" sind wohl plausible Nachbarn von \"inaccurate\".\n\n\n### Cosinus-√Ñhnlichkeit\n\n\nDie N√§he zweier Vektoren l√§sst sich, neben der euklidischen Distanz, auch z.B. √ºber die [Cosinus-√Ñhnlichkeit](https://en.wikipedia.org/wiki/Cosine_similarity) (Cosine similarity) berechnen, vgl. auch @fig-dotproduct:\n\n![Die Cosinus-√Ñhnlichkeit zweier Vektoren](img/dotproduct.png){#fig-dotproduct}\n\n[Quelle:  Mazin07, Lizenz: PD](https://en.wikipedia.org/wiki/Dot_product#/media/File:Dot_Product.svg)\n\n\n$${\\displaystyle {\\text{Cosinus-√Ñhnlichkeit}}=S_{C}(A,B):=\\cos(\\theta )={\\mathbf {A} \\cdot \\mathbf {B}  \\over \\|\\mathbf {A} \\|\\|\\mathbf {B} \\|}={\\frac {\\sum \\limits _{i=1}^{n}{A_{i}B_{i}}}{{\\sqrt {\\sum \\limits _{i=1}^{n}{A_{i}^{2}}}}{\\sqrt {\\sum \\limits _{i=1}^{n}{B_{i}^{2}}}}}},}$$\n\nwobei $A$ und $B$  zwei Vektoren sind und $\\|\\mathbf {A} \\|$ das Skalarprodukt von A (und B genauso).\nDas [Skalarprodukt](https://en.wikipedia.org/wiki/Dot_product) von $\\color {red} {a =  {\\displaystyle [a_{1},a_{2},\\cdots ,a_{n}]}}$ und $\\color {blue} {b =  {\\displaystyle [b_{1},b_{2},\\cdots ,b_{n}]}}$ ist so definiert:\n\n\n$${\\displaystyle \\mathbf {\\color {red}a} \\cdot \\mathbf {\\color {blue}b} =\\sum _{i=1}^{n}{\\color {red}a}_{i}{\\color {blue}b}_{i}={\\color {red}a}_{1}{\\color {blue}b}_{1}+{\\color {red}a}_{2}{\\color {blue}b}_{2}+\\cdots +{\\color {red}a}_{n}{\\color {blue}b}_{n}}$$\n\n\nEntsprechend ist die Funktion `nearest_neighbors` zu verstehen aus [Kap. 5.3](https://smltar.com/embeddings.html#exploring-cfpb-word-embeddings):\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-43_f05c50d82846ffd32f435d06e02c9002'}\n\n```{.r .cell-code}\nnearest_neighbors <- function(df, token) {\n  df %>%\n    widely(\n      ~ {\n        y <- .[rep(token, nrow(.)), ]\n        res <- rowSums(. * y) / \n          (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))\n        \n        matrix(res, ncol = 1, dimnames = list(x = names(res)))\n      },\n      sort = TRUE\n    )(item1, dimension, value) %>%\n    select(-item2)\n}\n```\n:::\n\n\nWobei mit `widely` zuerst noch von der Langform in die Breitform umformatiert wird,\nda die Breitform von der Formel verlangt wird bzw. das Rechnen einfacher macht.\n\nDer eine Vektor ist das Embedding des Tokens,\nder andere Vektor ist das *mittlere* Embedding √ºber alle Tokens des Corpus.\nWenn die Anzahl der Elemente konstant bleibt,\nkann man sich das Teilen durch $n$ schenken,\nwenn man einen Mittelwert berechnen;\nso h√§lt es auch die Syntax von `nearest_neighbors`.\n\nEin n√ºtzlicher Post zur Cosinus-√Ñhnlichkeit findet sich [hier](https://towardsdatascience.com/cosine-similarity-how-does-it-measure-the-similarity-maths-behind-and-usage-in-python-50ad30aad7db).\n[Dieses Bild](https://datascience-enthusiast.com/figures/cosine_sim.png) zeigt das\nKonzept der Cosinus-√Ñhnlichkeit anschaulich.\n\nZur Erinnerung: Der Cosinus eines Winkels ist definiert als Verh√§ltnis der L√§nge der Ankathete (das ist jene Kathete, die einen Schenkel des Winkels bildet) zur L√§nge der Hypotenuse^[Quelle: https://de.wikipedia.org/wiki/Sinus_und_Kosinus] in einem rechtwinkligen, vgl. @fig-dreieck.\n\n![Ein rechtwinkliges Dreieck als Grundlage der trigonometrischen Funktionen](img/dreieck.png){#fig-dreieck}\n\nAlso: ${\\displaystyle \\cos \\alpha ={\\frac {b}{c}}}$\n\n\n\n[Quelle: PaterSigmund, Wikipedia,dre CC-BY-SA 2.5](https://de.wikipedia.org/wiki/Sinus_und_Kosinus#/media/Datei:RechtwinkligesDreieck.svg)\n\n\nHilfreich ist auch [die Visualisierung von Sinus und Cosinus am Einheitskreis](https://de.wikipedia.org/wiki/Sinus_und_Kosinus#/media/Datei:Sinus_und_Kosinus_am_Einheitskreis_1.svg); gerne [animiert](https://upload.wikimedia.org/wikipedia/commons/f/f3/Sinus_und_Cosinus_am_Einheitskreis.gif) betrachten.\n\n\n## Word-Embeddings vorgekocht\n\n### Glove6B\n\nIn [Kap. 5.4](https://smltar.com/embeddings.html#glove) schreiben die Autoren:\n\n>   If your data set is too small, you typically cannot train reliable word embeddings.\n\nEin paar Millionen W√∂rter sollte der Corpus schon enthalten, so die Autoren.\nDa solche \"Worteinbettungen\" (word embedings) aufw√§ndig zu erstellen sind, \nkann man fertige, \"vorgekochte\" Produkte nutzen.\n\nGlove6B wurde anhand von Wikipedia und anderen Datenquellen erstellt [@pennington_glove_2014].\n\n:::callout-note\nDie zugeh√∂rigen Daten sind recht gro√ü; f√ºr [`glove6b`](https://nlp.stanford.edu/projects/glove/) [@pennington_glove_2014] ist fast ein Gigabyte f√§llig.\nSie sollten sich die Daten in einem ruhigen Moment (mit stabiler Internetverbindung) herunterladen und in einem Verzeichnis meiner Wahl abgespeichert (`datasets`).\nDa bei mir Download abbrach, als ich `embedding_glove6b(dimensions = 100)` aufrief, habe ich die Daten manuell heruntergeladen, s.u.\n:::\n\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/load-glove_d932151a2e1dd58a59d831c14d6bf68a'}\n\n```{.r .cell-code}\nglove6b <- \n  embedding_glove6b(dir = \"~/datasets\", dimensions = 50, manual_download = TRUE)\n\nglove6b %>% \n  select(1:5) %>% \n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"token\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"d1\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"d2\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"d3\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"d4\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"the\",\"2\":\"0.418000\",\"3\":\"0.249680\",\"4\":\"-0.41242\",\"5\":\"0.121700\"},{\"1\":\",\",\"2\":\"0.013441\",\"3\":\"0.236820\",\"4\":\"-0.16899\",\"5\":\"0.409510\"},{\"1\":\".\",\"2\":\"0.151640\",\"3\":\"0.301770\",\"4\":\"-0.16763\",\"5\":\"0.176840\"},{\"1\":\"of\",\"2\":\"0.708530\",\"3\":\"0.570880\",\"4\":\"-0.47160\",\"5\":\"0.180480\"},{\"1\":\"to\",\"2\":\"0.680470\",\"3\":\"-0.039263\",\"4\":\"0.30186\",\"5\":\"-0.177920\"},{\"1\":\"and\",\"2\":\"0.268180\",\"3\":\"0.143460\",\"4\":\"-0.27877\",\"5\":\"0.016257\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n\nDie ersten paar Tokens sind:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-44_459e2663d9d0e5826b684378ecdcc636'}\n\n```{.r .cell-code}\nglove6b$token %>% head(20)\n##  [1] \"the\"  \",\"    \".\"    \"of\"   \"to\"   \"and\"  \"in\"   \"a\"    \"\\\"\"   \"'s\"  \n## [11] \"for\"  \"-\"    \"that\" \"on\"   \"is\"   \"was\"  \"said\" \"with\" \"he\"   \"as\"\n```\n:::\n\n\n\n\nIn eine Tidyform bringen:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-45_a87f6bfdc33224cb8f086956719c3c6e'}\n\n```{.r .cell-code}\ntidy_glove <- \n  glove6b %>%\n  pivot_longer(contains(\"d\"),\n               names_to = \"dimension\") %>%\n  rename(item1 = token)\n\nhead(tidy_glove)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"item1\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"dimension\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"value\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"the\",\"2\":\"d1\",\"3\":\"0.418000\"},{\"1\":\"the\",\"2\":\"d2\",\"3\":\"0.249680\"},{\"1\":\"the\",\"2\":\"d3\",\"3\":\"-0.412420\"},{\"1\":\"the\",\"2\":\"d4\",\"3\":\"0.121700\"},{\"1\":\"the\",\"2\":\"d5\",\"3\":\"0.345270\"},{\"1\":\"the\",\"2\":\"d6\",\"3\":\"-0.044457\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\nGanz sch√∂n gro√ü:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-46_97284e095dfc5acc23b30fce49f2a073'}\n\n```{.r .cell-code}\ndim(glove6b)\n## [1] 400000     51\n```\n:::\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-47_5013d7b8247bb8f7d8e273b1b2cfd975'}\n\n```{.r .cell-code}\nobject.size(tidy_glove)\n## 503834736 bytes\n```\n:::\n\n\nIn Megabyte^[$1024 \\cdot 1024$ Byte, und $1024 =2^{10}$, daher $2^{10} \\cdot 2^{10} = 2^{20}$]\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-48_63dc420785ca3361917d69f8bc275ab2'}\n\n```{.r .cell-code}\nobject.size(tidy_glove) / 2^20\n## 480.5 bytes\n```\n:::\n\n\nEinfacher und genauer geht es so:\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-49_2d2fb2bff0a2d989929d1161bab53bd3'}\n\n```{.r .cell-code}\npryr::object_size(tidy_glove)\n## 503.83 MB\n```\n:::\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-50_091f420411c0315f52eac419fbd6cdf3'}\n\n```{.r .cell-code}\npryr::mem_used()\n## 875 MB\n```\n:::\n\n\n\n\nUm Speicher zu sparen, k√∂nnte man `glove6b` wieder direkt l√∂schen, wenn man nur mit der Tidyform weiterarbeitet.\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-51_5ade12a97cf07c843f26f223c49ca48e'}\n\n```{.r .cell-code}\nrm(glove6b)\n```\n:::\n\n\n\n\nJetzt k√∂nnen wir wieder nach Nachbarn fragen, im euklidischen Sinne, wie oben.\nProbieren wir aus, welche W√∂rter nah zu \"inaccurate\" stehen.\n\n\n:::callout-note\nWie wir oben gesehen haben, ist der Datensatz riesig^[zugegeben, ein subjektiver Ausdruck],\nwas die Berechnungen (zeitaufw√§ndig) und damit nervig machen k√∂nnen.\nDar√ºber hinaus kann es n√∂tig sein, dass Sie mehr Speicher auf Ihrem Computer zur Verf√ºgung stellen m√ºssen^[Kaufen...].\nWir m√ºssen noch `maximum_size = NULL`, um das Jonglieren mit riesigen Matrixen zu erlauben.\nM√∂ge der Gott der RAMs und Arbeitsspeicher uns gn√§dig sein!\n:::\n\n\n\n\n\nMit `pairwise_dist` dauert die Berechnung sehr lange und braucht wohl sehr viel Speicher.\nMitunter kam folgender Fehler auf: \"R error: vector memory exhausted (limit reached?)\".\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-52_b72cfe3a0353861c0b18b915d1359196'}\n\n```{.r .cell-code}\nword_neighbors_glove6b <- \ntidy_glove %>% \n  slice_head(prop = .1) %>% \n  pairwise_dist(item1, dimension, value, maximum_size = NULL)\n\nhead(word_neighbors_glove6b)\n\ntidy_glove %>% \n  filter(item1 == \"inaccurate\") %>% \n  arrange(-value) %>% \n  slice_head(n = 5)\n```\n:::\n\n\n\n\nDeswegen probieren wir doch die Funktion `nearest_neighbors`, so wie es im Buch vorgeschlagen wird, s. [Kap 5.3](https://smltar.com/embeddings.html#exploring-cfpb-word-embeddings).\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/fun-nearest-neighbors2_6d5c1842ea8e366a69fb86a84e4db81a'}\n\n```{.r .cell-code}\nnearest_neighbors <- function(df, token) {\n  df %>%\n    widely(\n      ~ {\n        y <- .[rep(token, nrow(.)), ]\n        res <- rowSums(. * y) / \n          (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))\n        matrix(res, ncol = 1, dimnames = list(x = names(res)))\n      },\n      sort = TRUE,\n      maximum_size = NULL\n    )(item1, dimension, value) %>%\n    select(-item2)\n}\n```\n:::\n\n::: {.cell hash='050-word-embedding_cache/html/tidy-glove-nearest-neighbors_c8ede9dbb648cf12259408aefc0e8d59'}\n\n```{.r .cell-code}\ntidy_glove %>%\n  # slice_head(prob = .1) %>% \n  nearest_neighbors(\"error\") %>% \n  head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"item1\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"value\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"error\",\"2\":\"1.0000000\"},{\"1\":\"errors\",\"2\":\"0.8462532\"},{\"1\":\"correct\",\"2\":\"0.7349197\"},{\"1\":\"mistake\",\"2\":\"0.7174614\"},{\"1\":\"accurate\",\"2\":\"0.7063602\"},{\"1\":\"precise\",\"2\":\"0.7035035\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\nEntschachteln wir unsere Daten zu `complaints`: \n\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnest-tidy-complaints3_982b2551f485d580b555b315c23fd2bc'}\n\n```{.r .cell-code}\ntidy_complaints3 <-\n  nested_words3 %>% \n  unnest(words)\n```\n:::\n\n\n\n\nDann erstellen wir uns eine Tabelle, in der nur die Schnittmenge der W√∂rter aus den Beschwerden und Glove vorkommen.\nDazu nutzen winr einen [inneren Join](https://github.com/gadenbuie/tidyexplain/blob/main/images/inner-join.gif)\n\n![Inner Join, Quelle: Garrick Adenbuie](img/inner-join.gif)\n\n[Quelle](https://github.com/gadenbuie/tidyexplain)\n\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/join-complaints-glove_5f410ec4d6e018bf82b111cbf522cbc8'}\n\n```{.r .cell-code}\ncomplaints_glove <- \ntidy_complaints3 %>% \n  inner_join(by = \"word\", \n  tidy_glove %>% \n  distinct(item1) %>% \n  rename(word = item1)) \n\nhead(complaints_glove)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"complaint_id\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"word\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"3384392\",\"2\":\"systems\"},{\"1\":\"3384392\",\"2\":\"inc\"},{\"1\":\"3384392\",\"2\":\"is\"},{\"1\":\"3384392\",\"2\":\"trying\"},{\"1\":\"3384392\",\"2\":\"to\"},{\"1\":\"3384392\",\"2\":\"collect\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nWie viele unique (distinkte) W√∂rter gibt es in unserem Corpus?\n\n\n::: {.cell hash='050-word-embedding_cache/html/tidy_complaints3_distinct_words_n_5bd614aaf88cfc0ff123137f186300cd'}\n\n```{.r .cell-code}\ntidy_complaints3_distinct_words_n <- \ntidy_complaints3 %>% \n  distinct(word) %>% \n  nrow()\n\ntidy_complaints3_distinct_words_n\n## [1] 222\n```\n:::\n\n\n\nIn `tidy_complaints` gibt es √ºbrigens 222 verschiedene W√∂rter.\n\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/word_matrix_f4fe304f4160a097c036c0c859fd83fc'}\n\n```{.r .cell-code}\nword_matrix <- tidy_complaints3 %>%\n  inner_join(by = \"word\",\n             tidy_glove %>%\n               distinct(item1) %>%\n               rename(word = item1)) %>%\n  count(complaint_id, word) %>%\n  cast_sparse(complaint_id, word, n)\n\n#word_matrix\n```\n:::\n\n\n`word_matrix` z√§hlt f√ºr jede der 10 Beschwerden, welche W√∂rter (und wie h√§ufig) vorkommen.\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-53_f9f4d07e51c5d2d2851f46c89fa6d75a'}\n\n```{.r .cell-code}\ndim(word_matrix)\n## [1]  10 222\n```\n:::\n\n\n10 Beschwerden (Dokumente) und 222 unique W√∂rter.\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/glove_matrix_f3f97c05d063452b160abda390abe68e'}\n\n```{.r .cell-code}\nglove_matrix <- tidy_glove %>%\n  inner_join(by = \"item1\",\n             tidy_complaints3 %>%\n               distinct(word) %>%\n               rename(item1 = word)) %>%\n  cast_sparse(item1, dimension, value)\n\n#glove_matrix\n```\n:::\n\n\n\n`glove_matrix` gibt f√ºr jedes unique Wort den Einbettungsvektor an.\n\n\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-54_dfff24fa5a238ad6a88f3f2c74ec2705'}\n\n```{.r .cell-code}\ndim(glove_matrix)\n## [1] 222  50\n```\n:::\n\n\nDas sind 222 unique W√∂rter und 50 Dimensionen des Einbettungsvektors.\n\n\n\nJetzt k√∂nnen wir noch pro Dokument (10 in diesem Beispiel) die mittlere \"Position\" jedes Dokuments im Einbettungsvektor ausrechnen.\nBildlich gesprochen: Was ist der mittlere Raumpunkt (Zentroid) des Bienenschwarms, wobei eine Biene ein Dokument darstellt und die Raumachsen die Dimensionen des Einbettungsvektors.\n\nDazu gewichten wir jedes Wort eines Dokuments mit den Ladungen des Einbettungsvektor und summieren diese Terme.\nEs resultiert eine Matrix mit einem Einbettungsvektor pro Dokument.\nDiese Matrix k√∂nnen wir jetzt als Pr√§diktorenmatrix hernehmen.\n\n\n::: {.cell hash='050-word-embedding_cache/html/doc_matrix_58d16b4502732f3be5f94d813addab54'}\n\n```{.r .cell-code}\ndoc_matrix <- word_matrix %*% glove_matrix\n#doc_matrix %>% head()\n```\n:::\n\n::: {.cell hash='050-word-embedding_cache/html/unnamed-chunk-55_530c9c6fc1e776daf293a85ac56d69d6'}\n\n```{.r .cell-code}\ndim(doc_matrix)\n## [1] 10 50\n```\n:::\n\n\n\nDie Anzahl der Dokumente ist 10 und die Anzahl der Dimensionen (des Einbettungsvektors) ist 50.\n\n\n### Wordembeddings f√ºr die deutsche Sprache\n\n\nIn diesem [Github-Projekt](https://devmount.github.io/GermanWordEmbeddings/) finden sich die Materialien f√ºr ein deutsches Wordembedding [@mueller2015].\n\n\n\n## Fazit\n\nWorteinbettungen sind eine aufw√§ndige Angelegenheit. \nPositiv gesprochen kann ein Analysti die Muskeln spielen lassen und zeigen, was sie oder er so alles drauf hat.\nIst ja schon cooles Zeugs, die Word Embeddings.\nEs besteht die Chance, dass man mit dieser Methode bessere Vorhersagen erreicht, als mit anderen, einfachen\nAns√§tzen wir Worth√§ufigkeiten oder tf-idf.\nAuf der anderen Seite ist es oft sinnvoll, mit einfachen Ans√§tzen zu starten,\nund zu sehen, wie weit man kommt.\nVielleicht ja weit genug.\n\n\n\n\n\n\n## Literatur\n\n\n### Wikipedia\n\nEs gibt eine Reihe n√ºtzlicher (und recht informationsdichter) Wikipedia-Eintr√§ge zum Thema Informationstheorie.\n\n- [Wikipedia: Pointwise Mutual Information](https://en.wikipedia.org/wiki/Pointwise_mutual_information)\n- [Wikipedia: Mutual information](https://en.wikipedia.org/wiki/Mutual_information)\n- [Wikipedia: Information theory](https://en.wikipedia.org/wiki/Information_theory)\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}