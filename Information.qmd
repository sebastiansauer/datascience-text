# Informationstheorie



### Lernziele


- Die grundlegenden Konzepte der Informationstheorie erkl√§ren k√∂nnen



### Vorbereitung

- Lesen Sie [diesen Text](https://www.science4all.org/article/shannons-information-theory/) als Vorbereitung



### Ben√∂tigte R-Pakete

```{r}
#| message: false
library(tidyverse)
library(tidytext)
library(hcandersenr)   # Textdaten
library(easystats)  # Komfort f√ºr deskriptive Statistiken, wie `describe_distribution`
library(entropy)  # Entropie berechnen
```





## Grundlagen


Die Informationstheorie ist eine der Sternstunden der Wissenschaft. [Manche sagen](http://www.science4all.org/article/shannons-information-theory/) dass Claude Shannon, der Autor der Theorie, auf einer Stufe mit Darwin und Einstein stehen sollte:

>    In this single paper, Shannon introduced this new fundamental theory. He raised the right questions, which no one else even thought of asking. This would have been enough to make this contribution earthshaking. But amazingly enough, Shannon also provided most of the right answers with class and elegance. In comparison, it took decades for a dozen of top physicists to define the basics of quantum theory. Meanwhile, Shannon constructed something equivalent, all by himself, in a single paper.
Shannon‚Äôs theory has since transformed the world like no other ever had, from information technologies to telecommunications, from theoretical physics to economical globalization, from everyday life to philosophy. (...)
I don‚Äôt think Shannon has had the credits he deserves. He should be right up there, near Darwin and Einstein, among the few greatest scientists mankind has ever had the chance to have. 


F√ºr die Statistik ist die Informationstheorie von hoher Bedeutung.
Im Folgenden schauen wir uns einige Grundlagen an.




### Shannon-Information

Mit der *Shannon-Information* (Information, Selbstinformation) quantifizieren wir, wie viel "√úberraschung" sich in einem Ereignis verbirgt [@shannon_1948].

Ein Ereignis mit ...

- *geringer* Wahrscheinlichkeit: *Viel* √úberraschung (Information)
- *hoher* Wahrscheinlichkeit: *Wenig* √úberraschung (Information)



Wenn wir also erfahren, dass ein unwahrscheinliches Ereignis eingetreten ist (Schnee im Sommer), sind wir √ºberraschter als wenn wir h√∂hen, dass ein wahrscheinliches Ereignis eingetreten ist (Schnee im Winter).

Die Information eines Ereignis ist also eng verbunden mit seiner Wahrscheinlichkeit oder auch mit den Odds eines Ereignisses.

Die Shannon-Information ist die einzige Gr√∂√üe, die einige w√ºnschenswerte Anforderungen^[Desiderata, sagt man] erf√ºllt:

1. Stetig
2. Je mehr Ereignisse in einem Zufallsexperiment m√∂glich sind, desto h√∂her die Information, wenn ein bestimmtes Ereignis eintritt
3. Additiv: Die Summe der Information zweier Teilereignisse ist gleich der Information des Gesamtereignis



:::{#def-info}

## Shannon-Information


Die Information ist so definiert:


$$I(x) = - \log_2 \left( Pr(x) \right)$$

:::

Andere Logaritmusbasen sind m√∂glich. 
Bei einem bin√§ren Logarithmus nennt man die Einheit *Bit*^[oder *shannon*].


Ein M√ºnwzurf^[wie immer, als fair angenommen, wenn sonst nichts anderes angegeben ist] hat *1 Bit* Information:

```{r}
-log(1/2, base = 2)
```




Damit gilt: $I = log \left(\frac{1}{Pr(x)}\right)$

Die Information ist eng verwandt mit den Odds bzw. Log-Odds (Logits):


$\text{log-odds}(x)=\log \left({\frac {p(x)}{p(\lnot x)}}\right)$

Logits k√∂nnen als Differenz zweier Shannon-Infos ausgedr√ºckt werden:


$\text{log-odds}(x)=I(\lnot x)-I(x)$



Die Information zweier unabh√§ngiger Ereignisse ist additiv.

Die gemeinsame Wahrscheinlichkeit zweier unabh√§ngiger Ereignisse ist das Produkt der einzelnen Wahrscheinlichkeiten:

$Pr(x,y) = Pr(x) \cdot Pr(y)$

Die *gemeinsame Information* ist dann

$$
{\displaystyle {\begin{aligned}\operatorname {I}(x,y)&=-\log _{2}\left[p(x,y)\right]=-\log _{2}\left[p(x)p(y)\right]\\[5pt]&=-\log _{2}\left[p{(x)}\right]-\log _{2}\left[p{(y)}\right]\\[5pt]&=\operatorname {I} (x)+\operatorname {I} (y)\end{aligned}}}
$$





:::{#exm-info1}


## Information eines wahrscheinlichen Ereignisses

Die Information eines fast sicheren Ereignisses ist gering.

```{r}
-log(99/100, base = 2)
```


:::





:::{#exm-info2}


## Information eines unwahrscheinlichen Ereignisses

Die Information eines unwahrscheinlichen Ereignisses ist hoch.

```{r}
-log(01/100, base = 2)
```


:::







:::{#exm-info3}


## Information eines W√ºrfelwurfs


Die Wahrscheinlichkeitsfunktion eines W√ºrfel ist

${\displaystyle Pr(k)={\begin{cases}{\frac {1}{6}},&k\in \{1,2,3,4,5,6\}\\0,&{\text{ansonsten}}\end{cases}}}$



Die Wahrscheinlichkeit, eine 6 zu w√ºrfeln, ist $Pr(X=6) = \frac{1}{6}$.



Die Information von $X=6$ betr√§gt also


$I(X=6) = -\log_2 \left( Pr(X=6) \right) = -\log_2(1/6) \approx 2.585 \, \text{bits}$.


```{r}
-log(1/6, base = 2)
```


:::







:::{#exm-info3}


## Information zweier  W√ºrfelwurfe







Die Wahrscheinlichkeit, mit zwei W√ºrfeln, $X$ und $Y$, jeweils *6* zu w√ºrfeln, 
betr√§gt $Pr(X=6, Y=6) = \frac{1}{36}$



Die Information betr√§gt also


$I(X=6, Y=6) = -\log_2 \left( Pr(6,6) \right)$


```{r}
-log(1/36, base = 2)
```


Aufgrund der Additivit√§t der Information gilt


$I(6,6) = I(6) + I(6)$


```{r}
-log(1/6, base = 2) + -log(1/6, base = 2)
```



:::









### Entropie


(Informations)entropie ist die Summe der Informationen der Ereignisse einer Zufallsvariablen, $X$.



:::{#def-entropie}


## Informationsentropie


*Informationsentropie*  ist so definiert:

$$H(p) = - \text{E log} (p_i) = - \sum_{i = 1}^n p_i \text{log} (p_i) = E\left[I(X) \right]$$

:::

Die Informationsentropie ist also die "mittlere" oder "erwartete Information einer Zufallsvariablen.


Die Entropie eines M√ºnzwurf-Versuchs (Bernoulli-Experiment) ist dann maximal (1 bit), wenn die Trefferwahrscheinlichkeit 50% betr√§gt: $Pr(X=x) = 1/2$, s. Abb. @fig-bernoulli-entropy.


![Der Zusammenhang von Wahrscheinlichkeit und Entropie bei einem Bernoulli-Versuch, Quelle: Wikipedia, CC-BY-SA, Brona, Rubber Duck](img/Binary_entropy_plot.svg.png){#fig-bernoulli-entropy width="30%"}



### Gemeinsame Information

Die *gemeinsame Information* (mutual information, MI) zweier Zufallsvariablen $X$ und $Y$, $I(X,Y)$, quantifiziert die Informationsmenge, die man √ºber $Y$ erh√§lt, wenn man $X$ beobachtet. Mit anderen Worten: Die MI ist ein Ma√ü des Zusammenhangs zweier (nominaler) Variablen. Im Gegensatz zur Korrelation ist die MI nicht auf lineare Abh√§ngigkeiten beschr√§nkt.



Die MI quantifiziert den Unterschied zwischen der gemeinsamen Verteilung $Pr(X,Y)$ und dem Produkt einer einzelnen^[auch als *marginalen* Wahrscheinlichkeiten oder *Randwahrscheinlichkeiten* bezeichnet] Wahrscheinlichkeitsverteilungen, d.h. $Pr(X)$ und $Pr(Y)$.

Wenn die beiden Variablen (stochastisch) unabh√§ngig^[F√ºr stochastische Unabh√§ngigkeit kann das Zeichen $\bot$ verwendet werden] sind, ist ihre gemeinsame Information Null:

$I(X,Y) = 0 \quad \text{gdw} \quad \bot(X,Y)$.

Dann gilt n√§mlich:

$\log \left( \frac{Pr(X,Y)} {Pr(X) \cdot Pr(Y)} \right) =\log(1) = 0$.


Das macht intuitiv Sinn: Sind zwei Variablen unabh√§ngig, so erf√§hrt man nichts √ºber die zweite, wenn man die erste kennt. So ist Kenntnis der Sternzeichens einer Person und ihrer K√∂rpergr√∂√üe unabh√§ngig.

Das Gegenteil ist auch wahr: Sind zwei Variablen voneinander komplett abh√§ngig, so wei√ü man alles √ºber die zweite, wenn man die erste kennt.


Die gemeinsame Information kann man sich als *Summe der einzelnen gemeinsamen Informationen* von $XY$ sehen (s. @tbl-mi1):

```{r}
#| label: tbl-mi1
#| tbl-cap: Summe der punktweisen gemeinsamen Informationen
#| echo: true
d <- tibble::tribble(
     ~x1,    ~x2,    ~x3,
  "x1y2", "x2y1", "x3y1",
  "x2y1", "x2y2", "x3y2",
  "x1y3", "x2y3", "x3y3"
  )
```



$I(X,Y) = \Sigma_Y \Sigma_y Pr(x,y) \underbrace{\log \left( \frac{Pr(X,Y)}{Pr(X) Pr(Y)} \right)}_\text{punktweise MI}$ 

Die Summanden der gemeinsamen Information bezeichnet man auch als *punktweise gemeinsame Information* (pointwise mutual information, PMI), entsprechend, s. @eq-pmi. MI ist also der Erwartungswert der PMI.


$${\displaystyle \operatorname {PMI} (x,y)\equiv \log_{2}{\frac {p(x,y)}{p(x)p(y)}}=\log _{2}{\frac {p(x|y)}{p(x)}}=\log _{2}{\frac {p(y|x)}{p(y)}}}
$${#eq-pmi}


Andere Basen als `log2` sind gebr√§uchlich, vor allem der nat√ºrliche Logarithmus.




::: {.remark}

Die zwei rechten Umformungen in @eq-pmi basieren auf der Umformung der gemeinsamen Wahrscheinlichkeit. 

Zur Erinnerung: $p(x,y) = p(y)p(x|y) = p(x)p(y|x)$
:::




::: {#exm-pmi}

## Interpretation der PMI

Sei $p(x) = p(y) = 1/10$ und $p(x,y) = 1/10$. W√§ren $x$ und $y$ unabh√§ngig, dann w√§re $p^{\prime}(x,y) = p(x)p(y) = 1/100$.
Das Verh√§ltnis der Produkte der einzelnen Wahrscheinlichkeit zur gemeinsamen Wahrscheinlichkeit w√§re dann 1 und der Logarithmus von 1 ist 0. Das Verh√§ltnis von 1 entspricht also der Unabh√§ngigkeit. Ist das Verh√§ltnis z.B. 5, so zeigt das eine gewisse Abh√§ngigkeit an.
Im obigen Beispiel gilt: $\frac{1/20}{1/100}=5$.

:::


Die MI wird auch √ºber die sog. *Kullback-Leibler-Divergenz* definiert,
die die Differenz zwischen gemeinsamer Wahrscheinlichkeitsfunktion bzw. -dichte und dem Produkt der jeweiligen Randwahrscheinlichkeiten bzw. -dichte angibt.



### Maximumentropie


:::{#def-maxent}


## Maximumentropie

Die Verteilungsform, f√ºr die es die meisten M√∂glichkeiten (Pfade im Baumdiagramm) gibt,
hat die h√∂chste Informationsentropie.

:::

@fig-muenz3 zeigt ein Baumdiagramm f√ºr einen 3-fachen M√ºnzwurf.
In den "Bl√§ttern" (Endknoten) sind die Ergebnisse des Experiments dargestellt
sowie die Zufallsvariable $X$, die die Anzahl der "Treffer" (Kopf) fasst.
Wie man sieht, vereinen einige Werte der Zufallsvariable mehr Pfade auf sich als andere:
Der Wert $X=1$ vereinigt 3 Pfade (von 8) auf sich; der Wert $X=3$ nur 1 Pfad.

![Pfade im Baumdiagramm: 3-facher M√ºnzwurf](img/muenz3.png){#fig-muenz3}



#### Ilustration 


Sagen wir, Sie stehen vor 5 Eimern und haben 10 Kieselsteine bei sich, die mit den Nummern 1 bis 10 beschriftet sind [@mcelreath_statistical_2020].
Weil Sie nichts besseres zu tun haben, werfen Sie die Kiesel in die Eimer und zwar so, 
dass die Wahrscheinlichkeit f√ºr einen Kiesel in einen bestimmten Eimer zu landen f√ºr alle Eimer gleich ist.
Sie werfen also Ihre 10 Kiesel und betrachten das Ergebnis; die Kiesel sind jetzt in einem bestimmten (zuf√§lligen)
Arrangement auf die Eimer verteilt.
Jede Aufteilung (der 10 Kiesel in den 5 Eimern) ist gleich wahrscheinlich^[so √§hnlich wie mit den Lottozahlen] -- 
die Wahrscheinlichkeit, dass alle 10 Kiesel in Eimer 1 landen ist also gleich hoch wie die Wahrscheinlichkeit,
dass jeder Eimer einen Kiesel abkriegt.
Jetzt kommt's: Manche Arrangements k√∂nnen auf mehrere Arten erzielt werden als andere. 
So gibt es nur *eine* Aufteilung f√ºr alle 10 Kiesel in einem Eimer (Teildiagramm a, in @fig-kiesel).
Aber es gibt 90 M√∂glichkeiten, die Kiesel so aufzuteilen, dass 2 in Eimer 2 landen, 8 in Eimer 4 und 2 in Eimer 4,
s. Teildiagramm b in @fig-kiesel [@kurz_statistical_2021]. 
Teildiagramme c bis e zeigen, dass die Anzahl der Aufteilungen schnell astronomisch hoch wird,
wenn sich die Kiesel "gleichm√§√üiger" auf die Eimer verteilen.
Die gleichm√§√üigste Aufteilung (Diagramm e) hat die gr√∂√üte Zahl an m√∂glichen Anordnungen.
Eine Aufteilung der Kiesel auch als Pfad durch ein Baumdiagramm beschrieben werden.

Hier sind ein paar verschiedene Arrangements, deren Anzahl von Aufteilungen wir hier anschauen:

```{r}
d <-
  tibble(a = c(0, 0, 10, 0, 0),
         b = c(0, 1, 8, 1, 0),
         c = c(0, 2, 6, 2, 0),
         d = c(1, 2, 4, 2, 1),
         e = 2) 
d
```



```{r}
#| echo: false
#| fig-cap: Anzahl von Aufteilungen der 10 Kiesel in 5 Eimer
#| label: fig-kiesel
d %>% 
  mutate(bucket = 1:5) %>% 
  gather(letter, pebbles, - bucket) %>% 
  
  ggplot(aes(x = bucket, y = pebbles)) +
  geom_col(width = 1/5) +
  geom_text(aes(y = pebbles + 1, label = pebbles)) +
  geom_text(data = tibble(
    letter  = letters[1:5],
    bucket  = 5.5,
    pebbles = 10,
    label   = str_c(c(1, 90, 1260, 37800, 113400), 
                    rep(c(" Pfad", " Pfade"), times = c(1, 4)))),
    aes(label = label), hjust = 1) +
  scale_y_continuous(breaks = c(0, 5, 10)) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~letter, ncol = 3)
```

Hier sind die Wahrscheinlichkeitsverteilungen der 5 Arrangements^[Ist das nicht eine elegante Syntax?! Solomon Kurz, der Autor der Syntax, hielt sich nicht an die aktuelle Version von `dplyr`; `mutate_all` ist eigentlich √ºberholt zugunsten von `mutate` mit `across`, aber die Pr√§gnanz der Syntax hier ist schon beeindruckend, wie ich finde.]:

```{r}
d %>% 
  mutate_all(~. / sum(.))
```


Dann kann man die Wahrscheinlichkeit einfach in Entropie umrechnen^[Syntax aus @kurz_statistical_2021]:

```{r}
d %>% 
  mutate_all(~ . / sum(.)) %>% 
  gather() %>% 
  group_by(key) %>% 
  summarise(h = -sum(ifelse(value == 0, 0, value * log(value))))
```


Das `ifelse` dient nur dazu, eine Wahrscheinlichkeit von 0 eine Entropie von 0 zu verpassen^[Regel von L'Hopital], denn sonst w√ºrden wir ein Problem rennen, wenn wir $log(0)$ ausrechnen.

```{r}
log(0)
```



## Zufallstext erkennen


### Entropie von Zufallstext


Kann man wohl Zufallstext maschinell erkennen? Sicher gibt es viele Ans√§tze,
um das Problem anzugehen.
Lassen Sie uns einen Ansatz erforschen. 
Erforschen hei√üt, *wir* erforschen f√ºr *uns*, es handelt sich um eine didaktische √úbung, 
das Ziel ist nicht, Neuland f√ºr die Menschheit zu betreten.

Aber zuerst m√ºssen wir √ºberlegen, was "Zufallstext" bedeuten soll.


Nehmen wir uns dazu zuerst einen richtigen Text, ein M√§rchen von H.C. Andersen zum Beispiel.
Nehmen wir das Erste aus der Liste in dem Tibble `hcandersen_de`, "das Feuerzeug".

```{r}
das_feuerzeug <-
  hcandersen_de  %>% 
  filter(book == "Das Feuerzeug") %>% 
  unnest_tokens(input = text, output = word) %>% 
  pull(word) 

head(das_feuerzeug)
```


Das M√§rchen ist `r length(das_feuerzeug)` W√∂rter lang.

```{r}
wortliste <- 
hcandersen_de  %>% 
  filter(book == "Das Feuerzeug") %>% 
  unnest_tokens(output = word, input = text) %>% 
  pull(word) %>% 
  unique()

head(wortliste)
```


Jetzt ziehen wir Stichproben (mit Zur√ºcklegen) aus dieser Liste und erstellen damit eine Art Zufallstext.

```{r}
zufallstext <- 
  sample(x = wortliste, size = length(das_feuerzeug)*10^2, replace = TRUE)
head(zufallstext)
```


Z√§hlen wir, wie h√§ufig jedes Wort vorkommt:

```{r}
zufallstext_count <-
tibble(zufallstext = zufallstext) %>% 
  count(zufallstext)

head(zufallstext_count)
```


Der H√§ufigkeitsvektor von `wortliste` besteht nur aus Einsen, 
so haben wir ja gerade die Wortliste definiert:

```{r}
wortliste_count <-
tibble(wortliste = wortliste) %>% 
  count(wortliste)

head(wortliste_count)
```


Daher ist ihre Informationsentropy maximal.

```{r}
entropy(wortliste_count$n, unit = "log2")
```



Die H√§ufigkeiten der W√∂rter in `zufallstext` hat eine hohe Entropie.

```{r}
entropy(zufallstext_count$n, unit = "log2")
```


Z√§hlen wir die H√§ufigkeiten in der Geschichte "Das Feuerzeug".

```{r}
das_feuerzeug_count <-
  tibble(text = das_feuerzeug) %>% 
  count(text)

head(das_feuerzeug_count)
```
Und berechnen dann die Entropie:

```{r}
entropy(das_feuerzeug_count$n, unit = "log2")
```


Der Zufallstext hat also eine h√∂here Entropie als der echte M√§rchentext.
Der Zufallstext ist also gleichverteilter in den Worth√§ufigkeiten.


Pro Bit weniger Entropie halbiert sich die Anzahl der M√∂glichkeiten einer H√§ufigkeitsverteilung.



### MI von Zufallstext



Left as an exercises for the reader^[[Vgl. hier](https://academia.stackexchange.com/questions/20084/is-using-the-phrase-is-left-as-an-exercise-for-the-reader-considered-good)] ü•≥.





## Literatur


### Wikipedia

Es gibt eine Reihe n√ºtzlicher (und recht informationsdichter) Wikipedia-Eintr√§ge zum Thema Informationstheorie.

- [Wikipedia: Pointwise Mutual Information](https://en.wikipedia.org/wiki/Pointwise_mutual_information)
- [Wikipedia: Mutual information](https://en.wikipedia.org/wiki/Mutual_information)
- [Wikipedia: Information theory](https://en.wikipedia.org/wiki/Information_theory)



