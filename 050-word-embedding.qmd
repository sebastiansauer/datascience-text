# Word Embedding


![Text als Datenbasis prädiktiver Modelle](img/text-mining-1476780_640.png){width=10%}
Bild von <a href="https://pixabay.com/de/users/mcmurryjulie-2375405/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1476780">mcmurryjulie</a> auf <a href="https://pixabay.com/de//?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1476780">Pixabay</a>



## Vorab


### Lernziele



<!-- - Die grundlegenden Konzepte der Informationstheorie erklären können -->
- Die Erstellung von Word-Embeddings anhand grundlegender R-Funktionen erläutern können.



### Vorbereitung

<!-- - Lesen Sie [diesen Text](https://www.science4all.org/article/shannons-information-theory/) als Vorbereitung -->
- Arbeiten Sie @smltar, [Kap. 5](https://smltar.com/embeddings.html) durch.






### Benötigte R-Pakete

```{r}
#| message: false
library(tidyverse)
library(easystats)  # Komfort für deskriptive Statistiken, wie `describe_distribution`
library(tidytext)
library(hcandersenr)   # Textdaten
library(slider)  # slide
library(widyr)  # pairwise_pmi
library(furrr)  # mehrere Kerne gleichzeitig nutzen
library(textdata)  # Worteinbettungen, vorgekocht
library(entropy)  # Entropie berechnen
library(widyr)  # Ähnlichkeit berechnen mit widyr_svd
library(furrr)  # Mehrere Kerne gleichzeitig
```


















## Daten


### Complaints-Datensatz

Der Datensatz `complaints` stammt aus [dieser Quelle](https://www.consumerfinance.gov/data-research/consumer-complaints/).

Den Datensatz `complaints` kann man [hier](https://github.com/EmilHvitfeldt/smltar/raw/master/data/complaints.csv.gz) herunterladen. Im Buch ist die Webseite nicht (direkt?) angegeben. Die Datei ist mit `gz` gepackt; `read_csv` sollte das automatisch entpacken. Achtung: Die Datei ist recht groß.

```{r read-complaints-data}
#| eval: false
d_path <- "https://github.com/EmilHvitfeldt/smltar/raw/master/data/complaints.csv.gz"
complaints <- read_csv(d_path)
```

Geschickter als jedes Mal die Datei herunterzuladen, ist es, sie einmal herunterzuladen, und dann lokal zu speichern,
etwa im Unterordner `data` des RStudio-Projektordners.


Nach dem Importieren wird der Datensatz in eine Tidy-Form gebracht (mit `unnest_tokens`) und dann verschachtelt, mit `nest`.


### Complaints verkürzt und geschachtelt

Um ein Herumprobieren zu erleichtern, ist hier der Datensatz `complaints` in zwei verkürzten Formen bereitgestellt:

```{r}
nested_words2_path <- "https://github.com/sebastiansauer/datascience-text/raw/main/data/nested_words2.rds"
nested_words3_path <- "https://github.com/sebastiansauer/datascience-text/raw/main/data/nested_words3.rds"
```


`nested_words2` enthält die ersten 10% des Datensatz `nested_words`und ist gut 4 MB groß (mit `gz` gezippt); er besteht aus ca. 11 Tausend Beschwerden.
`nested_words3` enthält nur die ersten 10 Beschwerden, was ihn gut zum Handhaben macht.

Beide sind verschachtelt und aus `tidy_complaints` (s. [Kap. 5.1](https://smltar.com/embeddings.html#motivatingsparse)) hervorgegangen.


```{r read-rds-nested-data}
nested_words3 <- read_rds(nested_words3_path)
```


Das sieht dann so aus:

```{r}
nested_words3 %>% 
  head(3)
```
Werfen wir einen Blick in den TExt der ersten Beschwerde des Datensatzes mit der ID `nested_words3_path$complaint_id[1]`.

```{r}
beschwerde1_text <- nested_words3$words[[1]]
```

Das ist ein Tibble mit einer Spalte und 17 Wörtern; 
da wir schon auf Unigramme aufgeteilt haben, ist jedes Wort ein Element des Vektors `word`: 

```{r}
beschwerde1_text %>% 
  head()
```

```{r}
beschwerde1_text$word
```



## Wordembeddings selber erstellen






### PMI berechnen


Rufen Sie sich die Definition der PMI ins Gedächtnis, s. @eq-pmi.

Mit R kann man die PMI z.B. so berechnen, s. `? pairwise_pmi` aus dem Paket `{widyr}`.


Zum Paket `widyr` von Robinson und Silge:

>   This package wraps the pattern of un-tidying data into a wide matrix, performing some processing, then turning it back into a tidy form. This is useful for several mathematical operations such as co-occurrence counts, correlations, or clustering that are best done on a wide matrix.


[Quelle](https://juliasilge.github.io/widyr/)

Erzeugen wir uns Dummy-Daten:

```{r}
dat <- tibble(feature = rep(1:5, each = 2),
              item = c("a", "b",
                       "a", "c",
                       "a", "c",
                       "b", "e",
                       "b", "f"))

dat
```

Aus der Hilfe der Funktion:

>   Find pointwise mutual information of pairs of items in a column, based on a "feature" column that links them together. This is an example of the spread-operate-retidy pattern.

Die Argumente der Funktion sind:

*item*

Item to compare; will end up in item1 and item2 columns

*feature*	

Column describing the feature that links one item to others


Manche Berechnungen (Operationen) lassen sich vielleicht leichter nicht in der Tidy-Form (Langform), sondern in der "breiten" oder Matrixform ausführen.
Wandeln wir mal `dat` von der Langform in die Breitform um:


```{r}
table(dat$item, dat$feature)
```

Silge und Robinson verdeutlichen das Prinzip von `widyr` so, s. @fig-widyr.


![Die Funktionsweise von widyr, Quelle: Silge und Robinson](img/widyr.jpeg){#fig-widyr}

(Vgl. auch die [Erklärung hier](https://bookdown.org/Maxine/tidy-text-mining/counting-and-correlating-pairs-of-words-with-widyr.html).)

Bauen wir das mal von Hand nach.



Randwahrscheinlichkeiten von `a` und `c` sowie deren Produkt, `p_a_p_c`:

```{r p_a_und_p_c}
p_a <- 3/5
p_c <- 2/5

p_a_p_c <- p_a * p_c
p_a_p_c
```


Gemeinsame Wahrscheinlichkeit von `a` und `c`:

```{r p_ac}
p_ac <- 2/5
```


PMI von Hand berechnet:

```{r}
log(p_ac/p_a_p_c)
```

Man beachte, dass hier als Basis $e$, der natürliche Logarithmus, verwendet wurde (nicht 2).

Jetzt berechnen wir die PMI mit `pairwise_pmi`.

```{r}
pairwise_pmi(dat, item = item, feature = feature)
```

Wie man sieht, entspricht unserer Berechnung von Hand der Berechnung mit `pairwise_pmi`. 





### Sliding

Sliding ist ein interessantes Konzept, aber man braucht vielleicht etwas Zeit, 
um sein Hirn um das Konzept zu wickeln...

Hier eine Illustration:

```{r}
txt_vec <- "Das ist ein Test, von dem nicht viel zu erwarten ist"

slider::slide(txt_vec, ~ .x, .before = 2)
```


Oh, da passiert nichts?! Kaputt? Nein, wir müssen jedes Wort als *ein Element* des Vektors auffassen.

```{r}
txt_df <-
  tibble(txt = txt_vec) %>% 
  unnest_tokens(input = txt, output = word)

head(txt_df)
```


```{r}
slider::slide(txt_df$word, ~ .x, .before = 2)
```


Ah!


Das Aufteilen in einzelne Wörter pro Element des Vektors könnte man auch so erreichen:


```{r slide2}
txt_vec2 <- str_split(txt_vec, pattern = boundary("word")) %>% 
  simplify() 

slide(txt_vec2, .f = ~.x, .before = 2)
```




In unserem Beispiel mit den Beschwerden:

```{r slide3}
slide(beschwerde1_text$word,  ~.x, .before = 2)
```



### Funktion `slide_windows`


Die Funktion `slide_windows` im [Kapitel 5.2](https://smltar.com/embeddings.html#understand-word-embeddings-by-finding-them-yourself) ist recht kompliziert. 
In solchen Fällen ist es hilfreich, sich jeden Schritt einzeln ausführen zu lassen. 
Das machen wir jetzt mal.

Hier ist die Syntax der Funktion `slide_windows`: 


```{r fun-slide-win}
slide_windows <- function(tbl, window_size) {
  skipgrams <- slider::slide(
    tbl, 
    ~.x,  # Syntax ähnlich zu purrr::map()
    .after = window_size - 1, 
    .step = 1, 
    .complete = TRUE
  )
  
  safe_mutate <- safely(mutate)
  
  out <- map2(skipgrams,
              1:length(skipgrams),
              ~ safe_mutate(.x, window_id = .y))
  
  out %>%
    transpose() %>%
    pluck("result") %>%
    compact() %>%
    bind_rows()
}
```




Erschwerend kommt eine große Datenmenge und eine lange Berechnungszeit dazu, was das Debuggen (Nachvollziehen und Durchdenken) der Schritte zusätzlich erschwert.
In solchen Fällen hilft die goldene Regel: Mach es dir so einfach wie möglich (aber nicht einfacher).
Wir nutzen also den stark verkleinerten Datensatz `nested_words3`, den wir oben importiert haben.


Zuerst erlauben wir mal, 
dasss unsere R-Session mehrere Kerne benutzen darf.


```{r}
plan(multisession)  ## for parallel processing
```

Die Funktion `slide_windows` ist recht kompliziert.
Es hilft oft, sich mit `debug(fun)` eine Funktion Schritt für Schritt anzuschauen.



Gehen wir Schritt für Schritt durch die Syntax von `slide_windows`.



Werfen wir einen Blick in `words`, erstes Element (ein Tibble mit einer Spalte). 
Denn `die einzelnen Elemente von `words` werden an die Funktion `slide_windows` als "Futter" übergeben.

```{r}
futter1 <- nested_words3[["words"]][[1]]
futter1
```

Das ist der Text der ersten Beschwerde.

Okay, also dann geht's los durch die einzelnen Schritte der Funktion `slide_windows`.


Zunächst holen wir uns die "Fenster" oder "Skipgrams":

```{r skipgrams1}
skipgrams1 <- slider::slide(
   futter1, 
    ~.x, 
    .after = 3, 
    .step = 1, 
    .complete = TRUE
  )
```


Bei `slide(tbl, ~.x)` geben wir die Funktion an, die auf `tbl` angewendet werden soll. 
Daher auch die Tilde, die uns von `purrr::map()` her bekannt ist.
In unserem Fall wollen wir nur die Elemente auslesen;
Elemente auslesen erreicht man,
in dem man sie mit Namen anspricht,
in diesem Fall mit dem Platzhalter `.x`.


Jedes Element von `skipgrams1` ist ein 4*1-Tibble und ist ein Skripgram.


```{r}
skipgrams1 %>% str()
```


Das zweite Skipgram von `skipgrams1` enthält, naja, das zweite Skipgram.

```{r}
skipgrams1[[2]] %>% str()
```

Und so weiter.




Okay, weiter im Programm. Jetzt mappen wir das Erstellen der Skipgrams 

```{r skipgrams2}
safe_mutate <- safely(mutate)
  
out1 <- map2(skipgrams1,
             1:length(skipgrams1),
             ~ safe_mutate(.x, window_id = .y))
  
out1 %>% 
  head(2) %>% 
  str()
```

`out1` ist eine Liste mit 17 Elementen; jedes Element  mit jeweils zwei Elementen: Den Ergebnissen und ob es einen Fehler gab bei `safe_mutate`.
Die 10 Elemente entsprechen den 10 Skipgrams.
Wir können aber `out1` auch "drehen", transponieren genauer gesagt.
so dass wir eine Liste mit *zwei* Elementen bekommen: 
das erste Element hat die (zehn) Ergebnisse (nämlich die Skipgrams) und das zweite Elemente, ob es Fehler gab.

Das Prinzip des Transponierens ist in @fig-Matrix-transpose dargestellt.

![Transponieren einer Matrix ("Tabelle")](img/Matrix_transpose.gif){#fig-Matrix-transpose}




```{r}
out2 <-
out1 %>%
  transpose() 
```


Puh, das ist schon anstrengendes Datenyoga...


Aber jetzt ist es einfach. 
Wir ziehen das erste der beiden Elemente, die Ergebnisse heraus (`pluck`), 
entfernen leere Elemente (`compact`) und machen einen Tibble daraus (`bind_rows`):

```{r skipgrams3}
out2 %>% 
  pluck("result") %>%
  compact() %>%
  bind_rows() %>% 
  head()
```

Geschafft!


### Ähnlichkeit berechnen


Nachdem wir jetzt `slide_windows` kennen, schauen wir uns die nächsten Schritte an:


```{r tidy-pmi1}
tidy_pmi1 <- nested_words3 %>%  # <--- Kleiner Datensatz!
  mutate(words = future_map(  # Schleife (mit mehreren Kernen) über ...
    words,   #  alle Wörter
    slide_windows,  # wobei jedes Mal diese Funtion angewendet wird
    4L  # Parameter an `slide_windows`: Window-Größe ist 4 (L wie "Long", für Integer)
  ))  
```

Wir werden `slide_windows` auf die Liste `words` an,
die die Beschwerden enthält. 
Für jede Beschwerde erstellen wir die Skipgrams;
diese Schleife wird realisiert über `map` bzw. `future_map`,
die uns erlaubt, mehrere Kerne des Rechners gleichzeitig zu nutzen,
damit es schneller geht.

Hier sehen wir z.B. die Skipgram-IDs der ersten Beschwerde.

```{r}
tidy_pmi1[["words"]][[1]] %>% 
  head()
```


Genestet siehst es so aus:

```{r}
tidy_pmi1 %>% 
  head(1)
```


Die Listenspalte entschachteln wir mal:

```{r}
tidy_pmi2 <- tidy_pmi1 %>% 
  unnest(words)  # entschachtele

tidy_pmi2 %>% 
  head()
```

Zum Berechnen der Ähnlichkeit brauchen wir eineindeutige IDs, 
nach dem Prinzip "1. Skipgram der 1. Beschwerde" etc:

```{r tidy-pmi3}
tidy_pmi3 <- tidy_pmi2 %>% 
  unite(window_id, complaint_id, window_id)  # führe Spalten zusammen

tidy_pmi3 %>% 
  head()
```

Schließlich berechnen wir die Ähnlichkeit mit `pairwise_pmi`,
das hatten wir uns oben schon mal näher angeschaut:

```{r tidy-pmi4}
tidy_pmi4 <- tidy_pmi3 %>% 
  pairwise_pmi(word, window_id)  # berechne Ähnlichkeit

tidy_pmi <- tidy_pmi4  # mit dem Objekt arbeiten wir dann weiter

tidy_pmi %>% 
  head()
```




### SVD 


Die *Singulärwertzerlegung* (singular value decomposition, SVD) ist ein Verfahren zur Verringerung der Spaltenzahl (Dimension), vergleichbar zur Faktorenanalyse.
Zur Anschaulichkeit - und ohne substanziellen Hintergrund - sei folgendes Beispiel genannt:
Die Verben "gehen", "rennen", "laufen", "schwimmen", "fahren", "rutschen" könnten zu einer gemeinsamen Dimension, etwa "fortbewegen" reduziert werden.
Jedes einzelne der eingehenden Verben erhält eine Zahl von 0 bis 1, 
das die konzeptionelle Nähe des Verbs
zur "dahinterliegenden" Dimension (fortbewegen) quantifiziert; 
die Zahl nennt man auch die "Ladung" des Items (Worts) auf die Dimension.
Sagen wir, wir identifizieren 10 Dimensionen.
Man erhält dann für jedes unique Wort im Corpus einen Vektor mit den Ladungen auf die Dimensionen.
Im genannten Beispiel wäre es ein 10-stelliger Vektor.
So wie ein 3-stelliger Vektor die Position einer Biene im Raum beschreibt^[Man könnte ergänzen: plus eine 4. Dimension für Zeit, plus noch ein paar Weitere für die Beschleunigung in verschiedene Richtungen...],
beschreibt hier unser 10-stelliger Vektor die "Position" eines Worts in unserem *Einbettungsvektor*.


Die Syntax dazu ist dieses Mal einfach:




```{r widely-svd}
tidy_word_vectors <- 
  tidy_pmi %>%
  widely_svd(
    item1, item2, pmi,
    nv = 100
  )

tidy_word_vectors %>% 
  (head)
```


Mit `nv = 100` haben wir die Anzahl (`n`) der Dimensionen (Variablen, `v`) auf 100 bestimmt.



### Wortähnlichkeit


Jetzt, da wir mit der SVD jedes Wort in einem Koordinatensystem verortet haben, können wir die Abstände der Wörter im Koordinatensystem bestimmen.
Das geht mit Hilfe des alten Pythagoras, s. @fig-euklid-distance.
Der Abstand, den man mit Hilfe des Satz des Pythagoras berechnet, nennt man auch *euklidische Distanz*.


![Euklidische Distanz in 2D, Quelle: Wikipedia, CC BY4.0, Kmhkmh](img/euklid-distance.png){#fig-euklid-distance width=50%}

Okay, wir sind in einem Raum mit vielen Dimensionen, was mein Hirn nicht mitmacht, [aber der Algebra ist das egal](https://mathworld.wolfram.com/Distance.html). 
Pythagoras' Satz lässt sich genauso anwenden, wenn es mehr als Dimensionen sind.





Die Autoren basteln sich selber eine Funktion in [Kap. 5.3](https://smltar.com/embeddings.html#exploring-cfpb-word-embeddings),
aber der Einfachheit halber nehme ich (erstmal) die entsprechende Funktion aus `widyr`:


```{r pairwise-dist}
#| cache: true
word_neighbors <- 
tidy_word_vectors %>% 
  pairwise_dist(item1, dimension, value)

word_neighbors %>% 
  head()
```






Schauen wir uns ein Beispiel an.
Was sind die Nachbarn von "inaccurate"?

```{r}
word_neighbors %>% 
  filter(item1 == "inaccurate") %>% 
  arrange(distance) %>% 
  head()
```

Hier ist die Datenmenge zu klein, um vernünftige Schlüsse zu ziehen.
Aber "incorrectly", "correct", "balance" sind wohl plausible Nachbarn von "inaccurate".


### Cosinus-Ähnlichkeit


Die Nähe zweier Vektoren lässt sich, neben der euklidischen Distanz, auch z.B. über die [Cosinus-Ähnlichkeit](https://en.wikipedia.org/wiki/Cosine_similarity) (Cosine similarity) berechnen, vgl. auch @fig-dotproduct:

![Die Cosinus-Ähnlichkeit zweier Vektoren](img/dotproduct.png){#fig-dotproduct}

[Quelle:  Mazin07, Lizenz: PD](https://en.wikipedia.org/wiki/Dot_product#/media/File:Dot_Product.svg)


$${\displaystyle {\text{Cosinus-Ähnlichkeit}}=S_{C}(A,B):=\cos(\theta )={\mathbf {A} \cdot \mathbf {B}  \over \|\mathbf {A} \|\|\mathbf {B} \|}={\frac {\sum \limits _{i=1}^{n}{A_{i}B_{i}}}{{\sqrt {\sum \limits _{i=1}^{n}{A_{i}^{2}}}}{\sqrt {\sum \limits _{i=1}^{n}{B_{i}^{2}}}}}},}$$

wobei $A$ und $B$  zwei Vektoren sind und $\|\mathbf {A} \|$ das Skalarprodukt von A (und B genauso).
Das [Skalarprodukt](https://en.wikipedia.org/wiki/Dot_product) von $\color {red} {a =  {\displaystyle [a_{1},a_{2},\cdots ,a_{n}]}}$ und $\color {blue} {b =  {\displaystyle [b_{1},b_{2},\cdots ,b_{n}]}}$ ist so definiert:


$${\displaystyle \mathbf {\color {red}a} \cdot \mathbf {\color {blue}b} =\sum _{i=1}^{n}{\color {red}a}_{i}{\color {blue}b}_{i}={\color {red}a}_{1}{\color {blue}b}_{1}+{\color {red}a}_{2}{\color {blue}b}_{2}+\cdots +{\color {red}a}_{n}{\color {blue}b}_{n}}$$


Entsprechend ist die Funktion `nearest_neighbors` zu verstehen aus [Kap. 5.3](https://smltar.com/embeddings.html#exploring-cfpb-word-embeddings):

```{r}
nearest_neighbors <- function(df, token) {
  df %>%
    widely(
      ~ {
        y <- .[rep(token, nrow(.)), ]
        res <- rowSums(. * y) / 
          (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))
        
        matrix(res, ncol = 1, dimnames = list(x = names(res)))
      },
      sort = TRUE
    )(item1, dimension, value) %>%
    select(-item2)
}
```

Wobei mit `widely` zuerst noch von der Langform in die Breitform umformatiert wird,
da die Breitform von der Formel verlangt wird bzw. das Rechnen einfacher macht.

Der eine Vektor ist das Embedding des Tokens,
der andere Vektor ist das *mittlere* Embedding über alle Tokens des Corpus.
Wenn die Anzahl der Elemente konstant bleibt,
kann man sich das Teilen durch $n$ schenken,
wenn man einen Mittelwert berechnen;
so hält es auch die Syntax von `nearest_neighbors`.

Ein nützlicher Post zur Cosinus-Ähnlichkeit findet sich [hier](https://towardsdatascience.com/cosine-similarity-how-does-it-measure-the-similarity-maths-behind-and-usage-in-python-50ad30aad7db).
[Dieses Bild](https://datascience-enthusiast.com/figures/cosine_sim.png) zeigt das
Konzept der Cosinus-Ähnlichkeit anschaulich.

Zur Erinnerung: Der Cosinus eines Winkels ist definiert als Verhältnis der Länge der Ankathete (das ist jene Kathete, die einen Schenkel des Winkels bildet) zur Länge der Hypotenuse^[Quelle: https://de.wikipedia.org/wiki/Sinus_und_Kosinus] in einem rechtwinkligen, vgl. @fig-dreieck.

![Ein rechtwinkliges Dreieck als Grundlage der trigonometrischen Funktionen](img/dreieck.png){#fig-dreieck}

Also: ${\displaystyle \cos \alpha ={\frac {b}{c}}}$



[Quelle: PaterSigmund, Wikipedia,dre CC-BY-SA 2.5](https://de.wikipedia.org/wiki/Sinus_und_Kosinus#/media/Datei:RechtwinkligesDreieck.svg)


Hilfreich ist auch [die Visualisierung von Sinus und Cosinus am Einheitskreis](https://de.wikipedia.org/wiki/Sinus_und_Kosinus#/media/Datei:Sinus_und_Kosinus_am_Einheitskreis_1.svg); gerne [animiert](https://upload.wikimedia.org/wikipedia/commons/f/f3/Sinus_und_Cosinus_am_Einheitskreis.gif) betrachten.


## Word-Embeddings vorgekocht

### Glove6B

In [Kap. 5.4](https://smltar.com/embeddings.html#glove) schreiben die Autoren:

>   If your data set is too small, you typically cannot train reliable word embeddings.

Ein paar Millionen Wörter sollte der Corpus schon enthalten, so die Autoren.
Da solche "Worteinbettungen" (word embedings) aufwändig zu erstellen sind, 
kann man fertige, "vorgekochte" Produkte nutzen.

Glove6B wurde anhand von Wikipedia und anderen Datenquellen erstellt [@pennington_glove_2014].

:::callout-note
Die zugehörigen Daten sind recht groß; für [`glove6b`](https://nlp.stanford.edu/projects/glove/) [@pennington_glove_2014] ist fast ein Gigabyte fällig.
Sie sollten sich die Daten in einem ruhigen Moment (mit stabiler Internetverbindung) herunterladen und in einem Verzeichnis meiner Wahl abgespeichert (`datasets`).
Da bei mir Download abbrach, als ich `embedding_glove6b(dimensions = 100)` aufrief, habe ich die Daten manuell heruntergeladen, s.u.
:::


Wie immer, Hilfe für eine Funktion bekommt man mit `?fun_name` oder interaktiv z.B. in RStudio.


```{r load-glove}
#| cache: true
#| eval: true
glove6b <- 
  embedding_glove6b(dir = "~/datasets", 
                    dimensions = 50,  # mit nur 50 Dimensionen
                    manual_download = FALSE)  # ist der Datensatz schon heruntergeladen?
```


```{r}
#| echo: false
#| eval: false
glove6b <- read_rds("~/datasets/glove6b/glove_6b_50.rds")
```



```{r}
glove6b %>% 
  select(1:5) %>% 
  head()
```



Die ersten paar Tokens sind:

```{r}
glove6b$token %>% head(20)
```



In eine Tidyform bringen:

```{r}
#| cache: true
tidy_glove <- 
  glove6b %>%
  pivot_longer(contains("d"),
               names_to = "dimension") %>%
  rename(item1 = token)

head(tidy_glove)
```


Ganz schön groß:

```{r}
dim(glove6b)
```


```{r}
object.size(tidy_glove)
```

In Megabyte^[$1024 \cdot 1024$ Byte, und $1024 =2^{10}$, daher $2^{10} \cdot 2^{10} = 2^{20}$]

```{r}
object.size(tidy_glove) / 2^20
```

Einfacher und genauer geht es so:

```{r}
pryr::object_size(tidy_glove)
```



```{r}
pryr::mem_used()
```



Um Speicher zu sparen, könnte man `glove6b` wieder direkt löschen, wenn man nur mit der Tidyform weiterarbeitet.

```{r}
rm(glove6b)
```



Jetzt können wir wieder nach Nachbarn fragen, im euklidischen Sinne, wie oben.
Probieren wir aus, welche Wörter nah zu "inaccurate" stehen.


:::callout-note
Wie wir oben gesehen haben, ist der Datensatz riesig^[zugegeben, ein subjektiver Ausdruck],
was die Berechnungen (zeitaufwändig) und damit nervig machen können.
Darüber hinaus kann es nötig sein, dass Sie mehr Speicher auf Ihrem Computer zur Verfügung stellen müssen^[Kaufen...].
Wir müssen noch `maximum_size = NULL`, um das Jonglieren mit riesigen Matrixen zu erlauben.
Möge der Gott der RAMs und Arbeitsspeicher uns gnädig sein!
:::





Mit `pairwise_dist` dauert die Berechnung sehr lange und braucht wohl sehr viel Speicher.
Mitunter kam folgender Fehler auf: "R error: vector memory exhausted (limit reached?)".


```{r}
#| eval: false
word_neighbors_glove6b <- 
tidy_glove %>% 
  slice_head(prop = .1) %>% 
  pairwise_dist(item1, dimension, value, maximum_size = NULL)

head(word_neighbors_glove6b)

tidy_glove %>% 
  filter(item1 == "inaccurate") %>% 
  arrange(-value) %>% 
  slice_head(n = 5)
```



Deswegen probieren wir doch die Funktion `nearest_neighbors`, so wie es im Buch vorgeschlagen wird, s. [Kap 5.3](https://smltar.com/embeddings.html#exploring-cfpb-word-embeddings).


```{r fun-nearest-neighbors2}
nearest_neighbors <- function(df, token) {
  df %>%
    widely(
      ~ {
        y <- .[rep(token, nrow(.)), ]
        res <- rowSums(. * y) / 
          (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))
        matrix(res, ncol = 1, dimnames = list(x = names(res)))
      },
      sort = TRUE,
      maximum_size = NULL
    )(item1, dimension, value) %>%
    select(-item2)
}
```



```{r tidy-glove-nearest-neighbors}
#| cache: true
tidy_glove %>%
  # slice_head(prob = .1) %>% 
  nearest_neighbors("error") %>% 
  head()
```


Entschachteln wir unsere Daten zu `complaints`: 


```{r unnest-tidy-complaints3}
tidy_complaints3 <-
  nested_words3 %>% 
  unnest(words)
```



Dann erstellen wir uns eine Tabelle, in der nur die Schnittmenge der Wörter aus den Beschwerden und Glove vorkommen.
Dazu nutzen winr einen [inneren Join](https://github.com/gadenbuie/tidyexplain/blob/main/images/inner-join.gif)

![Inner Join, Quelle: Garrick Adenbuie](img/inner-join.gif)

[Quelle](https://github.com/gadenbuie/tidyexplain)



```{r join-complaints-glove}
complaints_glove <- 
tidy_complaints3 %>% 
  inner_join(by = "word", 
  tidy_glove %>% 
  distinct(item1) %>% 
  rename(word = item1)) 

head(complaints_glove)
```

Wie viele unique (distinkte) Wörter gibt es in unserem Corpus?

```{r tidy_complaints3_distinct_words_n}
tidy_complaints3_distinct_words_n <- 
tidy_complaints3 %>% 
  distinct(word) %>% 
  nrow()

tidy_complaints3_distinct_words_n
```


In `tidy_complaints` gibt es übrigens `r tidy_complaints3_distinct_words_n` verschiedene Wörter.



```{r word_matrix}
word_matrix <- tidy_complaints3 %>%
  inner_join(by = "word",
             tidy_glove %>%
               distinct(item1) %>%
               rename(word = item1)) %>%
  count(complaint_id, word) %>%
  cast_sparse(complaint_id, word, n)

#word_matrix
```

`word_matrix` zählt für jede der 10 Beschwerden, welche Wörter (und wie häufig) vorkommen.


```{r}
dim(word_matrix)
```

`r dim(word_matrix)[1]` Beschwerden (Dokumente) und `r dim(word_matrix)[2]` unique Wörter.


```{r glove_matrix}
glove_matrix <- tidy_glove %>%
  inner_join(by = "item1",
             tidy_complaints3 %>%
               distinct(word) %>%
               rename(item1 = word)) %>%
  cast_sparse(item1, dimension, value)

#glove_matrix
```


`glove_matrix` gibt für jedes unique Wort den Einbettungsvektor an.


```{r}
dim(glove_matrix)
```

Das sind `r dim(glove_matrix)[1]` unique Wörter und `r dim(glove_matrix)[2]` Dimensionen des Einbettungsvektors.



Jetzt können wir noch pro Dokument (10 in diesem Beispiel) die mittlere "Position" jedes Dokuments im Einbettungsvektor ausrechnen.
Bildlich gesprochen: Was ist der mittlere Raumpunkt (Zentroid) des Bienenschwarms, wobei eine Biene ein Dokument darstellt und die Raumachsen die Dimensionen des Einbettungsvektors.

Dazu gewichten wir jedes Wort eines Dokuments mit den Ladungen des Einbettungsvektor und summieren diese Terme.
Es resultiert eine Matrix mit einem Einbettungsvektor pro Dokument.
Diese Matrix können wir jetzt als Prädiktorenmatrix hernehmen.

```{r doc_matrix}
doc_matrix <- word_matrix %*% glove_matrix
#doc_matrix %>% head()
```




```{r}
dim(doc_matrix)
```


Die Anzahl der Dokumente ist `r dim(doc_matrix)[1]` und die Anzahl der Dimensionen (des Einbettungsvektors) ist `r dim(doc_matrix)[2]`.


### Wordembeddings für die deutsche Sprache


In diesem [Github-Projekt](https://devmount.github.io/GermanWordEmbeddings/) finden sich die Materialien für ein deutsches Wordembedding [@mueller2015].



## Fazit

Worteinbettungen sind eine aufwändige Angelegenheit. 
Positiv gesprochen kann ein Analysti die Muskeln spielen lassen und zeigen, was sie oder er so alles drauf hat.
Ist ja schon cooles Zeugs, die Word Embeddings.
Es besteht die Chance, dass man mit dieser Methode bessere Vorhersagen erreicht, als mit anderen, einfachen
Ansätzen wir Worthäufigkeiten oder tf-idf.
Auf der anderen Seite ist es oft sinnvoll, mit einfachen Ansätzen zu starten,
und zu sehen, wie weit man kommt.
Vielleicht ja weit genug.






## Literatur


### Wikipedia

Es gibt eine Reihe nützlicher (und recht informationsdichter) Wikipedia-Einträge zum Thema Informationstheorie.

- [Wikipedia: Pointwise Mutual Information](https://en.wikipedia.org/wiki/Pointwise_mutual_information)
- [Wikipedia: Mutual information](https://en.wikipedia.org/wiki/Mutual_information)
- [Wikipedia: Information theory](https://en.wikipedia.org/wiki/Information_theory)




